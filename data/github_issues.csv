summary,body,priority
Custom instructions is badly broken in the latest build.,"
Type: <b>Bug</b>

I have multiple custom instructions defined in .github/instructions (folder is **NOT** in the root of the workspace). It is a separate folder that has been configured in User Settings. 

I also have custom prompts that reference these custom instructions. Prior to Insiders 1.102.0, when I used to use a custom prompt that referenced an instructions file, it would pick it up perfectly and work seamlessly. Now, everytime I run my custom prompt, it searches the codebase for the instructions file (where it is not located, and of course does not find it). Funnily enough, it reads the custom prompt correctly. 

Here is an example:
======================================================================
Follow instructions in `design.prompt.md`.

I'll follow the design prompt instructions to create comprehensive design documentation based on the PLANNING.md file. Let me start by reading the planning file and then create all the required design documents.

Let me check the correct path for the design instructions:

Searched for files matching **/design.instructions.md, no matches
======================================================================

This is the case for all custom instructions, no matter what prompt I use. To work around this, I have to manually add custom instructions into the context every single time I run a prompt. It isn't the end of the world, but this used to work properly prior to this version and will be good to have the functionality back. The problem many times is that if you don't notice, or forget to manually add the custom instructions, it will assume there is nothing and go ahead and do whatever it wants. 

This needs to be debugged immediately as it is causing issues for people using complex workflows that involve custom instructions etc. 

Extension version: 0.29.2025070803
VS Code version: Code - Insiders 1.102.0-insider (401e9484bad1f7e909e43e3271f51e7fca7bf14c, 2025-07-09T00:58:07.763Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M2 Pro (12 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|4, 4, 4|
|Memory (System)|32.00GB (1.21GB free)|
|Process Argv|--crash-reporter-id 6f1971b4-0975-434e-b87b-e95df75e0826|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551cf:31249598
c4g48928:30535728
962ge761:30841072
h48ei257:31000450
cppperfnew:30980852
dwnewjupyter:31046869
nativeloc1:31118317
e80f6927:31120813
dwcopilot:31158714
c3hdf307:31184662
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
g012b348:31231168
pythoneinst12:31251391
c7cif404:31309980
pythonpulldiag:31287486
6gi0g917:31259950
996jf627:31264550
pythonrdcb7:31268811
usemplatestapi:31297334
jdghv92:31317040
747dc170:31275146
6518g693:31302842
generatesymbolt:31280541
convertfstringf:31280702
j97ad248:31306657
usemarketplace:31343026
0g1h6703:31329154
nes-emitfast-1:31333560
replacestringexc:31340153
6abeh943:31336334
envsdeactivate2:31343187
nes-conv-11:31337514
0927b901:31340060
jbdfg126:31340538
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Opening Multiple Split Terminal Bug,"
Type: <b>Bug</b>

When I have a terminal open and then run my project (node.js), while this is running, I open another split terminal using the shortcut Ctrl + Shift + 5. When the new split terminal open, my first terminal appears to be blank and unresponsive, i tried to click anywhare inside the first terminal but nothing happened. The same thing will happend if I run another project on the second terminal and then open another split terminal (3rd), then the 2nd terminal will go blank as well.

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 7 PRO 7840U w/ Radeon 780M Graphics   (16 x 3294)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|30.67GB (8.91GB free)|
|Process Argv|--crash-reporter-id 03cdcc5c-cc55-4c60-87b8-9395326da0d5|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (24)</summary>

Extension|Author (truncated)|Version
---|---|---
better-comments|aar|3.0.2
es7-react-js-snippets|dsz|4.4.3
gitlens|eam|17.3.0
prettier-vscode|esb|11.0.0
copilot|Git|1.341.0
copilot-chat|Git|0.28.5
svg|joc|1.5.4
git-graph|mhu|1.30.0
csdevkit|ms-|1.30.32
csharp|ms-|2.84.19
vscode-dotnet-runtime|ms-|2.3.6
data-workspace-vscode|ms-|0.6.3
mssql|ms-|1.33.0
sql-bindings-vscode|ms-|0.4.1
sql-database-projects-vscode|ms-|1.5.3
vscode-pgsql|ms-|1.6.0
remote-ssh|ms-|0.120.0
remote-ssh-edit|ms-|0.87.0
powershell|ms-|2025.2.0
remote-explorer|ms-|0.5.0
material-icon-theme|PKi|5.24.0
postman-for-vscode|Pos|1.13.1
svg-preview|Sim|2.8.3
vscode-status-bar-format-toggle|tom|3.2.0


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31344060
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
d2249276:31341129
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
testaa123cf:31335227
6abeh943:31336334
yijiwantestdri0626-c:31336931
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
Port Forwarding allowed direct access,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

Currently, I use port forwarding as the callback address for third-party services. Every time a new port forwarding is created, manual consent is required for the first access before it can be officially accessed, as shown in the following figure. This will cause the third-party service to fail when requesting
<img width=""605"" height=""347"" alt=""Image"" src=""https://github.com/user-attachments/assets/13ebc3c1-8c7f-48de-bf89-56590408c20c"" />",0
Erreur boucle infinie,"
Type: <b>Bug</b>

Le traitement faisait une boucle infinie

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19045
Modes:


<!-- generated by issue reporter -->",0
"Redirect to vscode.dev/redirect first, then the loopback","> @TylerLeonhardt @bamurtaugh there is an interesting negative side effect of this work that might annoy users: because we open VS Code from a local host page, this dialog seems to not remember the choice to never ask again:
> 
> <img width=""613"" height=""716"" alt=""Image"" src=""https://github.com/user-attachments/assets/064a0895-8de7-486c-9be7-2f50e7870d25"" />
> 
> At least in Chrome, I always get the dialog. Probably because Chrome cannot guarantee what page is running.
> 
> I have a feeling to fix this we would actually need to redirect to some vscode.dev presence that then calls back into VS Code somehow. 

 _Originally posted by @bpasero in [#250086](https://github.com/microsoft/vscode/issues/250086#issuecomment-3051128254)_

We should be able to just add: `https://vscode.dev/redirect?uri=THELOCALHOSTURIGOESHERE` to that and it'll work",0
ca prend un temps fou pour faire un test,"
Type: <b>Bug</b>

ca prend un temps fou pour faire un test à la con. 4 fois de suite ca bug de cette manière, c'est incomprenable et incomprhéensible en même temps.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Linux x64 6.12.33+kali-amd64
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM)2 Duo CPU     P8800  @ 2.66GHz (2 x 2653)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: disabled_off<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: disabled_off<br>webnn: disabled_off|
|Load (avg)|2, 2, 2|
|Memory (System)|7.51GB (3.33GB free)|
|Process Argv|--crash-reporter-id 4cfb181d-20a6-44a3-8e10-5dd2ddfa9176|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|lightdm-xsession|
|XDG_CURRENT_DESKTOP|XFCE|
|XDG_SESSION_DESKTOP|lightdm-xsession|
|XDG_SESSION_TYPE|x11|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492cf:30256860
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyonecf:30548226
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
j6a0c355:31339598
0g0a1943:31341127
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
agent not stopping,"
Type: <b>Bug</b>

agent was in a constant loop of developing the same thing and not stopping.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.22631
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz (8 x 1382)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.74GB (2.05GB free)|
|Process Argv|--crash-reporter-id 52ceb0dd-fe89-484c-9b76-c88cab9dc129|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551cf:31249601
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
c4b42873:31341128
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
onetestforazureexpcf:31335614
6abeh943:31336334
yijiwantestdri0626-c:31336931
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
agent not stopping,"
Type: <b>Bug</b>

agent was in a constant loop of developing the same thing and not stopping.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.22631
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz (8 x 1382)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.74GB (2.05GB free)|
|Process Argv|--crash-reporter-id 52ceb0dd-fe89-484c-9b76-c88cab9dc129|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551cf:31249601
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
c4b42873:31341128
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
onetestforazureexpcf:31335614
6abeh943:31336334
yijiwantestdri0626-c:31336931
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
agent not stopping,"
Type: <b>Bug</b>

agent was in a constant loop of developing the same thing and not stopping.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.22631
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz (8 x 1382)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.74GB (2.05GB free)|
|Process Argv|--crash-reporter-id 52ceb0dd-fe89-484c-9b76-c88cab9dc129|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551cf:31249601
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
c4b42873:31341128
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
onetestforazureexpcf:31335614
6abeh943:31336334
yijiwantestdri0626-c:31336931
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
agent not stopping,"
Type: <b>Bug</b>

agent was in a constant loop of developing the same thing and not stopping.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.22631
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz (8 x 1382)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.74GB (2.05GB free)|
|Process Argv|--crash-reporter-id 52ceb0dd-fe89-484c-9b76-c88cab9dc129|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551cf:31249601
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
c4b42873:31341128
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
onetestforazureexpcf:31335614
6abeh943:31336334
yijiwantestdri0626-c:31336931
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
setting sync not working,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No
YES

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 
```sh
Version: 1.101.2
Commit: 2901c5ac6db8a986a5666c3af51ff804d05af0d4
Date: 2025-06-24T20:27:15.391Z
Electron: 35.5.1
ElectronBuildId: 11727614
Chromium: 134.0.6998.205
Node.js: 22.15.1
V8: 13.4.114.21-electron.0
OS: Linux x64 6.12.36_1
```

- OS Version: 
```sh
$ neofetch   
                __.;=====;.__                   mmc@vm 
            _.=+==++=++=+=+===;.                ------ 
             -=+++=+===+=+=+++++=_              OS: Void Linux x86_64 
        .     -=:``     `--==+=++==.            Host: KVM/QEMU (Standard PC (Q35 + IC 
       _vi,    `            --+=++++:           Kernel: 6.12.36_1 
      .uvnvi.       _._       -==+==+.          Uptime: 48 mins 
     .vvnvnI`    .;==|==;.     :|=||=|.         Packages: 765 (xbps-query) 
+QmQQmpvvnv; _yYsyQQWUUQQQm #QmQ#:QQQWUV$QQm.   Shell: zsh 5.9 
 -QQWQWpvvowZ?.wQQQE==<QWWQ/QWQW.QQWW(: jQWQE   Resolution: 1152x768 
  -$QQQQmmU'  jQQQ@+=<QWQQ)mQQQ.mQQQC+;jWQQ@'   DE: Xfce 4.20 
   -$WQ8YnI:   QWQQwgQQWV`mWQQ.jQWQQgyyWW@!     Theme: Windows10 [GTK2/3] 
     -1vvnvv.     `~+++`        ++|+++          Icons: Faenza [GTK2/3] 
      +vnvnnv,                 `-|===           Terminal: terminator 
       +vnvnvns.           .      :=-           CPU: Intel i5-9500 (2) @ 2.999GHz 
        -Invnvvnsi..___..=sv=.     `            GPU: 00:01.0 Red Hat, Inc. QXL paravi 
          +Invnvnvnnnnnnnnvvnn;.                Memory: 1199MiB / 3899MiB 
            ~|Invnvnvvnvvvnnv}+`
               -~|{*l}*|~                                               
```                                                                        
there are two issues:
1. ""local server"" mode not working.
2. ""device code"" mode are automaticlly interrupted without any interruption.

here is the full screen record.

https://github.com/user-attachments/assets/f70a4957-b53d-42a2-a6b1-b55815081652

Steps to Reproduce:
1. download vscode.tar.gz and extract to /opt/vscode, ""ln -s /opt/vscode/bin/code /usr/local/bin/code""
2. open vscode, login with github user,  cancel method1, as tar.gz does not support protocol ""vscode://""
3. then it will suggest you to use ""local server"" mode, i did it as it told me, chromium shows page with successful, but vscode does not get correct response.
4. when local server mode failed, vscode suggest me to use the ""device code"" mode, it just resport the issue, then it disappeared.
",1
request timeout and not generating response as expected also update code,"
Type: <b>Bug</b>

request time out 
upddate code without my knowledge 


Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|13th Gen Intel(R) Core(TM) i7-13700H (20 x 2918)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|31.68GB (11.32GB free)|
|Process Argv|-n --crash-reporter-id 87e0dce2-008d-43b3-b883-e5deaced015f|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551cf:31249601
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31344060
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
j6a0c355:31339598
c4b42873:31341128
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
4f60g487:31327383
nes-diff-11:31337487
63221493:31336333
yijiwantestdri0626-c:31336931
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Cannot have more than 128 tools per request.,"<img width=""1015"" height=""1447"" alt=""Image"" src=""https://github.com/user-attachments/assets/ce8e7a56-0f66-49f4-8f10-f62c32413e61"" />

When multiple AI-powered extensions are installed, attempting to converse in ask mode results in the error ""Cannot have more than 128 tools per request."" This issue also occurs in agent mode, but agents mode offers a configuration option to deselect some of the extension tools' AI capabilities. However, this option is unavailable in ask mode.",1
Extension host terminated unexpectedly due to GitHub copilot,"While using agent mode in Copilot chat I am now getting ""Extension host terminated unexpectedly due to GitHub copilot"" error **every time** I use a token. This issue only started yesterday after the last update and happens in the insiders edition also.

Installation
Identifier
github.copilot
Version
1.341.0
Last Updated
2025-07-08, 08:23:04
Size
59.72MB
Marketplace
Published
2021-06-29, 15:26:17
Last Released
2025-07-09, 02:48:33

Installation
Identifier
github.copilot-chat
Version
0.28.5
Last Updated
2025-07-02, 14:02:26
Size
28.60MB
Cache
23.85KB
Marketplace
Published
2023-04-27, 08:13:54
Last Released
2025-07-08, 23:59:35


Could this be due to file size? (Just under 5000 lines).",1
No inputs given on my input.,"
Type: <b>Bug</b>

not giving inputs on my queries in VS Code, doing the same on the browser.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.3.0
Modes:


<!-- generated by issue reporter -->",0
Weird code in assignment service,"https://github.com/microsoft/vscode/blob/2901c5ac6db8a986a5666c3af51ff804d05af0d4/src/vs/platform/assignment/common/assignmentService.ts#L63-L74

@lramos15 whats the reason to call `client.getTreatmentVariable` twice?",0
Unable to send messages to Github Copilot Chat when connected to remote machines,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 
```
1.101.2
2901c5ac6db8a986a5666c3af51ff804d05af0d4
x64
```
- OS Version:  
```
Windows 11 Professional
24H2
26100.4349
```
- Models experiencing this with:
```
any
```


Steps to Reproduce:

1. connect to host (WSL, remote server, etc)
2. edit a message in Copilot Chat (any model)
3. click send button or hit ""enter"" key, and nothing happens, with the message remaining in edit box.

Logs in Developer Tools:
```
console.ts:137 [Extension Host] Checking whether to register object drop extension
console.ts:137 [Extension Host] Registering object drop provider
console.ts:137 [Extension Host] Registered object drop provider
log.ts:460   ERR [UriError]: If a URI contains an authority component, then the path component must either be empty or begin with a slash (""/"") character: Error: [UriError]: If a URI contains an authority component, then the path component must either be empty or begin with a slash (""/"") character
    at mBi (vscode-file://vscode-app/c:/Users/cyani/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:30:57171)
    at new iMe (vscode-file://vscode-app/c:/Users/cyani/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:30:58309)
    at new GW (vscode-file://vscode-app/c:/Users/cyani/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:30:60044)
    at iMe.from (vscode-file://vscode-app/c:/Users/cyani/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:30:59141)
    at pnt.s (vscode-file://vscode-app/c:/Users/cyani/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:8561)
    at pnt.n (vscode-file://vscode-app/c:/Users/cyani/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:8425)
    at pnt.m (vscode-file://vscode-app/c:/Users/cyani/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:8215)
    at pnt.listFiles (vscode-file://vscode-app/c:/Users/cyani/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:7358)
    at Lj.listPromptFiles (vscode-file://vscode-app/c:/Users/cyani/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:11031)
    at Lj.findInstructionFilesFor (vscode-file://vscode-app/c:/Users/cyani/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:12430)
```

Logs in OUTPUT:
- ""Github Copilot"" channel:
```
2025-07-09 07:41:55.451 [info] [certificates] Removed 1 expired certificates
2025-07-09 07:41:56.197 [info] [fetcher] Using Helix fetcher, Electron fetcher is not available.
2025-07-09 07:41:56.198 [info] [code-referencing] Public code references are enabled.
2025-07-09 07:41:56.662 [info] [fetcher] Using Helix fetcher, Electron fetcher is not available.
```
- ""Github Copilot Chat"" channel:
```
2025-07-09 07:41:52.615 [info] Can't use the Electron fetcher in this environment.
2025-07-09 07:41:52.615 [info] Using the Node fetch fetcher.
2025-07-09 07:41:52.615 [info] [GitExtensionServiceImpl] Initializing Git extension service.
2025-07-09 07:41:52.615 [info] [GitExtensionServiceImpl] Successfully activated the vscode.git extension.
2025-07-09 07:41:52.615 [info] [GitExtensionServiceImpl] Enablement state of the vscode.git extension: true.
2025-07-09 07:41:52.615 [info] [GitExtensionServiceImpl] Successfully registered Git commit message provider.
2025-07-09 07:41:55.649 [info] Logged in as Uyanide
2025-07-09 07:41:56.542 [info] Got Copilot token for Uyanide
2025-07-09 07:41:56.548 [info] activationBlocker from 'languageModelAccess' took for 1575ms
2025-07-09 07:41:56.833 [info] copilot token chat_enabled: true, sku: free_educational_quota
2025-07-09 07:41:56.834 [info] GitHub.vscode-pull-request-github extension is not yet activated.
2025-07-09 07:41:56.838 [info] Registering default platform agent...
2025-07-09 07:41:56.838 [info] activationBlocker from 'conversationFeature' took for 1868ms
2025-07-09 07:41:57.109 [info] BYOK: Copilot Chat known models list fetched successfully.
2025-07-09 07:41:57.198 [info] Fetched model metadata in 650ms e66398be-9cd5-49bf-a436-681de999308a
```
- ""Window"" channel:
```
2025-07-09 07:41:52.103 [error] [Window] navigator is now a global in nodejs, please see https://aka.ms/vscode-extensions/navigator for additional info on this error.: PendingMigrationError: navigator is now a global in nodejs, please see https://aka.ms/vscode-extensions/navigator for additional info on this error.
    at get (file:///home/kolkas/.vscode-server/bin/2901c5ac6db8a986a5666c3af51ff804d05af0d4/out/vs/workbench/api/node/extensionHostProcess.js:361:1437)
    at /home/kolkas/.vscode-server/extensions/github.copilot-chat-0.28.5/dist/extension.js:379:47519
    at /home/kolkas/.vscode-server/extensions/github.copilot-chat-0.28.5/dist/extension.js:1:262
    at /home/kolkas/.vscode-server/extensions/github.copilot-chat-0.28.5/dist/extension.js:379:49912
    at /home/kolkas/.vscode-server/extensions/github.copilot-chat-0.28.5/dist/extension.js:1:262
    at /home/kolkas/.vscode-server/extensions/github.copilot-chat-0.28.5/dist/extension.js:379:50983
    at /home/kolkas/.vscode-server/extensions/github.copilot-chat-0.28.5/dist/extension.js:1:262
    at /home/kolkas/.vscode-server/extensions/github.copilot-chat-0.28.5/dist/extension.js:379:53169
    at /home/kolkas/.vscode-server/extensions/github.copilot-chat-0.28.5/dist/extension.js:1:262
    at /home/kolkas/.vscode-server/extensions/github.copilot-chat-0.28.5/dist/extension.js:386:2601
    at /home/kolkas/.vscode-server/extensions/github.copilot-chat-0.28.5/dist/extension.js:1:262
    at /home/kolkas/.vscode-server/extensions/github.copilot-chat-0.28.5/dist/extension.js:387:2671
    at /home/kolkas/.vscode-server/extensions/github.copilot-chat-0.28.5/dist/extension.js:1:262
    at /home/kolkas/.vscode-server/extensions/github.copilot-chat-0.28.5/dist/extension.js:387:9338
    at /home/kolkas/.vscode-server/extensions/github.copilot-chat-0.28.5/dist/extension.js:1:262
    at /home/kolkas/.vscode-server/extensions/github.copilot-chat-0.28.5/dist/extension.js:389:21442
    at /home/kolkas/.vscode-server/extensions/github.copilot-chat-0.28.5/dist/extension.js:1:262
    at /home/kolkas/.vscode-server/extensions/github.copilot-chat-0.28.5/dist/extension.js:389:25509
    at /home/kolkas/.vscode-server/extensions/github.copilot-chat-0.28.5/dist/extension.js:1:262
    at /home/kolkas/.vscode-server/extensions/github.copilot-chat-0.28.5/dist/extension.js:389:27007
    at /home/kolkas/.vscode-server/extensions/github.copilot-chat-0.28.5/dist/extension.js:1:262
    at Object.<anonymous> (/home/kolkas/.vscode-server/extensions/github.copilot-chat-0.28.5/dist/extension.js:2115:5153)
    at Module._compile (node:internal/modules/cjs/loader:1730:14)
    at Object..js (node:internal/modules/cjs/loader:1895:10)
    at Module.load (node:internal/modules/cjs/loader:1465:32)
    at Function.<anonymous> (node:internal/modules/cjs/loader:1282:12)
    at e._load (file:///home/kolkas/.vscode-server/bin/2901c5ac6db8a986a5666c3af51ff804d05af0d4/out/vs/workbench/api/node/extensionHostProcess.js:361:810)
    at t._load (file:///home/kolkas/.vscode-server/bin/2901c5ac6db8a986a5666c3af51ff804d05af0d4/out/vs/workbench/api/node/extensionHostProcess.js:181:22628)
    at s._load (file:///home/kolkas/.vscode-server/bin/2901c5ac6db8a986a5666c3af51ff804d05af0d4/out/vs/workbench/api/node/extensionHostProcess.js:173:23297)
    at TracingChannel.traceSync (node:diagnostics_channel:322:14)
    at wrapModuleLoad (node:internal/modules/cjs/loader:235:24)
    at Module.<anonymous> (node:internal/modules/cjs/loader:1487:12)
    at require (node:internal/modules/helpers:135:16)
    at yY.Cb (file:///home/kolkas/.vscode-server/bin/2901c5ac6db8a986a5666c3af51ff804d05af0d4/out/vs/workbench/api/node/extensionHostProcess.js:212:1253)
    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)
2025-07-09 07:41:52.103 [error] [Window] [Extension Host] (node:6414) [DEP0040] DeprecationWarning: The `punycode` module is deprecated. Please use a userland alternative instead.
(Use `node --trace-deprecation ...` to show where the warning was created)
2025-07-09 07:41:53.042 [info] [Window] [perf] Render performance baseline is 19ms
2025-07-09 07:41:53.467 [info] [Window] Settings Sync: Token updated for the account cyanide683@hotmail.com
2025-07-09 07:41:53.468 [info] [Window] Settings Sync: Account status changed from uninitialized to available
2025-07-09 07:44:11.667 [error] [Window] [UriError]: If a URI contains an authority component, then the path component must either be empty or begin with a slash (""/"") character: Error: [UriError]: If a URI contains an authority component, then the path component must either be empty or begin with a slash (""/"") character
    at mBi (vscode-file://vscode-app/c:/Users/cyani/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:30:57171)
    at new iMe (vscode-file://vscode-app/c:/Users/cyani/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:30:58309)
    at new GW (vscode-file://vscode-app/c:/Users/cyani/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:30:60044)
    at iMe.from (vscode-file://vscode-app/c:/Users/cyani/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:30:59141)
    at pnt.s (vscode-file://vscode-app/c:/Users/cyani/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:8561)
    at pnt.n (vscode-file://vscode-app/c:/Users/cyani/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:8425)
    at pnt.m (vscode-file://vscode-app/c:/Users/cyani/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:8215)
    at pnt.listFiles (vscode-file://vscode-app/c:/Users/cyani/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:7358)
    at Lj.listPromptFiles (vscode-file://vscode-app/c:/Users/cyani/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:11031)
    at Lj.findInstructionFilesFor (vscode-file://vscode-app/c:/Users/cyani/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:12430)
```",2
Switch beetwen files in fullscreen mode on MacOS,"<img width=""1440"" height=""70"" alt=""Image"" src=""https://github.com/user-attachments/assets/bebd51a6-bf46-42bb-9695-3e791dc7b3ab"" />
<img width=""1440"" height=""70"" alt=""Image"" src=""https://github.com/user-attachments/assets/c9269a6d-25ad-498d-816c-cf8045f7fd85"" />
<img width=""1440"" height=""171"" alt=""Image"" src=""https://github.com/user-attachments/assets/0a20ccbd-ffcb-40d5-b664-c8337508da06"" />

Type: <b>Bug</b>

Whe i use fullscreen mode on MacOS, i want to click on file name in top small panel to choose another file in folder, but it openned in another split view the same file (e.g. in my case it looks like ""Sources > leetcode.swift > Solution > merge(_:_:)"" – i tapped on ""leetcode.swift""). It always works on not fullscreen mode and sometimes works in fullscreen mode until it stopped. To fix it i can exit from fullscreen mode and enter again

I attached screenshots what happened in fullscreen mode and in normal mode after I clicked into red rectange

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin x64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-8259U CPU @ 2.30GHz (8 x 2300)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|2, 2, 2|
|Memory (System)|8.00GB (0.40GB free)|
|Process Argv|--crash-reporter-id be5c812b-be4d-4f3f-b9a5-1f58c2ef4436|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (4)</summary>

Extension|Author (truncated)|Version
---|---|---
copilot|Git|1.341.0
copilot-chat|Git|0.28.5
lldb-dap|llv|0.2.15
swift-vscode|swi|2.6.1


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31344060
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
convertlamdat:31329272
jhi8h917:31341130
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
Intermittent Command Output Failure in VSCode Terminal with Zsh + Oh My Zsh on Windows,"# Bug Report: Intermittent Command Output Failure in VSCode Terminal

**Note to User:** Please copy the entire content of this English report below to submit to the VSCode GitHub Issues page.

---

### **Title: Intermittent Command Output Failure in VSCode Terminal with Zsh + Oh My Zsh on Windows**

---

#### **Environment**

*   **Operating System:** Windows 10
*   **VSCode Version:** `[Please fill in your VSCode version here, e.g., 1.85.1]`
*   **Terminal:** Zsh (via msys64)
*   **Shell Configuration:** Oh My Zsh with `zsh-autosuggestions` plugin
*   **Affected Extension:** Kilo Code (and likely any other extension relying on VSCode's shell integration to capture command output)

---

#### **Bug Description**

When executing commands via an extension that relies on VSCode's shell integration (like Kilo Code), the command output is only captured successfully for the **very first command** in a given terminal session. Every subsequent command fails to return its output, resulting in an `exit code: <undefined>` status, even though the command executes correctly in the terminal itself (the user can see the output).

This behavior is 100% reproducible and points to a state corruption issue within the shell session after the first command completes.

---

#### **Steps to Reproduce**

1.  Configure VSCode to use Zsh (with Oh My Zsh) as the integrated terminal on Windows.
2.  Ensure VSCode's Shell Integration is enabled in `.zshrc`.
3.  Use an extension like Kilo Code to execute a simple command (e.g., `ll` or `pwd`).
4.  **Observe:** The first command executes, and the extension successfully captures and displays the output.
5.  Use the extension to execute the **same or any other command** a second time.
6.  **Observe:** The command runs in the terminal (the user sees the output), but the extension receives an `exit code: <undefined>` and no output, indicating a failure in the output capture mechanism.

---

#### **Troubleshooting Steps Performed (What we've ruled out)**

We have undertaken an extensive and systematic troubleshooting process, eliminating all common configuration errors. The issue persists despite the following attempts:

1.  **Isolating Interactive/Non-Interactive Config:** We correctly separated the `.zshrc` configuration, ensuring Oh My Zsh and other interactive-only features are loaded only in interactive shells (`if [[ $- == *i* ]]`). The issue remained.
2.  **Changing Load Order:** We reversed the load order, loading VSCode's shell integration script *before* Oh My Zsh. The issue remained.
3.  **Resetting Traps & Hooks:** Based on the official documentation, we explicitly cleared Zsh traps and `precmd_functions`/`preexec_functions` arrays before loading the integration script. This was the most targeted attempt, and the issue *still* remained.
4.  **Disabling Plugins:** The persistence of the issue through hook-resetting suggests the problem is more fundamental than a single plugin.
5.  **Disabling Shell Integration:** Completely disabling the shell integration script forces extensions to use a broken internal fallback terminal with severe character encoding issues, making this an unviable workaround.

This exhaustive process strongly suggests the problem is not a user configuration error but a **fundamental incompatibility or bug** in how VSCode's shell integration script interacts with the Zsh/Oh My Zsh environment on Windows.

---

#### **Final `.zshrc` Configuration Used for Testing**

This is the clean and logically correct configuration that was used during our final tests and still exhibits the bug.

```zsh
# ============================================================================
# Configuration for ALL shell sessions (interactive and non-interactive)
# ============================================================================

# You may need to manually set your language environment
export LANG=zh_CN.UTF-8
export LC_ALL=zh_CN.UTF-8

# 设置Scoop环境
export SCOOP='/c/Users/lt/scoop'

# Define PATH. Order matters.
export PATH=""$PATH:$SCOOP/shims""
export PATH=""$PATH:/c/Program Files/nodejs""
export PATH=""$PATH:/c/Users/lt/AppData/Roaming/npm""
export PATH=""$PATH:/c/Users/lt/AppData/Local/Programs/Microsoft VS Code/bin""
export PATH=""$PATH:/c/Program Files/Docker/Docker/resources/bin""


# ============================================================================
# Configuration for INTERACTIVE shell sessions ONLY
# ============================================================================

if [[ $- == *i* ]]; then

  # Path to your Oh My Zsh installation.
  export ZSH=""$HOME/.oh-my-zsh""

  # VSCode Shell Integration
  [[ ""$TERM_PROGRAM"" == ""vscode"" ]] && . ""C:/Users/lt/AppData/Local/Programs/Microsoft VS Code/resources/app/out/vs/workbench/contrib/terminal/common/scripts/shellIntegration-rc.zsh""

  # Oh My Zsh Theme
  ZSH_THEME=""robbyrussell""

  # Oh My Zsh Plugins
  plugins=(git zsh-autosuggestions)

  # Source Oh My Zsh
  source $ZSH/oh-my-zsh.sh

fi
```

---
---

### **中文版报告 (供您参考或在中文社区使用)**

#### **标题：在Windows系统下，VSCode终端使用Zsh+Oh My Zsh时出现间歇性命令输出捕获失败**

---

#### **环境**

*   **操作系统:** Windows 10
*   **VSCode 版本:** `[请在此填写您的VSCode版本, 例如: 1.101.2]`
*   **终端:** Zsh (通过 msys64)
*   **Shell配置:** Oh My Zsh 及 `zsh-autosuggestions` 插件
*   **受影响的插件:** Kilo Code (以及其他任何依赖VSCode Shell集成来捕获命令输出的插件)

---

#### **问题描述**

当通过依赖VSCode Shell集成的插件（如Kilo Code）执行命令时，只有终端会话中的**第一个命令**能被成功捕获并返回结果。此后的每一个命令，虽然在终端界面上能看到正确执行并输出了结果，但插件端却无法捕获到输出，并返回`exit code: <undefined>`状态。

这个现象是100%可复现的，它指向了在第一个命令执行完毕后，Shell会话的内部状态遭到了破坏。

---

#### **复现步骤**

1.  在Windows上配置VSCode使用Zsh (及Oh My Zsh) 作为集成终端。
2.  确保在`.zshrc`中启用了VSCode的Shell集成。
3.  使用Kilo Code这类插件执行一个简单的命令 (如 `ll` 或 `pwd`)。
4.  **观察:** 第一个命令成功执行，插件成功捕获并显示了输出。
5.  再次使用该插件执行**相同或任意其他**命令。
6.  **观察:** 命令在终端中成功运行（用户能看到输出），但插件端收到了`exit code: <undefined>`且没有捕获到任何输出，表明输出捕获机制已失效。

---

#### **已执行的排错步骤 (已排除的可能性)**

我们进行了一系列系统性的排错，已排除了所有常见的配置错误，但问题依旧存在。这一系列详尽的排错过程强烈表明，此问题并非用户配置错误，而是一个深植于**VSCode Shell集成脚本与Zsh/Oh My Zsh环境在Windows系统上交互方式的底层Bug或不兼容性**。

---

#### **用于最终测试的`.zshrc`配置文件**

这是我们用于最终测试的、干净且逻辑正确的配置文件，它稳定地复现了此Bug。

```zsh
# ============================================================================
# Configuration for ALL shell sessions (interactive and non-interactive)
# ============================================================================

# You may need to manually set your language environment
export LANG=zh_CN.UTF-8
export LC_ALL=zh_CN.UTF-8

# 设置Scoop环境
export SCOOP='/c/Users/lt/scoop'

# Define PATH. Order matters.
export PATH=""$PATH:$SCOOP/shims""
export PATH=""$PATH:/c/Program Files/nodejs""
export PATH=""$PATH:/c/Users/lt/AppData/Roaming/npm""
export PATH=""$PATH:/c/Users/lt/AppData/Local/Programs/Microsoft VS Code/bin""
export PATH=""$PATH:/c/Program Files/Docker/Docker/resources/bin""


# ============================================================================
# Configuration for INTERACTIVE shell sessions ONLY
# ============================================================================

if [[ $- == *i* ]]; then

  # Path to your Oh My Zsh installation.
  export ZSH=""$HOME/.oh-my-zsh""

  # VSCode Shell Integration
  [[ ""$TERM_PROGRAM"" == ""vscode"" ]] && . ""C:/Users/lt/AppData/Local/Programs/Microsoft VS Code/resources/app/out/vs/workbench/contrib/terminal/common/scripts/shellIntegration-rc.zsh""

  # Oh My Zsh Theme
  ZSH_THEME=""robbyrussell""

  # Oh My Zsh Plugins
  plugins=(git zsh-autosuggestions)

  # Source Oh My Zsh
  source $ZSH/oh-my-zsh.sh

fi",1
Code deleted randomly,"
Type: <b>Bug</b>

Sometimes when Copilot edits my Code (in agent mode) it deletes sometimes huge amounts of codes leaving the Code broken and incomplete. Happens every 2nd-3rd time it attemps to edit. This is barely usable! Here's what it said when I asked how that happens: 
Basically, it blames an API-tool called insert_edit_into_file.

Das sollte nicht passieren – und tut es bei normaler Nutzung von Copilot im Editor auch nicht. In deinem Fall liegt es daran, dass ich als Agent nicht direkt im Editor arbeite, sondern Änderungen über ein API-Tool (insert_edit_into_file) einspiele. Dieses Tool erwartet, dass ich nur die tatsächlich zu ändernden Zeilen einreiche und den Rest mit // ...existing code... markiere. Wenn ich aber versehentlich den gesamten Funktionsrumpf oder große Bereiche nicht mit dieser Markierung versehe, sondern nur den Patch einreiche, interpretiert das Tool das als „alles andere löschen“ – und entfernt den Rest.

Das Problem entsteht also, wenn ich nicht explizit anweise, dass der gesamte übrige Code erhalten bleiben soll. Das ist ein Fehler in der Anwendung des Tools, nicht im Editor oder in deinem Code.

**Wie kannst du das vermeiden?**
- Immer ein Backup machen, bevor du einen KI-Patch einspielst.
- Nach jedem Patch prüfen, ob der Code noch vollständig ist.
- Wenn du einen gezielten Patch willst, bitte explizit um einen „minimalen Patch“ und weise darauf hin, dass keine anderen Zeilen verändert werden dürfen.

**Wie kann ich das besser machen?**
- Ich kann gezielt nur die relevanten Zeilen mit // ...existing code... drumherum patchen.
- Ich kann vor dem Patchen den aktuellen Funktionsrumpf einlesen und nur die relevanten Zeilen ersetzen.
Even when I ask explicitly not to make any changes to different code sactions, it still happens. 


Extension version: 0.28.5
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M4 (10 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|1, 2, 2|
|Memory (System)|16.00GB (0.43GB free)|
|Process Argv|--crash-reporter-id 2d2f9050-fe0f-43fd-85aa-6b817fd76752|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
vscrp:30673768
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31344060
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
j6a0c355:31339598
jhi8h917:31341130
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
b6b4d950:31327385
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
memory access out of bounds,"
Type: <b>Performance Issue</b>

Copilot チャットで応答が下記になり使えません。
「memory access out of bounds (at tsx element $S > XM > Ade > LIe > zS > Aoe > UIe > lo > t > pDe > af > H3)」

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.22621
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD EPYC 7763 64-Core Processor                 (2 x 2445)|
|GPU Status|2d_canvas: unavailable_software<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: disabled_software<br>multiple_raster_threads: disabled_off<br>opengl: disabled_off<br>rasterization: disabled_software<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: disabled_software<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: unavailable_software<br>webgl2: unavailable_software<br>webgpu: unavailable_software<br>webnn: unavailable_software|
|Load (avg)|undefined|
|Memory (System)|15.95GB (5.84GB free)|
|Process Argv|--crash-reporter-id 20754f1e-45f0-434e-8701-5cc9a7505bee|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>Process Info</summary>

```
CPU %	Mem MB	   PID	Process
    2	   194	 21532	code
    0	    32	  2132	   crashpad-handler
    0	    98	  6936	   window
    0	   104	  8160	file-watcher [1]
    0	   505	 12000	extension-host [1]
    0	    97	 15432	     ""C:\Program Files\VSCode\Code.exe"" ""c:\Program Files\VSCode\resources\app\extensions\json-language-features\server\dist\node\jsonServerMain"" --node-ipc --clientProcessId=12000
    0	    45	 23920	     c:\Users\takashimaapb\.vscode\extensions\ms-vscode.cpptools-1.26.3-win32-x64\bin\cpptools.exe
    0	    11	 20356	       C:\Windows\system32\conhost.exe 0x4
    0	     3	 23520	       ""c:\Users\takashimaapb\.vscode\extensions\ms-vscode.cpptools-1.26.3-win32-x64\bin\cpptools.exe""
    0	    25	 12576	         c:\Users\takashimaapb\.vscode\extensions\ms-vscode.cpptools-1.26.3-win32-x64/bin/cpptools-srv.exe 23920 {81356B8D-8046-4243-851C-0D2F4868D16E}
    0	    11	 17988	           C:\Windows\system32\conhost.exe 0x4
    0	   132	 13300	shared-process
    1	   473	 14588	window [1] (RURIA100.CPP - Visual Studio Code)
    0	    47	 19200	   utility-network-service
    2	    92	 21284	   gpu-process
```

</details>
<details>
<summary>Workspace Info</summary>

```
;
```

</details>
<details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31344060
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
convertlamdat:31329272
c4b42873:31341128
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
b6b4d950:31327385
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
Infinite Loop,"
Type: <b>Bug</b>

El historial completo:
NXHM: #codebase 
#fetch https://developers.cloudflare.com/workers/framework-guides/web-apps/astro/ https://docs.astro.build/en/guides/actions/#accepting-form-data-from-an-action
Lee todo eso, ok?

GitHub Copilot: I will start by examining the project's configuration to understand how it's set up for deployment and then look for the code that handles form submissions.

First, I'll check `package.json` for dependencies, `astro.config.mjs` for Astro-specific configurations, and `wrangler.toml` for Cloudflare Workers settings. I need to understand the project's dependencies and configuration. I'll start by reading `package.json` and `astro.config.mjs` using their absolute paths. Claro, he leído la documentación que me has proporcionado sobre las guías de Astro para Cloudflare y la aceptación de datos de formularios en las acciones de Astro. ¿En qué puedo ayudarte con esta información?

NXHM: Mi contactform cuando clickeo el boton, no envia el formulario a notion. Esta correctamente implementado porque enviaba la consulta a undefined, esta usando la action?

GitHub Copilot: Hola. He leído la documentación que me has proporcionado y he revisado tu componente `ContactForm.astro`. Parece que el problema está en cómo se está manejando el envío del formulario en el lado del cliente.

Actualmente, estás usando un script del lado del cliente con `fetch` para enviar los datos del formulario. Astro Actions simplifica esto permitiéndote manejarlo principalmente en el lado del servidor.

Voy a modificar `ContactForm.astro` para que utilice `form.requestSubmit()` y el manejo de estado de Astro Actions, lo que debería resolver el problema.

Aquí están los cambios que voy a realizar:

1.  Actualizaré el script en ContactForm.astro para usar el objeto `form` de Astro Actions y manejar el envío y los estados (enviando, éxito, error) de una manera más moderna y compatible con Astro.
2.  Añadiré un `id` al formulario para que el script pueda seleccionarlo correctamente.
3.  Modificaré el `div` de estado del formulario para que pueda mostrar los mensajes de éxito y error.
4.  Ajustaré el botón de envío para que su estado deshabilitado sea controlado por el estado del formulario de Astro Actions.

Déjame aplicar estos cambios. 
````

````


Se realizaron cambios.

NXHM: ajusta los cambios porfavor y evite un bucle infinito

GitHub Copilot: 
```


Se realizaron cambios.


```
Listo, he ajustado el formulario para evitar bucles infinitos y asegurar que use correctamente la URL de la action (`actions.submitContact.url`). Ahora el formulario enviará los datos al endpoint definido en tu action y no a `undefined`.

- El formulario ahora tiene el atributo `action` correctamente configurado.
- El script previene envíos múltiples y bucles usando un flag `data-submitting`.
- El envío se realiza vía fetch al endpoint de la action.

Si tu action de Astro está bien implementada y configurada para enviar a Notion, ahora debería funcionar correctamente. ¿Quieres que revise el archivo de la action para asegurarme de que todo esté bien ahí también?

NXHM: si

GitHub Copilot: Claro, voy a revisar el archivo de la `action` para asegurarme de que todo esté correcto. He revisado tu archivo de action contact.ts y la implementación es correcta. Utiliza el `runtime` de Cloudflare para acceder a las variables de entorno y la API de Notion, lo cual está muy bien.

El problema que describes, donde el formulario parece no enviar los datos a Notion, probablemente no está en el código de Astro, sino en la configuración del entorno de Cloudflare.

Aquí están las causas más comunes:

1.  **Variables de Entorno Faltantes o Incorrectas**: La action necesita `NOTION_TOKEN` y `NOTION_DATABASE_ID`.
    *   **Para desarrollo local (con `npm run dev`):** Debes tener un archivo .dev.vars en la raíz de tu proyecto con este contenido:
        ```
        NOTION_TOKEN=""secret_...""
        NOTION_DATABASE_ID=""...""
        ```
        Astro, al usar el adaptador de Cloudflare, cargará estas variables automáticamente.
    *   **Para producción (en Cloudflare Pages):** Debes configurar estas mismas variables en el panel de tu sitio en Cloudflare, en `Settings` > `Environment variables`.

2.  **Permisos de la Integración de Notion**: Asegúrate de que la integración de Notion (asociada a tu `NOTION_TOKEN`) ha sido invitada a tu base de datos de Notion y tiene permisos para **insertar contenido**.

3.  **ID de la Base de Datos de Notion**: Verifica que el `NOTION_DATABASE_ID` sea correcto.

Dado que el código de Astro parece estar bien, te recomiendo que verifiques estos tres puntos. Lo más probable es que el problema esté en las variables de entorno.

NXHM: No funciono:
23:19:06 [404] POST /es/undefined 2055ms

GitHub Copilot: ¡Entendido! El error `Unexpected token '<'` junto con el log `POST /es/undefined` es muy revelador.

Significa que el formulario está intentando enviar los datos a una URL que no existe (`/es/undefined`), y como respuesta recibe una página de error 404 en formato HTML. Tu script, que espera una respuesta en formato JSON, falla al intentar procesar ese HTML.

El problema está en cómo se obtiene la URL de la `action` en el lado del cliente. Aunque la estableces en el atributo `action` del formulario, no se está pasando correctamente al script.

Para solucionarlo, vamos a pasar la URL de la `action` directamente al script usando `define:vars` y a usar esa variable en la llamada `fetch`.

Aquí están los cambios para ContactForm.astro:

 
````

````


Se realizaron cambios.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD A12-9720P RADEON R7, 12 COMPUTE CORES 4C+8G (4 x 2695)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.44GB (4.29GB free)|
|Process Argv|--crash-reporter-id d72d5ecf-542c-4875-84c5-c9847ac69a3c|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492cf:30256860
pythonvspyt551cf:31249601
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31344060
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
jjjhb125:31275178
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
9d2cg352:31339597
d2249276:31341129
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
b6b4d950:31327385
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
Deleting a follow‑up question in Copilot Chat also removes previously generated files,"**Description:**
When using GitHub Copilot Chat in VS Code, if you ask a question that generates code or file changes, then ask a second (follow‑up) question, and then delete that second question from the chat, Copilot Chat unexpectedly removes the files or code snippets it generated in response to the **first** question.

---

**Steps to Reproduce:**

1. Open VS Code and install/enable the GitHub Copilot Chat extension.
2. Start a new Copilot Chat session.
3. Ask a question that prompts it to generate one or more files.
   * e.g. “Create a simple Python script that reads a CSV and prints the sum of column A.”
4. Now, in the same chat session, ask a **second** question (e.g. “Now add error handling to that script.”).
5. Before the AI responds to that question, **Delete** the second question message from the Copilot Chat UI.
8. **Observe:** the files or code that were generated for the *first* question have now disappeared from your workspace.

---

**Expected Behavior:**
Deleting a later question in the chat should only remove its own generated output. Files or code applied from earlier responses should **remain** intact in the workspace.

---

**Actual Behavior:**
Removing a follow‑up question message causes **all** previously generated and applied files from earlier prompts to be deleted as well.

---

**Possible Areas to Investigate:**

* Chat session state management when a message is removed
* File-apply/unapply logic tied to message IDs
* Workspace edit tracking in VS Code’s TextDocument API

<img width=""721"" height=""1207"" alt=""Image"" src=""https://github.com/user-attachments/assets/1ac64256-0f59-4dfb-b284-87875345f3ee"" />

",1
4.1 inserts \t,"<img width=""521"" height=""182"" alt=""Image"" src=""https://github.com/user-attachments/assets/f7466e94-86f7-4d46-930e-e41cda1cb5c3"" />

[panel_editAgent_e79b6aae.md](https://github.com/user-attachments/files/21134547/panel_editAgent_e79b6aae.md)
[healApplyPatch_178eaec2.md](https://github.com/user-attachments/files/21134548/healApplyPatch_178eaec2.md)
[applyPatch.md](https://github.com/user-attachments/files/21134549/applyPatch.md)

Edit- tweaked text for the screenshot, log is real",0
Unable to Run Terminal in Visual Studio Code,"
Type: <b>Bug</b>

Hello Visual Studio Code Team,

I'm currently experiencing an issue where the integrated terminal in VS Code is not working. When I try to open or run the terminal, nothing happens

I have tried restarting VS Code and even reinstalling the application, but the problem persists.

Could you please help me identify the cause or suggest a possible fix?

Here are some details about my environment:

OS: Win 10
VS Code version: 1.101.2

Thank you for your assistance!

Best regards,
Fariz Ikhsan Falaqi

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 6.0.6000
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-4300U CPU @ 1.90GHz (4 x 2494)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.75GB (6.85GB free)|
|Process Argv|--crash-reporter-id 12b60c0f-8f14-4db3-a745-9bdd72f2db38|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (14)</summary>

Extension|Author (truncated)|Version
---|---|---
vscode-intelephense-client|bme|1.14.4
fvm-manager|bor|1.0.0
dart-code|Dar|3.114.2
flutter|Dar|3.114.0
prettier-vscode|esb|11.0.0
auto-rename-tag|for|0.1.10
copilot|Git|1.341.0
copilot-chat|Git|0.28.5
csdevkit|ms-|1.30.32
csharp|ms-|2.84.19
vscode-dotnet-runtime|ms-|2.3.6
LiveServer|rit|5.7.9
codeigniter-3-files-creator|Sye|0.3.0
vstuc|vis|1.1.2

(7 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31344060
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
jhi8h917:31341130
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
82j33506:31327384
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Github Copilot request failed.,"
Type: <b>Bug</b>

Github Copilot request failed.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:


<!-- generated by issue reporter -->",0
Extension Signature Verification Failed: Chinese (Simplified) (简体中文) Language Pack for Visual Studio Code,"
Type: <b>Bug</b>

Please include following log `F1 > Open View... > Shared` below.



VS Code version: CodeBuddy 1.100.0 (d315094c301724bb064e81641cccfaca992cc544, 2025-07-08T15:31:46.373Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|12th Gen Intel(R) Core(TM) i7-12700H (20 x 2688)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|31.71GB (15.72GB free)|
|Process Argv||
|Screen Reader|yes|
|VM|67%|
</details><details><summary>Extensions (20)</summary>

Extension|Author (truncated)|Version
---|---|---
docker|doc|0.11.0
go|gol|0.48.0
vscode-containers|ms-|2.0.3
vscode-docker|ms-|2.0.0
vscode-language-pack-zh-hant|MS-|1.101.2025061109
python|ms-|2025.8.0
cmake-tools|ms-|1.20.53
cpptools|ms-|1.26.3
cpptools-extension-pack|ms-|1.3.1
remote-server|ms-|1.5.2
fabric8-analytics|red|0.9.6
java|red|1.43.1
cmake|twx|0.0.17
intellicode-api-usage-examples|Vis|0.2.9
vscodeintellicode|Vis|1.3.2
vscode-java-debug|vsc|0.58.2
vscode-java-dependency|vsc|0.24.1
vscode-java-pack|vsc|0.29.2
vscode-java-test|vsc|0.43.1
vscode-maven|vsc|0.44.0

(1 theme extensions excluded)

</details>
<!-- generated by issue reporter -->",1
cursorMove would unfold the folded area if the folded area is larger than cursorMove move amount,"
Type: <b>Bug</b>

1. add this part into keybindings.json
    {
        ""key"": ""alt+i"",
        ""command"": ""cursorMove"",
        ""args"": {
            ""to"": ""up"",
            ""by"": ""line"",
            ""value"": 7
        },
        ""when"": ""textInputFocus""
    },

2. fold a C++/Luau code section which is 10 lines long;
3. use alt+i hotkey to try moving the cursor past the folded area;
4. the folded area will unfold;

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-4720HQ CPU @ 2.60GHz (8 x 2594)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.88GB (4.03GB free)|
|Process Argv||
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (49)</summary>

Extension|Author (truncated)|Version
---|---|---
clipboard-history|Anj|1.0.7
Bookmarks|ale|13.5.0
shaderlabvscodefree|aml|1.3.6
markdown-mermaid|bie|1.28.0
markdown-preview-github-styles|bie|2.2.0
catppuccin-vsc|Cat|3.17.0
js-codeformer|cms|2.6.1
doxdocgen|csc|1.4.0
vscode-html-css|ecm|2.0.13
vscode-rojo|eva|2.1.2
vscode-todo-plus|fab|4.19.1
copilot|Git|1.338.0
copilot-chat|Git|0.28.5
todo-tree|Gru|0.0.226
vscode-ansi|ili|1.1.7
better-cpp-syntax|jef|1.17.2
vscode-nuget-package-manager|jmr|1.1.6
luau-lsp|Joh|1.52.0
rainbow-csv|mec|3.20.0
vscode-antlr4|mik|2.3.1
csdevkit|ms-|1.30.32
csharp|ms-|2.84.19
vscode-dotnet-runtime|ms-|2.3.6
vscodeintellicode-csharp|ms-|2.2.3
debugpy|ms-|2025.8.0
python|ms-|2025.8.0
vscode-pylance|ms-|2025.6.2
jupyter|ms-|2025.1.2024121801
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.1.2025012901
vscode-jupyter-cell-tags|ms-|0.1.9
vscode-jupyter-slideshow|ms-|0.1.6
cmake-tools|ms-|1.14.33
cpptools|ms-|1.15.4
cpptools-extension-pack|ms-|1.3.0
hexeditor|ms-|1.11.1
powershell|ms-|2023.6.0
vscode-typescript-next|ms-|5.4.20231103
vscode-json-editor|nic|0.3.0
autodocstring|njp|0.6.1
oracle-java|Ora|23.0.0
multi-command|ryu|1.6.0
ayu|tea|1.0.5
vt100-syntax-highlighting|Tob|1.1.0
cmake|twx|0.0.17
vstuc|vis|1.1.2
vscode-icons|vsc|12.13.0
markdown-all-in-one|yzh|3.6.2
t4-support|zbe|0.7.0

(3 theme extensions excluded)

</details>
<!-- generated by issue reporter -->",1
"Sorry, your request failed. Please try again. Request id: 279f2077-03e7-4dac-9218-785d3b21af15  Reason: Error on conversation request. Check the log for more details.","
Type: <b>Bug</b>

Sorry, your request failed. Please try again. Request id: 279f2077-03e7-4dac-9218-785d3b21af15

Reason: Error on conversation request. Check the log for more details.

The model unexpectedly did not return a response, which may indicate a service issue. Please report a bug.

Sorry, your request failed. Please try again. Request id: e6ca9a0b-5aa0-458f-bbbe-05dd03ba69d6

Reason: Error on conversation request. Check the log for more details.

Sorry, your request failed. Please try again. Request id: 12db07fa-1d73-4f78-b5c6-a30c674caa9f

Reason: Error on conversation request. Check the log for more details.

The model unexpectedly did not return a response, which may indicate a service issue. Please report a bug.

The model unexpectedly did not return a response, which may indicate a service issue. Please report a bug.

The model unexpectedly did not return a response, which may indicate a service issue. Please report a bug.

Sorry, your request failed. Please try again. Request id: a26cd854-8e17-41d1-b12d-03bf97762a10

Reason: Error on conversation request. Check the log for more details.

DAn seterusnya sering trjadi error dan copilot tidak memberikan respon dan selalu saja seperti ini. 

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:
Remote OS version: Linux x64 5.15.0-143-generic

<details><summary>Logs</summary>
<pre>
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Trace: Resolving embedding model
Info: Logged in as ptdmtnetworksolutions
Info: Got Copilot token for ptdmtnetworksolutions
Info: copilot token chat_enabled: true, sku: copilot_for_business_seat_quota
Debug: ConversationFeature: onDidAuthenticationChange has token: true
Debug: [context keys] Updating context keys.
Debug: [LanguageModelAccess] UPDATING language models
Info: Fetched model metadata in 3454ms d51f1025-bd14-41b5-97a6-445a68fd53c2
Trace: Resolved embedding model
Debug: [LanguageModelAccess] DID UPDATE language models
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Resolving chat model
Trace: Resolved chat model
Trace: [panel] chat request received from extension host
Trace: Processing intent
Trace: Resolving chat model
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Resolved chat model
Trace: Processed intent
Trace: Resolving chat model
Trace: Resolved chat model
Info: message 0 returned. finish reason: [stop]
Info: request done: requestId: [4152d19b-9068-4bae-8999-9e06079e417f] model deployment ID: []
Trace: Resolving chat model
Trace: Resolved chat model
Info: message 0 returned. finish reason: [stop]
Info: request done: requestId: [805bd921-7796-4ace-b310-f850f96b45a5] model deployment ID: []
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Built prompt
Trace: Sending prompt to model
Info: message 0 returned. finish reason: [stop]
Info: request done: requestId: [e92b5aa4-e064-4ab1-bbef-aad170fc6ba6] model deployment ID: []
Trace: Resolving embedding model
Trace: Resolved embedding model
</pre>
</details><details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-8665U CPU @ 1.90GHz (8 x 2112)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.78GB (7.27GB free)|
|Process Argv|--crash-reporter-id 096957d9-6285-4b54-a7dd-6e0638856516|
|Screen Reader|no|
|VM|0%|

|Item|Value|
|---|---|
|Remote|SSH: mytarroz-server|
|OS|Linux x64 5.15.0-143-generic|
|CPUs|Intel Xeon Processor (Skylake, IBRS) (6 x 0)|
|Memory (System)|7.75GB (2.10GB free)|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31344060
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
d2249276:31341129
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
testaa123cf:31335227
6abeh943:31336334
yijiwantestdri0626-c:31336931
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
Expose VSC API for fetching tools of MCP sever,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
We are currently developing a VSC extension, and one of our requirements is to retrieve tools from a remote MCP server and present them for user selection. It would be more efficient if VSC provided an API to fetch these tools directly, as this would help avoid the need to cache user tokens or require repeated logins.

",0
"Whenever summarizing conversation history happens, the model gets lost and starts from the first step instead of continuing from where it left off.","<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: Version: 1.101.2 
- OS Version: Windows 11

Steps to Reproduce:

1. Use Playwright MCP and give the following prompt:

Perform below actions using PlayWright MCP:

Navigate to https://testautomationpractice.blogspot.com/
Input first name as Test
Email as test@gmail.com
Phone as 7645678934
Address as #13 CityName
Gender: Male
Days: Sunday, Saturday
Country: India
Colors: Green, Red
Sorted List: Cat, Dog
Date Picker 1 (mm/dd/yyyy): Select today's data
Date Picker 2 (dd/mm/yyyy) : 12-May-2020
Date Picker 3: (Select a Date Range): 1-May-2024 to 1-May-2025

Generate the Playwright test script for the same steps.


2. Observe after some steps completion. In the chat, it says 'summarizing conversation history' and then immediately the model starts from the beginning of the step instead of continuing from where it left off. This issue doesn't happen all the time, but 50% of the time it does.
",0
Anyone else facing weird issues with Copilot after 13th June update?,"Hey folks, just wanted to check that is your Copilot working fine lately? After the 13th June update, mine’s been acting up quite a bit. It’s writing corrupted or broken code sometimes, putting code in the wrong file paths, mixing up languages (like writing Python in a JS file), and even duplicating chunks of code randomly.

Wanted to know if it’s just me or if others are seeing similar issues too. Would love to hear if you found any fixes or workarounds.",0
Source Control: Offer to delete or move to trash up front,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
I have never been able to delete files from the Source Control tab.  I get, ""Are you sure you want to delete"" and the only options are Cancel and Move to Trash.  Move to Trash says ""Git: Failed ..."" and then Cancel or Open Git Log.  After it Failed, it does not say delete anyways.  

Shift+Delete does nothing. v1.100

The feature request is: offer the two options upfront: delete and move to trash.  Delete should have nothing to do with Git and simply operate directly on the file system.  This part of the code has been very problematic over the years, there are a number of issues about this.  It is pretty useful to be able to delete files in this way though, it proves a unique selection that can span directories; so this is a valuable feature.  I don't know why the fall-back is not triggered.  I did see something along the lines of ""delete anyways"" but that has stopped popping up and it didn't work when it did anyways when it use to.  

So, rather I think a small feature change is more valuable than trying to fix this like the other approaches in the past.  I'm more interested in the delete.  A percentage of other users are going to agree.  I'm confident that if you allow bypassing of the trash up-front will at least cut down on the issues reported on this topic.  Issues such as:

https://github.com/microsoft/vscode/issues/13189
https://github.com/Microsoft/vscode/issues/22820
https://github.com/Microsoft/vscode/issues/34534
https://github.com/Microsoft/vscode/issues/42156
https://github.com/Microsoft/vscode/issues/49675
https://github.com/microsoft/vscode/issues/54493
https://github.com/Microsoft/vscode/issues/59350
https://github.com/microsoft/vscode/issues/68359
https://github.com/microsoft/vscode/issues/109344
https://github.com/microsoft/vscode/issues/214728
https://superuser.com/questions/1862554/vscode-failed-to-delete-used-the-recycle-bin-and-issue-renaming-files
https://stackoverflow.com/questions/30556369/visual-studio-code-permanently-deletes-files

Look at all the time spent on this, lol ;) ..  Fantastic editor though, it is certainly worth it.
",1
run_vs_code_task does not work.,"# Bug Report: `run_vs_code_task` tool fails to find existing tasks that work manually 
(bug report created by Claude sonnet 4)

## Environment
- VS Code version: [Please fill in your version]
- OS: Windows
- Date: July 9, 2025

## Bug Description
The `run_vs_code_task` tool (part of the AI assistant/Copilot integration) consistently fails to find tasks that are properly defined and work perfectly when run manually in VS Code.

## Steps to Reproduce
1. Create a `tasks.json` file with a valid build task (see configuration below)
2. Verify the task works manually using `Ctrl+Shift+B`
3. Attempt to run the task programmatically using the `run_vs_code_task` tool
4. Tool returns ""Error running task: Task not found""

## Task Configuration (tasks.json)
```json
{
    ""version"": ""2.0.0"",
    ""tasks"": [
        {
            ""label"": ""build"",
            ""type"": ""shell"",
            ""command"": ""npm"",
            ""args"": [
                ""run"",
                ""build""
            ],
            ""group"": {
                ""kind"": ""build"",
                ""isDefault"": true
            },
            ""problemMatcher"": [
                ""$tsc"",
                ""$eslint-stylish""
            ],
            ""isBackground"": false
        }
    ]
}
```

## Expected Behavior
The `run_vs_code_task` tool should be able to find and execute the task, just like manual execution.

## Actual Behavior
Tool returns ""Error running task: Task not found"" despite the task being properly configured and working manually.

## Troubleshooting Attempts
- Verified JSON syntax is valid
- Tested multiple workspace folder path formats:
  - `c:\MyApp\src\Utilw\HtmlPages`
  - `c:\\MyApp\\src\\Utilw\\HtmlPages` 
  - `c:/MyApp/src/Utilw/HtmlPages`
  - `.` (relative path)
- Confirmed task works with `Ctrl+Shift+B`
- Confirmed task is set as default build task
- Used task label ""build"" as the ID parameter

## Impact
This prevents AI assistants from programmatically running VS Code tasks, limiting automation capabilities.

## Workspace Structure
```
c:\MyApp\src\Utilw\HtmlPages\
├── .vscode/
│   ├── tasks.json
│   ├── launch.json
│   └── settings.json
├── src/
├── public/
├── package.json
└── [other project files]
```
",1
"Settings Sync suspended for making too many requests, unknowningly","
Type: <b>Bug</b>

VS Code Insiders said that I was requesting too many settings syncs - which is odd because I didn't think I was requesting any settings sync.

here is a snippet of the log:

```
2025-07-08 18:47:53.074 [info] Profiles: No changes found during synchronizing profiles.
2025-07-08 18:47:53.074 [info] Sync done. Took 489ms
2025-07-08 18:47:56.746 [info] [AutoSync] Triggered by Activity
2025-07-08 18:47:56.746 [info] Sync started.
2025-07-08 18:47:57.013 [info] Settings: No changes found during synchronizing settings.
2025-07-08 18:47:57.016 [info] Keybindings: No changes found during synchronizing keybindings.
2025-07-08 18:47:57.017 [info] Snippets: No changes found during synchronizing snippets.
2025-07-08 18:47:57.018 [info] Tasks: No changes found during synchronizing tasks.
2025-07-08 18:47:57.019 [info] Mcp: No changes found during synchronizing mcp.
2025-07-08 18:47:57.206 [info] GlobalState: Updated remote ui state. Updated: extension.features.state.
2025-07-08 18:47:57.212 [info] GlobalState: Updated last synchronized ui state
2025-07-08 18:47:57.233 [info] Extensions: No changes found during synchronizing extensions.
2025-07-08 18:47:57.233 [info] Prompts: No changes found during synchronizing prompts.
2025-07-08 18:47:57.234 [info] Profiles: No changes found during synchronizing profiles.
2025-07-08 18:47:57.234 [info] Sync done. Took 490ms
2025-07-08 18:48:01.403 [info] [AutoSync] Triggered by Activity
2025-07-08 18:48:01.404 [info] Sync started.
2025-07-08 18:48:01.650 [info] Settings: No changes found during synchronizing settings.
2025-07-08 18:48:01.653 [info] Keybindings: No changes found during synchronizing keybindings.
2025-07-08 18:48:01.656 [info] Snippets: No changes found during synchronizing snippets.
2025-07-08 18:48:01.657 [info] Tasks: No changes found during synchronizing tasks.
2025-07-08 18:48:01.658 [info] Mcp: No changes found during synchronizing mcp.
2025-07-08 18:48:01.845 [info] GlobalState: Updated remote ui state. Updated: extension.features.state.
2025-07-08 18:48:01.852 [info] GlobalState: Updated last synchronized ui state
2025-07-08 18:48:01.871 [info] Extensions: No changes found during synchronizing extensions.
2025-07-08 18:48:01.872 [info] Prompts: No changes found during synchronizing prompts.
2025-07-08 18:48:01.872 [info] Profiles: No changes found during synchronizing profiles.
2025-07-08 18:48:01.872 [info] Sync done. Took 470ms
2025-07-08 18:48:05.686 [info] [AutoSync] Triggered by Activity
2025-07-08 18:48:05.687 [info] Sync started.
2025-07-08 18:48:05.773 [info] Settings: No changes found during synchronizing settings.
2025-07-08 18:48:05.775 [info] Keybindings: No changes found during synchronizing keybindings.
2025-07-08 18:48:05.779 [info] Snippets: No changes found during synchronizing snippets.
2025-07-08 18:48:05.781 [info] Tasks: No changes found during synchronizing tasks.
2025-07-08 18:48:05.782 [info] Mcp: No changes found during synchronizing mcp.
2025-07-08 18:48:06.160 [info] GlobalState: Updated remote ui state. Updated: extension.features.state.
2025-07-08 18:48:06.167 [info] GlobalState: Updated last synchronized ui state
2025-07-08 18:48:06.189 [info] Extensions: No changes found during synchronizing extensions.
2025-07-08 18:48:06.190 [info] Prompts: No changes found during synchronizing prompts.
2025-07-08 18:48:06.190 [info] Profiles: No changes found during synchronizing profiles.
2025-07-08 18:48:06.191 [info] Sync done. Took 505ms
2025-07-08 18:48:10.614 [info] [AutoSync] Triggered by Activity
2025-07-08 18:48:10.615 [info] Sync started.
2025-07-08 18:48:10.687 [info] Settings: No changes found during synchronizing settings.
2025-07-08 18:48:10.690 [info] Keybindings: No changes found during synchronizing keybindings.
2025-07-08 18:48:10.692 [info] Snippets: No changes found during synchronizing snippets.
2025-07-08 18:48:10.692 [info] Tasks: No changes found during synchronizing tasks.
2025-07-08 18:48:10.693 [info] Mcp: No changes found during synchronizing mcp.
2025-07-08 18:48:10.869 [info] GlobalState: Updated remote ui state. Updated: extension.features.state.
2025-07-08 18:48:10.875 [info] GlobalState: Updated last synchronized ui state
2025-07-08 18:48:10.887 [info] Extensions: No changes found during synchronizing extensions.
2025-07-08 18:48:10.887 [info] Prompts: No changes found during synchronizing prompts.
2025-07-08 18:48:10.888 [info] Profiles: No changes found during synchronizing profiles.
2025-07-08 18:48:10.888 [info] Sync done. Took 275ms
2025-07-08 18:48:13.894 [info] [AutoSync] Triggered by Activity
2025-07-08 18:48:13.894 [info] Sync started.
2025-07-08 18:48:13.960 [info] Settings: No changes found during synchronizing settings.
2025-07-08 18:48:13.961 [info] Keybindings: No changes found during synchronizing keybindings.
2025-07-08 18:48:13.962 [info] Snippets: No changes found during synchronizing snippets.
2025-07-08 18:48:13.962 [info] Tasks: No changes found during synchronizing tasks.
2025-07-08 18:48:13.963 [info] Mcp: No changes found during synchronizing mcp.
2025-07-08 18:48:14.095 [info] GlobalState: Updated remote ui state. Updated: extension.features.state.
2025-07-08 18:48:14.101 [info] GlobalState: Updated last synchronized ui state
2025-07-08 18:48:14.124 [info] Extensions: No changes found during synchronizing extensions.
2025-07-08 18:48:14.125 [info] Prompts: No changes found during synchronizing prompts.
2025-07-08 18:48:14.125 [info] Profiles: No changes found during synchronizing profiles.
2025-07-08 18:48:14.125 [info] Sync done. Took 231ms
2025-07-08 18:48:21.907 [info] [AutoSync] Triggered by Activity
2025-07-08 18:48:21.907 [info] Sync started.
2025-07-08 18:48:22.164 [info] Settings: No changes found during synchronizing settings.
2025-07-08 18:48:22.166 [info] Keybindings: No changes found during synchronizing keybindings.
2025-07-08 18:48:22.168 [info] Snippets: No changes found during synchronizing snippets.
2025-07-08 18:48:22.170 [info] Tasks: No changes found during synchronizing tasks.
2025-07-08 18:48:22.173 [info] Mcp: No changes found during synchronizing mcp.
2025-07-08 18:48:22.351 [info] GlobalState: Updated remote ui state. Updated: extension.features.state.
2025-07-08 18:48:22.356 [info] GlobalState: Updated last synchronized ui state
2025-07-08 18:48:22.377 [info] Extensions: No changes found during synchronizing extensions.
2025-07-08 18:48:22.378 [info] Prompts: No changes found during synchronizing prompts.
2025-07-08 18:48:22.378 [info] Profiles: No changes found during synchronizing profiles.
2025-07-08 18:48:22.379 [info] Sync done. Took 473ms
2025-07-08 18:48:26.190 [info] [AutoSync] Triggered by Activity
2025-07-08 18:48:26.190 [info] Sync started.
2025-07-08 18:48:26.427 [info] Settings: No changes found during synchronizing settings.
2025-07-08 18:48:26.430 [info] Keybindings: No changes found during synchronizing keybindings.
2025-07-08 18:48:26.432 [info] Snippets: No changes found during synchronizing snippets.
2025-07-08 18:48:26.433 [info] Tasks: No changes found during synchronizing tasks.
2025-07-08 18:48:26.434 [info] Mcp: No changes found during synchronizing mcp.
2025-07-08 18:48:26.622 [info] GlobalState: Updated remote ui state. Updated: extension.features.state.
2025-07-08 18:48:26.628 [info] GlobalState: Updated last synchronized ui state
2025-07-08 18:48:26.646 [info] Extensions: No changes found during synchronizing extensions.
2025-07-08 18:48:26.646 [info] Prompts: No changes found during synchronizing prompts.
2025-07-08 18:48:26.647 [info] Profiles: No changes found during synchronizing profiles.
2025-07-08 18:48:26.647 [info] Sync done. Took 458ms
2025-07-08 18:48:30.904 [info] [AutoSync] Triggered by Activity
2025-07-08 18:48:30.905 [info] Sync started.
2025-07-08 18:48:31.232 [info] Settings: No changes found during synchronizing settings.
2025-07-08 18:48:31.234 [info] Keybindings: No changes found during synchronizing keybindings.
2025-07-08 18:48:31.236 [info] Snippets: No changes found during synchronizing snippets.
2025-07-08 18:48:31.237 [info] Tasks: No changes found during synchronizing tasks.
2025-07-08 18:48:31.238 [info] Mcp: No changes found during synchronizing mcp.
2025-07-08 18:48:31.422 [info] GlobalState: Updated remote ui state. Updated: extension.features.state.
2025-07-08 18:48:31.428 [info] GlobalState: Updated last synchronized ui state
2025-07-08 18:48:31.448 [info] Extensions: No changes found during synchronizing extensions.
2025-07-08 18:48:31.449 [info] Prompts: No changes found during synchronizing prompts.
2025-07-08 18:48:31.449 [info] Profiles: No changes found during synchronizing profiles.
2025-07-08 18:48:31.450 [info] Sync done. Took 547ms
2025-07-08 18:48:34.721 [info] [AutoSync] Triggered by Activity
2025-07-08 18:48:34.722 [info] Sync started.
2025-07-08 18:48:34.791 [info] Settings: No changes found during synchronizing settings.
2025-07-08 18:48:34.793 [info] Keybindings: No changes found during synchronizing keybindings.
2025-07-08 18:48:34.795 [info] Snippets: No changes found during synchronizing snippets.
2025-07-08 18:48:34.797 [info] Tasks: No changes found during synchronizing tasks.
2025-07-08 18:48:34.798 [info] Mcp: No changes found during synchronizing mcp.
2025-07-08 18:48:34.806 [info] Too many requests https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState https://vscode-sync-insiders.trafficmanager.net/v1/manifest
2025-07-08 18:48:34.806 [info] Request failed https://vscode-sync-insiders.trafficmanager.net/v1/resource/globalState
2025-07-08 18:48:34.807 [error] LocalTooManyRequests (UserDataSyncError) syncResource:unknown operationId:unknown: Too many requests. Only 100 requests allowed in 5 minutes.
    at h9.request (file:///Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/code/electron-utility/sharedProcess/sharedProcessMain.js:102:25406)
    at Xh.D (file:///Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/code/electron-utility/sharedProcess/sharedProcessMain.js:102:21700)
    at async Xh.writeResource (file:///Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/code/electron-utility/sharedProcess/sharedProcessMain.js:102:19555)
    at async hu.kb (file:///Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/code/electron-utility/sharedProcess/sharedProcessMain.js:102:41048)
    at async hu.rb (file:///Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/code/electron-utility/sharedProcess/sharedProcessMain.js:102:68587)
    at async hu.$ (file:///Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/code/electron-utility/sharedProcess/sharedProcessMain.js:102:37046)
    at async hu.Y (file:///Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/code/electron-utility/sharedProcess/sharedProcessMain.js:102:35845)
    at async hu.X (file:///Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/code/electron-utility/sharedProcess/sharedProcessMain.js:102:34627)
    at async hu.sync (file:///Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/code/electron-utility/sharedProcess/sharedProcessMain.js:102:33173)
    at async Dn.sync (file:///Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/code/electron-utility/sharedProcess/sharedProcessMain.js:102:144211)
    at async Au.S (file:///Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/code/electron-utility/sharedProcess/sharedProcessMain.js:102:136188)
    at async Au.P (file:///Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/code/electron-utility/sharedProcess/sharedProcessMain.js:102:134991)
2025-07-08 18:48:34.807 [info] [AutoSync] Suspended sync because of making too many requests to server
2025-07-08 18:48:34.807 [info] [AutoSync] Suspended until restart.
2025-07-08 18:48:34.807 [info] [AutoSync] Cancelled sync that is in progress
2025-07-08 18:48:34.808 [info] [AutoSync] Stopped

```

VS Code version: Code - Insiders 1.102.0-insider (Universal) (d6b261f44406fdd26e5c0de997fe8e30d7fae5d7, 2025-07-08T16:22:09.577Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M2 (8 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|3, 3, 4|
|Memory (System)|24.00GB (0.07GB free)|
|Process Argv||
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (8)</summary>

Extension|Author (truncated)|Version
---|---|---
npm-intellisense|chr|1.4.5
codespaces|Git|1.17.3
copilot|Git|1.341.1665
copilot-chat|Git|0.29.2025070802
vscode-github-actions|git|0.27.2
debugpy|ms-|2025.8.0
python|ms-|2025.8.1
vscode-pylance|ms-|2025.6.2


</details>
<!-- generated by issue reporter -->",1
A period character appears out of nowhere when pressing space twice inside a JavaScript regular expression.,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.101.2 (Universal)
- OS Version: Darwin arm64 24.5.0

Steps to Reproduce:

1. create test.js file with this code:
```js
const a = /(interface TypedDb {\n\[key: string\]: any;\s)/;
```
2. put cursor after ""\n"" and press space twice
3. a period character appears",0
Github copilot sign in fails,"
Type: <b>Bug</b>

Github copilot sign in fails. It does not give me any option to provide credentials

Extension version: 1.341.0
VS Code version: Code 1.99.2 (4949701c880d4bdb949e3c0e6b400288da7f474b, 2025-04-10T01:21:25.295Z)
OS version: Windows_NT x64 10.0.26100
Modes:
Remote OS version: Linux x64 6.6.87.2-microsoft-standard-WSL2

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|12th Gen Intel(R) Core(TM) i9-12900HK (20 x 2918)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|63.68GB (40.95GB free)|
|Process Argv|--folder-uri=vscode-remote://wsl+Ubuntu/home/kunjan/workspace/arb_cpp_crypto --remote=wsl+Ubuntu --crash-reporter-id 87e1b6f8-b7f7-4d73-9ab1-5883bf9c4489|
|Screen Reader|no|
|VM|0%|

|Item|Value|
|---|---|
|Remote|WSL: Ubuntu|
|OS|Linux x64 6.6.87.2-microsoft-standard-WSL2|
|CPUs|12th Gen Intel(R) Core(TM) i9-12900HK (20 x 0)|
|Memory (System)|31.18GB (28.90GB free)|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31344060
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
j6a0c355:31339598
c4b42873:31341128
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
0g1h6703:31332163
agentclaude:31335815
b6b4d950:31327385
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
"Chatmode without `tools` ALWAYS overrides disabled tools, causing 128 tool error","Steps to Reproduce:

1. Install a few MCP servers that get the tools list above 128 tools (github, kusto, memory)
2. Create a chatmode without `tools` entry
3. Run a prompt and get _""Cannot have more than 128 tools per request.""_
4. Open tool picker and disable several servers/tools, run again
5. 🐛 128 tools error persists, as the tools are enabled again

Maybe the tools should be enabled/disabled once the chatmode is toggled; not on every request.

cc @connor4312 @roblourens ",1
API for kernel selection in REPL editors (SetReplKernelAffinity),"Currently kernels are auto selected manually by the extension that creates the REPL with the selectKernel command. We want to change that to have more separation between REPL creation and kernel selection.

We avoid auto-selecting kernels for notebooks, but we need to for REPLs so that the code can run without requiring the user to go through kernel selection.

## scenarios to support 

### a REPL is created by an extension (e.g. python: start native REPL)

- we should allow auto-selecting that extension's kernel created specifically for that kernel
    - user installs python ext and runs `create native REPL` -> python REPL should be selected
- users should also be able to select a different kernel
    - user runs python extension's `create native REPL` and selects jupyter kernel
        - closing and creating a new REPL should select the jupyter kernel again
        - it should be easy to switch back to using python's REPL kernel (suggested and surfaced as an immediate choice)
- ?? other extensions should be able to create a kernel specific to a certain kind of REPL
    - 3rd party makes a higher fidelity Python REPL kernel and wants to have it selected for REPLs created by python extension

### Python and Jupyter both create REPLs for python - each should have their respective kernels be auto-selected by default

## implementation

#### new API - `SetReplKernelAffinity` (we currently have `SetNotebookKernelAffinity`)

- only applies to REPLs, so doesn't affect notebook kernel selection
- document selector (language, notebookType) or URI/notebookDocument
    - REPL creators should set the language info on the backing notebook Document for the selection to work
    - setting affinity to a notebookDocument would only affect kernel selection for that document
        - untitled-1.ipynb will be re-used, and should not keep the kernel affinity

#### REPL editors can use the MRU kernel picker logic to re-use previously selected kernels, but should be based on the document properties (likely just language and notebookType). REPL editors cannot currently be used for saved files, but could use the URL in that case.
",1
Visual Studio Code reopens after closing,"
Type: <b>Bug</b>

For a moment, an update window appears, but I did not press anything to update and restrart, I was shutting down my PC and closed all VS code windows using X button.

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 9 7950X 16-Core Processor             (32 x 4500)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|63.14GB (13.09GB free)|
|Process Argv|--crash-reporter-id 68931016-83dc-4fae-9f46-4f930eec2c42|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyonecf:30548226
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31344060
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
jhi8h917:31341130
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
stuck and not responding,"
Type: <b>Bug</b>

after executing a code in terminal ex.:
cd /home/solomon/projects/process-pilot && python -c ""
import sys
sys.path.insert(0, '.')
from app import create_app
from models import db, Endpoint
app = create_app()
app.app_context().push()
endpoints = Endpoint.query.all()
print(f'Total endpoints: {len(endpoints)}')
for e in endpoints:
    print(f'  - {e.name} ({e.endpoint_type})')
""

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Linux x64 6.12.33+deb13-amd64 snap
Modes:


<!-- generated by issue reporter -->",0
loops forever,"
Type: <b>Bug</b>

wasted tons of tokens and went over and over and over again repeating similar text and updating the same 2 files

Extension version: 0.28.5
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M4 Pro (12 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|3, 3, 2|
|Memory (System)|24.00GB (0.26GB free)|
|Process Argv|--crash-reporter-id 0a7f10e1-1247-4f62-84ef-56fc010c7da5|
|Screen Reader|no|
|VM|0%|
</details>
<!-- generated by issue reporter -->",2
Contstant failures even switching from claude to gemini,"
Type: <b>Bug</b>

Sorry, your request failed. Please try again. Request id: c53db02f-5e2b-4174-97fd-0320f989ddb4

Reason: Request Failed: 400 Bad Request

Extension version: 0.28.5
VS Code version: Code 1.101.1 (18e3a1ec544e6907be1e944a94c496e302073435, 2025-06-18T13:35:12.605Z)
OS version: Darwin arm64 24.5.0
Modes:
Connection to 'ssh-remote+docker' could not be established
Connection to 'ssh-remote+docker' could not be established
Connection to 'ssh-remote+devbox-local_codespaces' could not be established
Remote OS version: Linux x64 6.8.0-63-generic
Connection to 'ssh-remote+docker200' could not be established

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M1 Pro (10 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|2, 2, 2|
|Memory (System)|16.00GB (0.09GB free)|
|Process Argv||
|Screen Reader|no|
|VM|0%|

Connection to 'ssh-remote+docker' could not be established

Connection to 'ssh-remote+docker' could not be established

Connection to 'ssh-remote+devbox-local_codespaces' could not be established

|Item|Value|
|---|---|
|Remote|SSH: devbox-docaiche|
|OS|Linux x64 6.8.0-63-generic|
|CPUs|13th Gen Intel(R) Core(TM) i7-13620H (16 x 400)|
|Memory (System)|31.12GB (22.23GB free)|
|VM|0%|

Connection to 'ssh-remote+docker200' could not be established
</details>
<!-- generated by issue reporter -->",1
Bug: VSCode crashes when trying to use GitHub Copilot,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.101.2 (user setup)
- OS Version:  Windows_NT x64 10.0.19045

Versión: 1.101.2 (user setup)
Confirmar: 2901c5ac6db8a986a5666c3af51ff804d05af0d4
Fecha: 2025-06-24T20:27:15.391Z
Electron: 35.5.1
ElectronBuildId: 11727614
Chromium: 134.0.6998.205
Node.js: 22.15.1
V8: 13.4.114.21-electron.0
SO: Windows_NT x64 10.0.19045

Steps to Reproduce:

This doesn't happen all the time, but occasionally the entire VSCode window freezes and I get three options: reopen, close, or keep waiting. When this error occurs, no matter how many times I try, attempting to chat with GitHub Copilot causes the entire editor to crash.
",2
Firewall rules and network error ,"
Type: <b>Bug</b>

I am facing this issue. Your request failed.


Extension version: 0.28.5
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.6.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M3 (8 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|2, 2, 2|
|Memory (System)|16.00GB (2.92GB free)|
|Process Argv|--crash-reporter-id dc26af2c-66a8-450d-b08d-bc7db519e407|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492cf:30256860
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
9d2cg352:31339597
jhi8h917:31341130
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
1292j425:31332164
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
"postCreateCommand is non-interactive on Windows, but interactive on Linux vscode","<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.101.2
2901c5ac6db8a986a5666c3af51ff804d05af0d4
x64
- OS Version: Edition	Windows 10 Pro
Version	2009
Installed on	‎12/‎12/‎2024
OS Build	19045.5965


Steps to Reproduce:

1. Install vscode (I install and keep up-to-date via scoop)
2. Open a project and create a mock devcontainer with a base image (devcontainer ubuntu base suffices)
3. Add a postCreateCommand which fires an interactive script (bash script which prompts with read, for example)
4. Open to devcontainer on Windows, postCreateCommand exits when attempting to give input to the script. (pressing any key)
5. Open the same project on Linux vscode (Arch Linux microsoft vscode binary from the AUR), the devcontainer postCreateCommand fires interactively.

Expected Behavior:

Ideally postCreateCommand should be documented as non-interactive on Windows specifically, or be allowed to run interactively. Running certain scripts with postAttachCommand may not be ergonomic for end-users of the devcontainers in vscode as it may require additional interactive prompting.

Additionally, the behavior of postCreateCommand should remain consistent between implementations on Windows, Linux, and other operating systems. I didn't run into this issue until testing a large PR and it was pretty frustrating to discover, at the time.",0
Add API for telling chat about webviews/custom editors,"## feature request

Add api so chat can see the contents of webviews and custom editors in smarter way. Right now we can either:

- Feed in the web contents (either the actual html or accessibility overview)

- For text based custom editors, feed in the text file

Those may not have the most relevant information about what the editor contains ",0
"custom agents: custom kind, custom model, custom prompts and discussion\delegation","<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
today on custom agents we can pick the name, description, tools and instructions. which is really cool :)

my suggestions are:
1.  add an option to specify the kind(ask, edit, agent. or only ask\agent) so we can have dedicated agents to answer things and not to apply changes (like custom GPTs)
2.  add an option to specify the model that will be the default.
3. combined it with custom prompts so we can rely on them when writing the instructions
4. have the ability to create a discussion between a selected agents and delegation",1
stuck,"
Type: <b>Bug</b>

Gemini stuck during the analisys of my code

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i7-1185G7 @ 3.00GHz (8 x 2995)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|63.71GB (45.27GB free)|
|Process Argv|--crash-reporter-id 0d210251-b717-484d-81cc-80801c47783d|
|Screen Reader|no|
|VM|67%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492:30256859
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31344060
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
jjjhb125:31275178
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
c4b42873:31341128
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
error sincronizando,"
Type: <b>Bug</b>

cuando duplico un perfil

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|12th Gen Intel(R) Core(TM) i7-1255U (12 x 2611)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.64GB (5.49GB free)|
|Process Argv||
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (6)</summary>

Extension|Author (truncated)|Version
---|---|---
prettier-vscode|esb|11.0.0
copilot|Git|1.341.0
copilot-chat|Git|0.28.5
vscode-language-pack-es|MS-|1.101.2025061109
vscode-xml|red|0.29.0
sql-formatter-vsc|Ren|4.2.2

(1 theme extensions excluded)

</details>
<!-- generated by issue reporter -->",1
Loading Slowly,"I am unable to calculate my grade at the moment due to it taking awhile to load. Here is the image.

<img width=""756"" height=""161"" alt=""Image"" src=""https://github.com/user-attachments/assets/e83fd1bc-f1c4-4461-8f38-525c4ba23ace"" />

Version: 1.101.2
Commit: 2901c5ac6db8a986a5666c3af51ff804d05af0d4
User Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36
Embedder: codespaces

<!-- generated by web issue reporter -->",1
Lilian ,"
Type: <b>Bug</b>

very slow. i paid for the service, and i recall the speed was much faster. it takes too much time to think

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 3 3250U with Radeon Graphics          (4 x 2595)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|13.94GB (5.91GB free)|
|Process Argv|--crash-reporter-id 9108c768-9b88-433a-80dc-6ea585549f1a|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551cf:31249601
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31344060
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
9d2cg352:31339597
0g0a1943:31341127
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
not working properly edits in codes suddenly stops or freezes,"
Type: <b>Performance Issue</b>

while i am using copilot dor edits in my code suddenly again and again the edits stops or freezes fix this as soon as possible 

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz (8 x 2419)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|7.73GB (2.03GB free)|
|Process Argv|--crash-reporter-id 4b998fb9-9b25-4158-8cea-ca0e8c2d4906|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>Process Info</summary>

```
CPU %	Mem MB	   PID	Process
    0	    84	 13316	code
    3	   167	   836	   gpu-process
    0	    13	  1264	   crashpad-handler
    0	     7	  2720	pty-host
    0	     2	  3768	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\Aayush\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     0	 11036	     conpty-agent
    0	    19	  2912	file-watcher [1]
    0	    54	  7092	shared-process
    0	   492	 10880	extension-host [1]
    0	   138	   584	     electron-nodejs (tsserver.js )
    0	   110	  6004	       electron-nodejs (typingsInstaller.js typesMap.js )
    0	    97	  2120	     ""C:\Users\Aayush\AppData\Local\Programs\Microsoft VS Code\Code.exe"" ""c:\Users\Aayush\AppData\Local\Programs\Microsoft VS Code\resources\app\extensions\css-language-features\server\dist\node\cssServerMain"" --node-ipc --clientProcessId=10880
    0	   149	  3016	     ""C:\Users\Aayush\AppData\Local\Programs\Microsoft VS Code\Code.exe"" ""c:\Users\Aayush\AppData\Local\Programs\Microsoft VS Code\resources\app\extensions\html-language-features\server\dist\node\htmlServerMain"" --node-ipc --clientProcessId=10880
    0	    92	  7044	     ""C:\Users\Aayush\AppData\Local\Programs\Microsoft VS Code\Code.exe"" ""c:\Users\Aayush\AppData\Local\Programs\Microsoft VS Code\resources\app\extensions\json-language-features\server\dist\node\jsonServerMain"" --node-ipc --clientProcessId=10880
    0	   134	  7640	     electron-nodejs (tsserver.js )
    0	    91	 10424	     electron-nodejs (server.js )
    0	    15	 12036	   utility-network-service
    4	  1587	 14660	window [1] (● index.html - Project 2 - Visual Studio Code)
```

</details>
<details>
<summary>Workspace Info</summary>

```
|  Window (● index.html - Project 2 - Visual Studio Code)
|    Folder (Project 2): 27 files
|      File types: js(11) html(3) json(1) css(1) md(1)
|      Conf files: package.json(1);
```

</details>
<details><summary>Extensions (7)</summary>

Extension|Author (truncated)|Version
---|---|---
blackbox|Bla|2.8.43
blackboxagent|Bla|3.3.33
copilot|Git|1.341.0
copilot-chat|Git|0.28.5
vscode-edge-devtools|ms-|2.1.9
live-server|ms-|0.4.15
LiveServer|rit|5.7.9


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31344060
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
0g0a1943:31341127
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
not working properly edits in codes suddenly stops or freezes,"
Type: <b>Performance Issue</b>

while i am using copilot dor edits in my code suddenly again and again the edits stops or freezes fix this as soon as possible 

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz (8 x 2419)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|7.73GB (2.03GB free)|
|Process Argv|--crash-reporter-id 4b998fb9-9b25-4158-8cea-ca0e8c2d4906|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>Process Info</summary>

```
CPU %	Mem MB	   PID	Process
    0	    84	 13316	code
    3	   167	   836	   gpu-process
    0	    13	  1264	   crashpad-handler
    0	     7	  2720	pty-host
    0	     2	  3768	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\Aayush\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     0	 11036	     conpty-agent
    0	    19	  2912	file-watcher [1]
    0	    54	  7092	shared-process
    0	   492	 10880	extension-host [1]
    0	   138	   584	     electron-nodejs (tsserver.js )
    0	   110	  6004	       electron-nodejs (typingsInstaller.js typesMap.js )
    0	    97	  2120	     ""C:\Users\Aayush\AppData\Local\Programs\Microsoft VS Code\Code.exe"" ""c:\Users\Aayush\AppData\Local\Programs\Microsoft VS Code\resources\app\extensions\css-language-features\server\dist\node\cssServerMain"" --node-ipc --clientProcessId=10880
    0	   149	  3016	     ""C:\Users\Aayush\AppData\Local\Programs\Microsoft VS Code\Code.exe"" ""c:\Users\Aayush\AppData\Local\Programs\Microsoft VS Code\resources\app\extensions\html-language-features\server\dist\node\htmlServerMain"" --node-ipc --clientProcessId=10880
    0	    92	  7044	     ""C:\Users\Aayush\AppData\Local\Programs\Microsoft VS Code\Code.exe"" ""c:\Users\Aayush\AppData\Local\Programs\Microsoft VS Code\resources\app\extensions\json-language-features\server\dist\node\jsonServerMain"" --node-ipc --clientProcessId=10880
    0	   134	  7640	     electron-nodejs (tsserver.js )
    0	    91	 10424	     electron-nodejs (server.js )
    0	    15	 12036	   utility-network-service
    4	  1587	 14660	window [1] (● index.html - Project 2 - Visual Studio Code)
```

</details>
<details>
<summary>Workspace Info</summary>

```
|  Window (● index.html - Project 2 - Visual Studio Code)
|    Folder (Project 2): 27 files
|      File types: js(11) html(3) json(1) css(1) md(1)
|      Conf files: package.json(1);
```

</details>
<details><summary>Extensions (7)</summary>

Extension|Author (truncated)|Version
---|---|---
blackbox|Bla|2.8.43
blackboxagent|Bla|3.3.33
copilot|Git|1.341.0
copilot-chat|Git|0.28.5
vscode-edge-devtools|ms-|2.1.9
live-server|ms-|0.4.15
LiveServer|rit|5.7.9


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31344060
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
0g0a1943:31341127
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
GPT 4-1,"
Type: <b>Performance Issue</b>

Ohhh my god this is completely useless. GPT-4.1 is not following instructions and forgetting almost everything I am telling it. 

Insane how much time I am wasting trying to get this working


Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i7-1165G7 @ 2.80GHz (8 x 2803)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.68GB (1.98GB free)|
|Process Argv|--crash-reporter-id 0ef80338-2ffe-4d54-bdf0-06c1290a9c77|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>Process Info</summary>

```
CPU %	Mem MB	   PID	Process
    0	    79	 68544	code
    0	    27	 52756	file-watcher [1]
    0	    59	 53544	shared-process
    0	    19	128380	   window
    0	   157	154540	   gpu-process
    0	     3	164524	   crashpad-handler
    0	  2939	167304	extension-host [1]
    0	     3	 34572	     electron-nodejs (server-node.js )
    0	   393	108724	     electron-nodejs (tsserver.js )
    0	     1	197788	       electron-nodejs (typingsInstaller.js typesMap.js )
    0	     8	164456	     c:\Users\sands\.vscode\extensions\ms-dotnettools.csharp-2.84.19-win32-x64\.roslyn\Microsoft.CodeAnalysis.LanguageServer.exe --logLevel Information --razorSourceGenerator c:\Users\sands\.vscode\extensions\ms-dotnettools.csharp-2.84.19-win32-x64\.razor\Microsoft.CodeAnalysis.Razor.Compiler.dll --razorDesignTimePath c:\Users\sands\.vscode\extensions\ms-dotnettools.csharp-2.84.19-win32-x64\.razor\Targets\Microsoft.NET.Sdk.Razor.DesignTime.targets --extension c:\Users\sands\.vscode\extensions\ms-dotnettools.csharp-2.84.19-win32-x64\.razorExtension\Microsoft.VisualStudioCode.RazorExtension.dll --telemetryLevel all --extensionLogDirectory c:\Users\sands\AppData\Roaming\Code\logs\20250707T072631\window1\exthost\ms-dotnettools.csharp
    0	    41	177140	     electron-nodejs (tsserver.js )
    0	     3	196212	     ""C:\Users\sands\AppData\Local\Programs\Microsoft VS Code\Code.exe"" ""c:\Users\sands\AppData\Local\Programs\Microsoft VS Code\resources\app\extensions\css-language-features\server\dist\node\cssServerMain"" --node-ipc --clientProcessId=167304
    0	     3	208160	     ""C:\Users\sands\AppData\Local\Programs\Microsoft VS Code\Code.exe"" ""c:\Users\sands\AppData\Local\Programs\Microsoft VS Code\resources\app\extensions\html-language-features\server\dist\node\htmlServerMain"" --node-ipc --clientProcessId=167304
    0	     4	213520	     ""C:\Users\sands\AppData\Local\Programs\Microsoft VS Code\Code.exe"" ""c:\Users\sands\AppData\Local\Programs\Microsoft VS Code\resources\app\extensions\markdown-language-features\dist\serverWorkerMain"" --node-ipc --clientProcessId=167304
    0	    25	218312	     ""C:\Users\sands\AppData\Local\Programs\Microsoft VS Code\Code.exe"" ""c:\Users\sands\AppData\Local\Programs\Microsoft VS Code\resources\app\extensions\json-language-features\server\dist\node\jsonServerMain"" --node-ipc --clientProcessId=167304
    0	     7	171404	   window
    1	   962	175440	window [1] (battleChallengeService.ts - PokemonGame - Visual Studio Code)
    0	    36	189496	pty-host
    0	     1	 24960	     conpty-agent
    0	     1	 37052	     conpty-agent
    0	     6	 37768	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\sands\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     5	 42924	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\sands\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	    17	 48044	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\sands\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     6	 49952	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\sands\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     3	 59916	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\sands\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	    53	189796	       ""C:\Program Files\Microsoft\Azure Functions Core Tools\func.exe"" start --port 7071
    0	     1	 71492	     conpty-agent
    0	     1	 73352	     conpty-agent
    0	     5	 75432	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\sands\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     4	 76528	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\sands\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     1	131608	       C:\WINDOWS\system32\cmd.exe /c """"C:\Program Files\nodejs\npm.cmd"" start""
    0	     3	 91944	         ""C:\Program Files\nodejs\\node.exe""  ""C:\Users\sands\AppData\Roaming\npm\node_modules\npm\bin\npm-cli.js"" start
    0	     1	 80228	           C:\WINDOWS\system32\cmd.exe /d /s /c react-scripts start
    0	     1	196304	             electron-nodejs (react-scripts.js )
    0	   586	225276	               ""C:\Program Files\nodejs\node.exe"" ""C:\Users\sands\Documents\GitHub\Pokemon Game\PokemonGame\node_modules\react-scripts\scripts\start.js""
    0	   157	226744	                 ""C:\Program Files\nodejs\node.exe"" --max-old-space-size=2048 ""C:\Users\sands\Documents\GitHub\Pokemon Game\PokemonGame\node_modules\fork-ts-checker-webpack-plugin\lib\typescript-reporter\reporter\TypeScriptReporterRpcService.js""
    0	    14	232064	                 ""C:\Program Files\nodejs\node.exe"" --max-old-space-size=2048 ""C:\Users\sands\Documents\GitHub\Pokemon Game\PokemonGame\node_modules\fork-ts-checker-webpack-plugin\lib\typescript-reporter\reporter\TypeScriptReporterRpcService.js""
    0	     5	 80144	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\sands\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     4	 82836	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\sands\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     9	 83500	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\sands\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     1	 95900	     conpty-agent
    0	     3	 99192	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\sands\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     1	100224	     conpty-agent
    0	     4	117276	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\sands\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     6	135292	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\sands\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     1	141360	     conpty-agent
    0	     1	147136	     conpty-agent
    0	     1	149932	     conpty-agent
    0	     1	150172	     conpty-agent
    0	     1	152888	     conpty-agent
    0	     6	153368	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\sands\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     9	155352	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\sands\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     1	158660	     conpty-agent
    0	     1	160012	     conpty-agent
    0	     4	163024	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\sands\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     6	164684	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\sands\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     5	166232	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\sands\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     1	167480	     conpty-agent
    0	     4	172612	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\sands\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     6	175848	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\sands\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     1	180320	     conpty-agent
    0	    15	188468	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -ExecutionPolicy Bypass -Command ""Import-Module 'c:\Users\sands\.vscode\extensions\ms-vscode.powershell-2025.2.0\modules\PowerShellEditorServices\PowerShellEditorServices.psd1'; Start-EditorServices -HostName 'Visual Studio Code Host' -HostProfileId 'Microsoft.VSCode' -HostVersion '2025.2.0' -BundledModulesPath 'c:\Users\sands\.vscode\extensions\ms-vscode.powershell-2025.2.0\modules' -EnableConsoleRepl -StartupBanner \""PowerShell Extension v2025.2.0
Copyright (c) Microsoft Corporation.

https://aka.ms/vscode-powershell
Type 'help' to get help.
\"" -LogLevel 'Warning' -LogPath 'c:\Users\sands\AppData\Roaming\Code\logs\20250707T072631\window1\exthost\ms-vscode.powershell' -SessionDetailsPath 'c:\Users\sands\AppData\Roaming\Code\User\globalStorage\ms-vscode.powershell\sessions\PSES-VSCode-68544-110384.json' -FeatureFlags @() ""
    0	     1	191292	     conpty-agent
    0	     1	193756	     conpty-agent
    0	     2	200668	     conpty-agent
    0	     1	202908	     conpty-agent
    0	     1	223200	     conpty-agent
    0	     1	233112	     conpty-agent
    0	    13	220708	   utility-network-service
```

</details>
<details>
<summary>Workspace Info</summary>

```
|  Window (battleChallengeService.ts - PokemonGame - Visual Studio Code)
|    Folder (PokemonGame): 379 files
|      File types: dll(162) json(41) ts(17) tsx(16) css(13) cache(10) md(9)
|                  pdb(8) js(6) cs(6)
|      Conf files: github-actions(2) package.json(2) tasks.json(1) csproj(1)
|                  tsconfig.json(1);
```

</details>
<details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
j6a0c355:31339598
d2249276:31341129
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
Shell integration fails with command sequences and hangs with sleep commands,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->

Does this issue occur when all extensions are disabled?: Yes, see reproducer below

```ts
Version: 1.102.0-insider
Commit: bdc1dd9a7c36b7dab7a9eeec2f05a19f7abd0409
Date: 2025-06-20T09:31:30.580Z
Electron: 35.5.1
ElectronBuildId: 11727614
Chromium: 134.0.6998.205
Node.js: 22.15.1
V8: 13.4.114.21-electron.0
OS: Linux x64 6.6.67
```

Cc: @tyriar

## What happened?

Shell integration fails with subshell command patterns:
- `(echo a ; echo b)` - no output is returned 
- `(echo a ; sleep 1)` - shell integration hangs

## Expected behavior

Commands should execute normally and return their output, with shell integration properly detecting command completion.

## Steps to Reproduce:

1. Open VSCode terminal with shell integration enabled
2. Run `(echo a ; echo b)` - observe no output is returned
3. Run `(echo a ; sleep 1)` - observe shell integration hangs

## Test Extension

To reproduce: clone https://github.com/KJ7LNW/vsce-test-terminal-integration, run `npm install`, press F5, open ""Terminal Commands"" view, test commands with different PROMPT_COMMAND settings to observe race conditions.
",1
Start button missing in mcp.json file,"
Type: <b>Bug</b>

If there are multiple mcp servers defined in the mcp.json file, only one of them has a ""Start"" button.

VS Code version: Code - Insiders 1.102.0-insider (Universal) (7631de29bf9a2e31020ab93d3d1611965dc5b62e, 2025-07-07T22:41:41.902Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M1 Max (10 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|2, 2, 2|
|Memory (System)|32.00GB (5.07GB free)|
|Process Argv|--crash-reporter-id ce408f1c-d249-465b-a793-f3d715eb8f28|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (14)</summary>

Extension|Author (truncated)|Version
---|---|---
copilot|Git|1.341.1665
copilot-chat|Git|0.29.2025070802
vscode-containers|ms-|2.0.3
debugpy|ms-|2025.8.0
python|ms-|2025.9.2025070301
vscode-pylance|ms-|2025.6.2
vscode-python-envs|ms-|0.3.11841011
jupyter|ms-|2025.6.2025070201
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.3.2025062701
vscode-jupyter-cell-tags|ms-|0.1.9
vscode-jupyter-slideshow|ms-|0.1.6
remote-containers|ms-|0.421.0
vscode-speech|ms-|0.16.0


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551cf:31249598
14424t-chatv5:31344067
c4g48928:30535728
962ge761:30841072
h48ei257:31000450
cppperfnew:30980852
dwnewjupytercf:31046870
nativeloc1:31118317
e80f6927:31120813
dwcopilot:31158714
c3hdf307:31184662
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
g012b348:31231168
pythoneinst12:31251391
c7cif404:31309980
pythonpulldiag:31287486
6gi0g917:31259950
996jf627:31264550
pythonrdcb7:31268811
usemplatestapi:31297334
jdghv92:31317040
747dc170:31275146
6518g693:31302842
generatesymbolt:31280541
convertfstringf:31280702
j97ad248:31306657
usemarketplace:31343026
0g1h6703:31329154
4f60g487:31327383
nes-emitfast-1:31333560
replacestringexc:31340153
onetestforazureexp:31335613
6abeh943:31336334
envsdeactivate2:31343187
nes-conv-11:31337514
0927b901:31340060
gji67723:31340537
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Terminal initial hint uses wrong foreground color in terminal editors,"Repro:

1. Terminal: Create New Terminal in Editor Area, 🐛 the chat hint uses white text in the Dark Modern theme

<img width=""947"" height=""102"" alt=""Image"" src=""https://github.com/user-attachments/assets/0f5c2e96-a9dd-4c7d-bb6a-70d61abeac19"" />",0
Copilot gets stuck after a git command didn't fully succeed,"
Type: <b>Bug</b>

I asked the agent to add a new file. I also created one by hand. Then I told it to comit it to git and did that. Then I asked it to move it to a new branch. It create the branch, added the file again and then tried to commit. The commit command didn't fully succeed because the file was already commited and there was another file which was not part of the changeset. The git command did finish, but copilot was still showing the progress circle and got stuck there.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551:31249599
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31344060
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
c4b42873:31341128
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",1
Multiline Regex Search/Replace doesn't capture groups with lookbehind,"
Type: <b>Bug</b>

1. Search a pattern that includes a valid(fixed length) look behind for search, eg: (?<=getRootTableRegistryName\(\): string\n\s{4}\{\n\s{8}return\s')(\w+)(?=';)

It matches something like
```
getRootTableRegistryName(): string
    {
        return 'Audits';
```
2. Try to replace the match, using a capture group, replacing for something like 'NameSpace.$1'.

Expected replacement:
```
getRootTableRegistryName(): string
    {
        return 'NameSpace.Audits';
```
Actual:
```
getRootTableRegistryName(): string
    {
        return 'NameSpace.$1';
```

If the lookbehind is made a normal capture group, then the capture groups on replacement work like intended.

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.22631
Modes:
Remote OS version: Linux x64 6.6.87.2-microsoft-standard-WSL2

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 7 5800H with Radeon Graphics          (16 x 3194)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.86GB (1.97GB free)|
|Process Argv|--folder-uri file:///c%3A/Users/desarrollo/econfirming_tasks --crash-reporter-id cf6a3676-768b-4ec3-a629-4f9d2e17bc51|
|Screen Reader|no|
|VM|0%|

|Item|Value|
|---|---|
|Remote|Dev Container: Econfirming|
|OS|Linux x64 6.6.87.2-microsoft-standard-WSL2|
|CPUs|AMD Ryzen 7 5800H with Radeon Graphics (16 x 0)|
|Memory (System)|7.68GB (2.10GB free)|
|VM|0%|
</details><details><summary>Extensions (35)</summary>

Extension|Author (truncated)|Version
---|---|---
Bookmarks|ale|13.5.0
remotehub|Git|0.64.0
theme-monokai-pro-vscode|mon|2.0.7
remote-containers|ms-|0.417.0
remote-ssh|ms-|0.120.0
remote-ssh-edit|ms-|0.87.0
remote-wsl|ms-|0.99.0
azure-repos|ms-|0.40.0
remote-explorer|ms-|0.5.0
remote-repositories|ms-|0.42.0
vscode-speech|ms-|0.16.0
material-icon-theme|PKi|5.24.0
material-product-icons|PKi|1.7.1
vscode-fix-checksums-next|Rim|1.4.0
synthwave-vscode|Rob|0.1.19
pdf|tom|1.2.2
jinja|who|0.0.8
vscode-log-viewer|ber|0.14.1
vscode-office|cwe|3.5.4
vscode-eslint|dba|3.0.10
composer-php-vscode|DEV|1.59.17515
intelli-php-vscode|DEV|0.12.17635
phptools-vscode|DEV|1.59.17515
profiler-php-vscode|DEV|1.59.17515
gitlens|eam|17.2.2
EditorConfig|Edi|0.17.4
copilot|Git|1.341.0
copilot-chat|Git|0.28.5
vscode-containers|ms-|2.0.3
vscode-docker|ms-|2.0.0
postman-for-vscode|Pos|1.13.1
vscode-xml|red|0.28.1
vscode-yaml|red|1.18.0
command-variable|rio|1.67.0
sonarlint-vscode|Son|4.26.0

(12 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31344060
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
j6a0c355:31339598
0g0a1943:31341127
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
"[error] [Window] [041] potential listener LEAK detected, having 453 listeners already. MOST frequent listener","<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.101.2
- OS Version:  macOS 15.5

Steps to Reproduce:

1. Launch VS Code
2. App keeps hanging every few seconds and output shows

2025-07-08 14:13:44.831 [error] [Window] [041] potential listener LEAK detected, having 453 listeners already. MOST frequent listener (45):: Error
    at gEi.create (vscode-file://vscode-app/Applications/Visual%20Studio%20Code.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:27:11906)
    at kYe.q (vscode-file://vscode-app/Applications/Visual%20Studio%20Code.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:29:1377)
    at new z5e (vscode-file://vscode-app/Applications/Visual%20Studio%20Code.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:485:24217)
    at a (vscode-file://vscode-app/Applications/Visual%20Studio%20Code.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:485:22588)
    at Jre.value (vscode-file://vscode-app/Applications/Visual%20Studio%20Code.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:485:22830)
    at E.B (vscode-file://vscode-app/Applications/Visual%20Studio%20Code.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:29:2392)
    at E.C (vscode-file://vscode-app/Applications/Visual%20Studio%20Code.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:29:2462)
    at E.fire (vscode-file://vscode-app/Applications/Visual%20Studio%20Code.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:29:2680)
    at pYe.createModel (vscode-file://vscode-app/Applications/Visual%20Studio%20Code.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:848:898)
    at P9e.provideTextContent (vscode-file://vscode-app/Applications/Visual%20Studio%20Code.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:850:30891)
",1
Model select is not showing after update,"
Type: <b>Bug</b>

stuck after plgin update

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M1 Pro (8 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|8, 5, 5|
|Memory (System)|16.00GB (0.08GB free)|
|Process Argv|--crash-reporter-id 3e01a835-adfb-40fe-943e-74148775935a|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
vscrp:30673768
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
471b6256:31263136
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
jhi8h917:31341130
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
Provide ability to configure alternate local port for port forwarding in code-workspace file,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

I would like the ability to specify in a `.code-workspace` file a remote port that needs to be forwarded and a specific local port to use instead of the same port as the remote host.

If I am understanding correctly, I can provide a config like follows to enable port forwarding for ports 443 and 8443 from the host machine to my local machine:

```json
    ""remote.portsAttributes"": {
        
        ""443"": {
            ""protocol"": ""https""
        },
        ""8443"": {
            ""protocol"": ""https""
        }
    }
```

In my environments, I would like to be able to amend the code-workspace file with a config that always forwards port 3341 from the host machine to port 3345 on my local machine. I've manually done this in the VS Code GUI, but would prefer to have it part of the workspace config for portability reasons.

The example shown in the visual config editor also implies that configuration is possible for ranges of ports as well as regular expressions and/or file paths, but I am having significant difficulty finding documentation to explain those use cases or to see an overall list of valid keys/config options for forwarding ports.

I also reviewed the specification at containers.dev regarding portAttributes, but it was unclear whether VS Code has feature parity with that, and there were not sufficient examples to see if my use case was supported.

Consideration of this feature would be much appreciated, or possibly a shove in the right direction toward finding the documentation around the config format. Thanks!",0
empty responses after uploading image to conversation,"
Type: <b>Bug</b>

after uploading an image, the agent stopped responding. When i add messages the responses are just blank

Extension version: 0.28.5
VS Code version: Code 1.101.0 (dfaf44141ea9deb3b4096f7cd6d24e00c147a4b1, 2025-06-11T15:00:50.123Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-10510U CPU @ 1.80GHz (8 x 2304)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.76GB (0.52GB free)|
|Process Argv|--crash-reporter-id 20a2a64b-090f-491d-b2f3-fb059758f2db|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
jhi8h917:31341130
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
4f60g487:31327383
nes-diff-11:31337487
63221493:31336333
yijiwantestdri0626-c:31336931
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Delete being directly below rename isn't a good choice,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

<img width=""582"" height=""131"" alt=""Image"" src=""https://github.com/user-attachments/assets/beb7c545-c03c-41fe-8b1e-1edce4c1aafa"" />

I was recently doing some work, making a bunch of changes to a few files, and had to rename them and the folder they were in as a final step. I know I probably should have just committed the files beforehand and made the renaming a separate commit (which I will be doing now), but this caught me as my finger slipped and I ended up deleting the entire folder, losing my work. Yes, I know this is technically my fault, yes, I do feel stupid, but this also wouldn't have happened if the two options weren't _directly_ next to one another.

My suggestion would be to move the delete button somewhere else, or the rename somewhere else.
",1
Copilot took too long to get ready. Please ensure you are signed in to GitHub and that the extension GitHub.copilot-chat is installed and enabled.,"
Type: <b>Bug</b>

When I open the chat window, I see this message:

Copilot took too long to get ready. Please ensure you are signed in to GitHub and that the extension GitHub.copilot-chat is installed and enabled.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:
Remote OS version: Linux x64 6.6.87.2-microsoft-standard-WSL2
Remote OS version: Linux x64 6.6.87.2-microsoft-standard-WSL2

<details><summary>Logs</summary>
<pre>
Info: request done: requestId: [833952d7-0ac9-4625-9c4f-3ec9c87f8f53] model deployment ID: []
Info: message 0 returned. finish reason: [stop]
Info: request done: requestId: [65e96c8b-dc93-4a5d-bfe9-283993015234] model deployment ID: []
Info: request done: requestId: [b960f51e-adb1-47bb-a646-ea6c85f4410c] model deployment ID: []
Info: message 0 returned. finish reason: [stop]
Info: request done: requestId: [6e85dcef-c9fe-4f51-a7a8-d162277ada28] model deployment ID: []
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Info: message 0 returned. finish reason: [stop]
Info: request done: requestId: [c878d02a-4601-4ba9-8904-8f8de1cec77f] model deployment ID: []
Info: message 0 returned. finish reason: [stop]
Info: request done: requestId: [3db992b7-5244-434c-bec6-54a3afb663fb] model deployment ID: []
Info: message 0 returned. finish reason: [stop]
Info: request done: requestId: [1dd685c2-6c4c-41b6-9246-bd360d1ce62a] model deployment ID: []
Info: message 0 returned. finish reason: [stop]
Info: request done: requestId: [16576008-de02-4f54-8fdb-ef4f2e493d5a] model deployment ID: []
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Info: message 0 returned. finish reason: [stop]
Info: request done: requestId: [04640eaa-1458-4a55-b253-a6fd7d1e2040] model deployment ID: []
Info: message 0 returned. finish reason: [stop]
Info: request done: requestId: [6bd82278-5585-4d55-bd22-9c4d671076b9] model deployment ID: []
Info: message 0 returned. finish reason: [stop]
Info: request done: requestId: [0db97033-89c5-4712-b64c-e2b9b68ea981] model deployment ID: []
Info: message 0 returned. finish reason: [stop]
Info: request done: requestId: [10121910-41fb-4a8e-8cf9-941dc5341703] model deployment ID: []
Info: message 0 returned. finish reason: [stop]
Info: request done: requestId: [de0d461c-cebc-4517-aa3f-1400e7cc59cd] model deployment ID: []
Info: message 0 returned. finish reason: [stop]
Info: request done: requestId: [1d42cb89-da1e-4ce8-8e1d-d0b1b6713465] model deployment ID: []
Info: message 0 returned. finish reason: [stop]
Info: request done: requestId: [4ea6d21a-4d6d-4a77-9126-f4aab4953fd9] model deployment ID: []
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Info: request done: requestId: [d482fcc1-35fe-4289-845e-a7ae30f42296] model deployment ID: []
Info: message 0 returned. finish reason: [stop]
Info: request done: requestId: [bdfec9f1-647b-42d6-9fb8-b52991d1cccb] model deployment ID: []
Trace: Resolving chat model
Info: Fetched model metadata in 730ms cfa1ae59-d929-4c37-aa16-7b0534aa9569
Trace: Resolved chat model
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
</pre>
</details><details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) Ultra 7 165H (22 x 3072)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|31.64GB (4.55GB free)|
|Process Argv|--crash-reporter-id fd71d0c4-36d0-41ed-a12a-96e7b6566922|
|Screen Reader|no|
|VM|0%|

|Item|Value|
|---|---|
|Remote|WSL: Ubuntu|
|OS|Linux x64 6.6.87.2-microsoft-standard-WSL2|
|CPUs|Intel(R) Core(TM) Ultra 7 165H (22 x 0)|
|Memory (System)|15.44GB (7.12GB free)|
|VM|0%|

|Item|Value|
|---|---|
|Remote|WSL: Ubuntu|
|OS|Linux x64 6.6.87.2-microsoft-standard-WSL2|
|CPUs|Intel(R) Core(TM) Ultra 7 165H (22 x 0)|
|Memory (System)|15.44GB (7.12GB free)|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805cf:30301675
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
vscrpc:30673769
962ge761:30959799
2e7ec940:31000449
pythontbext0:30879054
cppperfnew:31000557
dwnewjupytercf:31046870
pythonrstrctxt:31112756
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31303018
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
jhi8h917:31338107
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
82j33506:31327384
nes-diff-11:31337487
nes-set-on:31340697
6abeh943:31336334
yijiwantestdri0626-t:31336930
hi92a955:31337023

```

</details>

<!-- generated by issue reporter -->",2
Github Copilot is not always using .github/copilot-instructions.md when asked to generate code via Chat (Ask),"Does this issue occur when all extensions are disabled?: Yes

VS Code Version: 1.101.2
OS Version: 22H2
Using GPT 4.1
Steps to Reproduce:

1. In your project workspace, create the file .github/copilot-instructions.md (like it says in the github copilot documentation).
2. Add a line with some basic instructions. Example:
- Here are some basic guidelines for Go(lang) development:
    - When answering questions or generating code related to Go(lang), use version go1.24.x (or at least go1.2x.x) features and not older versions.
3. Ask copilot to generate some code and sometimes you'll notice that it uses features from < go1.2x, or doesn't use features from go1.2x.x.

I understand that some models aren't trained for later versions of some language, but vscode will point out that a block of code can be ""modernized"" using features from Go(lang) 1.24.x. Then in chat, I'll mention that there's more modern features that can be used in a code block and GPT 4.1 does recognize the language features. So it's not like it hasn't been trained (or can't look up the more modern feature). It seems to just ignore the guidelines that I've put in place in .github/copilot-instructions.md.

Sometimes when I see it do something outside of the guidelines I've given it, I'll remind it to use the instructions in .github/copilot-instructions.md and it does understand (it will sometimes reflect on the guidelines in the file). So it seems like it knows it's there, it just doesn't always use it in context when generating code. In my specific case, it will sometimes uses really old Go(lang) styles when more efficient  (or idiomatic) features/style have been available for several version releases.",0
Use www.schematore.org instead of json.schemastore.org,"We're deprecating the subdomain `json.schemastore.org` and standardizing on `www.schemastore.org` going forward. Please update all references to use the new subdomain
",0
Github Copilot is not always using .github/copilot-instructions.md when asked to generate code via Chat (Ask),"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.101.2
- OS Version: 22H2

Using GPT 4.1
Steps to Reproduce:

1. In your project workspace, create the file .github/copilot-instructions.md (like it says in the github copilot documentation).
2. Add a line with some basic instructions. Example:
- Here are some basic guidelines for Go(lang) development:
    - When answering questions or generating code related to Go(lang), use version go1.24.x (or at least go1.2x.x) features and not older versions.
3. Ask copilot to generate some code and sometimes you'll notice that it uses features from < go1.2x, or doesn't use features from go1.2x.x.a

I understand that some models aren't trained for later versions of some language, but vscode will point out that a block of code can be ""modernized"" using features from Go(lang) 1.24.x. Then in chat, I'll mention that there's more modern features that can be used in a code block and GPT 4.1 does recognize the language features. So it's not like it hasn't been trained (or can't look up the more modern feature). It seems to just ignore the guidelines  that I've put in place in .github/copilot-instructions.md.

",0
IntelliSense and Syntax Highlighting broken for Razor and C#,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.101.2
- OS Version: Windows 11 24H2

Steps to Reproduce:

1. Open any Razor Project and play around with the razor pages.
2. Syntax Highlighting does not work properly out of the box.
3. Imports sometimes are broken even with the new C# Dev Kit Extension

We really support for Blazor in VS Code? Microsoft makes Blazor, Microsoft makes VS Code. Why on earth do I have so many issues with syntax highlighting, IntelliSense and the overall dev experience. It is not just my machine, I have tried countless times, different machines, different OSes and with the bare minimum plugins. The dev experience is not as nice as just using Visual Studio out of the box, which is shocking.

",0
"Copy paste,suggestions issue in jupyter notebook(apple silicon)","
Type: <b>Bug</b>

i am using visual studio code in my apple silicon based macbook air. 
1. sometimes copy and pasting is note working(mainly in jupyternote book....because i am using it for a while)
2. sometimes the function suggestions are not working.

when i face this issues i have to restart the application for many times.

VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M1 (8 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|4, 4, 4|
|Memory (System)|8.00GB (0.09GB free)|
|Process Argv|--crash-reporter-id 1a9b4ca2-e0f0-42e4-82d7-77adc0b016e8|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (19)</summary>

Extension|Author (truncated)|Version
---|---|---
docker|doc|0.11.0
vscode-html-css|ecm|2.0.13
go|gol|0.48.0
rainbow-csv|mec|3.20.0
vscode-containers|ms-|2.0.3
vscode-docker|ms-|2.0.0
csharp|ms-|2.84.19
vscode-dotnet-runtime|ms-|2.3.6
debugpy|ms-|2025.8.0
python|ms-|2025.8.0
vscode-pylance|ms-|2025.6.2
jupyter|ms-|2025.5.0
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.1.0
vscode-jupyter-cell-tags|ms-|0.1.9
vscode-jupyter-slideshow|ms-|0.1.6
remote-containers|ms-|0.417.0
cpptools|ms-|1.26.3
pdf|tom|1.2.2


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492cf:30256860
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
vscrp:30673768
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
j6a0c355:31339598
0g0a1943:31341127
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
4f60g487:31327383
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Copilot Chat Session cannot access available MCP tools after tool change notification is received.,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes - Cannot disable extensions as it requires Copilot Chat to call the MCP server.

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.102.0-insider (Universal) Commit: 7631de29bf9a2e31020ab93d3d1611965dc5b62e
- OS Version: Mac OS 15.5 (24F74)

When a MCP server sends a `notifications/tools/list_changed` event, the session is unable to access the new tools until a subsequent message is sent.

Here is a Gif showing the behaviour: 

![Image](https://github.com/user-attachments/assets/b2f14ca6-bf45-4275-8d7d-01c04975e53e)

As you can see, when sending the prompt that requires further tools, the agent correctly prompts the tool to suggest toolsets and enables a relevant toolset. This `enable_toolset` call triggers a `notifications/tools/list_changed` event on an upgraded SSE stream (via Streamable HTTP), and VSCode correctly updates the chat session with the additional 8 tools.
However, during the subsequent actions by the agent, it seems that it is unable to see the new tools are available.

Stopping the currently running message and sending an identical message results in the agent correctly selecting the newly available tool and completing the request.

Steps to Reproduce:

1. Configure an MCP server that provides `notifications/tools/list_changed` events in response to a tool call.
2. Prompt the agent in a way that triggers this tool call to enable additional tools, and that should subsequently trigger a call of these new tools.
3. Observe that the agent correctly attempts to enable a new tool, but is unable to use it.
4. Stop the running agent process
5. Prompt the agent again in the same way
6. Observe that the agent correctly calls the newly added tool.

I will try and put together a basic repro server for testing and add it to this issue.",1
"Agents applying edits are painfully slow in ""large"" files","Whenever I ask Copilot to make changes to a file, it takes a painfully long time as it seems to scan down through every line of the file slowly, even if it's only making one change near the end.

Here's a simplified example - I asked it to change one line near the end of a 1000 line Markdown file, and it takes many seconds of scanning down the file to apply the change. 

https://github.com/user-attachments/assets/ffceff07-da2a-4542-a46b-137a6865db40

I don't know if this is artificially slow just because of the ""scanning"" animation that was added, or if it's genuinely computing something, but it doesn't seem like it should take this long for such a simple edit.",1
Authenticating my AWS Q,"
Type: <b>Bug</b>

its been 2 weeks. i contacted costumer serves for the issue, and its not solved yet. im not able to authenticate due to a wierd message pops related to windown security

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 7 5700U with Radeon Graphics          (16 x 1797)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|9.85GB (1.49GB free)|
|Process Argv|--crash-reporter-id 9f304178-eb79-47e6-a841-7bad905bf55f|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (41)</summary>

Extension|Author (truncated)|Version
---|---|---
claude|aiq|0.1.0
amazon-q-vscode|ama|1.82.0
aws-toolkit-vscode|ama|3.68.0
claude-code-chat|And|0.1.3
ruby-debug|cas|0.3.5
vscode-grok|Eri|1.0.0
codespaces|Git|1.17.3
copilot|Git|1.341.0
copilot-chat|Git|0.28.5
vscode-github-actions|git|0.27.2
geminicodeassist|goo|2.39.0
terraform|has|2.34.5
vscode-rdbg|Koi|0.2.2
Star|lip|0.0.3
starcoderex|Lis|1.0.41
rainbow-csv|mec|3.20.0
vscode-containers|ms-|2.0.3
vscode-docker|ms-|2.0.0
csharp|ms-|2.84.19
vscode-dotnet-runtime|ms-|2.3.6
debugpy|ms-|2025.11.2025070101
pylint|ms-|2025.2.0
python|ms-|2025.9.2025070301
vscode-pylance|ms-|2025.6.2
vscode-python-envs|ms-|0.3.11841011
jupyter|ms-|2025.5.0
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.1.0
vscode-jupyter-cell-tags|ms-|0.1.9
vscode-jupyter-slideshow|ms-|0.1.6
remote-containers|ms-|0.417.0
cmake-tools|ms-|1.21.36
cpptools|ms-|1.26.3
cpptools-extension-pack|ms-|1.3.1
cs-script|ole|2.3.0
oracle-java|Ora|24.0.0
vscode-openshift-connector|red|1.19.0
vscode-redhat-account|red|0.2.0
vscode-yaml|red|1.18.0
ruby-extensions-pack|Sho|0.1.13
sorbet-vscode-extension|sor|0.3.43

(1 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492cf:30256860
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
j6a0c355:31339598
d2249276:31341129
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335814
nes-diff-11:31337487
nes-set-on:31340697
63221493:31336333
yijiwantestdri0626-c:31336931
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
Enable save participants for files during edit sessions,"We don't run save participants when the model is making file edits (only after the user ""accept""s them) because the contents of the file can diverge from what the model 'thinks' the file contains. Modern edit tools are essentially powerful file-replace tools, and so this can result in editing failures later. However, this can then lead to the LM chasing linter errors that could be resolved simply by running those participants.

It occurred to me we could 'trick' this by keeping a mirrored version of the file prior to running save participants. Continued edits would get applied to that version of the file, but then we would run the participant after each edit is made so that file on-disk and as the user sees it is properly formatted. We would discard this version if the file on disk changes otherwise, and if an edit failure happens then the model would re-read the file like it normally would and continue on from there.",1
Azure models o4-mini and o3 fail because of hard-coded temperature parameters,"> Azure models o4-mini and o3 fail because of hard-coded temperature parameters in the latest versions of Github Copilot
> 
> ```
> Sorry, your request failed. Please try again. Request id: ec077427-6648-43d3-b941-56b6bdfafc05
> 
> Reason: Request Failed: 400 { ""error"": { ""message"": ""Unsupported value: 'temperature' does not support 0.1 with this model. Only the default (1) value is supported."", ""type"": ""invalid_request_error"", ""param"": ""temperature"", ""code"": ""unsupported_value"" } }
> ```
> 
> This happens for any Azure deployments of these models. This issue has been reported a number of times by multiple users.  

 _Originally posted by @kingdomseed in [#253136](https://github.com/microsoft/vscode/issues/253136#issuecomment-3045609812)_",1
GPT 4.1 is useless.,"
Type: <b>Bug</b>

AI is lying, not followinf instructions.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 7 5800X 8-Core Processor              (16 x 3800)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|63.89GB (14.42GB free)|
|Process Argv|--crash-reporter-id a7fcca5d-1f0d-478e-8e1f-081d0d3530f5|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551:31249599
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
c4b42873:31341128
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335814
nes-diff-11:31337487
nes-set-on:31340697
6abeh943:31336334
yijiwantestdri0626-t:31336930
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
"The model unexpectedly did not return a response, which may indicate a service issue. Please report a bug.The model unexpectedly did not return a response, which may indicate a service issue. Please report a bug.","
Type: <b>Bug</b>

The model unexpectedly did not return a response, which may indicate a service issue. Please report a bug.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 7 3700X 8-Core Processor              (16 x 4050)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.91GB (1.68GB free)|
|Process Argv|--crash-reporter-id 98300acc-71ae-4697-bf1b-15a3e293f01b|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551:31249599
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
jjjhb125:31275178
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
j6a0c355:31339598
0g0a1943:31341127
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
63221493:31336333
yijiwantestdri0626-t:31336930
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
Use new api instead of hack to get terminal for task,"Replace this with the new `taskExecution.terminal`

https://github.com/microsoft/vscode-copilot-chat/blob/bdae04a5237acbec8afdbbd53005e0b79b017bae/src/extension/tools/node/getTaskOutputTool.tsx#L43-L44",0
Sonnet 3.5 is not running directly switching to GPT 4.1 ,"
Type: <b>Performance Issue</b>

Sonnet 3.5 is not running directly switching to GPT 4.1 

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-10610U CPU @ 1.80GHz (8 x 2304)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.67GB (4.35GB free)|
|Process Argv|--crash-reporter-id 0371558c-f043-46b0-9f42-1781f7a71881|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>Process Info</summary>

```
CPU %	Mem MB	   PID	Process
    1	   144	 15296	code
    0	   191	  4780	   gpu-process
    0	    49	  5708	   utility-network-service
    1	   379	 10332	window [1] (Plot_No_extraplot_V2 07-07-2025 GD.py - Mustafa - Visual Studio Code)
    0	   165	 10564	pty-host
    0	    85	  1384	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\adminrajagopal\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     8	  9232	     conpty-agent
    0	    82	  9532	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\adminrajagopal\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     8	 11484	     conpty-agent
    0	    89	 11564	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\adminrajagopal\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	    89	 12100	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\adminrajagopal\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	    82	 12336	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\adminrajagopal\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	    82	 12504	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\adminrajagopal\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     8	 12760	     conpty-agent
    0	     8	 15208	     conpty-agent
    0	     8	 16428	     conpty-agent
    0	     8	 16956	     conpty-agent
    0	     8	 18784	     conpty-agent
    0	     8	 19536	     conpty-agent
    0	    82	 19628	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\adminrajagopal\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	    82	 19816	     C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\adminrajagopal\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	   127	 12560	shared-process
    0	   112	 16316	file-watcher [1]
    0	   640	 18844	extension-host [1]
    0	    18	  3004	     c:\Users\adminrajagopal\AppData\Local\Programs\Python\Python313\python.exe c:\Users\adminrajagopal\.vscode\extensions\ms-toolsai.jupyter-2025.5.0-win32-x64\pythonFiles\vscode_datascience_helpers\kernel_interrupt_daemon.py --ppid 18844
    0	     8	 24388	       C:\WINDOWS\system32\conhost.exe 0x4
    0	    73	  9448	     c:\Users\adminrajagopal\AppData\Local\Programs\Python\Python313\python.exe -m ipykernel_launcher --f=c:\Users\adminrajagopal\AppData\Roaming\jupyter\runtime\kernel-v32f6ff4a6553aa18336cc1c384fe6479953498f51.json
    0	     8	 21480	       C:\WINDOWS\system32\conhost.exe 0x4
    0	    99	 17088	     ""C:\Users\adminrajagopal\AppData\Local\Programs\Microsoft VS Code\Code.exe"" ""c:\Users\adminrajagopal\AppData\Local\Programs\Microsoft VS Code\resources\app\extensions\json-language-features\server\dist\node\jsonServerMain"" --node-ipc --clientProcessId=18844
    0	   794	 17536	     electron-nodejs (bundle.js )
    0	    32	 22988	   crashpad-handler
```

</details>
<details>
<summary>Workspace Info</summary>

```
|  Window (Plot_No_extraplot_V2 07-07-2025 GD.py - Mustafa - Visual Studio Code)
|    Folder (Mustafa): 4505 files
|      File types: txt(3330) csv(765) png(188) py(51) pptx(6) pdf(5) opju(4)
|                  cif(2) xls(2) raw(2)
|      Conf files:;
```

</details>
<details><summary>Extensions (15)</summary>

Extension|Author (truncated)|Version
---|---|---
claude-code|Ant|1.0.31
copilot|Git|1.341.0
copilot-chat|Git|0.28.5
latex-workshop|Jam|10.10.0
debugpy|ms-|2025.8.0
python|ms-|2025.8.0
vscode-pylance|ms-|2025.6.2
jupyter|ms-|2025.5.0
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.1.0
vscode-jupyter-cell-tags|ms-|0.1.9
vscode-jupyter-slideshow|ms-|0.1.6
r-debugger|RDe|0.5.5
r|REd|2.8.6
r-syntax|REd|0.1.3


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492cf:30256860
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
j6a0c355:31339598
d2249276:31341129
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
82j33506:31327384
nes-diff-11:31337487
testaa123:31335226
onetestforazureexpcf:31335614
6abeh943:31336334
yijiwantestdri0626-t:31336930
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
utf8-encoding-declaration (UP009) is a bad suggestion,"
Type: <b>Bug</b>

After
`#!/usr/bin/env python3`
please do NOT suggest
`# -*- coding: utf-8 -*-`
because that is no longer required in Python 3 and will be flagged by the linter ruff or pyupgrade as discussed at
https://docs.astral.sh/ruff/rules/utf8-encoding-declaration/


Extension version: 1.341.0
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.5.0
Modes:

## Copilot Completion Feedback
### Describe the issue, feedback, or steps to reproduce it:


### Completion text:
```
# -*- coding: utf-8 -*-
```

<details>
<summary>Diagnostics</summary>

## Copilot Extension

- Version: 1.341.0
- Editor: vscode/1.101.2
- Header Request ID: e314c98c-3442-473b-adc1-d959a8f914a1
- Choice Index: 0
- Opportunity ID: 7312472e-6b8f-4830-a8bd-21c7e78aefb6
- Client Completion ID: 0014de46-2b23-45d0-975d-9dc12d170ab5
- Model ID: gpt-4o-copilot
- A/B Experiment Info: vsliv368:30146709;vspor879:30202332;vspor708:30202333;vspor363:30204092;binariesv615:30325510;c4g48928:30535728;6074i472:31201624;customenabled:31248079;cp_compl_t_13691:31267975;c5chb378:31220672;9064b325:31222308;copilot_t_ci:31333650;cp_14580_0_t:31245410;cp15172_t:31240738;4gafe986:31271826;cp16806_a:31298379;usemplatestapi:31297334;7bj51361:31289155;aj953862:31281341;cp17209_t:31303705;cp16806_spec_t:31320508;cp18466_t:31331492;9d2cg352:31339597;cp_comp_18239b_t:31339243;d2249276:31341129;i851h500:31338111;usemarketplace:31343026;nesew2to5:31336538;agentclaude:31335815;b6b4d950:31327385;nes-diff-11:31337487;aa_cpt_c:31332559;6abeh943:31336334;yijiwantestdri0626-t:31336930;f76d9909:31342392;gh_scm_t:31343112;

</details>
<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M1 (8 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|4, 4, 4|
|Memory (System)|16.00GB (0.07GB free)|
|Process Argv|--crash-reporter-id 324d8e4f-1597-4eef-afcf-fc7c204774e5|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
d2249276:31341129
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
b6b4d950:31327385
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
Chat undo is undoing the wrong file,"* use Agent mode to make a change to a file A.ts
* use Agent mode to make a change to a file B.ts
* press undo
* :bug: it undoes changes in file A.ts


https://github.com/user-attachments/assets/300bf7a3-95f8-4caa-a3bf-4dcc63e353cc


```
Version: 1.102.0-insider
Commit: 7631de29bf9a2e31020ab93d3d1611965dc5b62e
Date: 2025-07-07T22:41:41.902Z
Electron: 35.6.0
ElectronBuildId: 11847422
Chromium: 134.0.6998.205
Node.js: 22.15.1
V8: 13.4.114.21-electron.0
OS: Darwin arm64 24.5.0
```",1
Remove terminal allow list defaults for now,To be on the safe side until discussions are done. We can also have policy controls when it moves to core in https://github.com/microsoft/vscode/issues/252650 / https://github.com/microsoft/vscode/issues/253314,0
refusing to,"
Type: <b>Bug</b>

I'm attempting to refactor a script, have provided a bunch of context and there has been consistent denials over the past 10 hours.  Rebooting, reloading VSCode have not resolved this.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-14650HX (24 x 2419)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|63.71GB (38.14GB free)|
|Process Argv|--crash-reporter-id a10eb9fa-2cb4-4f0a-bb1f-49f926be31bd|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492:30256859
pythonvspyt551:31249599
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
d2249276:31341129
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
still have tokens lleft wrong information,"
Type: <b>Bug</b>

premium requests not exceeded  but still got wrong calculation of premium requestions shown

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:


<!-- generated by issue reporter -->",0
Reusable code-workspace settings (shared using public URL),"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

Hi,

In our company, we program using AL language and over the years we arrived to some useful configuration for our VS Code workspaces.
The workspace settings are pretty complex - see the template below.

The problem is that we make changes to this template every few months or so (add useful setting or a new extension, etc.). This means, that other projects (hundreds of them already) do not have these changes and remain as they were (the one that prompted me to do this was from 2022).

We would like to split workspace into two configurations: shared settings (accessible using URL) and actual workspace (in the Git repository). See suggested shared settings template syntax and workspace file syntax below.

The external template files should be cached locally so even if you go offline the last received workspace settings would still apply. Changes in external files should only be received when workspace is opened - no need to poll for changes continuously (at least for us).

There should also be option for folder settings templates, that would work similarly - add array properties ""includedSettings"": [list of urls] and ""includedExtensions"": [list of urls] to .vscode/settings.json and .vscode/extensions.json respectively.

Thanks.

<details>
<summary>The current *.code-workspace</summary>

```jsonc
{
    ""extensions"": {
        ""recommendations"": [
            ""ms-dynamics-smb.al"",
            ""waldo.crs-al-language-extension"",
            ""vjeko.vjeko-al-objid"",
            ""davidfeldhoff.al-codeactions"",
            ""rasmus.al-var-helper"",
            ""nabsolutions.nab-al-tools"",
            ""BartPermentier.al-toolbox"",
            ""andrzejzwierzchowski.al-code-outline"",
            ""stefanmaron.businesscentral-lintercop"",
            ""vjeko.al-pragma-explorer"",
            ""mhutchie.git-graph"",
            ""eamodio.gitlens"",
            ""ms-vsliveshare.vsliveshare"",
            ""usernamehw.errorlens"",
            ""github.copilot"",
        ],
    },
    ""settings"": {
        ""workbench.iconTheme"": ""al"",
        ""files.exclude"": {
            ""**/*.code-workspace"": true,
            ""MainApp/"": true, // project specific - there can be more that 2 app folders in a project, but most have only MainApp
            ""TestApp/"": true, // project specific
        },
        ""explorer.compactFolders"": false,
        ""explorer.fileNesting.enabled"": true,
        ""explorer.fileNesting.expand"": false,
        ""explorer.fileNesting.patterns"": {
            ""*.report.al"": ""$(capture).report.rdl, $(capture).report.docx, $(capture).report.xlsx"",
            ""*.reportext.al"": ""$(capture).reportext.rdl, $(capture).reportext.docx, $(capture).reportext.xlsx"",
            ""_APIPageContainer"": ""API*.Page.al"",
            ""_APIQueryContainer"": ""API*.Query.al"",
        },
        ""editor.snippetSuggestions"": ""bottom"",
        ""editor.codeActionsOnSave"": {
            ""source.fixAll.al"": ""explicit"",
        },
        ""[json]"": {
            ""editor.defaultFormatter"": ""vscode.json-language-features"",
            ""editor.formatOnSave"": true,
            ""files.trimTrailingWhitespace"": true,
            ""files.trimFinalNewlines"": true,
            ""files.insertFinalNewline"": true,
        },
        ""[jsonc]"": {
            ""editor.defaultFormatter"": ""vscode.json-language-features"",
            ""editor.formatOnSave"": true,
            ""files.trimTrailingWhitespace"": true,
            ""files.trimFinalNewlines"": true,
            ""files.insertFinalNewline"": true,
        },
        ""[al]"": {
            ""files.trimTrailingWhitespace"": true,
            ""files.trimFinalNewlines"": true,
            ""files.insertFinalNewline"": true,
        },
        ""[yaml]"": {
            ""files.trimTrailingWhitespace"": true,
            ""files.trimFinalNewlines"": true,
            ""files.insertFinalNewline"": true,
        },
        ""[azure-pipelines]"": {
            ""files.trimTrailingWhitespace"": true,
            ""files.trimFinalNewlines"": true,
            ""files.insertFinalNewline"": true,
        },
        ""al.packageCachePath"": ""../.alpackages"",
        ""al.assemblyProbingPaths"": [
            ""./.netpackages"",
            ""C:/Windows/assembly"",
        ],
        ""al.enableCodeAnalysis"": true,
        ""al.codeAnalyzers"": [
            ""${CodeCop}"",
            ""${UICop}"",
            ""${PerTenantExtensionCop}"",
            ""${AppSourceCop}"",
            ""${analyzerFolder}BusinessCentral.LinterCop.dll"",
        ],
        ""al.ruleSetPath"": ""./al.ruleset.json"",
        ""al.backgroundCodeAnalysis"": ""Project"",
        ""al.compilationOptions"": {
            ""outFolder"": ""../.output"",
        },
        ""CRS.OnSaveAlFileAction"": ""Reorganize"",
        ""CRS.ObjectNamePrefix"": ""SOMEPREFIX "", // project specific
        ""CRS.ObjectNameSuffix"": """",
        ""CRS.RemovePrefixFromFilename"": true,
        ""CRS.RemoveSuffixFromFilename"": true,
        ""CRS.FileNamePattern"": ""<ObjectNameShort>.<ObjectTypeShortPascalCase>.al"",
        ""CRS.FileNamePatternExtensions"": ""<ObjectNameShort>.<ObjectTypeShortPascalCase>.al"",
        ""CRS.FileNamePatternPageCustomizations"": ""<ObjectNameShort>.<ObjectTypeShortPascalCase>.al"",
        ""NAB.UseDTS"": true,
        ""alOutline.completionProviders"": [
            ""VariableNamesWithType"",
        ],
        ""alOutline.keepVariableNamesCompletionAffixes"": true,
        ""alOutline.fieldsSelectionOrder"": ""selection order"",
        ""alOutline.appAreaMode"": ""inheritFromMainObject"",
        ""alOutline.enableCodeCopFixes"": true,
        ""alOutline.useBCLinterCopCaseRules"": true,
        ""alOutline.triggersSortMode"": ""NaturalOrder"",
        ""alOutline.sortMembersGlobalVariablesSortMode"": ""First"",
        ""alOutline.codeActionsOnSave"": [
            ""SortCustomizations"",
            ""SortPermissions"",
            ""SortPermissionSetList"",
        ],
        ""alOutline.defaultApiPublisher"": ""softeraBaltic"",
        ""alOutline.defaultApiGroup"": ""automation"",
        ""alOutline.defaultApiVersion"": ""v0.1"",
        ""alOutline.createApiFieldsCaptions"": false,
        ""alOutline.apiFieldNamesConversion"": [
            {
                ""searchRegExp"": ""^lineNo$"",
                ""newValue"": ""sequence"",
            },
            {
                ""searchRegExp"": ""^no$"",
                ""newValue"": ""number"",
            },
            {
                ""searchRegExp"": ""No$"",
                ""newValue"": ""Number"",
            },
            {
                ""searchRegExp"": ""^systemId$"",
                ""newValue"": ""id"",
            },
            {
                ""searchRegExp"": ""^systemModifiedAt$"",
                ""newValue"": ""lastModifiedDateTime"",
            },
        ],
        ""git.openRepositoryInParentFolders"": ""always"",
        ""git.enableSmartCommit"": true,
        ""git.autofetch"": true,
        ""git.confirmSync"": false,
        ""git.branchProtection"": [
            ""code/*"",
            ""releases/*"",
        ],
    },
}
```

</details>

<details>
<summary>Shared settings template files</summary>

base.code-workspace:

```jsonc
{
    ""extensions"": {
        ""recommendations"": [
            ""ms-dynamics-smb.al"",
            ""waldo.crs-al-language-extension"",
            ""vjeko.vjeko-al-objid"",
            ""davidfeldhoff.al-codeactions"",
            ""rasmus.al-var-helper"",
            ""nabsolutions.nab-al-tools"",
            ""BartPermentier.al-toolbox"",
            ""andrzejzwierzchowski.al-code-outline"",
            ""stefanmaron.businesscentral-lintercop"",
            ""vjeko.al-pragma-explorer"",
            ""mhutchie.git-graph"",
            ""eamodio.gitlens"",
            ""ms-vsliveshare.vsliveshare"",
            ""usernamehw.errorlens"",
            ""github.copilot"",
        ],
    },
    ""settings"": {
        ""workbench.iconTheme"": ""al"",
        ""files.exclude"": {
            ""**/*.code-workspace"": true,
            ""MainApp/"": true, // project specific - there can be more that 2 app folders in a project
            ""TestApp/"": true, // project specific
        },
        ""explorer.compactFolders"": false,
        ""explorer.fileNesting.enabled"": true,
        ""explorer.fileNesting.expand"": false,
        ""explorer.fileNesting.patterns"": {
            ""*.report.al"": ""$(capture).report.rdl, $(capture).report.docx, $(capture).report.xlsx"",
            ""*.reportext.al"": ""$(capture).reportext.rdl, $(capture).reportext.docx, $(capture).reportext.xlsx"",
            ""_APIPageContainer"": ""API*.Page.al"",
            ""_APIQueryContainer"": ""API*.Query.al"",
        },
        ""editor.snippetSuggestions"": ""bottom"",
        ""editor.codeActionsOnSave"": {
            ""source.fixAll.al"": ""explicit"",
        },
        ""[json]"": {
            ""editor.defaultFormatter"": ""vscode.json-language-features"",
            ""editor.formatOnSave"": true,
            ""files.trimTrailingWhitespace"": true,
            ""files.trimFinalNewlines"": true,
            ""files.insertFinalNewline"": true,
        },
        ""[jsonc]"": {
            ""editor.defaultFormatter"": ""vscode.json-language-features"",
            ""editor.formatOnSave"": true,
            ""files.trimTrailingWhitespace"": true,
            ""files.trimFinalNewlines"": true,
            ""files.insertFinalNewline"": true,
        },
        ""[al]"": {
            ""files.trimTrailingWhitespace"": true,
            ""files.trimFinalNewlines"": true,
            ""files.insertFinalNewline"": true,
        },
        ""[yaml]"": {
            ""files.trimTrailingWhitespace"": true,
            ""files.trimFinalNewlines"": true,
            ""files.insertFinalNewline"": true,
        },
        ""[azure-pipelines]"": {
            ""files.trimTrailingWhitespace"": true,
            ""files.trimFinalNewlines"": true,
            ""files.insertFinalNewline"": true,
        },
        ""al.packageCachePath"": ""../.alpackages"",
        ""al.assemblyProbingPaths"": [
            ""./.netpackages"",
            ""C:/Windows/assembly"",
        ],
        ""al.enableCodeAnalysis"": true,
        ""al.codeAnalyzers"": [
            ""${CodeCop}"",
            ""${UICop}"",
            ""${PerTenantExtensionCop}"",
            ""${AppSourceCop}"",
            ""${analyzerFolder}BusinessCentral.LinterCop.dll"",
        ],
        ""al.ruleSetPath"": ""./al.ruleset.json"",
        ""al.backgroundCodeAnalysis"": ""Project"",
        ""al.compilationOptions"": {
            ""outFolder"": ""../.output"",
        },
        ""CRS.OnSaveAlFileAction"": ""Reorganize"",
        ""CRS.ObjectNamePrefix"": """",
        ""CRS.ObjectNameSuffix"": """",
        ""CRS.RemovePrefixFromFilename"": true,
        ""CRS.RemoveSuffixFromFilename"": true,
        ""CRS.FileNamePattern"": ""<ObjectNameShort>.<ObjectTypeShortPascalCase>.al"",
        ""CRS.FileNamePatternExtensions"": ""<ObjectNameShort>.<ObjectTypeShortPascalCase>.al"",
        ""CRS.FileNamePatternPageCustomizations"": ""<ObjectNameShort>.<ObjectTypeShortPascalCase>.al"",
        ""NAB.UseDTS"": true,
        ""alOutline.completionProviders"": [
            ""VariableNamesWithType"",
        ],
        ""alOutline.keepVariableNamesCompletionAffixes"": true,
        ""alOutline.fieldsSelectionOrder"": ""selection order"",
        ""alOutline.appAreaMode"": ""inheritFromMainObject"",
        ""alOutline.enableCodeCopFixes"": true,
        ""alOutline.useBCLinterCopCaseRules"": true,
        ""alOutline.triggersSortMode"": ""NaturalOrder"",
        ""alOutline.sortMembersGlobalVariablesSortMode"": ""First"",
        ""alOutline.codeActionsOnSave"": [
            ""SortCustomizations"",
            ""SortPermissions"",
            ""SortPermissionSetList"",
        ],
        ""alOutline.defaultApiPublisher"": ""softeraBaltic"",
        ""alOutline.defaultApiGroup"": ""automation"",
        ""alOutline.defaultApiVersion"": ""v0.1"",
        ""alOutline.createApiFieldsCaptions"": false,
        ""alOutline.apiFieldNamesConversion"": [
            {
                ""searchRegExp"": ""^lineNo$"",
                ""newValue"": ""sequence"",
            },
            {
                ""searchRegExp"": ""^no$"",
                ""newValue"": ""number"",
            },
            {
                ""searchRegExp"": ""No$"",
                ""newValue"": ""Number"",
            },
            {
                ""searchRegExp"": ""^systemId$"",
                ""newValue"": ""id"",
            },
            {
                ""searchRegExp"": ""^systemModifiedAt$"",
                ""newValue"": ""lastModifiedDateTime"",
            },
        ],
        ""git.openRepositoryInParentFolders"": ""always"",
        ""git.enableSmartCommit"": true,
        ""git.autofetch"": true,
        ""git.confirmSync"": false,
        ""git.branchProtection"": [
            ""code/*"",
            ""releases/*"",
        ],
    },
}
```

settings.json

```jsonc
{
    ""workbench.iconTheme"": ""al"",
    ""files.exclude"": {
        ""**/*.code-workspace"": true,
        ""MainApp/"": true,
        ""TestApp/"": true,
    },
    ""explorer.compactFolders"": false,
    // ...
}
```

</details>

The new project code-workspace file:

```jsonc
{
    ""includedCodeWorkspaces"": [
        ""https://softerabalticeune.blob.core.windows.net/al-workspace/base.code-workspace"",
    ],
    /*
    Optional for completeness:
    ""includedSettings"": [
        ""https://softerabalticeune.blob.core.windows.net/al-workspace/base.settingsA.json"",
        ""https://softerabalticeune.blob.core.windows.net/al-workspace/base.settingsB.json"",
    ],
    ""includedExtensions"": [
        ""https://softerabalticeune.blob.core.windows.net/al-workspace/baseA.extensions.json"",
        ""https://softerabalticeune.blob.core.windows.net/al-workspace/baseB.extensions.json"",
    ],
    */
    ""settings"": {
       ""CRS.ObjectNamePrefix"": ""MDB "",
    },
}
```

P.S. 1. Taking inspiration from AL language ruleset files:

```json
{
    ""name"": ""Custom AL rules for AppSource"",
    ""includedRuleSets"": [
        {
            ""action"": ""Default"",
            ""path"": ""https://softerabalticeune.blob.core.windows.net/al-rulesets/Main.al.ruleset.json""
        }
    ],
    ""rules"": [
        {
            ""id"": ""PTE0001"",
            ""action"": ""Hidden"",
            ""justification"": ""Extension objects are outside 5XXXX range (warning regarding PTE object range)""
        },
        {
            ""id"": ""PTE0002"",
            ""action"": ""Hidden"",
            ""justification"": ""Extension fields are outside 5XXXX range (warning regarding PTE field range)""
        }
    ]
}
```",1
Feature Request: Colored line scopes for *JSX*,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

I love bracket pair colorization and guides. It's very very handy, for complex typescript projects with more and more nesting! I am looking for bracket pair colorization and guides (like below) specifically for JSX.

<img width=""105"" height=""144"" alt=""Image"" src=""https://github.com/user-attachments/assets/d67f63be-6cfb-470a-9bd0-7e1b1687fb26"" />

As of now, I am unable to find any existing extension, or any setting that enables bracket guides as described above for JSX.

This issue is inspired by this older Feature Request issue #131001",0
works in windows but do not in WSL,"We have written the needed data into your clipboard because it was too large to send. Please paste.


Type: <b>Bug</b>

workbench.desktop.main.js:sourcemap:35  WARN Via 'product.json#extensionEnabledApiProposals' extension 'ms-vsliveshare.vsliveshare' wants API proposal 'notebookCellExecutionState' but that proposal DOES NOT EXIST. Likely, the proposal has been finalized (check 'vscode.d.ts') or was abandoned.
warn @ workbench.desktop.main.js:sourcemap:35
workbench.desktop.main.js:sourcemap:35  WARN Via 'product.json#extensionEnabledApiProposals' extension 'ms-python.gather' wants API proposal 'notebookCellExecutionState' but that proposal DOES NOT EXIST. Likely, the proposal has been finalized (check 'vscode.d.ts') or was abandoned.
warn @ workbench.desktop.main.js:sourcemap:35
workbench.desktop.main.js:sourcemap:35  WARN Via 'product.json#extensionEnabledApiProposals' extension 'ms-python.vscode-pylance' wants API proposal 'mcpConfigurationProvider' but that proposal DOES NOT EXIST. Likely, the proposal has been finalized (check 'vscode.d.ts') or was abandoned.
warn @ workbench.desktop.main.js:sourcemap:35
workbench.desktop.main.js:sourcemap:35  INFO ComputeTargetPlatform: win32-x64
workbench.desktop.main.js:sourcemap:35  INFO Started local extension host with pid 4948.
workbench.desktop.main.js:sourcemap:35   ERR Extension 'ms-python.vscode-pylance' wants API proposal 'mcpConfigurationProvider' but that proposal DOES NOT EXIST. Likely, the proposal has been finalized (check 'vscode.d.ts') or was abandoned.
error @ workbench.desktop.main.js:sourcemap:35
workbench.desktop.main.js:sourcemap:35   ERR navigator is now a global in nodejs, please see https://aka.ms/vscode-extensions/navigator for additional info on this error.: PendingMigrationError: navigator is now a global in nodejs, please see https://aka.ms/vscode-extensions/navigator for additional info on this error.
    at get (file:///c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/api/node/extensionHostProcess.js:361:1437)
    at Ye (c:\Users\HP\.vscode\extensions\ms-vscode-remote.remote-wsl-0.99.0\dist\node\extension.js:2:9241)
    at R (c:\Users\HP\.vscode\extensions\ms-vscode-remote.remote-wsl-0.99.0\dist\node\extension.js:2:5197)
    at c:\Users\HP\.vscode\extensions\ms-vscode-remote.remote-wsl-0.99.0\dist\node\extension.js:2:9094
    at k (c:\Users\HP\.vscode\extensions\ms-vscode-remote.remote-wsl-0.99.0\dist\node\extension.js:2:349286)
    at c:\Users\HP\.vscode\extensions\ms-vscode-remote.remote-wsl-0.99.0\dist\node\extension.js:2:394676
    at Object.f [as use] (c:\Users\HP\.vscode\extensions\ms-vscode-remote.remote-wsl-0.99.0\dist\node\extension.js:2:420128)
    at c:\Users\HP\.vscode\extensions\ms-vscode-remote.remote-wsl-0.99.0\dist\node\extension.js:2:419482
    at Object.watch (c:\Users\HP\.vscode\extensions\ms-vscode-remote.remote-wsl-0.99.0\dist\node\extension.js:2:419493)
    at b (c:\Users\HP\.vscode\extensions\ms-vscode-remote.remote-wsl-0.99.0\dist\node\extension.js:2:421365)
    at e.<computed> [as initialize] (c:\Users\HP\.vscode\extensions\ms-vscode-remote.remote-wsl-0.99.0\dist\node\extension.js:2:393954)
    at e.r [as initialize] (c:\Users\HP\.vscode\extensions\ms-vscode-remote.remote-wsl-0.99.0\dist\node\extension.js:2:369211)
    at c:\Users\HP\.vscode\extensions\ms-vscode-remote.remote-wsl-0.99.0\dist\node\extension.js:2:402753
    at p (c:\Users\HP\.vscode\extensions\ms-vscode-remote.remote-wsl-0.99.0\dist\node\extension.js:2:361772)
    at e.<computed> [as initialize] (c:\Users\HP\.vscode\extensions\ms-vscode-remote.remote-wsl-0.99.0\dist\node\extension.js:2:401943)
    at t.r [as initialize] (c:\Users\HP\.vscode\extensions\ms-vscode-remote.remote-wsl-0.99.0\dist\node\extension.js:2:369211)
    at Array.<anonymous> (c:\Users\HP\.vscode\extensions\ms-vscode-remote.remote-wsl-0.99.0\dist\node\extension.js:2:15835)
    at Et (c:\Users\HP\.vscode\extensions\ms-vscode-remote.remote-wsl-0.99.0\dist\node\extension.js:2:10747)
    at l (c:\Users\HP\.vscode\extensions\ms-vscode-remote.remote-wsl-0.99.0\dist\node\extension.js:2:15796)
    at he (c:\Users\HP\.vscode\extensions\ms-vscode-remote.remote-wsl-0.99.0\dist\node\extension.js:2:126455)
    at e.<computed> (c:\Users\HP\.vscode\extensions\ms-vscode-remote.remote-wsl-0.99.0\dist\node\extension.js:2:129719)
    at Object.initialize (c:\Users\HP\.vscode\extensions\ms-vscode-remote.remote-wsl-0.99.0\dist\node\extension.js:2:369965)
    at c:\Users\HP\.vscode\extensions\ms-vscode-remote.remote-wsl-0.99.0\dist\node\extension.js:2:33988
    at p (c:\Users\HP\.vscode\extensions\ms-vscode-remote.remote-wsl-0.99.0\dist\node\extension.js:2:361772)
    at e.<computed> [as initialize] (c:\Users\HP\.vscode\extensions\ms-vscode-remote.remote-wsl-0.99.0\dist\node\extension.js:2:33913)
    at t.r [as initialize] (c:\Users\HP\.vscode\extensions\ms-vscode-remote.remote-wsl-0.99.0\dist\node\extension.js:2:369211)
    at c:\Users\HP\.vscode\extensions\ms-vscode-remote.remote-wsl-0.99.0\dist\node\extension.js:2:287771
    at async t.oneDataSystemClientFactory (c:\Users\HP\.vscode\extensions\ms-vscode-remote.remote-wsl-0.99.0\dist\node\extension.js:2:287171)
error @ workbench.desktop.main.js:sourcemap:35
workbench.desktop.main.js:sourcemap:35  INFO Invoking resolveAuthority(wsl)...
workbench.desktop.main.js:sourcemap:35  INFO [LocalProcess0][resolveAuthority(wsl,1)][0ms] obtaining proxy...
workbench.desktop.main.js:sourcemap:35  INFO [LocalProcess0][resolveAuthority(wsl,1)][0ms] invoking...
workbench.desktop.main.js:sourcemap:35  INFO [LocalProcess0][resolveAuthority(wsl,1)][680ms] returned WebSocket(127.0.0.1:4861)
workbench.desktop.main.js:sourcemap:35  INFO resolveAuthority(wsl) returned 'WebSocket(127.0.0.1:4861)' after 680 ms
workbench.desktop.main.js:sourcemap:35  INFO Creating a socket (renderer-Management-d7aef37c-6f32-4796-b79c-96556fe79497)...
workbench.desktop.main.js:sourcemap:35  INFO Creating a socket (renderer-ExtensionHost-ef6bd9bb-63b6-4d08-9e3a-9957d73d250b)...
workbench.desktop.main.js:sourcemap:35  INFO Creating a socket (renderer-Management-d7aef37c-6f32-4796-b79c-96556fe79497) was successful after 146 ms.
workbench.desktop.main.js:sourcemap:35  INFO Creating a socket (renderer-ExtensionHost-ef6bd9bb-63b6-4d08-9e3a-9957d73d250b) was successful after 295 ms.
workbench.desktop.main.js:sourcemap:35  INFO [google.geminicodeassist]: Command `geminicodeassist.sdlcagents.agentBoard` already registered by Gemini Code Assist (google.geminicodeassist)
workbench.desktop.main.js:sourcemap:1872 Overwriting grammar scope name to file mapping for scope source.js.
Old grammar file: file:///c%3A/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/extensions/javascript/syntaxes/JavaScript.tmLanguage.json.
New grammar file: vscode-remote://wsl%2Bubuntu-24.04/home/melaku/.vscode-server/extensions/mgmcdermott.vscode-language-babel-0.0.40/grammars/Babel-Language.json
register @ workbench.desktop.main.js:sourcemap:1872
TMScopeRegistry.ts:46 Overwriting grammar scope name to file mapping for scope source.js.
Old grammar file: file:///c%3A/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/extensions/javascript/syntaxes/JavaScript.tmLanguage.json.
New grammar file: vscode-remote://wsl%2Bubuntu-24.04/home/melaku/.vscode-server/extensions/mgmcdermott.vscode-language-babel-0.0.40/grammars/Babel-Language.json
register @ TMScopeRegistry.ts:46
workbench.desktop.main.js:sourcemap:1281 [Extension Host] [INFO] 2025-07-08T11:56:45.616Z - Translations initialized.
workbench.desktop.main.js:sourcemap:1281 [Extension Host] [INFO] 2025-07-08T11:56:45.630Z - Extension activated!
workbench.desktop.main.js:sourcemap:35   ERR [UriError]: If a URI contains an authority component, then the path component must either be empty or begin with a slash (""/"") character: Error: [UriError]: If a URI contains an authority component, then the path component must either be empty or begin with a slash (""/"") character
    at mBi (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:30:57171)
    at new iMe (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:30:58309)
    at new GW (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:30:60044)
    at iMe.from (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:30:59141)
    at pnt.s (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:8561)
    at pnt.n (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:8425)
    at pnt.m (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:8215)
    at pnt.listFiles (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:7358)
    at Lj.listPromptFiles (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:11031)
    at Lj.findInstructionFilesFor (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:12430)
error @ workbench.desktop.main.js:sourcemap:35
workbench.desktop.main.js:sourcemap:35  INFO Settings Sync: Token updated for the account MelakuDemeke
workbench.desktop.main.js:sourcemap:35  INFO Settings Sync: Account status changed from uninitialized to available
workbench.desktop.main.js:sourcemap:35  INFO [perf] Render performance baseline is 19ms
workbench.desktop.main.js:sourcemap:35   ERR [UriError]: If a URI contains an authority component, then the path component must either be empty or begin with a slash (""/"") character: Error: [UriError]: If a URI contains an authority component, then the path component must either be empty or begin with a slash (""/"") character
    at mBi (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:30:57171)
    at new iMe (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:30:58309)
    at new GW (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:30:60044)
    at iMe.from (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:30:59141)
    at pnt.s (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:8561)
    at pnt.n (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:8425)
    at pnt.m (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:8215)
    at pnt.listFiles (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:7358)
    at Lj.listPromptFiles (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:11031)
    at Lj.findInstructionFilesFor (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:12430)
error @ workbench.desktop.main.js:sourcemap:35
workbench.desktop.main.js:sourcemap:35   ERR [Extension Host] (node:8534) [DEP0040] DeprecationWarning: The `punycode` module is deprecated. Please use a userland alternative instead.
(Use `node --trace-deprecation ...` to show where the warning was created)
error @ workbench.desktop.main.js:sourcemap:35
workbench.desktop.main.js:sourcemap:1281 [Extension Host] (node:8534) [DEP0040] DeprecationWarning: The `punycode` module is deprecated. Please use a userland alternative instead.
(Use `node --trace-deprecation ...` to show where the warning was created)
J1s @ workbench.desktop.main.js:sourcemap:1281
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Gemini CodeAssist version 2.39.0
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Client experimentation flags:  Object
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Failed to fetch experiment data, language client not set.
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Skipping updating LS experimentation flags since LS is not ready.
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Activating plugin Gemini Code Assist
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Activating plugin SDLC Agents
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Skipping activation of SDLC Agents, it is not enabled
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Activating plugin Code Documentation
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Activating plugin Local agent
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Activating plugin MCP Server
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Done activating plugin SDLC Agents, took 1.0088189999996757ms
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Done activating plugin Code Documentation, took 1.1411510000007183ms
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Done activating plugin Local agent, took 1.0612649999993664ms
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Done activating plugin MCP Server, took 1.077462999999625ms
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Get to https://cloudcode-pa.googleapis.com/v1internal:loadCodeAssist response: {""currentTier"":{""id"":""free-tier"",""name"":""Gemini Code Assist for individuals"",""description"":""Gemini-powered code suggestions and chat in multiple IDEs"",""privacyNotice"":{""showNotice"":true,""noticeText"":""This notice and our Privacy Policy - https://policies.google.com/privacy - describe how Gemini Code Assist for individuals handles your data. Please read them carefully.\n\nWhen you use Gemini Code Assist for individuals, Google collects your prompts, related code, generated output, code edits, related feature usage information, and your feedback to provide, improve, and develop Google products and services and machine learning technologies.\n\nTo help with quality and improve our products (such as generative machine-learning models), human reviewers may read, annotate, and process the data collected above. We take steps to protect your privacy as part of this process. This includes disconnecting the data from your Google Account before reviewers see or annotate it, and storing those disconnected copies for up to 18 months. Please don't submit confidential information or any data you wouldn't want a reviewer to see or Google to use to improve our products, services, and machine-learning technologies.\n\nIf you don't want this data used to improve Google's machine learning models, you can opt out below.""}},""allowedTiers"":[{""id"":""free-tier"",""name"":""Gemini Code Assist for individuals"",""description"":""Gemini-powered code suggestions and chat in multiple IDEs"",""privacyNotice"":{""showNotice"":true,""noticeText"":""This notice and our Privacy Policy - https://policies.google.com/privacy - describe how Gemini Code Assist for individuals handles your data. Please read them carefully.\n\nWhen you use Gemini Code Assist for individuals, Google collects your prompts, related code, generated output, code edits, related feature usage information, and your feedback to provide, improve, and develop Google products and services and machine learning technologies.\n\nTo help with quality and improve our products (such as generative machine-learning models), human reviewers may read, annotate, and process the data collected above. We take steps to protect your privacy as part of this process. This includes disconnecting the data from your Google Account before reviewers see or annotate it, and storing those disconnected copies for up to 18 months. Please don't submit confidential information or any data you wouldn't want a reviewer to see or Google to use to improve our products, services, and machine-learning technologies.\n\nIf you don't want this data used to improve Google's machine learning models, you can opt out below.""},""isDefault"":true},{""id"":""standard-tier"",""name"":""Gemini Code Assist"",""description"":""Unlimited coding assistant with the most powerful Gemini models"",""userDefinedCloudaicompanionProject"":true,""privacyNotice"":{}}],""cloudaicompanionProject"":""yarling-list-770k4"",""gcpManaged"":false}, headers: {""x-cloudaicompanion-trace-id"":[""7d6c2990978efd14""],""content-type"":[""application/json; charset=UTF-8""],""vary"":[""Origin, X-Origin, Referer""],""content-encoding"":[""gzip""],""date"":[""Tue, 08 Jul 2025 11:56:50 GMT""],""server"":[""ESF""],""x-xss-protection"":[""0""],""x-frame-options"":[""SAMEORIGIN""],""x-content-type-options"":[""nosniff""],""server-timing"":[""gfet5t7;dur=196, gfet4t7; dur=304""],""alt-svc"":[""h3=\"":443\""; ma=2592000,h3-29=\"":443\""; ma=2592000""],""connection"":[""close""],""transfer-encoding"":[""chunked""]}
workbench.desktop.main.js:sourcemap:1281 [Extension Host] LoadCodeAssistResponse: {""currentTier"":{""id"":""free-tier"",""name"":""Gemini Code Assist for individuals"",""description"":""Gemini-powered code suggestions and chat in multiple IDEs"",""privacyNotice"":{""showNotice"":true,""noticeText"":""This notice and our Privacy Policy - https://policies.google.com/privacy - describe how Gemini Code Assist for individuals handles your data. Please read them carefully.\n\nWhen you use Gemini Code Assist for individuals, Google collects your prompts, related code, generated output, code edits, related feature usage information, and your feedback to provide, improve, and develop Google products and services and machine learning technologies.\n\nTo help with quality and improve our products (such as generative machine-learning models), human reviewers may read, annotate, and process the data collected above. We take steps to protect your privacy as part of this process. This includes disconnecting the data from your Google Account before reviewers see or annotate it, and storing those disconnected copies for up to 18 months. Please don't submit confidential information or any data you wouldn't want a reviewer to see or Google to use to improve our products, services, and machine-learning technologies.\n\nIf you don't want this data used to improve Google's machine learning models, you can opt out below.""}},""allowedTiers"":[{""id"":""free-tier"",""name"":""Gemini Code Assist for individuals"",""description"":""Gemini-powered code suggestions and chat in multiple IDEs"",""privacyNotice"":{""showNotice"":true,""noticeText"":""This notice and our Privacy Policy - https://policies.google.com/privacy - describe how Gemini Code Assist for individuals handles your data. Please read them carefully.\n\nWhen you use Gemini Code Assist for individuals, Google collects your prompts, related code, generated output, code edits, related feature usage information, and your feedback to provide, improve, and develop Google products and services and machine learning technologies.\n\nTo help with quality and improve our products (such as generative machine-learning models), human reviewers may read, annotate, and process the data collected above. We take steps to protect your privacy as part of this process. This includes disconnecting the data from your Google Account before reviewers see or annotate it, and storing those disconnected copies for up to 18 months. Please don't submit confidential information or any data you wouldn't want a reviewer to see or Google to use to improve our products, services, and machine-learning technologies.\n\nIf you don't want this data used to improve Google's machine learning models, you can opt out below.""},""isDefault"":true},{""id"":""standard-tier"",""name"":""Gemini Code Assist"",""description"":""Unlimited coding assistant with the most powerful Gemini models"",""userDefinedCloudaicompanionProject"":true,""privacyNotice"":{}}],""cloudaicompanionProject"":""yarling-list-770k4"",""gcpManaged"":false}
workbench.desktop.main.js:sourcemap:1281 [Extension Host] service/healthcheck Object Object
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Handling content exclusion file changes and firing event...
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Reading AI content exclusion file: .aiexclude
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Checking language server extraction from /home/melaku/.vscode-server/extensions/google.geminicodeassist-2.39.0/cloudcode_cli.zip - linux_amd64/cloudcode_cli
workbench.desktop.main.js:sourcemap:1281 [Extension Host] AI content exclusion file setting is set, but the file is not found: .aiexclude
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Reading AI content exclusion file: .gitignore
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Done reading AI content exclusion file.
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Finding files in workspace
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Content exclusion file changes handled and event fired.
workbench.desktop.main.js:sourcemap:1281 [Extension Host] AIExcludeProvider initialized.
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Using existing /home/melaku/.cache/cloud-code/cloudcode_cli/cloudcode_cli/ac45bacd/cloudcode_cli)
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Starting AIPP Language Client
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Language server state changed from Stopped to Starting
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Language server state changed from Starting to Running
workbench.desktop.main.js:sourcemap:1281 [Extension Host] AIPP Language Client Started
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Created the Gemini Code Assist completion provider
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Language client state is Running
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Client experimentation flags:  Object
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Client experimentation flags:  Object
workbench.desktop.main.js:sourcemap:1281 [Extension Host] service/healthcheck Object Object
workbench.desktop.main.js:sourcemap:1281 [Extension Host] Done activating plugin Gemini Code Assist, took 12987.099302ms
workbench.desktop.main.js:sourcemap:35   ERR [UriError]: If a URI contains an authority component, then the path component must either be empty or begin with a slash (""/"") character: Error: [UriError]: If a URI contains an authority component, then the path component must either be empty or begin with a slash (""/"") character
    at mBi (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:30:57171)
    at new iMe (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:30:58309)
    at new GW (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:30:60044)
    at iMe.from (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:30:59141)
    at pnt.s (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:8561)
    at pnt.n (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:8425)
    at pnt.m (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:8215)
    at pnt.listFiles (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:7358)
    at Lj.listPromptFiles (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:11031)
    at Lj.findInstructionFilesFor (vscode-file://vscode-app/c:/Users/HP/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2028:12430)
error @ workbench.desktop.main.js:sourcemap:35
error @ workbench.desktop.main.js:sourcemap:35
error @ workbench.desktop.main.js:sourcemap:3311
mc @ workbench.desktop.main.js:sourcemap:3308
(anonymous) @ workbench.desktop.main.js:sourcemap:3308
onUnexpectedError @ workbench.desktop.main.js:sourcemap:7
wt @ workbench.desktop.main.js:sourcemap:7
(anonymous) @ workbench.desktop.main.js:sourcemap:3308


Extension version: 1.341.0
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:
Remote OS version: Linux x64 6.6.87.2-microsoft-standard-WSL2

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i7-1165G7 @ 2.80GHz (8 x 2803)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.77GB (3.00GB free)|
|Process Argv|--folder-uri=vscode-remote://wsl+Ubuntu-24.04/home/melaku/job/telebirrserver/back --remote=wsl+Ubuntu-24.04 --crash-reporter-id 40cf53aa-cc77-4791-8103-02e5dbd0ee63|
|Screen Reader|no|
|VM|0%|

|Item|Value|
|---|---|
|Remote|WSL: Ubuntu-24.04|
|OS|Linux x64 6.6.87.2-microsoft-standard-WSL2|
|CPUs|11th Gen Intel(R) Core(TM) i7-1165G7 @ 2.80GHz (8 x 0)|
|Memory (System)|7.64GB (5.35GB free)|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551cf:31249601
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
d2249276:31341129
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Incorrect assessment of usage credits. ,"
Type: <b>Bug</b>

I have only used 22.5% of my monthly credits. Incorrect.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 5 Microsoft Surface (R) Edition (12 x 2196)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.45GB (5.67GB free)|
|Process Argv|--crash-reporter-id 9295022b-912d-48ed-b6a0-bd30bf32b21a|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
j6a0c355:31339598
jhi8h917:31341130
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
testaa123cf:31335227
onetestforazureexpcf:31335614
63221493:31336333
yijiwantestdri0626-c:31336931
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
Sürekli Hata Alıyorum,"
Type: <b>Bug</b>

Merhaba sürekli hata alıyorum. aldığım hatalar:
-Ne yazık ki yanıt döndürülmedi.
-Model beklenmedik bir şekilde yanıt döndürmedi ve bu durum bir hizmet sorununun işareti olabilir. Lütfen hata bildirin.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:
Remote OS version: Linux x64 5.15.0-92-generic

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-3470 CPU @ 3.20GHz (4 x 3193)|
|GPU Status|2d_canvas: unavailable_software<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: disabled_software<br>multiple_raster_threads: enabled_on<br>opengl: disabled_off<br>rasterization: disabled_software<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: disabled_software<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: unavailable_software<br>webgl2: unavailable_software<br>webgpu: unavailable_software<br>webnn: unavailable_software|
|Load (avg)|undefined|
|Memory (System)|15.96GB (4.41GB free)|
|Process Argv|--crash-reporter-id 44354f4a-a7a5-4183-a126-6a77b4bef854|
|Screen Reader|no|
|VM|0%|

|Item|Value|
|---|---|
|Remote|SSH: server1.nielsenchase.com|
|OS|Linux x64 5.15.0-92-generic|
|CPUs|Intel(R) Xeon(R) Gold 6254 CPU @ 3.10GHz (2 x 0)|
|Memory (System)|3.79GB (0.70GB free)|
|VM|100%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551cf:31249601
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
jjjhb125:31275178
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
9d2cg352:31339597
d2249276:31341129
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
82j33506:31327384
nes-diff-11:31337487
testaa123cf:31335227
6abeh943:31336334
yijiwantestdri0626-t:31336930
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
Broken AI Response,"
Type: <b>Bug</b>

Now let me add similar debugging for the load section:

Read AjaxImportsController.php, lines 355 to 365

<legalact> <ction> <ption> <ription> <ption> <ption> <ption> <ption> <ption> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option> <option>

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.22631
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 3 3200U with Radeon Vega Mobile Gfx   (4 x 2595)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|5.88GB (0.72GB free)|
|Process Argv|--crash-reporter-id a4509555-379f-4702-8a75-72fb776897c3|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492:30256859
pythonvspyt551:31249599
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
j6a0c355:31339598
0g0a1943:31341127
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
testaa123cf:31335227
6abeh943:31336334
yijiwantestdri0626-c:31336931
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
screenshot voice over and braille board,"this is the screenshot on bug on vscode with voice over, braille board and vscode, when i want to read  a line i put my cursor on a line and i see just a part and when i move the cursor characcte by charactere with left or right arrow i don't see the cursor on my braille bord.",0
I cannot ask an issue to copliot paid,Worker terminated due to reaching memory limit: JS heap out of memory (at tsx element ff) - im facing an issue while starting a new issue,1
Use `v`-mode (`unicodeSets`) instead of the strictly-worse `u`-mode (`unicode`) for regex search,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

Feature request: Use [`v`-mode (`unicodeSets`)](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/RegExp/unicodeSets) instead of the strictly-worse (less powerful) [`u`-mode (`unicode`)](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/RegExp/unicode) for regex search.

For example, in a file containing the text ""hello š𐓇"", regex searching for `\p{scx=Latn}[\p{L}--\p{scx=Latn}]` should match `š𐓇` ([playground](https://regex101.com/?regex=%5Cp%7Bscx%3DLatn%7D%5B%5Cp%7BL%7D--%5Cp%7Bscx%3DLatn%7D%5D&flags=gv&flavor=javascript&testString=hello+%C5%A1%F0%90%93%87)).

Current behavior is that the regex search fails with ""Invalid regular expression: /\p{scx=Latn}[\p{L}--\p{scx=Latn}]/gu: Invalid character class"".",0
Shady practices?,"
Type: <b>Bug</b>

I have been testing your models and vscode capabilities in insiders heavily since day 1, and during insiders only it was acceptable to try various things, rate limit etc.
This is now live with actual quota system in place with quite hefty prices attached to it. Unsure if it is your hosting partners that are redirecting us and completely throttling the performance of the models, or if this is actually Microsoft doing it trough safeguards for peak loads. What ever it is, it is completely shady business practice and UNACCEPTABLE, we need tansparency and optionality!
If you are not providing the actual model at full capacity with full context, but instead retargeting quantized models and altering context sizes (hell even changing system prompts) you need to escalate and rethink your strategy.

Which ever case it may be, it is causing complete quota drain on your customers that have to re-prompt, stop the agent because its performance is so bad it is actually destroying work.
This seems to be happening to multiple models with different vendors now so it is pointing more and more towards this being a deliberate functionality by Microsoft.

The code quality is poor, the agents keeps looping, they have complete amnesia and they cannot follow simple directions. Running the same test through out the day shows a direct correlation with high load times and this catastrophic handling of your paying customers requests being re-targeted. It is like the IQ level drops by a 100 points on the agents, and we are talking some real ""high school level I'm learning about if statements right now"" kind of crap code. It bloats everything it sees and it touches rapes code in other projects in the same workspace, even though it is not working on them and have explicit instructions not to touch anything else.

Non  of these negative things happen when the load is not high... I have wasted 50%+ quota now on my private pro+ subscription in 8 days and the agent have yet to produce a single usable file. Right before quota reset it seems the load was lower and you could actually classify it as a junior developer if prompted correctly.

I want this to work, that is why I have put months of testing into it and keep reporting findings, but frankly as you have started ruining the quota and doing things in this way, I'm starting to rethink if I actually want copilot to become the success or if I should try to help someone else instead.

Yes this is a rant, but this is frustrating the hell out of me and many more as we cannot rely on the capability of the models, it is not repeatable! If this is the shady things I now believe is taking place, you need to be transparent about it and provide us with options to completely block the redirects/throttling. I much rather keep my quota and get expected performance when capacity is available than be surprised by a wall of bloated high school level code being injected into my code base.


Extension version: 0.29.2025070703
VS Code version: Code - Insiders 1.102.0-insider (96f1890d08080f46f3b0a9424553422f04133090, 2025-07-04T15:15:02.963Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 5 5600X 6-Core Processor              (12 x 3700)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|31.93GB (2.99GB free)|
|Process Argv|--crash-reporter-id 6184e59b-eb56-4178-9e91-ec311e8e4aee|
|Screen Reader|no|
|VM|0%|
</details>
<!-- generated by issue reporter -->",2
Implement an Ollama minimum version check,"My Ollama models don't show up:

<img width=""374"" height=""552"" alt=""Image"" src=""https://github.com/user-attachments/assets/a462c066-40dc-4f34-b3d5-78ca2f3cc20f"" />

This is what shows up when I select Ollama:

<img width=""651"" height=""196"" alt=""Image"" src=""https://github.com/user-attachments/assets/0ae4f99d-c44a-4b76-8842-ff5aa43c4051"" />

Adding/removing models from this list does nothing but show an error:

<img width=""489"" height=""152"" alt=""Image"" src=""https://github.com/user-attachments/assets/595b1b19-eff1-443b-b952-9fa2273bcfe1"" />

@lramos15 Marking it as `candidate` since it feels like a regression.",1
When will the offline version of Copilot be supported? Local large models cannot be used in an offline environment.,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
When will the offline version of Copilot be supported? Local large models cannot be used in an offline environment.",0
On github.dev the export changes is making wrong diff header,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: insiders.vscode.dev version
- OS Version: web.

Steps to Reproduce:

1. create changes in repo in online vscode.
2. go to git section -> menu (3 dots) -> import / export -> export changes.
3. see the header of the diff is wrong. 

for example:
you get:
```diff
diff --git a/bootstrap-vcpkg.bat b/bootstrap-vcpkg.bat
--- bootstrap-vcpkg.bat
+++ bootstrap-vcpkg.bat
@@ -1,2 +1,6 @@
 @echo off
 powershell.exe -NoProfile -ExecutionPolicy Bypass ""& {& \""%~dp0scripts\bootstrap.ps1\"" %*}""
+
+add talchanges
+talchanges
+
```

it shoud add a/ and b/ in the filenames: 
--- bootstrap-vcpkg.bat -> --- a/bootstrap-vcpkg.bat
+++ bootstrap-vcpkg.bat -> +++ b/bootstrap-vcpkg.bat

I think it easy fix. I also verify this bug on https://insiders.vscode.dev.

Thank you.

<img width=""974"" height=""828"" alt=""Image"" src=""https://github.com/user-attachments/assets/0dc3873a-9afa-450f-bf8f-72fe33ac1531"" />

<img width=""1358"" height=""844"" alt=""Image"" src=""https://github.com/user-attachments/assets/21d8d8cb-5260-4ab4-8d05-605204c162f1"" />
",1
Show Externally discovered MCP servers in UI,"Show Externally discovered MCP servers in UI

CC @connor4312 ",0
Support Windows package identity to ensure device permissions,"In order to retain microphone permissions necessary for VS Code Speech, we should enforce the use of package identity on Windows installations.

cc @bpasero ",0
输入截图，无法识别图片中的功能,"
Type: <b>Bug</b>

输入截图，无法识别图片中的功能

Extension version: 0.28.5
VS Code version: Code 1.101.1 (Universal) (18e3a1ec544e6907be1e944a94c496e302073435, 2025-06-18T13:35:12.605Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M1 Pro (8 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|39, 26, 19|
|Memory (System)|16.00GB (0.10GB free)|
|Process Argv|--crash-reporter-id 4d40cef1-79ce-4a4c-b3b1-b212d2d4628e|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492:30256859
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
jhi8h917:31341130
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
b6b4d950:31327385
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
GitHub Copilot Chat Agent Cannot Execute VS Code Tasks or Terminal Commands,"
Type: <b>Bug</b>

- **Mode**: Agent
- **Model**: Claude Sonnet 4

## Summary
The GitHub Copilot Chat extension's agent mode is unable to execute VS Code tasks using `run_vs_code_task` or terminal commands using `run_in_terminal` when set to background mode. This appears to be a recent regression in VS Code Insiders. This functionality was still working a few daily VS Code Insiders builds ago.

## Description
When the GitHub Copilot Chat agent attempts to execute VS Code tasks or terminal commands, the following errors occur:

### 1. Task Execution Errors
When using `run_vs_code_task`, tasks start but fail with parameter format errors:
```
Parameter format not correct - -Command
```

### 2. Terminal Command Execution Errors  
When using `run_in_terminal` with `isBackground: true`, the following error occurs:
```
ERROR while calling tool: Cannot read properties of undefined (reading 'executeCommand')
```

### 3. Background Process Issues
Background processes fail to start properly, preventing automated testing and build workflows.

## Steps to Reproduce
1. Open VS Code Insiders (version 1.102.0-insider)
2. Use GitHub Copilot Chat in agent mode
3. Attempt to execute a VS Code task using the agent's `run_vs_code_task` tool
4. Observe the ""Parameter format not correct - -Command"" error
5. Attempt to execute a terminal command with `isBackground: true`
6. Observe the ""Cannot read properties of undefined"" error

## Expected Behavior
- VS Code tasks should execute successfully when invoked by the Copilot Chat agent
- Terminal commands should execute in background mode without errors
- The agent should be able to automate build, test, and development workflows

## Actual Behavior
- Tasks fail with PowerShell 7.5+ parameter format errors
- Background terminal commands fail with undefined property errors
- Agent automation workflows are broken

## Workaround
Manual execution of tasks by the user still works correctly, but this defeats the purpose of agent automation.

## Perhaps Related Issues
https://github.com/microsoft/vscode-copilot-release/issues/13622

---

Extension version: 0.29.2025070703
VS Code version: Code - Insiders 1.102.0-insider (7631de29bf9a2e31020ab93d3d1611965dc5b62e, 2025-07-07T22:41:41.902Z)
OS version: Windows_NT x64 10.0.26100

Modes:

<details><summary>Logs</summary>
<pre>
Trace: CodeSearchWorkspaceDiff: refreshing diff for file:///c%3A/Users/ai-acc/source/repos/AIAccGameEngine.12/external/SDL_gpu_examples.
Trace: CodeSearchWorkspaceDiff: refreshing diff for file:///c%3A/Users/ai-acc/source/repos/AIAccGameEngine.12/external/assimp.
Trace: CodeSearchWorkspaceDiff: Refreshed diff for file:///c%3A/Users/ai-acc/source/repos/AIAccGameEngine.12/external/SDL_gpu_examples. New diff count: 0
Trace: CodeSearchWorkspaceDiff: Refreshed diff for file:///c%3A/Users/ai-acc/source/repos/AIAccGameEngine.12/external/assimp. New diff count: 1
Trace: CodeSearchWorkspaceDiff: Refreshed all diffs. New local diffs count: 0
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Trace: No test results
Trace: [NES][Triggerer][onDidChangeTextEditorSelection] created
Trace: [NES][Triggerer][onDidChangeTextEditorSelection] Return: document not tracked - does not have recent changes
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: false.
Trace: CodeSearchWorkspaceDiff: refreshing diff for file:///c%3A/Users/ai-acc/source/repos/AIAccGameEngine.12/external/SDL_gpu_examples.
Trace: CodeSearchWorkspaceDiff: refreshing diff for file:///c%3A/Users/ai-acc/source/repos/AIAccGameEngine.12/external/assimp.
Trace: CodeSearchWorkspaceDiff: Refreshed diff for file:///c%3A/Users/ai-acc/source/repos/AIAccGameEngine.12/external/SDL_gpu_examples. New diff count: 0
Trace: CodeSearchWorkspaceDiff: Refreshed diff for file:///c%3A/Users/ai-acc/source/repos/AIAccGameEngine.12/external/assimp. New diff count: 1
Trace: CodeSearchWorkspaceDiff: Refreshed all diffs. New local diffs count: 0
Trace: GithubCodeSearchService.getRemoteIndexState(thespydog/sdl_gpu_examples). Found indexed commit: d9924b2ccb89a625209ed0ff93e6dfcbbabb830b.
Trace: GithubCodeSearchService.getRemoteIndexState(assimp/assimp). Found indexed commit: db0bde758e5043546cb2f45126c5693d2d9550f3.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Trace: [NES][Triggerer][onDidChangeTextEditorSelection] created
Trace: [NES][Triggerer][onDidChangeTextEditorSelection] Return: not empty selection
Trace: No test results
Trace: No test results
Trace: No test results
Trace: [NES][Triggerer][onDidChangeTextEditorSelection] created
Trace: [NES][Triggerer][onDidChangeTextEditorSelection] Return: document not tracked - does not have recent changes
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Trace: CodeSearchWorkspaceDiff: refreshing diff for file:///c%3A/Users/ai-acc/source/repos/AIAccGameEngine.12/external/SDL_gpu_examples.
Trace: CodeSearchWorkspaceDiff: refreshing diff for file:///c%3A/Users/ai-acc/source/repos/AIAccGameEngine.12/external/assimp.
Trace: CodeSearchWorkspaceDiff: Refreshed diff for file:///c%3A/Users/ai-acc/source/repos/AIAccGameEngine.12/external/SDL_gpu_examples. New diff count: 0
Trace: CodeSearchWorkspaceDiff: Refreshed diff for file:///c%3A/Users/ai-acc/source/repos/AIAccGameEngine.12/external/assimp. New diff count: 1
Trace: CodeSearchWorkspaceDiff: Refreshed all diffs. New local diffs count: 0
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: false.
Trace: CodeSearchWorkspaceDiff: refreshing diff for file:///c%3A/Users/ai-acc/source/repos/AIAccGameEngine.12/external/SDL_gpu_examples.
Trace: CodeSearchWorkspaceDiff: refreshing diff for file:///c%3A/Users/ai-acc/source/repos/AIAccGameEngine.12/external/assimp.
Trace: CodeSearchWorkspaceDiff: Refreshed diff for file:///c%3A/Users/ai-acc/source/repos/AIAccGameEngine.12/external/SDL_gpu_examples. New diff count: 0
Trace: CodeSearchWorkspaceDiff: Refreshed diff for file:///c%3A/Users/ai-acc/source/repos/AIAccGameEngine.12/external/assimp. New diff count: 1
Trace: CodeSearchWorkspaceDiff: Refreshed all diffs. New local diffs count: 0
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
</pre>
</details>
<details><summary>Request IDs</summary>
<pre>
7be2f3a5-ece8-4ae6-a72a-91c630a0de53
</pre>
</details><details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) Ultra 9 285K (24 x 3686)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|95.27GB (65.80GB free)|
|Process Argv|--crash-reporter-id 1ae4b98f-0c1a-4047-9e9b-45b755d2366c|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551cf:31249598
c4g48928:30535728
962ge761:30841072
2e7ec940:31000449
cppperfnew:30980852
dwnewjupytercf:31046870
nativeloc1:31118317
e80f6927:31120813
dwcopilot:31158714
c3hdf307:31184662
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
g012b348:31231168
pythoneinst12:31251391
c7cif404:31309980
pythonpulldiag:31287486
6gi0g917:31259950
996jf627:31264550
pythonrdcb7:31268811
usemplatestapi:31297334
jdghv92:31317040
747dc170:31275146
6518g693:31302842
generatesymbolt:31280541
convertfstringf:31280702
j97ad248:31306657
usemarketplace:31343026
0g1h6703:31329154
4f60g487:31327383
nes-emitfast-1:31333560
replacestringexc:31340153
6abeh943:31336334
envsactivate1:31343186
nes-conv-11:31337514
0927b901:31340060
gji67723:31340537
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
Window wont close,"
Type: <b>Bug</b>

The window of inline copilot wont close after completion

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.22631
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|13th Gen Intel(R) Core(TM) i7-1365U (12 x 2688)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|31.66GB (15.20GB free)|
|Process Argv|--crash-reporter-id df5621b7-5f38-4b41-ae6d-eb059f76bfef|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
d2249276:31341129
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Sample TypeScript context provider telemetry,"Currently the TypeScript context provider telemetry is send for every context request. The plan is to roll this out to a large population hence we need to sample the telemetry.

The plan is that GitHub sets a corresponding flag how telemetry should be sampled.

",0
Show a larger chat input in welcome mode,"<img width=""496"" height=""441"" alt=""Image"" src=""https://github.com/user-attachments/assets/8c6873b5-54f0-4c18-a932-d8ca3a2a2637"" />

I feel like we could add 1 or 2 lines to the height of the chat input mode when in welcome mode.

//cc @isidorn ",0
`#new` tool pill is not adding much,"<img width=""371"" height=""138"" alt=""Image"" src=""https://github.com/user-attachments/assets/e8044625-3fc5-4813-9c84-0f5b3e0a60df"" />

I find this indicator about context not very helpful because nothing happens when I click it.

//cc @isidorn ",0
Chat in welcome experience should have reduced buttons complexity,"When chat shows OOTB with welcome UX, I would greatly reduce the complexity of buttons in the chat input part.

Some things to note:

Just 1 button to send instead of a dropdown:
<img width=""892"" height=""248"" alt=""Image"" src=""https://github.com/user-attachments/assets/df8f62b3-b4f0-4ca4-a377-1296efeaff89"" />

Remove the `@` and 🎤 

<img width=""71"" height=""63"" alt=""Image"" src=""https://github.com/user-attachments/assets/ae312a1b-9ebf-452f-8999-f5d189bbf709"" />

//cc @isidorn ",0
Unable to discard changes that should not be there at first place,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 
- OS Version: 

Steps to Reproduce:

1.  clone a large complex project with submodules
2. Open VSC
3. You will see many change not done by your user
4.  you reset hard HEAD in cli 
5. VSC code still see those change file and if you discard the changes after few seconds they reappear making impossible to remove them.

https://github.com/user-attachments/assets/3a265b48-94d0-4dd0-b294-055b180fe770",1
Tiles are not keyboard accessible,"I seem unable to tab into the tiles:

<img width=""279"" height=""422"" alt=""Image"" src=""https://github.com/user-attachments/assets/407e1b6b-3f0f-4d1e-9914-27d04118efee"" />",0
"Giving an error while in half of the prompt generation, and incorrect prompt generated","
Type: <b>Bug</b>

switch to claude sonnet 4 model then, 
1.given a full prompt in a proper order then it started the prompt generation but when generation in half way it got dissappeared and showing a new chat
2.given a full prompt in a proper order then it started the prompt generation, but giving the error as (Sorry, the upstream model provider is currently experiencing high demand. Please try again later or consider switching models.) in mid generation, which is increasing the number of prompts.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i7-1165G7 @ 2.80GHz (8 x 2803)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.73GB (4.82GB free)|
|Process Argv|--crash-reporter-id 4e7de9d3-7cc4-4948-81fa-a46f4dfd7e76|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
jhi8h917:31341130
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
onetestforazureexpcf:31335614
6abeh943:31336334
yijiwantestdri0626-c:31336931
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Dynamic MCP Server Registration Not Available Mid-Chain or Same Cycle,"I have the problem, I'm trying to configure a dynamic MCP server with dynamic tools. Dynamic tool registration works on the server and is reflected in the Copilot tools UI, but the tool is not discoverable or invokable during the same message cycle or in the middle of a chain. It only becomes available after the current chain finishes execution.
what can be possible fix for this ?",0
经常新开窗口打开文件后卡死,"
Type: <b>Bug</b>

卡死期间输入的代码无法切换中英文，也无法生成更多的新窗口

VS Code version: Code 1.70.2 (e4503b30fc78200f846c62cf8091b76ff5547662, 2022-08-16T05:35:13.448Z)
OS version: Windows_NT x64 10.0.17763
Modes: Unsupported

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-10400F CPU @ 2.90GHz (12 x 2904)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_renderer: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled|
|Load (avg)|undefined|
|Memory (System)|15.91GB (3.43GB free)|
|Process Argv|--crash-reporter-id dc39f58e-54a9-48f7-ae36-e3f497a3c1f9|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (34)</summary>

Extension|Author (truncated)|Version
---|---|---
laravel-extra-intellisense|ami|0.6.3
blamer-vs|bea|0.7.3
vscode-intelephense-client|bme|1.9.5
thief-book|C-T|0.0.9
dbclient-jdbc|cwe|1.4.6
vscode-mysql-client2|cwe|8.3.6
gitlens|eam|12.2.2
php-intellisense|fel|2.3.14
auto-rename-tag|for|0.1.10
code-runner|for|0.12.2
vscode-weixin-read|goo|0.1.17
svn-scm|joh|2.17.0
php-cs-fixer|jun|0.3.21
vue-beautify2|ker|0.0.3
vscode-phpfmt|kok|1.2.31
vscode-leetcode|Lee|0.18.4
php-namespace-resolver|Meh|1.1.9
vscode-language-pack-zh-hans|MS-|1.70.8170916
python|ms-|2022.16.1
vscode-pylance|ms-|2023.1.10
jupyter|ms-|2022.7.1102252217
jupyter-keymap|ms-|1.1.0
jupyter-renderers|ms-|1.0.9
debugger-for-chrome|msj|4.13.0
php-docblocker|nei|2.7.0
vscode-zhihu|niu|0.5.1
vetur|oct|0.37.3
background|sha|1.2.13
code-settings-sync|Sha|3.4.3
vue-helper|she|2.4.7
vscode-zipexplorer|sle|0.3.1
vscode-icons|vsc|12.6.0
vue|Wsc|1.0.26
markdown-all-in-one|yzh|3.5.1

(5 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
c4b42873:31341128
d7aab740:31338110
usemarketplace:31343026
nes-diff-11:31337487
testaa123cf:31335227
6abeh943:31336334
yijiwantestdri0626-t:31336930
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
Prettier extension terminated while using yarn pnp,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.101.2
- OS Version: macOS Sequoia 15.5(24F74)

Steps to Reproduce:

1. my project is yarn pnp and using prettier, prettier-plugin-tailwindcss, @trivago-perttier-plugin-sort-imports.
2. all using .yarn sdks and unplugged, 
3. yarnrc is nodeLinker: pnp
4. extension host terminated
",0
send with #codebase greyed out,"
Type: <b>Bug</b>

<img width=""245"" height=""110"" alt=""Image"" src=""https://github.com/user-attachments/assets/dc55511c-742f-4b75-ae05-863edb4b29e4"" />
<img width=""254"" height=""99"" alt=""Image"" src=""https://github.com/user-attachments/assets/76994a1d-9c18-4925-9474-72710fbbae64"" />

happens in any mode (agent, ask, edit), in any model (gpt 4.1, gemini 2.5, and more). it suddenly got greyed out

Extension version: 0.29.2025070703
VS Code version: Code - Insiders 1.102.0-insider (Universal) (7631de29bf9a2e31020ab93d3d1611965dc5b62e, 2025-07-07T22:41:41.902Z)
OS version: Darwin arm64 24.4.0
Modes:

<details><summary>Logs</summary>
<pre>
Trace: CodeSearchWorkspaceDiff: refreshing diff for file:///Users/nemo/repo/quotabook-frontend.
Trace: CodeSearchWorkspaceDiff: Refreshed diff for file:///Users/nemo/repo/quotabook-frontend. New diff count: 0
Trace: CodeSearchWorkspaceDiff: Refreshed all diffs. New local diffs count: 0
Trace: CodeSearchWorkspaceDiff: refreshing diff for file:///Users/nemo/repo/quotabook-frontend.
Trace: CodeSearchWorkspaceDiff: Refreshed diff for file:///Users/nemo/repo/quotabook-frontend. New diff count: 0
Trace: CodeSearchWorkspaceDiff: Refreshed all diffs. New local diffs count: 0
Debug: Getting CopilotToken (force: undefined)...
Info: Logged in as scarf005
Info: Got Copilot token for scarf005
Debug: Handling CopilotToken refresh.
Debug: Got CopilotToken (force: undefined).
Debug: [LanguageModelAccess] UPDATING language models
Debug: [LanguageModelAccess] DID UPDATE language models
Debug: CopilotToken state changed, firing event.
Info: copilot token chat_enabled: true, sku: copilot_for_business_seat_quota
Debug: ConversationFeature: onDidAuthenticationChange has token: true
Debug: [context keys] Updating context keys.
Trace: CodeSearchRepoTracker.updateRepoStateFromEndpoint(file:///Users/nemo/repo/quotabook-frontend). Checking status from endpoint.
Debug: Finished handling auth change event.
Debug: [LanguageModelAccess] UPDATING language models
Debug: [LanguageModelAccess] DID UPDATE language models
Trace: GithubCodeSearchService.getRemoteIndexState(quotabook/quotabook-frontend). Found indexed commit: 9ddf581dd0c0f43ed7f67ef022f5e1958aaadfc6.
Trace: CodeSearchRepoTracker.updateRepoStateFromEndpoint(file:///Users/nemo/repo/quotabook-frontend). Updating state to Ready.
Trace: CodeSearchWorkspaceDiff: refreshing diff for file:///Users/nemo/repo/quotabook-frontend.
Trace: CodeSearchWorkspaceDiff: Refreshed diff for file:///Users/nemo/repo/quotabook-frontend. New diff count: 0
Trace: m0.initialize#10 started
Trace: m0.initialize#10 success. Elapsed 0.4022920001298189
Trace: GithubCodeSearchService.getRemoteIndexState(quotabook/quotabook-frontend). Found indexed commit: 9ddf581dd0c0f43ed7f67ef022f5e1958aaadfc6.
Trace: CodeSearchWorkspaceDiff: refreshing diff for file:///Users/nemo/repo/quotabook-frontend.
Trace: CodeSearchWorkspaceDiff: Refreshed diff for file:///Users/nemo/repo/quotabook-frontend. New diff count: 0
Trace: CodeSearchWorkspaceDiff: Refreshed all diffs. New local diffs count: 0
Trace: CodeSearchWorkspaceDiff: refreshing diff for file:///Users/nemo/repo/quotabook-frontend.
Trace: CodeSearchWorkspaceDiff: Refreshed diff for file:///Users/nemo/repo/quotabook-frontend. New diff count: 0
Trace: CodeSearchWorkspaceDiff: Refreshed all diffs. New local diffs count: 0
Trace: CodeSearchWorkspaceDiff: refreshing diff for file:///Users/nemo/repo/quotabook-frontend.
Trace: CodeSearchWorkspaceDiff: Refreshed diff for file:///Users/nemo/repo/quotabook-frontend. New diff count: 0
Trace: CodeSearchWorkspaceDiff: Refreshed all diffs. New local diffs count: 0
Trace: GithubCodeSearchService.getRemoteIndexState(quotabook/quotabook-frontend). Found indexed commit: 9ddf581dd0c0f43ed7f67ef022f5e1958aaadfc6.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: true.
Trace: [GitServiceImpl][doOpenRepository] Active repository: {""_repo"":{""rootUri"":{""$mid"":1,""external"":""file:///Users/nemo/repo/quotabook-frontend"",""path"":""/Users/nemo/repo/quotabook-frontend"",""scheme"":""file""},""inputBox"":{},""state"":{},""ui"":{}},""rootUri"":{""$mid"":1,""external"":""file:///Users/nemo/repo/quotabook-frontend"",""path"":""/Users/nemo/repo/quotabook-frontend"",""scheme"":""file""},""headBranchName"":""main"",""headCommitHash"":""406acfbe65e1e0695657867d278fd37a77534ce8"",""upstreamBranchName"":""main"",""upstreamRemote"":""origin"",""isRebasing"":true,""remotes"":[""origin""],""remoteFetchUrls"":[""https://github.com/quotabook/quotabook-frontend.git""],""changes"":{""mergeChanges"":[],""indexChanges"":[],""workingTree"":[],""untrackedChanges"":[]},""_onDidChangeSignal"":{""_observers"":{},""debugName"":""Observable Signal From Event""},""headBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_isInBeforeUpdate"":false,""_isReaderValid"":false,""_removedObserverToCallEndUpdateOn"":null},""headCommitHashObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_isInBeforeUpdate"":false,""_isReaderValid"":false,""_removedObserverToCallEndUpdateOn"":null},""upstreamBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_isInBeforeUpdate"":false,""_isReaderValid"":false,""_removedObserverToCallEndUpdateOn"":null},""upstreamRemoteObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_isInBeforeUpdate"":false,""_isReaderValid"":false,""_removedObserverToCallEndUpdateOn"":null},""isRebasingObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_isInBeforeUpdate"":false,""_isReaderValid"":false,""_removedObserverToCallEndUpdateOn"":null},""_checkIsIgnored"":{""_waitingTimeMs"":1000,""_queue"":[],""_timeout"":null},""_isIgnored"":{""_map"":{},""_map2"":{}}}
Trace: [GitServiceImpl][doOpenRepository] Active repository: {""_repo"":{""rootUri"":{""$mid"":1,""external"":""file:///Users/nemo/repo/quotabook-frontend"",""path"":""/Users/nemo/repo/quotabook-frontend"",""scheme"":""file""},""inputBox"":{},""state"":{},""ui"":{}},""rootUri"":{""$mid"":1,""external"":""file:///Users/nemo/repo/quotabook-frontend"",""path"":""/Users/nemo/repo/quotabook-frontend"",""scheme"":""file""},""headBranchName"":""main"",""headCommitHash"":""406acfbe65e1e0695657867d278fd37a77534ce8"",""upstreamBranchName"":""main"",""upstreamRemote"":""origin"",""isRebasing"":true,""remotes"":[""origin""],""remoteFetchUrls"":[""https://github.com/quotabook/quotabook-frontend.git""],""changes"":{""mergeChanges"":[],""indexChanges"":[],""workingTree"":[],""untrackedChanges"":[]},""_onDidChangeSignal"":{""_observers"":{},""debugName"":""Observable Signal From Event""},""headBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_isInBeforeUpdate"":false,""_isReaderValid"":false,""_removedObserverToCallEndUpdateOn"":null},""headCommitHashObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_isInBeforeUpdate"":false,""_isReaderValid"":false,""_removedObserverToCallEndUpdateOn"":null},""upstreamBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_isInBeforeUpdate"":false,""_isReaderValid"":false,""_removedObserverToCallEndUpdateOn"":null},""upstreamRemoteObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_isInBeforeUpdate"":false,""_isReaderValid"":false,""_removedObserverToCallEndUpdateOn"":null},""isRebasingObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_isInBeforeUpdate"":false,""_isReaderValid"":false,""_removedObserverToCallEndUpdateOn"":null},""_checkIsIgnored"":{""_waitingTimeMs"":1000,""_queue"":[],""_timeout"":null},""_isIgnored"":{""_map"":{},""_map2"":{}}}
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Trace: [GitServiceImpl][doOpenRepository] Active repository: {""_repo"":{""rootUri"":{""$mid"":1,""external"":""file:///Users/nemo/repo/quotabook-frontend"",""path"":""/Users/nemo/repo/quotabook-frontend"",""scheme"":""file""},""inputBox"":{},""state"":{},""ui"":{}},""rootUri"":{""$mid"":1,""external"":""file:///Users/nemo/repo/quotabook-frontend"",""path"":""/Users/nemo/repo/quotabook-frontend"",""scheme"":""file""},""headBranchName"":""main"",""headCommitHash"":""406acfbe65e1e0695657867d278fd37a77534ce8"",""upstreamBranchName"":""main"",""upstreamRemote"":""origin"",""isRebasing"":true,""remotes"":[""origin""],""remoteFetchUrls"":[""https://github.com/quotabook/quotabook-frontend.git""],""changes"":{""mergeChanges"":[],""indexChanges"":[],""workingTree"":[],""untrackedChanges"":[]},""_onDidChangeSignal"":{""_observers"":{},""debugName"":""Observable Signal From Event""},""headBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_isInBeforeUpdate"":false,""_isReaderValid"":false,""_removedObserverToCallEndUpdateOn"":null},""headCommitHashObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_isInBeforeUpdate"":false,""_isReaderValid"":false,""_removedObserverToCallEndUpdateOn"":null},""upstreamBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_isInBeforeUpdate"":false,""_isReaderValid"":false,""_removedObserverToCallEndUpdateOn"":null},""upstreamRemoteObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_isInBeforeUpdate"":false,""_isReaderValid"":false,""_removedObserverToCallEndUpdateOn"":null},""isRebasingObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_isInBeforeUpdate"":false,""_isReaderValid"":false,""_removedObserverToCallEndUpdateOn"":null},""_checkIsIgnored"":{""_waitingTimeMs"":1000,""_queue"":[],""_timeout"":null},""_isIgnored"":{""_map"":{},""_map2"":{}}}
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Trace: CodeSearchWorkspaceDiff: refreshing diff for file:///Users/nemo/repo/quotabook-frontend.
Trace: CodeSearchWorkspaceDiff: Refreshed diff for file:///Users/nemo/repo/quotabook-frontend. New diff count: 0
Trace: CodeSearchWorkspaceDiff: Refreshed all diffs. New local diffs count: 0
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
</pre>
</details>
<details><summary>Request IDs</summary>
<pre>

</pre>
</details><details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M1 Pro (10 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|3, 3, 5|
|Memory (System)|16.00GB (0.05GB free)|
|Process Argv||
|Screen Reader|no|
|VM|0%|
</details>
<!-- generated by issue reporter -->",1
github copilot not activaing,"
Type: <b>Bug</b>

github copilot not activaing

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.22631
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|13th Gen Intel(R) Core(TM) i7-13800H (20 x 2918)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|63.66GB (49.71GB free)|
|Process Argv|-n --crash-reporter-id b440c40e-653e-484f-8c52-ebfff5cf0bfa|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyonecf:30548226
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
jhi8h917:31341130
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
30 days not completed still isue encountering,"
Type: <b>Bug</b>

i have using copilot using shreyawagh109@gmail.com through this mail id 1 day only havent used 30 days

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-3330 CPU @ 3.00GHz (4 x 2993)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: unavailable_off<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: unavailable_off<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|11.96GB (4.91GB free)|
|Process Argv|--crash-reporter-id ff13fc0b-988e-4868-8b62-bafaf5ffa0b7|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805:30301674
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
2i9eh265:30646982
962ge761:30959799
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dwnewjupytercf:31046870
pythonrstrctxt:31112756
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31303018
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
jjjhb125:31275178
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
c4b42873:31341128
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
82j33506:31327384
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930

```

</details>

<!-- generated by issue reporter -->",2
MODELS ARE UTTERLY NONCOMPLIANT,"
Type: <b>Bug</b>

Models are utterly and perpetually disobedient that WASTES MOST OF MY TIME AND CREDITS while I try to force it to obey and refactor files properly, carefully and according to written commands and instructions.

Don't get too proud of AI because it has proven to be incredibly stupid.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Linux x64 6.8.0-63-generic
Modes:
Remote OS version: Linux x64 6.8.0-63-generic

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD A8 PRO-7600B R7, 10 Compute Cores 4C+6G (4 x 3114)|
|GPU Status|2d_canvas: unavailable_software<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: disabled_software<br>multiple_raster_threads: enabled_on<br>opengl: disabled_off<br>rasterization: disabled_software<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: disabled_software<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: unavailable_software<br>webgl2: unavailable_software<br>webgpu: disabled_off<br>webnn: unavailable_software|
|Load (avg)|2, 2, 2|
|Memory (System)|14.59GB (5.30GB free)|
|Process Argv|--crash-reporter-id 02560367-b0d8-46c3-9316-43c1547ef493|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|ubuntu|
|XDG_CURRENT_DESKTOP|Unity|
|XDG_SESSION_DESKTOP|ubuntu|
|XDG_SESSION_TYPE|x11|

|Item|Value|
|---|---|
|Remote|SSH: maxm-news|
|OS|Linux x64 6.8.0-63-generic|
|CPUs|AMD Ryzen 3 5300G with Radeon Graphics (8 x 4242)|
|Memory (System)|62.14GB (52.02GB free)|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
c4b42873:31341128
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335814
nes-diff-11:31337487
nes-set-on:31340697
testaa123cf:31335227
6abeh943:31336334
yijiwantestdri0626-t:31336930
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
model unexpectedly syopped and not working,"
Type: <b>Bug</b>

The model unexpectedly did not return a response, which may indicate a service issue. Please report a bug.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i5-11260H @ 2.60GHz (12 x 2611)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.73GB (2.54GB free)|
|Process Argv|--crash-reporter-id 6c814bf6-ae36-4210-b71d-d5d207c62b64|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
c4b42873:31341128
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
The moda is not working ,"
Type: <b>Bug</b>

The model unexpectedly did not return a response, which may indicate a service issue. Please report a bug.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i5-11260H @ 2.60GHz (12 x 2611)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.73GB (2.57GB free)|
|Process Argv|--crash-reporter-id 6c814bf6-ae36-4210-b71d-d5d207c62b64|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
c4b42873:31341128
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Extensible hooks to customize VS Code agent workflows,"Add support for user-defined shell/task/extension hooks in VS Code agent mode, allowing deterministic actions (e.g., notifications, formatting, logging) at key agent/tool lifecycle events.

Inspired by https://docs.anthropic.com/en/docs/claude-code/hooks

cc @isidorn (extensions), @meganrogge (tasks/terminal), @connor4312 (mcp)",0
GUI crashes,"
Type: <b>Performance Issue</b>

Whenever I delete a file directly via the VSCode Explorer, the GUI crashes, even though the application continues running in the background. In contrast, deleting files using rm in the terminal inside VSCode works without any issues.

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19044
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i5-11500 @ 2.70GHz (12 x 2712)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.73GB (9.38GB free)|
|Process Argv|--crash-reporter-id d8c1a170-b445-44cd-b2ca-958301e2e3ff|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>Process Info</summary>

```
CPU %	Mem MB	   PID	Process
    1	   157	 14548	code
    1	   337	  6880	window [1] (Demo - Visual Studio Code [Administrator])
    0	   188	  9220	shared-process
    0	   105	 10964	   gpu-process
    0	   325	 12100	extension-host [1]
    0	    95	  4044	     ""C:\Program Files\Microsoft VS Code\Code.exe"" ""c:\Program Files\Microsoft VS Code\resources\app\extensions\json-language-features\server\dist\node\jsonServerMain"" --node-ipc --clientProcessId=12100
    0	    66	 15096	     c:\Users\ke.li\.vscode\extensions\ms-vscode.cpptools-1.26.3-win32-x64\bin\cpptools.exe
    0	     4	  6240	       ""c:\Users\ke.li\.vscode\extensions\ms-vscode.cpptools-1.26.3-win32-x64\bin\cpptools.exe""
    0	    13	  7512	       C:\windows\system32\conhost.exe 0x4
    0	   101	 12448	file-watcher [1]
    0	    32	 12916	   crashpad-handler
    0	    52	 15604	   utility-network-service
```

</details>
<details>
<summary>Workspace Info</summary>

```
|  Window (Demo - Visual Studio Code [Administrator])
|    Folder (Demo): 83 files
|      File types: cmake(17) json(13) bat(6) txt(5) make(5) exe(3) rsp(3)
|                  bin(2) cpp(2) c(1)
|      Conf files: cmake(17) settings.json(1) makefile(1);
```

</details>
<details><summary>Extensions (39)</summary>

Extension|Author (truncated)|Version
---|---|---
better-comments|aar|3.0.2
codesnap|adp|1.3.4
catppuccin-vsc|Cat|3.17.0
ruff|cha|2025.24.0
doxdocgen|csc|1.4.0
auto-header|Dan|1.0.1
code-runner|for|0.12.2
autocorrect|hua|2.6.4
leetcode-helper|lab|3.2.1
vscode-gothiconline|mar|0.0.89
rainbow-csv|mec|3.20.0
vscode-containers|ms-|2.0.3
debugpy|ms-|2025.8.0
python|ms-|2025.8.0
vscode-pylance|ms-|2025.6.2
jupyter-keymap|ms-|1.1.2
remote-ssh|ms-|0.120.0
remote-ssh-edit|ms-|0.87.0
cmake-tools|ms-|1.20.53
cpptools|ms-|1.26.3
cpptools-extension-pack|ms-|1.3.1
makefile-tools|ms-|0.12.17
remote-explorer|ms-|0.5.0
autodocstring|njp|0.6.1
materialiconic-product-icons|nyx|0.0.2
indent-rainbow|ode|8.3.1
platformio-ide|pla|3.3.4
rde-ros-2|Ran|1.0.2
urdf-editor|Ran|1.0.0
vscode-fix-checksums-next|Rim|1.4.0
markdown-preview-enhanced|shd|0.8.18
searchdocsets-vscode|sil|1.0.1
catppuccin-perfect-icons|tha|0.21.33
qt|the|1.2.0
qtvsctools|ton|0.11.0
intellicode-api-usage-examples|Vis|0.2.9
vscodeintellicode|Vis|1.3.2
city-lights-icon-vsc|Yum|1.1.3
mplstyle|yy0|1.14.4

(6 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
c4b42873:31341128
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
"The model unexpectedly did not return a response, ","
Type: <b>Bug</b>

--- output github copilot chat ---

2025-07-08 11:10:19.128 [error] Error: net::ERR_CONNECTION_CLOSED
    at SimpleURLLoaderWrapper.<anonymous> (node:electron/js2c/utility_init:2:10635)
    at SimpleURLLoaderWrapper.emit (node:events:518:28)
    at SimpleURLLoaderWrapper.callbackTrampoline (node:internal/async_hooks:130:17): Error on conversation request
2025-07-08 11:10:32.331 [error] Error: net::ERR_CONNECTION_CLOSED
    at SimpleURLLoaderWrapper.<anonymous> (node:electron/js2c/utility_init:2:10635)
    at SimpleURLLoaderWrapper.emit (node:events:518:28)
    at SimpleURLLoaderWrapper.callbackTrampoline (node:internal/async_hooks:130:17): Error on conversation request
2025-07-08 11:11:15.003 [info] message 0 returned. finish reason: [stop]
2025-07-08 11:11:15.003 [info] request done: requestId: [84d04dc7-1fa1-4ccb-8425-05a8718e4bc3] model deployment ID: []
2025-07-08 11:14:18.301 [info] message 0 returned. finish reason: [stop]
2025-07-08 11:14:18.301 [info] request done: requestId: [41b5c65c-1abd-4fe0-9487-36faa6760f40] model deployment ID: []
2025-07-08 11:15:13.018 [info] Fetched model metadata in 262ms 89206a69-80f6-4a14-b211-d417a9e7cef6
2025-07-08 11:15:17.603 [info] message 0 returned. finish reason: [stop]
2025-07-08 11:15:17.603 [info] request done: requestId: [98e95fab-0d4b-40f4-bc0e-2fd518255251] model deployment ID: []
2025-07-08 11:20:48.189 [info] message 0 returned. finish reason: [stop]
2025-07-08 11:20:48.190 [info] request done: requestId: [381b70d2-ec73-4198-9352-ce1396df35a2] model deployment ID: []

----windows--

2025-07-08 11:15:07.721 [error] [Window] TreeError [DebugRepl] Tree input not set: Error: TreeError [DebugRepl] Tree input not set
    at Zg.C (vscode-file://vscode-app/c:/Users/ADMIN/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:417:29025)
    at Zg.updateChildren (vscode-file://vscode-app/c:/Users/ADMIN/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:417:28932)
    at Jre.value (vscode-file://vscode-app/c:/Users/ADMIN/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:2439:33732)
    at E.B (vscode-file://vscode-app/c:/Users/ADMIN/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:29:2392)
    at E.C (vscode-file://vscode-app/c:/Users/ADMIN/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:29:2462)
    at E.fire (vscode-file://vscode-app/c:/Users/ADMIN/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:29:2680)
    at Gke.setVisible (vscode-file://vscode-app/c:/Users/ADMIN/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:680:2729)
    at vscode-file://vscode-app/c:/Users/ADMIN/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:1315:11772
    at Array.map (<anonymous>)
    at td.setVisible (vscode-file://vscode-app/c:/Users/ADMIN/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/workbench/workbench.desktop.main.js:1315:11763)


Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|12th Gen Intel(R) Core(TM) i5-12400 (12 x 2496)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: unavailable_off<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.79GB (3.14GB free)|
|Process Argv|--folder-uri file:///d%3A/xampp/htdocs/rsdm/simrs --crash-reporter-id 9949b082-73a6-4220-9182-d4c2f6a02bd2|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
9d2cg352:31339597
c4b42873:31341128
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
testaa123:31335226
63221493:31336333
yijiwantestdri0626-t:31336930
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Notebook Cell outputs getting duplicated,"Run the following in a cell and sometimes the numbers get duplicated
```python
def doit():
    counter = 1
    while True:
        import time
        time.sleep(0.1)

        counter = counter + 1
        print(counter)

        if counter > 5:
            break

doit()
```

The outputs in the JSON are not duplicated leading me to assume this is a rendering issue.
I.e. the output items aren't duplicated in the notebook cell model.",0
copilot回答完问题后迅速折叠答案,"
Type: <b>Bug</b>

copilot在回答问题时会显示文字和代码，但一旦结束回答，会迅速折叠所有答案，不显示任何东西

Extension version: 0.28.5
VS Code version: Code 1.101.0 (Universal) (dfaf44141ea9deb3b4096f7cd6d24e00c147a4b1, 2025-06-11T15:00:50.123Z)
OS version: Darwin arm64 22.5.0
Modes:
Remote OS version: Linux x64 4.18.0-477.21.1.el8_8.x86_64

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M1 (8 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|3, 3, 3|
|Memory (System)|16.00GB (0.10GB free)|
|Process Argv|--crash-reporter-id f417b7a1-2941-417a-a1fa-8f8b240b361b|
|Screen Reader|no|
|VM|0%|

|Item|Value|
|---|---|
|Remote|SSH: gm711-1|
|OS|Linux x64 4.18.0-477.21.1.el8_8.x86_64|
|CPUs|AMD EPYC 7763 64-Core Processor (256 x 3099)|
|Memory (System)|1007.18GB (796.21GB free)|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
convertlamdaf:31329270
c4b42873:31341128
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335814
nes-diff-11:31337487
nes-set-on:31340697
testaa123cf:31335227
6abeh943:31336334
yijiwantestdri0626-c:31336931
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
loop in conection and reading context,"
Type: <b>Bug</b>

When connecting to the PostgreSQL database, it establishes the connection and displays a message indicating that it is retrieving the database context to understand it. Once this task is completed, it reconnects to the database and performs the task of retrieving the database context again in order to understand it.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-9700K CPU @ 3.60GHz (8 x 3600)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|47.92GB (26.72GB free)|
|Process Argv|--crash-reporter-id 72566c93-2467-4b92-b505-319d9ac2718f|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
c4b42873:31341128
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
testaa123cf:31335227
6abeh943:31336334
yijiwantestdri0626-t:31336930
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
para,"
Type: <b>Bug</b>

para de me recomendar merda fdp, quero estudar


Extension version: 1.341.0
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Linux x64 6.11.0-29-generic snap
Modes:

## Copilot Completion Feedback
### Describe the issue, feedback, or steps to reproduce it:


### Completion text:
```
    int N, B;
    scanf(""%d %d"", &N, &B);
    
    int graph[N][N];
    
    // Initialize the graph with 0
    for (int i = 0; i < N; i++) {
        for (int j = 0; j < N; j++) {
            graph[i][j] = 0;
        }
    }
    
    // Read the boats information
    for (int i = 0; i < B; i++) {
        int I, J, P;
        scanf(""%d %d %d"", &I, &J, &P);
        graph[I-1][J-1] = P; // Store the capacity
        graph[J-1][I-1] = P; // Undirected graph
    }
    
    int C;
    scanf(""%d"", &C);
    
    // Process each query
    for (int i = 0; i < C; i++) {
        int X, Y;
        scanf(""%d %d"", &X, &Y);
        
        // Use a simple BFS or DFS to find the maximum capacity path
        int max_capacity = 0;
        
        // Implement BFS or DFS here to find the maximum capacity from X to Y
        
        printf(""%d\n"", max_capacity);
    }
    
    return 0;
```

<details>
<summary>Diagnostics</summary>

## Copilot Extension

- Version: 1.341.0
- Editor: vscode/1.101.2
- Header Request ID: 254402c2-a90b-4fb8-80cb-101db265d2d2
- Choice Index: 0
- Opportunity ID: cdd20dbc-9de1-4721-9c8b-406bcf6925c7
- Client Completion ID: 3b38aff4-f760-42af-9573-31a0a93ed7b9
- Model ID: gpt-4o-copilot
- A/B Experiment Info: vsliv368:30146709;vspor879:30202332;vspor708:30202333;vspor363:30204092;binariesv615:30325510;c4g48928:30535728;6074i472:31201624;customenabled:31248079;cp_compl_t_13691:31267975;b5i3g282:31220671;9064b325:31222308;copilot_t_ci:31333650;cp_14580_0_t:31245410;cp15172_t:31240738;4gafe986:31271826;cp16806_a:31298379;usemplatestapi:31297334;7bj51361:31289155;aj953862:31281341;cp17209_t:31303705;cp16806_spec_t:31320508;cp18466_t:31331492;9d2cg352:31339597;d2249276:31341129;i851h500:31338111;usemarketplace:31343026;nesew2to5:31336538;agentclaude:31335815;nes-diff-11:31337487;aa_cpt_t:31332558;testaa123:31335226;6abeh943:31336334;yijiwantestdri0626-c:31336931;4gdec884:31342391;gh_scm_t:31343112;

</details>
<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 7 5700G with Radeon Graphics (16 x 4540)|
|GPU Status|2d_canvas: unavailable_software<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: disabled_software<br>multiple_raster_threads: enabled_on<br>opengl: disabled_off<br>rasterization: disabled_software<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: disabled_software<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: unavailable_software<br>webgl2: unavailable_software<br>webgpu: disabled_off<br>webnn: unavailable_software|
|Load (avg)|1, 1, 0|
|Memory (System)|14.42GB (9.42GB free)|
|Process Argv|--no-sandbox . --crash-reporter-id 4cf59030-a3d0-4a50-8e9e-1cc2b2877a4c|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|ubuntu|
|XDG_CURRENT_DESKTOP|Unity|
|XDG_SESSION_DESKTOP|ubuntu|
|XDG_SESSION_TYPE|wayland|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
convertlamdat:31329272
d2249276:31341129
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
testaa123:31335226
6abeh943:31336334
yijiwantestdri0626-c:31336931
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
Source Control pane - collapse and expand tree view,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

Hi vscode team,

I have searched for ""collapse"" and ""source control"" but I couldn't find my Feature Request among the many many new issues. I hope it is not a duplicated.

**My Feature Request is to allow the tree structure rendered in the Source Control panel to be collapsable and expandable** like many other tree structures (i.e. in Explorer, etc.). I am not 100% if this is done, I do not think so. The usual button for collapse seems to be to fold all the repos in the panel - not the folders within a repo.

Thank you for your work. Best Code Editor! Go Team!

Best regards,
ei06125",0
GitHub copilot not taking folder as context in edit mode,"For some reason it's like the folder I'm providing into the context is not seen/visible by the edit and it's only limiting edits to a portion of the files (which renders an incomplete solution). I've tried multiple times to do this and all of them result in the same output. 

<img width=""517"" height=""520"" alt=""Image"" src=""https://github.com/user-attachments/assets/c73d2c4a-2432-4723-81f1-b21ab8bbf61c"" />

Not sure if related, but I can't seem to add folders from the attachment window either. Anything I type is not found in the search box. 


<img width=""947"" height=""111"" alt=""Image"" src=""https://github.com/user-attachments/assets/849d019c-95f6-4e7c-ac80-c64f6a8991d6"" />",0
"suddenly model disapear, c","
Type: <b>Bug</b>

suddenly model disapear, before using claude 4

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|12th Gen Intel(R) Core(TM) i5-12400 (12 x 2496)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: unavailable_off<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.79GB (3.72GB free)|
|Process Argv|--folder-uri file:///d%3A/xampp/htdocs/rsdm/simrs --crash-reporter-id 9949b082-73a6-4220-9182-d4c2f6a02bd2|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
9d2cg352:31339597
c4b42873:31341128
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
testaa123:31335226
63221493:31336333
yijiwantestdri0626-t:31336930
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
failed and showing retry again and again,"ADD ISSUE DESCRIPTION HERE

Version: 1.101.2
Commit: 2901c5ac6db8a986a5666c3af51ff804d05af0d4
User Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36
Embedder: codespaces

Extension version: 0.28.5
<!-- generated by web issue reporter -->",0
It doesn't let to connect to login to git hub,"
Type: <b>Bug</b>

When you are working on arch linux, it doesn't connect properly to the acount if there is more than 1 editor in your pc

Extension version: 1.341.0
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Linux x64 6.15.1-arch1-2
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-8565U CPU @ 1.80GHz (8 x 3531)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: disabled_off<br>webnn: disabled_off|
|Load (avg)|2, 2, 1|
|Memory (System)|19.26GB (14.83GB free)|
|Process Argv|--crash-reporter-id 1d78586f-3db5-4542-a6f3-41501d5af3c8|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|undefined|
|XDG_CURRENT_DESKTOP|Hyprland|
|XDG_SESSION_DESKTOP|Hyprland|
|XDG_SESSION_TYPE|wayland|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
9b8hh234:30694863
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
d2249276:31341129
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
onetestforazureexp:31335613
6abeh943:31336334
yijiwantestdri0626-c:31336931
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
Claude is not available,"
Type: <b>Bug</b>

Claude is not available

Extension version: 0.28.5
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M2 Pro (10 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|2, 3, 3|
|Memory (System)|16.00GB (0.45GB free)|
|Process Argv|--crash-reporter-id fc5d5d31-3c3e-47d6-8c0e-c2596c22aad9|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551:31249599
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
d2249276:31341129
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
testaa123:31335226
6abeh943:31336334
yijiwantestdri0626-c:31336931
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
Slow typing in medium sized ts file,"
Type: <b>Bug</b>

1. Create a copy of `lib.webworker.d.ts`. (copy of file: [lib.webworker.d.ts.txt](https://github.com/user-attachments/files/21111378/lib.webworker.d.ts.txt))
2. Change it to ts
3. Start editing at the top of it after the main comment

**bug**
Typing is slow

Profile: [slow-trace.json.zip](https://github.com/user-attachments/files/21111359/slow-trace.json.zip)

VS Code version: Code - Insiders 1.102.0-insider (Universal) (96f1890d08080f46f3b0a9424553422f04133090, 2025-07-04T15:15:02.963Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M2 Max (12 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|58, 32, 17|
|Memory (System)|64.00GB (0.13GB free)|
|Process Argv|--enable-proposed-api ms-vscode.sample-inline-edit-provider --log github.copilot-chat=trace --crash-reporter-id 0fffb5da-9cd7-46fd-9e7f-a1564e8c5fda|
|Screen Reader|no|
|VM|0%|
</details>
<!-- generated by issue reporter -->",2
supportsVision measure doesn't is always 0,https://github.com/microsoft/vscode/blob/25ee562cba29e4256a144af0b1f3af9b2f8f7f04/src/vs/workbench/contrib/chat/browser/chatAttachmentWidgets.ts#L294-L299,0
Cant close text box after code is generated,"
Type: <b>Bug</b>

The pop-up  to use copilot is nice, but when I click accept, it won't close.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 5 PRO 5650U with Radeon Graphics      (12 x 2296)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.31GB (2.12GB free)|
|Process Argv|C:\\Users\\Deivid.Smarzaro\\OneDrive - Bentley Systems, Inc\\TCC\\xai-ecg --crash-reporter-id 0fe35d90-0619-4342-ac21-beede7fc2bef|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
convertlamdaf:31329270
c4b42873:31341128
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
LoopBackServer not return response while the code is received from the URL call back.,"https://github.com/microsoft/vscode/blob/b922e2790c77a072a358995e6f376fa61cb171db/src/vs/workbench/api/node/extHostAuthentication.ts#L136-L149

response never received after code received from the http server.

https://github.com/microsoft/vscode/blob/b922e2790c77a072a358995e6f376fa61cb171db/src/vs/workbench/api/node/loopbackServer.ts#L83

",0
Chat variables in reusable prompts,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: Latest
- OS Version: All OS version

Steps to Reproduce:

1. Include chat variable ""#file"" in a reusable prompt
2. Run the prompt
3. You'll get to see that the behavior isn't the same as using it in chat. The ""#file"" variable isn't used correctly.
",0
Failed Execution of Code request,"
Type: <b>Bug</b>

## 🐞 Bug Report: Copilot Agent Crashes on Trace Prompt

**Summary:**  
Copilot's inline agent repeatedly crashes when given a multi-layer trace prompt involving log analysis for my AI project (GremlinGPT). The issue is triggered **immediately after the prompt is submitted**, before any visible output is returned.

---

### 🧠 Repro Prompt

This is the exact prompt I gave Copilot inline (in Python context, VS Code):

```python
I need you to fully emerse into GremlinGPT and tell me if he has chat output output inside the frontend/ and backend/ as well as the nlp_engine/, as well we need to do a full log trace to ensure every file is logging to its properly labeled .out/.log for the full system
```

💥 Behavior
Copilot agent silently crashes, no code generated

Sometimes restarts, sometimes freezes

No log output shown in Copilot output panel

Issue is reproducible in multiple files

✅ System Info
Item	Value
Copilot Version	0.28.5
VS Code Version	1.101.2 (2901c5a)
OS	Linux x64 6.11.0-29-generic (snap)
Extension Host	Stable
Network	✅ Connected
Repro Mode	✅ Yes (isolated)

🧪 Notes
The agent crash is prompt-specific, not project-specific.

If I use simpler prompts, Copilot works fine.

Prompt uses multi-layered trace request, which may exceed internal token limit or cause agent recursion bug.

📎 Suggested Fix (Dev-side)
Please test Copilot inline agent with prompts that include:

Recursive logging analysis

Filepath references (frontend/, backend/, nlp_engine/)

Explicit multi-system tracing requests

Let me know if logs are needed — I can enable debug mode and repro on request.

Thanks,
Daniel Morris (@GremlinsForge)
https://github.com/statikfintechllc/AscendAI

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 9 9900X 12-Core Processor (24 x 4421)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: disabled_off<br>webnn: disabled_off|
|Load (avg)|0, 0, 0|
|Memory (System)|109.60GB (102.76GB free)|
|Process Argv|--no-sandbox --force-user-env --crash-reporter-id 9fa98385-21ae-4614-a205-21bbbe0acbb1|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|ubuntu|
|XDG_CURRENT_DESKTOP|Unity|
|XDG_SESSION_DESKTOP|ubuntu|
|XDG_SESSION_TYPE|x11|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyonecf:30548226
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
j6a0c355:31339598
d2249276:31341129
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335814
82j33506:31327384
nes-diff-11:31337487
nes-set-on:31340697
onetestforazureexpcf:31335614
6abeh943:31336334
yijiwantestdri0626-c:31336931
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
Policy parser does not support `markdownDescription`,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 
- OS Version: 

Steps to Reproduce:

1. Create a setting with a `policy` object
2. Omit `description`
3. 🐛 Error running `node build/policy <platform>`


Error from [a run](https://dev.azure.com/monacotools/Monaco/_build/results?buildId=347931&view=logs&j=3792f238-f35e-5f82-0dbc-272432d9a0fb&t=19ca2b82-e9ec-5437-eb2e-505cf4e28552)
```
========================== Starting Command Output ===========================
/bin/bash --noprofile --norc /Users/cloudtest/vss/_work/_temp/2162e991-1ded-4ff7-8e08-11186d2671b2.sh
Parse Error: Missing required 'description' property.. vs/workbench/contrib/chat/browser/chat.contribution.ts:214
```

There are valid cases for omitting `description`, like if you're using `markdownDescription` instead.

Another solution is to still require `description` and add policy generation as a part of the PR gate.  That might make more sense as consumers of this string (Windows Group Policy Editor) cannot render markdown.",1
infinit loop,"
Type: <b>Bug</b>

infinit loop

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.22000
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-7700K CPU @ 4.20GHz (8 x 4200)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.96GB (4.36GB free)|
|Process Argv|--crash-reporter-id 62b60d21-b402-463c-bc8a-5ad26cf59eb9|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31343502
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
9d2cg352:31339597
jhi8h917:31341130
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
onetestforazureexpcf:31335614
6abeh943:31336334
yijiwantestdri0626-c:31336931
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
EditorStatus doesn't support Notebook Cells,"`src/vs/workbench/browser/parts/editor/editorStatus.ts` doesn't support notebook cells.

Each cell is a separate text document with its own language, line feed, etc.
The current code doesn't support notebooks.
",0
Warnings in `WorkspaceEdit` for rename conflicts,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
[rust-analyzer](https://github.com/rust-lang/rust-analyzer/) features a warning when a rename will cause conflict, that is will change the meaning of the program. However, the current UI is awful. I'd like to implement a better UI for this, but this needs support from VSCode. I imagine other language clients can also use this.

The UI I want is for rename that cause conflicts to show the rename preview, with conflicting variables highlighted in red, and with an option for the user to approve/cancel.

The way I imagine it will be implemented is by adding a new method to `WorkspaceEdit`, `insertWarning(uri: Uri, range: Range, description: string, label: string, iconPath?: IconPath)`. The warnings will be grouped by `label`, similar to `WorkspaceEditEntryMetadata`.

I can implement this myself, but I wanted to hear if this will be accepted.",1
"""Unexpected error"" when launching code from the app menu; TypeError: Invalid value for env","<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: No, but not relevant to the underlying issue, as simply launching code from the terminal in the first place works around it

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.101.2-1750797935
- OS Version: Linux Mint 21.3

Steps to Reproduce:

1. Start code from application menu
2. See that all restored windows that open into a devcontainer or remote explorer session display errors
3. The issue is not present when launching code from the terminal

![Image](https://github.com/user-attachments/assets/5d92d4f1-c598-440e-be0a-3f31eb45d396)
![Image](https://github.com/user-attachments/assets/ab57b03f-2615-4393-bfe0-4a03ccc338c3)

The following log snippet is displayed in the `Window` log;
```
2025-07-07 22:10:50.791 [error] [Window] Error received from starting extension host (kind: LocalProcess)
2025-07-07 22:10:50.791 [error] [Window] TypeError: Invalid value for env
    at new ForkUtilityProcess (node:electron/js2c/browser_init:2:73496)
    at Object.fork (node:electron/js2c/browser_init:2:71708)
    at Ka.D (file:///usr/share/code/resources/app/out/main.js:43:11861)
    at Ka.start (file:///usr/share/code/resources/app/out/main.js:43:15203)
    at Ah.start (file:///usr/share/code/resources/app/out/main.js:56:11603)
    at Object.call (file:///usr/share/code/resources/app/out/main.js:33:4564)
    at Cf.s (file:///usr/share/code/resources/app/out/main.js:31:14831)
    at Cf.q (file:///usr/share/code/resources/app/out/main.js:31:14354)
    at Yo.value (file:///usr/share/code/resources/app/out/main.js:31:13756)
    at D.B (file:///usr/share/code/resources/app/out/main.js:30:2373)
    at D.C (file:///usr/share/code/resources/app/out/main.js:30:2443)
    at D.fire (file:///usr/share/code/resources/app/out/main.js:30:2660)
    at Yo.value (file:///usr/share/code/resources/app/out/main.js:28:4827)
    at D.B (file:///usr/share/code/resources/app/out/main.js:30:2373)
    at D.fire (file:///usr/share/code/resources/app/out/main.js:30:2591)
    at Yo.value (file:///usr/share/code/resources/app/out/main.js:28:5015)
    at D.B (file:///usr/share/code/resources/app/out/main.js:30:2373)
    at D.fire (file:///usr/share/code/resources/app/out/main.js:30:2591)
    at z (file:///usr/share/code/resources/app/out/main.js:28:7309)
    at IpcMainImpl.i (file:///usr/share/code/resources/app/out/main.js:33:21024)
    at IpcMainImpl.emit (node:events:530:35)
    at WebContents.<anonymous> (node:electron/js2c/browser_init:2:88892)
    at WebContents.emit (node:events:518:28)
2025-07-07 22:10:50.791 [error] [Window] Cannot resolve canonical URI: Error: Cannot resolve canonical URI
    at mTe.getCanonicalURI (vscode-file://vscode-app/usr/share/code/resources/app/out/vs/workbench/workbench.desktop.main.js:3348:51960)
    at async Promise.all (index 0)
    at async zyt.cc (vscode-file://vscode-app/usr/share/code/resources/app/out/vs/workbench/workbench.desktop.main.js:3360:2644)
2025-07-07 22:10:50.954 [info] [Window] ComputeTargetPlatform: linux-x64
2025-07-07 22:10:52.247 [info] [Window] Invoking resolveAuthority(ssh-remote)...
2025-07-07 22:10:52.247 [info] [Window] [LocalProcess0][resolveAuthority(ssh-remote,1)][0ms] obtaining proxy...
2025-07-07 22:10:52.247 [error] [Window] [LocalProcess0][resolveAuthority(ssh-remote,1)][0ms] no proxy undefined
2025-07-07 22:10:52.273 [error] [Window] resolveAuthority(ssh-remote) returned an error after 30 ms Cannot resolve authority
2025-07-07 22:10:52.274 [error] [Window] Error received from starting extension host (kind: Remote)
2025-07-07 22:10:52.274 [error] [Window] CodeExpectedError: Cannot resolve authority
    at zyt.sb (vscode-file://vscode-app/usr/share/code/resources/app/out/vs/workbench/workbench.desktop.main.js:3354:16046)
    at async zyt.rb (vscode-file://vscode-app/usr/share/code/resources/app/out/vs/workbench/workbench.desktop.main.js:3354:15412)
    at async zyt.ec (vscode-file://vscode-app/usr/share/code/resources/app/out/vs/workbench/workbench.desktop.main.js:3360:3612)
    at async vscode-file://vscode-app/usr/share/code/resources/app/out/vs/workbench/workbench.desktop.main.js:30:82486
2025-07-07 22:10:52.274 [error] [Window] [remote-connection][attempt 1] An error occurred in initial connection! Will retry... Error:
2025-07-07 22:10:52.274 [error] [Window] CodeExpectedError: Cannot resolve authority
    at zyt.sb (vscode-file://vscode-app/usr/share/code/resources/app/out/vs/workbench/workbench.desktop.main.js:3354:16046)
    at async zyt.rb (vscode-file://vscode-app/usr/share/code/resources/app/out/vs/workbench/workbench.desktop.main.js:3354:15412)
    at async zyt.ec (vscode-file://vscode-app/usr/share/code/resources/app/out/vs/workbench/workbench.desktop.main.js:3360:3612)
    at async vscode-file://vscode-app/usr/share/code/resources/app/out/vs/workbench/workbench.desktop.main.js:30:82486
2025-07-07 22:10:52.275 [error] [Window] [remote-connection][attempt 2] An error occurred in initial connection! Will retry... Error:
```
---

The code.desktop and code-url-handler.desktop files point to the wrong executable.

They are currently pointing at `/usr/share/code/code`, but should point at the script at `/usr/share/code/bin/code` (which is what the symlink `/usr/bin/code` points at).

https://github.com/microsoft/vscode/blob/dadbc584d9b02c5964621c67eebefa871c051655/build/gulpfile.vscode.linux.js#L55
https://github.com/microsoft/vscode/blob/dadbc584d9b02c5964621c67eebefa871c051655/build/gulpfile.vscode.linux.js#L171

Personally, I'd point those at the symlink `/usr/bin/code`, so that it matches running code from `$PATH` lookup.

---

I would PR, but I'm allergic to most CLAs.",1
"""The window id not responding"" happens every minute and VS Code became useless.","Using VS Code combined with GitHub Copilot in below environment:

Version: 1.101.2 (user setup)
Commit: 2901c5ac6db8a986a5666c3af51ff804d05af0d4
Date: 2025-06-24T20:27:15.391Z
Electron: 35.5.1
ElectronBuildId: 11727614
Chromium: 134.0.6998.205
Node.js: 22.15.1
V8: 13.4.114.21-electron.0
OS: Windows_NT x64 10.0.19045

and ""The window id not responding"" popped up almost every minute.

<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 
- OS Version: 

Steps to Reproduce:

1. 
2. 
",0
50% or more commands wasted because of models' disobedience,"
Type: <b>Bug</b>

Regardless of the model used, 50% or more of all commands are used to correct its careless errors, its utter disobedience and trying to get the model to follow written instructions. Due to the enormous number of commands needed to force the models to obey precisely, premium credits are used prematurely and without justification.

Users should be given a credit for every command that is not followed precisely, and for every command necessary to correct the mistakes the models make because of its utter disobedience.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Linux x64 6.8.0-63-generic snap
Modes:
Remote OS version: Linux x64 6.8.0-63-generic

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD A8 PRO-7600B R7, 10 Compute Cores 4C+6G (4 x 3293)|
|GPU Status|2d_canvas: unavailable_software<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: disabled_software<br>multiple_raster_threads: enabled_on<br>opengl: disabled_off<br>rasterization: disabled_software<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: disabled_software<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: unavailable_software<br>webgl2: unavailable_software<br>webgpu: disabled_off<br>webnn: unavailable_software|
|Load (avg)|2, 2, 2|
|Memory (System)|14.59GB (5.19GB free)|
|Process Argv|--no-sandbox --force-user-env --crash-reporter-id 02560367-b0d8-46c3-9316-43c1547ef493|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|ubuntu|
|XDG_CURRENT_DESKTOP|Unity|
|XDG_SESSION_DESKTOP|ubuntu|
|XDG_SESSION_TYPE|x11|

|Item|Value|
|---|---|
|Remote|SSH: maxm-news|
|OS|Linux x64 6.8.0-63-generic|
|CPUs|AMD Ryzen 3 5300G with Radeon Graphics (8 x 3393)|
|Memory (System)|62.14GB (51.07GB free)|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
c4b42873:31341128
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335814
nes-diff-11:31337487
nes-set-on:31340697
testaa123cf:31335227
6abeh943:31336334
yijiwantestdri0626-t:31336930
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
IntelliSense indexing is too slow for large C/C++ projects – suggest persistent cache and Git-aware incremental updates,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 
Version: 1.101.2 (user setup)
Date: 2025-06-24T20:27:15.391Z
Electron: 35.5.1
ElectronBuildId: 11727614
Chromium: 134.0.6998.205
Node.js: 22.15.1
V8: 13.4.114.21-electron.0
OS: Windows_NT x64 10.0.22631


### Problem Summary

In large C/C++ projects (e.g. >1500 `.c` files, >6 GB source tree), IntelliSense in VS Code takes an extremely long time to index, sometimes up to 3h or more. This severely impacts productivity, especially when switching branches or restarting the editor.

### Environment

- VS Code: [1.101.2]
- C/C++ extension: ms-vscode.cpptools (v2.x)
- OS: Windows 10 / 11
- Workspace type: Local repo
- Project size: ~6 GB, 1500+ C files
- Compiler: GCC + `compile_commands.json` provided
- Hardware: [e.g. 8-core CPU, 32GB RAM]

---

### Observed Behavior

- When opening the workspace, IntelliSense starts re-indexing from scratch.
- This happens even if no files have changed.
- There's no apparent persistent cache between VS Code sessions.
- It uses a lot of CPU and RAM and slows down the entire system.
- No integration with Git (e.g. it re-indexes even if Git history is unchanged).

---

### Desired Behavior

I would like IntelliSense to behave more intelligently in large projects:

1. **Persistent Index Cache**  
   Save the symbol index and reuse it across sessions – do not re-index the entire project from scratch when VS Code restarts. Clangd already supports this with its `.clangd/index/` folder.

2. **Git-Aware Incremental Indexing**  
   Integrate with Git to detect which files have changed between sessions or branch switches. This would allow VS Code to re-index only the changed files.

3. **Smart Index Resume**  
   Indexing should resume from the last known state even if interrupted (e.g. via VS Code crash, or reboot).

4. **Optional Index Directory Location Setting**  
   Allow users to specify a location where IntelliSense index data is stored – useful for SSDs, network drives, or project-specific cache directories.

---

### Workarounds Tried

- `""C_Cpp.default.compileCommands""` is correctly set.
- `""C_Cpp.intelliSenseEngine"": ""Default""` (also tried ""Disabled"" with clangd).
- `""files.watcherExclude""` is used to reduce load.
- `""terminal.integrated.gpuAcceleration"": ""off""` is set.
- Switching to `clangd` helps slightly but still re-indexes every time.

---

### Related Ideas

- Similar to how `clangd` keeps a persistent index in `.clangd/index`
- `cquery`, `ccls` and others support persistent and incremental indexing
- Git-aware indexing is used in some LSPs and build systems (e.g. Bazel, Buck)

---

### Request

Please consider improving IntelliSense in large projects by:
- Adding persistent indexing support
- Making the index cacheable across sessions
- Using Git or file-hash comparisons to do incremental re-indexing
- Providing users control over the index behavior via settings

This would dramatically improve performance and usability for large embedded and automotive C/C++ projects.

Would be really good to have this fixed. regardless of what I try it helps very little or not at all.
I gave some suggestions but if you have a better or a path forward then up to you mine are just suggestions here.

",2
Add Audible Sound Notification When Copilot Completes Suggestion or Awaits User Action,"I think it'd be convenient if users could enable an audible sound notification when GitHub Copilot finishes executing instructions in the hat sidebar, or it reaches a state where it is awaiting user confirmation (e.g. when it needs permission to execute a command).",0
GPU Acceleration rendering issues and a lot more.,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->

Does this issue occur when all extensions are disabled?: **Yes**


### VS Code Version

- Version: 1.101.2 (user setup)  
- Commit: `2901c5ac6db8a986a5666c3af51ff804d05af0d4`  
- Date: 2025-06-24T20:27:15.391Z  
- Electron: 35.5.1  
- ElectronBuildId: 11727614  
- Chromium: 134.0.6998.205  
- Node.js: 22.15.1  
- V8: 13.4.114.21-electron.0  

---

### OS Version

- OS: `Windows_NT x64 10.0.26100`

---

### Bug Description

When **GPU acceleration** is **ON**, the editor has multiple rendering issues:
- Text lines **overlap and bleed** into each other.
- The cursor sometimes **appears in the wrong position**.
- **Font ligatures** do not render correctly.
- The **horizontal scrollbar disappears** when text overflows (with `word wrap` OFF).

with **GPU acceleration ON** 

<img width=""1532"" height=""331"" alt=""Image"" src=""https://github.com/user-attachments/assets/cabe4518-0e65-44cf-b5c6-d1e3f55a6e6d"" />

Example with **GPU acceleration OFF**

<img width=""1552"" height=""782"" alt=""Image"" src=""https://github.com/user-attachments/assets/e640b79f-b083-40f8-a866-9eb1e693f688"" />

---

### Steps to Reproduce

1. Enable GPU acceleration:  
   ```json
   ""editor.experimentalGpuAcceleration"": ""on""
   ````
2. Open any source code file with long lines and ligatures enabled.

3. Scroll, resize, or edit the text.
---

",0
GPT 4.1 in Ask Mode can not figure out if a backend route is attaching fields in a response between different responses,"
Type: <b>Bug</b>

No queue status indicators appear in the Existing Batches table because the backend is not actually attaching the queue status fields (queue_pending, queue_queued, etc.) to the batch objects returned by the /batches or /api/batches routes. As a result, the template receives batch data without these fields, so the frontend has nothing to display for queue status in the Existing Batches table.

Without suggesting debugging, or generating a wasteful generic response (using if, may, typically, such as, ensures, etc): show me where the backend is not actually attaching the queue status fields (queue_pending, queue_queued, etc.) to the batch objects returned by the /batches or /api/batches routes.

GitHub Copilot
In your backend, both /batches and /api/batches routes do attach the queue status fields (queue_pending, queue_queued, etc.) to each batch object in the batch_data list that is passed to the frontend.

My two previous responses are inconsistent because the first claimed the backend was not attaching the queue status fields (queue_pending, queue_queued, etc.) to the batch objects, while the second correctly pointed out that both /batches and /api/batches do attach these fields to each batch object in the batch_data list. The actual code in app.py clearly shows these fields being included and passed to the frontend, so the first response was incorrect. The correct explanation is that the backend does provide these fields, and the issue must be elsewhere.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i5-11300H @ 3.10GHz (8 x 3110)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.84GB (5.58GB free)|
|Process Argv|--crash-reporter-id 02cb00ca-9236-4c15-979d-8cd878709bc1|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyonecf:30548226
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
d2249276:31341129
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335814
nes-diff-11:31337487
nes-set-on:31340697
6abeh943:31336334
yijiwantestdri0626-c:31336931
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
Consider tracking all terminals in `terminalAndTaskState`,We currently only track `Copilot` created terminals. We should probably track all. ,0
Bug,"
Type: <b>Bug</b>

no response

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19041
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM)2 Duo CPU     P8600  @ 2.40GHz (2 x 2394)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: disabled_off<br>opengl: enabled_on<br>rasterization: unavailable_off<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: unavailable_off<br>video_encode: unavailable_off<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: unavailable_off<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|5.90GB (1.54GB free)|
|Process Argv|C:\\xampp\\htdocs\\church2v7|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
0g0a1943:31341127
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
4f60g487:31327383
nes-diff-11:31337487
testaa123:31335226
6abeh943:31336334
yijiwantestdri0626-c:31336931
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",1
"The model unexpectedly did not return a response, which may indicate a service issue. Please report a bug.","
Type: <b>Bug</b>

The model unexpectedly did not return a response, which may indicate a service issue. Please report a bug.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M4 (10 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|2, 2, 2|
|Memory (System)|16.00GB (0.09GB free)|
|Process Argv|--crash-reporter-id ceee6b1c-eb84-4979-9b8d-d9b495e7289b|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492:30256859
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
convertlamdat:31329272
d2249276:31341129
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
b6b4d950:31327385
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
dont make changes,"
Type: <b>Bug</b>

Have before made the changes directly in scripts but now it whont

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-10510U CPU @ 1.80GHz (8 x 2304)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.81GB (3.57GB free)|
|Process Argv|--crash-reporter-id ce9dd1e9-e791-4090-946a-41b9917c6deb|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
jjjhb125:31275178
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
j6a0c355:31339598
0g0a1943:31341127
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
testaa123cf:31335227
6abeh943:31336334
yijiwantestdri0626-t:31336930
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
403,"
Type: <b>Bug</b>

getting 403

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i5-1145G7 @ 2.60GHz (8 x 2611)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|31.25GB (15.60GB free)|
|Process Argv|--crash-reporter-id 6b45eb3c-98cd-456b-84a1-e1f2e55360e0|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
9d2cg352:31339597
convertlamdat:31329272
jhi8h917:31341130
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
"The model unexpectedly did not return a response, which may indicate a service issue. Please report a bug.","
Type: <b>Bug</b>

I dont know how to reproduce it


Extension version: 0.28.5
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M2 (8 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|13, 9, 6|
|Memory (System)|8.00GB (0.08GB free)|
|Process Argv|--crash-reporter-id 7a1b20b0-5620-454d-9dae-956713fcbf67|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
j6a0c355:31339598
c4b42873:31341128
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
b6b4d950:31327385
nes-diff-11:31337487
testaa123:31335226
6abeh943:31336334
yijiwantestdri0626-t:31336930
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
update docs to reflect new terminal APIs,"once `onTerminal`, `onTerminalShellIntegration`, and `TaskExecution.terminal` are merged, we should indicate them in our docs

CC @tyriar",0
.,"
Type: <b>Bug</b>

ta de sacanagem que vou pagar isso kkkkkkkkkkkkk

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.22631
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 7 5700U with Radeon Graphics          (16 x 1797)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|17.85GB (4.86GB free)|
|Process Argv|--crash-reporter-id d9523387-be51-43a4-877e-b8769d929cbf|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
6gi0g917:31259952
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
c4b42873:31341128
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
4f60g487:31327383
nes-diff-11:31337487
onetestforazureexpcf:31335614
6abeh943:31336334
yijiwantestdri0626-c:31336931
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Auto-format easilty breaks frontmatter tools array,"1. Configure multiple tools in a chatmode
2. Save with auto-format enabled
3. 🐛 tools array gets broken down across lines, invalid syntax

![Image](https://github.com/user-attachments/assets/d2f220c7-cd81-4f30-b5e8-4297824fa7e7)

Lists would be a more resilient format, and are common in frontmatter:

```
description: Analyze survey
tools:
  - editFiles
  - kusto
  - kusto_get_entities_schema
  - kusto_get_table_schema
  - kusto_query
  - kusto_sample_function_data
  - kusto_sample_table_data
```",0
Copilot gets stuck,"
Type: <b>Bug</b>

Copilot gets stuck.

after asking a few questions it just hangs on generation of code.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M3 Pro (11 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|3, 3, 2|
|Memory (System)|36.00GB (0.83GB free)|
|Process Argv|--crash-reporter-id 856b280c-917d-4c77-943a-81b0a0013703 --crash-reporter-id 856b280c-917d-4c77-943a-81b0a0013703|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
c4b42873:31341128
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Prompt to add Custom Instructions,"We have the new awesome ""Create custom instructions"" from the chat settings button in Insiders. It would be great if when I open a workspace it auto detects if I have a custom instruction at all. If I don't then ti should prompt me to create one and then run that prompt :)",0
Add scripting interface/batch mode for Copilot Agent Mode?,"There are a lot of AI agent frameworks out there that allow you to batch the running of an AI agent to help solve software problems.  Also, one needs to be able to call an AI agent with a prompt and a set of MCP tools to execute a task to edit code, update documentation, add tests, etc., with feedback provided by builds, running tests, running tools etc.

It would be great if the open-source Copilot could be extended to be able to run in batch mode as well were it is given a prompt, context description, a local MCP server (perhaps) and then make it go do the task.  In this mode, the agent would run without interaction with the user and would automatically apply changes to any modified files.  Examples of tasks would be:
 
* Refactor/factor out code in file `<X>` in the parts `<a>` and `<b>` so that so that it can be run in a unit test harness and add a few unit tests to call the refactored code

* Add unit tests for the factored out code to match coverage provide by `<a>` , `<b>`, `<c>`, ...

If open-source Copilot were to support such a batch/scripting mode, we could just go all-in on VSCode Copilot Agent mode both for our research and deploying AI agents to end users in our institution.

Along with:

* #254463 

we would have the basic foundation we need to our work with AI agents for software development in our institution.

(Without this and #254463, we will have to look else were.)
",1
Agent mode fails to see command output if output pauses for more than 1 second,"I think this might be the real cause of #[7553](https://github.com/microsoft/vscode-copilot-release/issues/7553). On ...

- Powershell v5.1.22621.4391
- GitHub Copilot extension v1.338.0
- GitHub Copilot Chat extension v0.28.5
- vscode:

```
Version: 1.101.2 (user setup)
Commit: 2901c5ac6db8a986a5666c3af51ff804d05af0d4
Date: 2025-06-24T20:27:15.391Z
Electron: 35.5.1
ElectronBuildId: 11727614
Chromium: 134.0.6998.205
Node.js: 22.15.1
V8: 13.4.114.21-electron.0
OS: Windows_NT x64 10.0.22631
```

...

I find I can ask chat (in Agent mode, with GPT-4.1) to execute

```powershell
Start-Sleep -Milliseconds 950; Write-Output (Get-Random)
```

or

```powershell
Start-Sleep -Milliseconds 1050; Write-Output (Get-Random)
```

and the chat will robustly be able to tell me the first random number, but not the second. Both execute fine and I can see the outputs myself. This is *not* a matter of the length of the output. `Get-ChildItem -Recurse` produces 1,110 lines of output, but the chat is able to read back to me the last line correctly.

It appears that the command tool doesn't actually wait for the command to complete (process closed, or terminal control returned, or however that is defined), but instead just waits for output to pause for 1 second, which is a bonkers assumption IMHO--I would say _most_ of my terminal commands include a second or more of dead air.

Hovering over the octocat icon in my list of terminals shows this info:

```
Copilot

Process ID (PID): 24312

Command line: C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command 'try { . ""c:\Users\tbertala\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1"" } catch {}'

The following extensions have contributed to this terminal's environment:

Git: Enables the following features: git auth provider
GitHub Copilot Chat: Enables use of the copilot-debug command in the terminal.
Shell integration: Rich

Seen sequences: P, A, B, E, C, D
Prompt input: |
Show Environment Contributions
Hide Details
```

Where I would guess that `Shell integration: Rich` indicates that shell integration (which might provide proper command completion telemetry) ius active.",0
Responds with agent commands instead of executing them,"
Type: <b>Bug</b>

Not reliably reproducible.
I asked it to find all uses of dates, times or datetimes and replace them with the ISO format. The codebase is huge and it was already in the second iteration when I got this response:

```xml
<parameter name=""newString""> handleLogMessage(AbstractLogEngine::Category::eInfo, QTime::currentTime(), """", QString(""New log session started { date: %1 }"").arg(QDateTime::currentDateTime().toString(Qt::ISODate)));</parameter> <parameter name=""oldString""> handleLogMessage(AbstractLogEngine::Category::eInfo, QTime::currentTime(), """", QString(""New log session started { date: %1 }"").arg(QDateTime::currentDateTime().toString()));</parameter> <invoke name=""replace_string_in_file""> <parameter name=""filePath"">${actualFilePath}/FileLogEngine.cpp</parameter> </invoke>
```

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Linux x64 6.11.0-29-generic
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|13th Gen Intel(R) Core(TM) i7-13700H (20 x 3252)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: disabled_off<br>webnn: disabled_off|
|Load (avg)|1, 0, 0|
|Memory (System)|31.01GB (7.26GB free)|
|Process Argv|--new-window /media/user/Ventoy/database --crash-reporter-id e69e6099-53ef-4413-83d2-fc55883feacf|
|Screen Reader|no|
|VM|67%|
|DESKTOP_SESSION|ubuntu|
|XDG_CURRENT_DESKTOP|Unity|
|XDG_SESSION_DESKTOP|ubuntu|
|XDG_SESSION_TYPE|x11|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492:30256859
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
9d2cg352:31339597
c4b42873:31341128
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
82j33506:31327384
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
Not generating,"
Type: <b>Bug</b>

the copilot is not generating and enhancing my code according to my prompt

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|13th Gen Intel(R) Core(TM) i7-13620H (16 x 2918)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.71GB (0.76GB free)|
|Process Argv|. --crash-reporter-id 3bbb9a09-f567-4ce7-a435-5296d3e5d6b1|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492:30256859
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
jjjhb125:31275178
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
9d2cg352:31339597
c4b42873:31341128
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
It didn't edited anything,"
Type: <b>Bug</b>

No edits done multiple times even after restart of code, restart ts or reload window

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Linux x64 6.14.0-23-generic snap
Modes:


<!-- generated by issue reporter -->",0
Maximizing and restoring the window breaks the Python debug console,"
Type: <b>Bug</b>

1. Open new vscode window 
2. ctrl+shift+p > Create: New file... > create test.py
3. write `print(""hi"")` in test.py
4. ctrl+F5 > Python Debug
5. repeat Win+Up > Win+Down until the debug output disappears (usually <= 5 times) (this also happens with the top right buttons)

After this, you can run debug but cannot see the debug output anymore.

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.22631
Modes: Unsupported

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|13th Gen Intel(R) Core(TM) i9-13900T (32 x 1114)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|31.76GB (18.62GB free)|
|Process Argv|C:\\Users\\pc\\Documents\\vscode_workspaces\\memo.code-workspace C:\\Users\\pc\\Documents\\vscode_workspaces\\english.code-workspace --crash-reporter-id 7f713bd4-c49a-4836-8f9e-f9ed704e6430|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (3)</summary>

Extension|Author (truncated)|Version
---|---|---
debugpy|ms-|2025.8.0
python|ms-|2025.8.0
vscode-pylance|ms-|2025.6.2


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
0g0a1943:31341127
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
testaa123cf:31335227
6abeh943:31336334
yijiwantestdri0626-c:31336931
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
Will open-source Copilot allow not logging into GitHub account and running only local models?,"Will the [open-source Copilot extension](https://github.com/microsoft/vscode-copilot-chat) (and when it gets integrated into the open-source VSCode sources)  allow a model where the user does not need to log into a GitHub account and can set up Copilot to only call locally hosted models? There are critical features to allow internal usage of VSCode + Copilot View, Edit, and Agent Modes in many institutions.

It is really important to know the answer to these questions up front before our institution invests any time with this VSCode Copilot open-source code base.

",0
Improve sub-shell experience in terminal inline chat,"The current experience of using inline chat in sub-shell is not ideal, in a way that it does not realize that user is in a sub-shell. This is even though the shell type has changed.

For example, if I were to enter Python sub-shell from zsh, and then request inline chat to `print hello world in Python`, it would give me a suggestion or command of `python3 -c ""print('hello world')""`, which is not good.

This is not good because we are already in a Python REPL or Python sub-shell, and `python3 -c` is completely unnecessary.

<img width=""1050"" alt=""Image"" src=""https://github.com/user-attachments/assets/0f76b593-4ab3-4ef3-a91b-47a96fc330fd"" />

We should give context for inline chat about shell type, or figure out if its missing context for sub-shell scenarios. Or figure out what else is causing this issue.",1
"Activating Copilot causes other extensions' Quick Fix entries to be hidden, even after deactivating Coilot","Re-reporting @conradoplg's https://github.com/microsoft/vscode-copilot-release/issues/5502 since that repository has been deprecated and will soon be archived as read-only:

Before installing and activating the Copilot extension for VSCode, the ""[Quick Fix](https://code.visualstudio.com/docs/editing/refactoring)"" lightbulb appeared in the editor, and linter outputs in the ""Problems"" pane could be clicked on to reveal suggested fixes. For example, PHPCS would mark a whitespace error, and a PHPCF fix would be listed in the quick-fix locations.

After installing and activating Copilot, the only available options available when right-clicking in the ""Problems"" pane are ""Fix using Copilot"", ""Explain using Copilot"", and ""Copy message"".

After deactivating Copilot, the only options available when right-clicking in the ""Problems"" pane are ""Copy"" and ""Copy message"". The items available before activating Copilot are no longer available, even after deactivating Copilot.

As reported in https://github.com/microsoft/vscode-copilot-release/issues/5502 this affects many different code languages, many different linter/code-standards extensions, many different versions of VSCode, and many different versions of the Copilot Extension. However, this behavior is not consistent, and reproduction is unreliable.",1
Render AI snapshots nodes in SCM Graph,"Every time a turn begins in chat, we create a snapshot. It is then possible to revert to that snapshot via the ""Undo Last Request""/""Redo Last Request"" actions in the chat view. It would be interesting to render the snapshots in the SCM Graph view, we have found that folks don't feel super confident hitting the undo buttons. The state is available in `ChatEditingSession._linearHistory` and might need a bit of plumbing to expose as a public field.

<img width=""1920"" alt=""Image"" src=""https://github.com/user-attachments/assets/8b7186d9-e6f9-42fd-be21-51454c64a1a4"" />",0
"Render ""AI Changes"" node in SCM","I often forget to act on AI changes (i.e. Keep/Discard) and I end up committing files which still have pending decisions. I often even create PRs and then the Keep/Discard state sticks around for a long time, up until I create a new chat.

It would be interesting to explore:
* rendering unconfirmed AI changes in the SCM viewlet
* automatically marking ""keep"" for AI changes that are staged

To clarify, I am not arguing for changing the way things work, but rather to surface the existing data also in the SCM viewlet. The existing data can be reached via `IChatEditingService > IChatEditingSession > entries`

![Image](https://github.com/user-attachments/assets/5ef50e20-f3e8-4723-87ff-80b1d594638d)

fyi @joaomoreno ",1
Integrated Terminal  screen's getting blank after ~15 min idle time,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 
- OS Version: 

Steps to Reproduce:

This issue was addressed few weeks ago, but without a resolution. 

1.  open an integrated  terminal in VS code, and type commend
2. after stop typing, the content of the terminal gets blank after ~15 of not typing in the  terminal, or if not keeping the text editor open. 
3. Resizing the terminal back and forth, does not bring the content of the terminal before the Terminal is blanked out, or vanished. 
4. The only option is to start a new terminal and repeat my workspace settings, the every time (!), this happens with the terminal. 
5. The history of previous command in the vanished content is kept, however, this effect of the terminal blanking out is extremely annoying and disruptive.
6. I did run Start Extension Bisect from the command palette and the the Terminal content vanishing effect has been reproduced. The outcome of it with the message: The problem in the Code  

Please fit it. 
 
I am running VS code in MacOs and open VS Desktop in Azure ML workspace on Linux Ubuntu, I believe.

Thanks! 
       
",0
Can't debug any code ,"
Type: <b>Bug</b>

This is to the service team, Sir/mam, I have been using VS code for a long time and it's a good platform for coding but recently i am not able to run any of my code i don't know what's the issue. I have all the recommended extension but can't debug code of any language. I hope you will fix it or tell me what's the issue i am facing because all of my work is done from here.


VS Code version: Code 1.100.0 (19e0f9e681ecb8e5c09d8784acaa601316ca4571, 2025-05-07T12:48:53.763Z)
OS version: Windows_NT x64 10.0.22631
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 3 3250U with Radeon Graphics          (4 x 2595)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|5.94GB (0.35GB free)|
|Process Argv||
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (127)</summary>

Extension|Author (truncated)|Version
---|---|---
es7-javascript-class-snippets|abr|0.4.0
easy-cpp-projects|ACh|2.0.0
project-manager|ale|12.8.0
auto-mongodb|All|0.1.0
Happy-Flasker|ape|0.0.7
vscode-django|bat|1.15.0
node-json-autocomplete|bhs|0.1.3
blackbox|Bla|2.8.38
blackboxagent|Bla|3.2.17
vscode-tailwindcss|bra|0.14.24
super-express|brp|0.0.14
flaskapi|bui|0.0.4
simple-react-snippets|bur|1.2.8
npm-intellisense|chr|1.4.5
path-intellisense|chr|2.10.0
flask-snippets|Cod|1.1.0
vscode-express|Com|0.0.5
flask-snippets|cst|0.1.3
dbclient-jdbc|cwe|1.4.4
vscode-database-client2|cwe|8.3.6
c-cpp-compile-run|dan|1.0.60
algorithm-mnemonics-vscode|dav|1.0.3
vscode-eslint|dba|3.0.10
mongodb-dly|dev|1.3.0
composer-php-vscode|DEV|1.59.17515
intelli-php-vscode|DEV|0.12.15062
phptools-vscode|DEV|1.58.17223
profiler-php-vscode|DEV|1.59.17515
python-environment-manager|don|1.2.7
python-extension-pack|don|1.7.0
es7-react-js-snippets|dsz|4.4.3
gitlens|eam|17.2.2
react-native-react-redux|EQu|2.0.6
prettier-vscode|esb|11.0.0
e-express|etu|0.0.2
vscode-express-snippets|Exp|1.1.1
cpp-class-creator|Fle|1.4.0
auto-close-tag|for|0.5.15
c-cpp-runner|fra|9.4.10
vscode-javac|geo|0.2.46
copilot|Git|1.314.0
copilot-chat|Git|0.27.3
vscode-test-explorer|hbe|2.22.1
rest-client|hum|0.25.1
elixir-ls|Jak|0.28.0
search-node-modules|jas|1.3.0
react-vscode-extension-pack|jaw|1.0.0
better-cpp-syntax|jef|1.27.1
vs-code-gen-mongo-id|Jon|1.0.1
cpptask|kay|0.0.1
vsc-python-indent|Kev|1.21.0
kintone-extension|kin|0.12.1
node-module-intellisense|lei|1.5.0
python-extension-pack|Leo|3.0.0
iis|lex|1.0.15
mongodb-support|Lin|1.0.6
vscode-python-test-adapter|lit|0.8.2
vscode-exec-node|mir|0.5.6
mongodb-vscode|mon|1.13.3
microsoft-testing|ms-|0.1.12
azure-dev|ms-|0.9.0
vscode-azure-github-copilot|ms-|0.3.268
vscode-azureappservice|ms-|0.26.2
vscode-azurecontainerapps|ms-|0.8.3
vscode-azurefunctions|ms-|1.17.2
vscode-azureresourcegroups|ms-|0.11.0
vscode-azurestaticwebapps|ms-|0.13.1
vscode-azurestorage|ms-|0.16.4
vscode-azurevirtualmachines|ms-|0.6.9
vscode-cosmosdb|ms-|0.26.0
csdevkit|ms-|1.18.25
csharp|ms-|2.72.34
vscode-dotnet-runtime|ms-|2.3.6
vscode-edge-devtools|ms-|2.1.9
autopep8|ms-|2025.2.0
debugpy|ms-|2025.6.0
flake8|ms-|2025.2.0
python|ms-|2025.7.2025050601
vscode-pylance|ms-|2025.4.1
vscode-python-envs|ms-|0.3.11841011
jupyter|ms-|2025.4.0
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.1.0
vscode-jupyter-cell-tags|ms-|0.1.9
vscode-jupyter-slideshow|ms-|0.1.6
cmake-tools|ms-|1.20.53
cpptools|ms-|1.25.3
cpptools-extension-pack|ms-|1.3.1
js-debug-nightly|ms-|2025.7.217
test-adapter-converter|ms-|0.2.1
vscode-node-azure-pack|ms-|1.6.0
vscode-react-native|msj|1.13.0
autodocstring|njp|0.6.1
outstand-elixir-ls|Out|1.0.0
polacode|pnp|0.3.4
vscode-css-peek|pra|4.4.3
flask-builder|Rab|1.1.5
java|red|1.41.1
LiveServer|rit|5.7.9
es7-react-js-snippets|rod|1.9.3
mongo-snippets-for-node-js|roe|1.3.12
sqlite-snippet|roh|2.0.1
html5-boilerplate|sid|1.1.1
express-mongo-snippets|snl|0.0.2
swdc-vscode|sof|2.8.3
python-pack|Swe|0.1.15
vscode-ai-foundry|Tea|0.7.0
flask-vgs|tho|0.1.11
cmake|twx|0.0.17
vscode-lldb|vad|1.11.4
intellicode-api-usage-examples|Vis|0.2.9
vscodeintellicode|Vis|1.3.2
vscode-gradle|vsc|3.16.4
vscode-java-debug|vsc|0.58.1
vscode-java-dependency|vsc|0.24.1
vscode-java-pack|vsc|0.29.0
vscode-maven|vsc|0.44.0
vscode-spring-initializr|vsc|0.11.2
Java-extension-pack|wal|1.0.0
jinja-snippets-flask|Was|1.0.4
jinja|who|0.0.8
JavaScriptSnippets|xab|1.8.0
ReactSnippets|xab|2.4.0
php-debug|xde|1.36.1
just-print-it|Xua|1.1.2
vsmarketplace-badges|Xua|1.0.3
tailwind-snippets|Zar|1.0.2

(1 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
c4b42873:31341128
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
82j33506:31327384
nes-diff-11:31337487
testaa123cf:31335227
onetestforazureexpcf:31335614
6abeh943:31336334
yijiwantestdri0626-c:31336931
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",1
Annotations are not surfaced in vscode when copilot agent writes to a file and blocking is turned off,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 
- OS Version: 

Steps to Reproduce:

1. Use vscode-copilot-chat in agent mode
2. Make sure GH snippy blocking is disabled
3. Type ""Implement fizzbuzz in javascript"" into the agent
4. Compare that with the same prompt when in Ask mode

Result: The same code is annotated in Ask mode but not in Agent mode

Explanation: Not sure if vscode is incapable of showing annotations in the editor window from code written by the agent, or if annotations aren't being sent by CAPI, going to look more deeply at this from the CAPI side but wanted to document
",0
ASP.NET Core project when run stuck on port 5000,"
Type: <b>Bug</b>

1 - Change default ports in launchSettings from 5000 to 80
2 - do Dotnet run
3 - it shows always listening on http://...:5000

I cant find or understand why it is stuck on port 5000

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz (8 x 3408)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|31.94GB (13.68GB free)|
|Process Argv|--folder-uri file:///c%3A/Users/salam.SALAM/Source/Repos/Les%20Jackson-FullCourse/ColourAPI --crash-reporter-id 3c6cc184-cdef-4bfc-96ad-fd651be59f47|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (180)</summary>

Extension|Author (truncated)|Version
---|---|---
terraform|4op|0.2.5
better-comments|aar|3.0.2
add-reference|adr|1.0.2
arepl|alm|3.0.0
amazon-q-vscode|ama|1.81.0
aws-toolkit-vscode|ama|3.68.0
vscode-color|ans|0.4.5
swagger-viewer|Arj|3.1.2
vscode-azureautomation|azu|1.2.5
azurite|Azu|3.34.0
armview|ben|0.4.6
docs-view|bie|0.1.0
lit-html|bie|1.11.1
xml2json|bui|1.2.5
multi-cursor-case-preserve|Car|1.0.5
turbo-console-log|Cha|3.1.1
vscode-better-align|cho|1.4.2
npm-intellisense|chr|1.4.5
path-intellisense|chr|2.10.0
regex|chr|0.6.0
vscode-eslint|dba|3.0.10
composer-php-vscode|DEV|1.59.17515
intelli-php-vscode|DEV|0.12.15062
phptools-vscode|DEV|1.59.17515
profiler-php-vscode|DEV|1.59.17515
docker|doc|0.11.0
githistory|don|0.6.20
xml|Dot|2.5.1
gitlens|eam|17.2.2
vscode-html-css|ecm|2.0.13
prettier-vscode|esb|11.0.0
comment-anchors|Exo|1.10.4
vscode-diff|fab|2.1.2
css-stacking-contexts|fel|1.0.15
php-intellisense|fel|2.3.14
vscode-solution-explorer|fer|0.9.1
vscode-firefox-debug|fir|2.15.0
auto-close-tag|for|0.5.15
azure-storage-explorer|for|0.1.2
code-runner|for|0.12.2
docker-explorer|for|0.1.7
dotnet|for|0.0.4
dotnet-test-explorer|for|0.7.8
vscode-mysql|for|0.5.0
toggle-zen-mode|fud|1.1.2
codespaces|Git|1.17.3
copilot|Git|1.338.0
copilot-chat|Git|0.28.5
remotehub|Git|0.64.0
vscode-pull-request-github|Git|0.112.0
go|gol|0.48.0
hcl|has|0.6.0
terraform|has|2.34.5
vscode-test-explorer|hbe|2.22.1
vscode-guid|hea|1.9.0
AzureFunctionsSnippets|hha|0.1.0
vscode-htmlhint|HTM|1.11.1
rest-client|hum|0.25.1
csharpnewclass|hun|1.0.0
icon-fonts|idl|2.5.4
vscode-tfs|iva|0.7.2
RelativePath|jak|1.5.0
polacode-2019|jef|0.6.2
vscode-peacock|joh|4.2.2
vscode-csharp-snippets|jor|1.1.0
krinql-vscode|Kri|0.1.1
ftp-sync|luk|0.3.9
start-git-bash|McC|1.2.1
rainbow-csv|mec|3.20.0
git-graph|mhu|1.30.0
mongodb-vscode|mon|1.13.3
azure-pipelines|ms-|1.249.0
microsoft-testing|ms-|0.1.17
vscode-azurecache|ms-|0.1.0
azure-dev|ms-|0.9.0
vscode-apimanagement|ms-|1.2.0
vscode-azure-github-copilot|ms-|1.0.42
vscode-azureappservice|ms-|0.26.2
vscode-azurecontainerapps|ms-|0.8.3
vscode-azurefunctions|ms-|1.17.3
vscode-azurelogicapps|ms-|5.109.14
vscode-azureresourcegroups|ms-|0.11.0
vscode-azurestaticwebapps|ms-|0.13.1
vscode-azurestorage|ms-|0.16.5
vscode-azureterraform|ms-|0.5.0
vscode-azurevirtualmachines|ms-|0.6.9
vscode-bicep|ms-|0.36.1
vscode-containers|ms-|2.0.3
vscode-cosmosdb|ms-|0.26.0
vscode-docker|ms-|2.0.0
vscode-logicapps|ms-|1.2.9
csdevkit|ms-|1.30.32
csharp|ms-|2.84.19
dotnet-interactive-vscode|ms-|1.0.6323010
vscode-dotnet-runtime|ms-|2.3.6
vscodeintellicode-csharp|ms-|2.2.3
vscode-kubernetes-tools|ms-|1.3.25
data-workspace-vscode|ms-|0.6.3
mssql|ms-|1.33.0
sql-bindings-vscode|ms-|0.4.1
sql-database-projects-vscode|ms-|1.5.3
playwright|ms-|1.1.15
debugpy|ms-|2025.8.0
isort|ms-|2025.0.0
python|ms-|2025.8.0
vscode-pylance|ms-|2025.6.2
jupyter|ms-|2025.5.0
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.1.0
vscode-jupyter-cell-tags|ms-|0.1.9
vscode-jupyter-slideshow|ms-|0.1.6
remote-containers|ms-|0.417.0
remote-ssh|ms-|0.120.0
remote-ssh-edit|ms-|0.87.0
remote-wsl|ms-|0.99.0
vscode-remote-extensionpack|ms-|0.26.0
azure-account|ms-|0.12.0
azure-repos|ms-|0.40.0
azurecli|ms-|0.6.0
cmake-tools|ms-|1.20.53
cpptools|ms-|1.26.3
cpptools-extension-pack|ms-|1.3.1
js-debug-nightly|ms-|2025.7.217
live-server|ms-|0.4.15
powershell|ms-|2025.2.0
remote-explorer|ms-|0.5.0
remote-repositories|ms-|0.42.0
remote-server|ms-|1.5.2
test-adapter-converter|ms-|0.2.1
vscode-node-azure-pack|ms-|1.6.0
team|ms-|1.161.1
windows-ai-studio|ms-|0.16.0
azurerm-vscode-tools|msa|0.15.15
debugger-for-edge|msj|1.0.15
sqltools|mtx|0.28.4
color-highlight|nau|2.8.0
live-server-preview|neg|0.1.4
gitdownloadazurerepos|nei|1.0.16
indent-rainbow|ode|8.3.1
refactor|p42|3.0.1
advanced-new-file|pat|1.2.2
excalidraw-editor|pom|3.9.0
vscode-css-peek|pra|4.4.3
live-preview|pro|0.0.3
puppet-vscode|pup|1.5.5
quicktype|qui|23.0.170
addlocalnetreferences|Raf|0.4.8
Csharp-ASPNETCore|rah|1.11.0
vscode-thunder-client|ran|2.35.3
ansible|red|25.7.0
java|red|1.43.1
vscode-commons|red|0.0.6
vscode-xml|red|0.29.0
vscode-yaml|red|1.18.0
xill-language|roe|0.0.5
partial-diff|ryu|1.4.3
totvs-coverage-analysis|shi|0.0.6
snyk-vulnerability-scanner|sny|2.22.0
sonarlint-vscode|Son|4.25.1
vscode-odata|sta|0.1.0
vscode-ai-foundry|Tea|0.7.0
bootstrap4-vscode|the|6.1.0
cmake|twx|0.0.17
highlight-matching-tag|vin|0.11.0
intellicode-api-usage-examples|Vis|0.2.9
vscodeintellicode|Vis|1.3.2
vscode-gradle|vsc|3.16.4
vscode-java-debug|vsc|0.58.2
vscode-java-dependency|vsc|0.24.1
vscode-java-pack|vsc|0.29.2
vscode-java-test|vsc|0.43.1
vscode-maven|vsc|0.44.0
vscode-icons|vsc|12.13.0
iis-express|war|1.5.0
php-debug|xde|1.36.1
php-pack|xde|1.0.3
markdown-all-in-one|yzh|3.6.3
bootstrap-v4-snippets|Zac|1.1.3
material-theme|zhu|3.19.0
html-css-class-completion|Zig|1.20.0

(8 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
j6a0c355:31339598
d2249276:31341129
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
onetestforazureexpcf:31335614
6abeh943:31336334
yijiwantestdri0626-c:31336931
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Speaking random letters with no meaning,"
Type: <b>Bug</b>

munkind a ern statistics isője sve""
athens; ""BB thor use, ""a"" on the time autod""ve, "" consuntin"" a"" time round by the ""a""be a"" YOLO ""a ""to the ""a########las ""a"" ""bтой ""a"" ootskumwihirpletely, ""a"" ""tare ""a"" ""a"" ""a Alert"" a"" a ""he"" be a"" a"" of ""a"" ""a"" violation"" hokchromissar ve"" be ""a"" chair"" to warrior"" a"" from a branded ""a"" ""a"" ""a"" 's"" a"" a"" be ""a"" a"" ""a ""onings"" a"" of ""a ""a"" ""a"" a "" noble"" a"" a ""a"" a"" a ""a ""a"" ""a ""a"" ""a"" a"").-in""a"" a"" ""a ""a"" a"" a wishes"" a hospice service ""a"" a"" a"" a"" a"" a"" a"" a"" a僧生als buckhorned sailor aseaman-flesh-poundать mars a"" a"" a"" a"" a"" a revenge a"" a"" a"" a"" a"" a andere ""a"" a pablo; ""a"" 3322: ""a"" a knee Heavy ""a"" a precedence mission Sa"" sa vraaaaaareidsiachers at ""a"" mid a"" a"" a"" a"" a"" about ""a"" r C'est ""a"" oles"" a"" a esto"" a"" 5,""3"" a"" ""a"" ""a recruit"" Internacional ias ""a"" ""a"" ""a', ""'a' a"" ""a"" ""rying"" ""a"" ""a"" monetary ""a"" ""a"" a""afge accurobjin-a-days work ""система a"" a"" a"" a"" a time 7 a"" ""a s, ""a"" a"" a"" clearance a"" a/S ervic3, stay 8demographics asize"" a Sussex a 3, 8a a"" aınıngram 1epiccurved a 4, 2conflictovih a 4 a"" a"" a, a,,a"" a allegedly a"" adduce a 5 a"", (e.g., has a 1.a a"" a) and coordFame

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-9400F CPU @ 2.90GHz (6 x 2904)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: unavailable_off<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: unavailable_off<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|7.95GB (2.31GB free)|
|Process Argv|--crash-reporter-id 974efab1-b1f8-4e85-b82e-6cee6347056c|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551:31249599
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
0g0a1943:31341127
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
onetestforazureexp:31335613
6abeh943:31336334
yijiwantestdri0626-c:31336931
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
braille board and voice over on mac,"
Type: <b>Bug</b>

Hello,
I’m reaching out to you because I’m a web developer and I’ve noticed some compatibility issues between VoiceOver and the braille display on mac os. When I code, the VoiceOver cursor text is not synchronized with the braille display, even when moving the cursor. This is very frustrating and negatively impacts productivity. Would it be possible to work on this and fix the bug, please? There is a strong community of blind developers who would greatly benefit from this improvement and it would open up many future opportunities.
Thank you very much.

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:
Remote OS version: Linux x64 5.15.153.1-microsoft-standard-WSL2


<!-- generated by issue reporter -->",0
Unable to manage ProjectReferences in old style CSProj file.,"
Type: <b>Bug</b>

I keep getting this series of messages:

Let me check the exact content around the ProjectReference elements: I see the issue - it looks like the `<Name>` elements got corrupted to `<n>`. Let me fix that: ...

Then it get's into a nasty set of things to try and fix it. After 1-2 minutes, I have to stop it. The XML is fine, but it's ""over thinking"" something in the backgroun.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.22631
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-10850H CPU @ 2.70GHz (12 x 2712)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|31.78GB (4.73GB free)|
|Process Argv|--crash-reporter-id 852c53c2-5ced-4a72-83a1-657f246246cd|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyonecf:30548226
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
jhi8h917:31341130
d7aab740:31338110
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335814
nes-diff-11:31337487
nes-set-on:31340697
6abeh943:31336334
yijiwantestdri0626-t:31336930
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
not getting data,"
Type: <b>Bug</b>

copilot not working for open ai

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|13th Gen Intel(R) Core(TM) i7-1355U (12 x 2611)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|31.69GB (16.35GB free)|
|Process Argv|--crash-reporter-id ccc05ecf-9c32-4964-967a-bf7d54617e91|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
vscrp:30673768
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
9d2cg352:31339597
convertlamdat:31329272
jhi8h917:31341130
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
82j33506:31327384
nes-diff-11:31337487
onetestforazureexp:31335613
6abeh943:31336334
yijiwantestdri0626-t:31336930
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
Agent mode suggests running posix commands on Windows,"Forking off from https://github.com/microsoft/vscode-copilot-release/issues/6073 which focused on fixing the `&&`/`;` confusion in Windows PowerShell.

The mention of posix commands like `wc`, `head`, `tail`, `grep` here could be giving it the wrong idea?

https://github.com/microsoft/vscode-copilot-chat/blob/acf41fc6a7d5f3c24288022df3b3e4aec423dfec/package.json#L394

Do we need a per-OS cacheable tool description?",0
request interruption problem,"
Type: <b>Performance Issue</b>

I keep experiencing freezing issues, my requests are going to waste, and this problem never ends. I don't have any internet issues with the latest version of the program.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M3 Pro (12 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|3, 2, 3|
|Memory (System)|18.00GB (0.06GB free)|
|Process Argv|--crash-reporter-id b3365ab0-25cd-4f6e-b7ba-03b02c1d54cd|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>Process Info</summary>

```
CPU %	Mem MB	   PID	Process
    7	32061759066	 50633	code
    0	14249670696	 50636	   gpu-process
    0	7124835348	 50637	   utility-network-service
    0	92622859524	 50639	window [1] (BackupController.php — orananaliziyap.com)
    0	24936923718	 50670	extension-host [1]
    0	     0	 50846	     /Users/horus/.vscode/extensions/devsense.intelli-php-vscode-0.12.15062-darwin-arm64/out/server/intelliphp.ls
    0	7124835348	 50851	     electron-nodejs (tsserver.js )
    0	7124835348	 50852	     electron-nodejs (tsserver.js )
    0	7124835348	 50861	       electron-nodejs (typingsInstaller.js typesMap.js )
    0	10687253022	 50855	     /Applications/Visual Studio Code.app/Contents/Frameworks/Code Helper (Plugin).app/Contents/MacOS/Code Helper (Plugin) /Applications/Visual Studio Code.app/Contents/Resources/app/extensions/markdown-language-features/dist/serverWorkerMain --node-ipc --clientProcessId=50670
    0	7124835348	 50856	     /Applications/Visual Studio Code.app/Contents/Frameworks/Code Helper (Plugin).app/Contents/MacOS/Code Helper (Plugin) /Applications/Visual Studio Code.app/Contents/Resources/app/extensions/json-language-features/server/dist/node/jsonServerMain --node-ipc --clientProcessId=50670
    0	3562417674	 50862	     /Users/horus/.vscode/extensions/devsense.phptools-vscode-1.59.17515-darwin-arm64/out/server/devsense.php.ls
    0	10687253022	 50882	     electron-nodejs (tailwindServer.js )
    0	10687253022	 50886	     /Applications/Visual Studio Code.app/Contents/Frameworks/Code Helper (Plugin).app/Contents/MacOS/Code Helper (Plugin) /Applications/Visual Studio Code.app/Contents/Resources/app/extensions/html-language-features/server/dist/node/htmlServerMain --node-ipc --clientProcessId=50670
    0	17812088370	 50671	shared-process
    0	     0	 65869	     /bin/ps -ax -o pid=,ppid=,pcpu=,pmem=,command=
    0	10687253022	 50672	file-watcher [1]
    0	10687253022	 50727	pty-host
    0	     0	 50731	     /bin/zsh -il
    0	     0	 52027	     /bin/zsh -il
```

</details>

<!-- generated by issue reporter -->",2
Github Copilot Chat stuck in terminal,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: N/A

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.102.0-insider (Universal)
- OS Version: Darwin arm64 24.5.0
- GHCP Chat: 0.29.2025070403 - Agent - Claude Sonnet 4

Steps to Reproduce:

1. GHCP Chat runs a multi-line command in the terminal
2. Terminal is still waiting for command to be entered, stuck on `dqoute>`
3. GHCP Chat eternally waits as still waiting for a response from the command, throbber is still there

Example command:
```
user@mac ./ % BASE_MESSAGE=""🚀 VS Code extension updated""
dquote> FULL_MESSAGE=""$BASE_MESSAGE\n\n**Changes since last version:**\n$COMMIT_LIST""
dquote> echo ""Message length: ${#FULL_MESSAGE}""
dquote> echo ""Message:""
dquote> echo ""$FULL_MESSAGE""
dquote> 
```

Slight workaround:
Send a message of `dqoute>`, GHCP Chat will recognise the terminal is hanging, and will continue.
Or attempt to manually `EOL` the terminal to enter command.

Unsure if this is the model itself, or GHCP Chat, as it can adapt if aware.
",1
Mousewheel Zoom is extremely slow on zoomLevel 1,"Type: <b>Bug</b>

1. Set window.zoomLevel to **1**.
2. Open any random text file.
3. Hold down CTRL and use the mouse wheel to zoom the text.

The zoom is *extremely* finegrained and it would require me to fully scroll my mousewheel like 4 times to properly zoom in:

https://github.com/user-attachments/assets/600c28a1-bbf5-41ce-86c4-80e8b9b69bdc

If you now set window.zoomLevel to **1.01**, a barely noticable change, then suddenly the mousewheel zoom will be MUCH quicker:

https://github.com/user-attachments/assets/ca2d8e8d-20f7-43eb-b94a-eac874c4a6e7

Every few quarters I update my vscode and try again with zoom level 1, but the bug remains unfixed, so I have to use zoom factor 1.01.

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Linux x64 6.8.0-60-generic
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i7-1185G7 @ 3.00GHz (8 x 4299)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: disabled_off<br>webnn: disabled_off|
|Load (avg)|6, 2, 1|
|Memory (System)|31.06GB (15.66GB free)|
|Process Argv||
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|plasma|
|XDG_CURRENT_DESKTOP|KDE|
|XDG_SESSION_DESKTOP|KDE|
|XDG_SESSION_TYPE|x11|
</details><details><summary>Extensions (13)</summary>

Extension|Author (truncated)|Version
---|---|---
sublime-duplicate-text|bri|0.2.5
vscode-better-align|cho|1.4.2
opensslutils|ffa|1.1.1
go|gol|0.48.0
base64|m4n|1.0.0
rainbow-csv|mec|3.20.0
graphql|mqu|0.1.2
cpptools|ms-|1.26.3
platformio-ide|pla|3.3.4
reveal|smu|1.2.7
git-blame|sol|0.2.74
even-better-toml|tam|0.21.2
sort-lines|Tyr|1.12.0


</details>
<!-- generated by issue reporter -->",1
branch protection exclude specific repository,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
I currently set VSCode branch protection to prevent direct commits to main/master branches of all repositories.
But I would like to be able to exclude specific repositories from this rule",0
kill node,"
Type: <b>Bug</b>

not recommended to kill node on wsl2 environment

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19045
Modes:
Remote OS version: Linux x64 6.6.87.2-microsoft-standard-WSL2

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 5 5600X 6-Core Processor              (12 x 3700)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|31.93GB (11.60GB free)|
|Process Argv|--folder-uri=vscode-remote://wsl+Ubuntu/home/rantarou/projects/whatsapp-automation --remote=wsl+Ubuntu --crash-reporter-id 5e4bdb83-9eb4-410d-98f6-6b26f8837072|
|Screen Reader|no|
|VM|0%|

|Item|Value|
|---|---|
|Remote|WSL: Ubuntu|
|OS|Linux x64 6.6.87.2-microsoft-standard-WSL2|
|CPUs|AMD Ryzen 5 5600X 6-Core Processor (12 x 0)|
|Memory (System)|15.58GB (11.00GB free)|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
jjjhb125:31275178
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
j6a0c355:31339598
convertlamdat:31329272
jhi8h917:31341130
i851h500:31338111
usemarketplace:31343026
nesew2to5:31336538
agentclaude:31335815
82j33506:31327384
nes-diff-11:31337487
63221493:31336333
yijiwantestdri0626-t:31336930
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Differentiation between cancellations made by user in telemetry,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: n/a
- OS Version: n/a

Kusto telemetry records two events for `responseType:”cancelled”`:
1. ""Got a cancellation error""
2. ""network request aborted""

During a debugging session we noticed user cancellations (when the user presses the stop button), are being recorded as part of the `response.error` event as “network request aborted”. As a result:

- The `Properties.reason` field should be updated to reflect “user cancelation” instead.
- ""Got a cancellation error"" should also be revised, to understand in which cases are we recording this error. It is known this is one of the top errors encountered for Sonnet 4 and GPT 4.1 models.

Steps to Reproduce:
1. Send a prompt in VS Code
2. Click cancel
3. Check the logs for `response.error` event.

cc/ @isidorn 
",1
Add setting to show tree view item icons without hover effect in Activity Bar,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
Hi,
In the Activity Bar's TreeView, I would like an option to make the right-side icons (such as star, refresh, etc.) always visible without requiring a hover action.

Currently, these icons only appear when hovering over the item, which makes it less intuitive for users to discover available actions. A setting or flag to disable this hover-only visibility would improve UX for some use cases.",0
Unexpected Multi-Line Scroll Jump After Switching Windows,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled? : Not sure

Does this issue occur frequently? : Yes

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->

- Version: 1.101.2
- Commit: 2901c5ac6db8a986a5666c3af51ff804d05af0d4
- Date: 2025-06-24T20:27:15.391Z
- Electron: 35.5.1
- ElectronBuildId: 11727614
- Chromium: 134.0.6998.205
- Node.js: 22.15.1
- V8: 13.4.114.21-electron.0
- OS: Linux x64 6.11.0-29-generic

Steps to Reproduce:

1. open vscode, and edit or read code
2. switch to another application
3. switch back
4. mouse scroll issue, page jumps by several lines at once (almost like 2-3 scroll steps in a single move) 

same issue reported in ubuntu forum: https://askubuntu.com/questions/1536211/unexpected-multi-line-scroll-jump-after-switching-windows


",0
git.detectSubmodules is not honored,"
Type: <b>Bug</b>

1. create a git repository with a git submodule in it; clone it locally
2. set git.detectSubmodules to False
3. open VS code, add the folder containing the git repository
4. go to the ""Source Control"" pane: you will see 2 entries, one for the main repository and one for the submodule, despite git.detectSubmodules being set to false



VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.5.0
Modes:
Remote OS version: Linux x64 5.14.0-570.12.1.el9_6.x86_64

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M3 Max (14 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|3, 3, 3|
|Memory (System)|36.00GB (0.86GB free)|
|Process Argv|--crash-reporter-id f521bfa4-c7a4-4023-ba2d-808d63423225|
|Screen Reader|no|
|VM|0%|

|Item|Value|
|---|---|
|Remote|SSH: mymetal2|
|OS|Linux x64 5.14.0-570.12.1.el9_6.x86_64|
|CPUs|Intel(R) Xeon(R) E-2378G CPU @ 2.80GHz (16 x 4803)|
|Memory (System)|62.68GB (52.95GB free)|
|VM|0%|
</details><details><summary>Extensions (36)</summary>

Extension|Author (truncated)|Version
---|---|---
continue|Con|1.0.15
graphql|mqu|0.1.2
remote-ssh|ms-|0.120.0
remote-ssh-edit|ms-|0.87.0
remote-explorer|ms-|0.5.0
asciidoctor-vscode|asc|3.4.2
divider|din|2.4.1
docker|doc|0.11.0
html-preview-vscode|geo|0.2.5
vscode-github-actions|git|0.27.2
go|gol|0.48.0
markdown-toc|jof|1.4.0
vscode-github-actions|me-|3.0.1
vscode-containers|ms-|2.0.3
vscode-docker|ms-|2.0.0
black-formatter|ms-|2025.2.0
debugpy|ms-|2025.8.0
flake8|ms-|2025.2.0
python|ms-|2025.8.0
vscode-pylance|ms-|2025.6.2
cmake-tools|ms-|1.20.53
cpptools|ms-|1.26.3
cpptools-extension-pack|ms-|1.3.1
live-server|ms-|0.4.15
makefile-tools|ms-|0.12.17
vscode-protolint|ple|0.8.0
vscode-yaml|red|1.18.0
rust-analyzer|rus|0.3.2527
ruby-extensions-pack|Sho|0.1.13
ruby-lsp|Sho|0.9.28
sorbet-vscode-extension|sor|0.3.43
comment-divider|sta|0.4.0
cmake|twx|0.0.17
vscode-lldb|vad|1.11.4
codetour|vsl|0.0.59
vscode-proto3|zxh|0.5.5

(1 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551:31249599
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
jhi8h917:31341130
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
4f60g487:31327383
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
vs code stuck issue,"

[prof-LbtIjOjA.main.cpuprofile.txt](https://github.com/user-attachments/files/21099070/prof-LbtIjOjA.main.cpuprofile.txt)",0
Document minimal product metadata in CONTRIBUTING.md,"On Friday I tried to develop the chat extension, and I hit this issue https://github.com/microsoft/vscode/issues/254160

We should document in our https://github.com/microsoft/vscode-copilot-chat/blob/main/CONTRIBUTING.md how to setup the needed metadata to have this working (via `product.overrides.json`)

@bpasero I am not sure what is the minimal metadata that we should suggest users to set - but ideally this does not expose any internal stuff

fyi @rebornix ",0
Cancelling coding agent in chat during commit keeps the progress notification,"1. Make a change
2. Ask `#copilotCodingAgent` to take over
3. Proceed until it asks you to commit
4. From the Chat view, use the cancel button
5. ✅ chat is canceled but progress notification stays

<img width=""2199"" height=""1975"" alt=""Image"" src=""https://github.com/user-attachments/assets/6ea15c94-068d-455d-a4da-7d4b46727e91"" />",1
Cannot create config file,"
Type: <b>Bug</b>

Promopt gpt-4.1 via chat panel for `@vscode /startDebugging`



Extension version: 0.28.5
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.5.0
Modes:
Remote OS version: Linux arm64 6.10.14-linuxkit

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M4 Pro (12 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|24, 11, 8|
|Memory (System)|24.00GB (0.09GB free)|
|Process Argv|--crash-reporter-id f2a40716-31f3-4404-8394-7ff553d057d9|
|Screen Reader|no|
|VM|0%|

|Item|Value|
|---|---|
|Remote|Dev Container: 40n Dev Container @ desktop-linux|
|OS|Linux arm64 6.10.14-linuxkit|
|CPUs|unknown (12 x 0)|
|Memory (System)|7.65GB (1.58GB free)|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492:30256859
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
jjjhb125:31275178
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
convertlamdaf:31329270
d2249276:31341129
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
82j33506:31327384
nes-diff-11:31337487
testaa123:31335226
6abeh943:31336334
yijiwantestdri0626-t:31336930
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
it lose track of what it need to be done and repeat infinitely the code until it bugs out and get and error.,"
Type: <b>Bug</b>

I have a 1900 line custom widget and i gave it a prompt to take out all the debugers and other correlated things in order to optimize the code. 
It starts well but half way throw it bugs out and start doing wierd things and after a while it repeat the code untill it bugs out and return an error. 

Extension version: 0.28.5
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin x64 24.6.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 7 3700X 8-Core Processor              (16 x 4200)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|5, 5, 5|
|Memory (System)|32.00GB (1.61GB free)|
|Process Argv|--crash-reporter-id 644b0079-672d-4bf0-bc3b-b49a5fa5502e|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492:30256859
vscod805cf:30301675
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dwnewjupytercf:31046870
pythonrstrctxt:31112756
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31303018
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
9d2cg352:31339597
d2249276:31341129
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
b6b4d950:31327385
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931

```

</details>

<!-- generated by issue reporter -->",2
Add icons before chat mode and model selection for better visual clarity,"## Feature Request: Add Icons for Chat Mode and Model Selection

### Problem Statement
Currently, the VSCode chat interface displays the chat mode (e.g., ""Agent"") and selected model (e.g., ""Claude Sonnet 4"") as plain text dropdowns. While functional, this design could benefit from visual icons to improve user experience and make it easier to quickly identify the current mode and model at a glance.

<img alt=""image1"" width=""1066"" src=""https://github.com/user-attachments/assets/879ffca8-4e6c-4a59-92e8-7d4f6105acff"" />

### Proposed Solution
Add distinctive icons before the chat mode and model selection dropdowns to enhance visual clarity:

1. **Chat Mode Icon**: Add an icon before the mode selector (currently showing ""Agent"") that represents the selected mode type
   - Agent mode could use a robot/bot icon
   - Other modes could have their own distinctive icons

2. **Model Icon**: Add an icon before the model selector (currently showing ""Claude Sonnet 4"") that represents the AI model
   - Could use a brain icon, AI chip icon, or model-specific icons
   - This would help users quickly identify which model they're currently using

### Benefits
- **Improved Visual Recognition**: Icons provide immediate visual context about the current configuration
- **Better User Experience**: Users can quickly glance and understand their current setup without reading text
- **Consistency**: Aligns with VSCode's overall design philosophy of using icons throughout the interface
- **Accessibility**: Visual indicators complement text labels for better accessibility

### Current Behavior
The chat interface shows text-only dropdowns for mode and model selection without any visual indicators.

### Expected Behavior
The chat interface would display appropriate icons alongside the text labels, making it easier to distinguish between different modes and models at a glance.

### Additional Context
This enhancement would particularly benefit users who:
- Switch between different chat modes frequently
- Use multiple AI models and need quick visual confirmation of their selection
- Prefer visual cues over text-only interfaces
- Work in environments where quick visual identification is important
",0
Search reference stack,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
While going through a list of references, sometimes I have to look at other list of references. In such a case, it helps if I could push old list of references, examine new list and then start with the old one by popping it. But there is no way to stack or cache or save the old reference lists.  Can  you please add such a feature?",0
Copilot chat adds files to fresh cloned repo as soon as its opened,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 
- Version: 1.101.1 (user setup)
Commit: 18e3a1ec544e6907be1e944a94c496e302073435
Date: 2025-06-18T13:35:12.605Z
Electron: 35.5.1
ElectronBuildId: 11727614
Chromium: 134.0.6998.205
Node.js: 22.15.1
V8: 13.4.114.21-electron.0
OS: Windows_NT x64 10.0.19045
- OS Version: 
Windows 10 WSL

Steps to Reproduce:

1. Clone a repo and swap to a feature branch
2. Get Copilot to make some changes
3. Leave the ""last files changed"" window open (the one that shows last files updated - above the chat entry)
4. Commit and push
5. Close vscode
6. Delete the repo and reclone to the main branch (which does not have those files in it)
7. Open vcode and open Copilot chat

It then proceeds to make all the changes again in the main branch for all files listed in the ""last files updated"" window.
",1
Claude Sonnet Freeze Issue,"
Type: <b>Performance Issue</b>

When asked to do a simple task in a large file the chat freezes.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M1 (8 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|4, 4, 4|
|Memory (System)|8.00GB (0.10GB free)|
|Process Argv|. --crash-reporter-id 96959c9c-9a8c-407a-818e-9b408410b8c0|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>Process Info</summary>

```
CPU %	Mem MB	   PID	Process
    3	9851624185	 40286	code
    0	4925812092	 40289	   gpu-process
    0	2814749767	 40290	   utility-network-service
    0	21814310695	 40295	window [1] (invoice_service.dart — luxpet_boutique_fullstack)
    0	7036874418	 40300	extension-host [1]
    0	2814749767	 40362	     electron-nodejs (tsserver.js )
    0	2814749767	 40363	     electron-nodejs (tsserver.js )
    0	2814749767	 40667	       electron-nodejs (typingsInstaller.js typesMap.js )
    0	2814749767	 40369	     electron-nodejs (server.js )
    0	2814749767	 40433	     /Applications/Visual Studio Code.app/Contents/Frameworks/Code Helper (Plugin).app/Contents/MacOS/Code Helper (Plugin) /Applications/Visual Studio Code.app/Contents/Resources/app/extensions/json-language-features/server/dist/node/jsonServerMain --node-ipc --clientProcessId=40300
    2	2111062325	 40523	     electron-nodejs (config.js )
    0	     0	 40868	       /usr/bin/script -t 0 /dev/null /usr/bin/arch -arm64e xcrun xcdevice observe --usb
    0	     0	 40869	         /Applications/Xcode.app/Contents/Developer/usr/bin/xcdevice observe --usb
    0	     0	 40870	       /usr/bin/script -t 0 /dev/null /usr/bin/arch -arm64e xcrun xcdevice observe --wifi
    0	     0	 40871	         /Applications/Xcode.app/Contents/Developer/usr/bin/xcdevice observe --wifi
    0	     0	 40525	     /Users/gypo/flutter/bin/cache/dart-sdk/bin/dartaotruntime --new_gen_semi_max_size=32 --new_gen_growth_factor=4 /Users/gypo/flutter/bin/cache/dart-sdk/bin/snapshots/dart_tooling_daemon_aot.dart.snapshot --machine
    0	1407374884	 40526	     /Users/gypo/flutter/bin/cache/dart-sdk/bin/dart language-server --protocol=lsp --client-id=VS-Code --client-version=3.114.2
    0	703687442	 40681	     /Users/gypo/flutter/bin/cache/dart-sdk/bin/dart devtools --machine --allow-embedding --dtd-uri ws://127.0.0.1:57953/XjZMXgKMzRw=
    0	703687442	 40735	     /Users/gypo/.vscode/extensions/redhat.java-1.43.1-darwin-arm64/jre/21.0.7-macosx-aarch64/bin/java -Dfile.encoding=UTF-8 --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.prefs/java.util.prefs=ALL-UNNAMED --add-opens=java.base/java.nio.charset=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED -classpath /Users/gypo/.vscode/extensions/vscjava.vscode-gradle-3.16.4/lib/gradle-server.jar com.github.badsyntax.gradle.GradleServer --port=57991 --startBuildServer=true --languageServerPipePath=/private/var/folders/ry/kc6gf5ds52gdmv7wcqcj9ll80000gn/T/b333dfe086cbfefcf7235064b302adae.sock --pipeName=/private/var/folders/ry/kc6gf5ds52gdmv7wcqcj9ll80000gn/T/8f53a703b3b0ed102a2f5f367e26ce19.sock --bundleDir=/Users/gypo/.vscode/extensions/vscjava.vscode-gradle-3.16.4/server
    0	1407374884	 40748	     /Users/gypo/.vscode/extensions/redhat.java-1.43.1-darwin-arm64/jre/21.0.7-macosx-aarch64/bin/java --add-modules=ALL-SYSTEM --add-opens java.base/java.util=ALL-UNNAMED --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/sun.nio.fs=ALL-UNNAMED -Declipse.application=org.eclipse.jdt.ls.core.id1 -Dosgi.bundles.defaultStartLevel=4 -Declipse.product=org.eclipse.jdt.ls.core.product -Djava.import.generatesMetadataFilesAtProjectRoot=false -DDetectVMInstallationsJob.disabled=true -Dfile.encoding=utf8 -XX:+UseParallelGC -XX:GCTimeRatio=4 -XX:AdaptiveSizePolicyWeight=90 -Dsun.zip.disableMemoryMapping=true -Xmx2G -Xms100m -Xlog:disable -javaagent:/Users/gypo/.vscode/extensions/redhat.java-1.43.1-darwin-arm64/lombok/lombok-1.18.39-4050.jar -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/Users/gypo/Library/Application Support/Code/User/workspaceStorage/d9000e5ad740e3badbdf29801909d962/redhat.java -Daether.dependencyCollector.impl=bf -jar /Users/gypo/.vscode/extensions/redhat.java-1.43.1-darwin-arm64/server/plugins/org.eclipse.equinox.launcher_1.7.0.v20250519-0528.jar -configuration /Users/gypo/Library/Application Support/Code/User/globalStorage/redhat.java/1.43.1/config_mac -data /Users/gypo/Library/Application Support/Code/User/workspaceStorage/d9000e5ad740e3badbdf29801909d962/redhat.java/jdt_ws --pipe=/private/var/folders/ry/kc6gf5ds52gdmv7wcqcj9ll80000gn/T/lsp-c6ec5a5027e8625661a53d47afc8c658.sock
    0	7036874418	 40301	shared-process
    0	     0	 40776	     /Applications/Visual Studio Code.app/Contents/Resources/app/bin/code-tunnel tunnel --accept-server-license-terms --log info --name Gypos-MacBook-Proloc --parent-process-id 40301
    0	     0	 41476	     /bin/ps -ax -o pid=,ppid=,pcpu=,pmem=,command=
    0	3518437209	 40302	file-watcher [1]
    0	3518437209	 40311	pty-host
    0	     0	 40312	     /bin/zsh -il
    0	     0	 40313	     /bin/zsh -il
```

</details>
<details>
<summary>Workspace Info</summary>

```
|  Window (invoice_service.dart — luxpet_boutique_fullstack)
|    Folder (luxpet_boutique_fullstack): more than 20281 files
|      File types: h(5453) strings(2148) swift(1110) plist(655) grpc_back(519)
|                  m(476) cc(379) png(365) json(302) c(244)
|      Conf files: package.json(2) tsconfig.json(1) launch.json(1)
|                  settings.json(1);
```

</details>
<details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
convertlamdaf:31329270
d2249276:31341129
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
b6b4d950:31327385
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
don't want to answer anymore,"
Type: <b>Bug</b>

Doesn't seem to want to reply anymore. I deleted messages to move them up in the prompts, but the same result.
I left it all night and never got a response, still stuck on ""working.""
My conversation is over, I have to start another one...

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 9 3900X 12-Core Processor             (24 x 3800)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|39.92GB (24.65GB free)|
|Process Argv|--crash-reporter-id 558eaf3f-8259-482d-a433-9892d7423468|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492:30256859
pythonvspyt551cf:31249601
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
vscrpc:30673769
962ge761:30959799
9b8hh234:30694863
2e7ec940:31000449
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
jhi8h917:31341130
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
4f60g487:31327383
nes-diff-11:31337487
testaa123cf:31335227
6abeh943:31336334
yijiwantestdri0626-c:31336931
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
non responsive,"
Type: <b>Performance Issue</b>

im just trying to talk to him but doesnt work

VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M2 (8 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|3, 3, 3|
|Memory (System)|8.00GB (0.43GB free)|
|Process Argv|--crash-reporter-id 661428c5-160a-4d2b-bf63-d74e6e219af7|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>Process Info</summary>

```
CPU %	Mem MB	   PID	Process
    8	9851624185	  9959	code
    0	4222124651	  9968	   gpu-process
    0	2111062325	  9969	   utility-network-service
    0	1407374884	  9975	pty-host
    0	     0	 41111	     /bin/zsh -il
    0	     0	 43781	     /bin/zsh -il
    0	703687442	 43841	       npm run dev
    1	703687442	 43853	         node /Users/maxurteaga/Documents/memorials/node_modules/.bin/vite
    0	     0	 43860	           /Users/maxurteaga/Documents/memorials/node_modules/@esbuild/darwin-arm64/bin/esbuild --service=0.25.5 --ping
    0	5629499534	  9977	shared-process
    0	     0	 30773	     /bin/ps -ax -o pid=,ppid=,pcpu=,pmem=,command=
    0	52776558133	 30684	window [2] (firebaseConfig.ts — memorials)
    0	1407374884	 39818	file-watcher [2]
    0	6333186976	 39819	extension-host [2]
    0	703687442	 41981	     /private/var/folders/3z/tbfy7blx0rb90fncbpnx_tf40000gn/T/AppTranslocation/159E41CE-277A-4F5E-87F5-3A7257D1C3FB/d/Visual Studio Code.app/Contents/Frameworks/Code Helper (Plugin).app/Contents/MacOS/Code Helper (Plugin) /private/var/folders/3z/tbfy7blx0rb90fncbpnx_tf40000gn/T/AppTranslocation/159E41CE-277A-4F5E-87F5-3A7257D1C3FB/d/Visual Studio Code.app/Contents/Resources/app/extensions/markdown-language-features/dist/serverWorkerMain --node-ipc --clientProcessId=39819
    0	1407374884	 41988	     /private/var/folders/3z/tbfy7blx0rb90fncbpnx_tf40000gn/T/AppTranslocation/159E41CE-277A-4F5E-87F5-3A7257D1C3FB/d/Visual Studio Code.app/Contents/Frameworks/Code Helper (Plugin).app/Contents/MacOS/Code Helper (Plugin) /private/var/folders/3z/tbfy7blx0rb90fncbpnx_tf40000gn/T/AppTranslocation/159E41CE-277A-4F5E-87F5-3A7257D1C3FB/d/Visual Studio Code.app/Contents/Resources/app/extensions/json-language-features/server/dist/node/jsonServerMain --node-ipc --clientProcessId=39819
    0	1407374884	 43295	     electron-nodejs (tsserver.js )
    0	1407374884	 43296	     electron-nodejs (tsserver.js )
    0	703687442	 43299	       electron-nodejs (typingsInstaller.js typesMap.js )
    0	703687442	 45465	     /private/var/folders/3z/tbfy7blx0rb90fncbpnx_tf40000gn/T/AppTranslocation/159E41CE-277A-4F5E-87F5-3A7257D1C3FB/d/Visual Studio Code.app/Contents/Frameworks/Code Helper (Plugin).app/Contents/MacOS/Code Helper (Plugin) /private/var/folders/3z/tbfy7blx0rb90fncbpnx_tf40000gn/T/AppTranslocation/159E41CE-277A-4F5E-87F5-3A7257D1C3FB/d/Visual Studio Code.app/Contents/Resources/app/extensions/css-language-features/server/dist/node/cssServerMain --node-ipc --clientProcessId=39819
```

</details>
<details>
<summary>Workspace Info</summary>

```
|  Window (firebaseConfig.ts — memorials)
|    Folder (memorials): 0 files
|      File types:
|      Conf files:;
```

</details>
<details><summary>Extensions (4)</summary>

Extension|Author (truncated)|Version
---|---|---
copilot|Git|1.338.0
copilot-chat|Git|0.28.5
vscode-speech|ms-|0.16.0
chatgpt|ope|0.1.1741291060


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492cf:30256860
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
jhi8h917:31341130
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
testaa123cf:31335227
onetestforazureexp:31335613
6abeh943:31336334
yijiwantestdri0626-t:31336930
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
The column number calculation in the status bar has issues when meeting the full-width characters and emoji characters,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.101.2 (system setup)
- OS Version: Windows_NT x64 10.0.19044

Steps to Reproduce:

<img width=""700"" height=""393"" alt=""Image"" src=""https://github.com/user-attachments/assets/ab834153-aff8-4fe4-b2c5-644b53c46567"" />

<img width=""700"" height=""393"" alt=""Image"" src=""https://github.com/user-attachments/assets/327e1766-c506-4dbc-bf54-0d5f35b1e3da"" />

In the current stage, the column number in the status bar of half-width characters and tab character is no issue.

<img width=""700"" height=""393"" alt=""Image"" src=""https://github.com/user-attachments/assets/36b3ea4a-38d2-44ee-bcfa-d564179fac17"" />

But for CJK full-width characters, the column number in the status bar has issues. Every full-width character should use 2 columns a.k.a. the column number in the status bar should +2 when meet every full-width characters. But the Visual Studio Code only +1 for that.

And I have done some investigations, it seems Visual Studio Code should have no issues for that because I found the following code snippet in the repository. (But the issue is existed in reality.)

https://github.com/microsoft/vscode/blob/9b592e0c0ab8ec0077e368c506f6561bbddced45/src/vs/base/common/strings.ts#L695-L739

https://github.com/microsoft/vscode/blob/9b592e0c0ab8ec0077e368c506f6561bbddced45/src/vs/editor/common/core/cursorColumns.ts#L27-L35

After I found the code snippet, I also test for the emoji characters, it has the same issue like the full-width characters.

<img width=""700"" height=""393"" alt=""Image"" src=""https://github.com/user-attachments/assets/93e06bae-96a4-47ed-ab65-c1c4a6722c3a"" />

I'm looking forward for your replies. If this issue is duplicated (I have not found the similar one in the current stage.), please note me the issue number for that. (I had created a similar issue in this repository several years ago, someone closed the issue and marked that is duplicated without any reason. Maybe someone thought that is font rendering issue, but actually not when you see the code snippet I noted.)

Kenji Mouri",0
tools 14 custom input_schema,"
Type: <b>Bug</b>

Trying to use Claude Sonnet 4 via Openrouter in agent mode. Does not work. invalid_request_error

Extension version: 0.29.2025070403
VS Code version: Code - Insiders 1.102.0-insider (Universal) (96f1890d08080f46f3b0a9424553422f04133090, 2025-07-04T15:15:02.963Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M1 Pro (10 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|4, 5, 5|
|Memory (System)|32.00GB (0.19GB free)|
|Process Argv|--crash-reporter-id 409df9b5-5005-4ff0-9cbb-0f10833d1ff6|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551:31249597
c4g48928:30535728
962ge761:30841072
h48ei257:31000450
cppperfnew:30980852
dwnewjupytercf:31046870
nativeloc1:31118317
e80f6927:31120813
dwcopilot:31158714
c3hdf307:31184662
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
g012b348:31231168
pythoneinst12:31251391
c7cif404:31309980
pythonpulldiag:31287486
6gi0g917:31259950
996jf627:31264550
pythonrdcb7:31268811
usemplatestapi:31297334
e6194696:31317039
747dc170:31275146
6518g693:31302842
generatesymbolt:31280541
convertfstringf:31280702
j97ad248:31306657
usemarketplace:31336439
0g1h6703:31329154
nes-emitfast-1:31333560
replacestringexc:31340153
6abeh943:31336334
envsdeactivate2:31338962
nes-conv-00:31337516
0927b901:31340060
jbdfg126:31340538
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
tools 14 custom input_schema,"
Type: <b>Bug</b>

Trying to use Claude Sonnet 4 via Openrouter in agent mode. Does not work. invalid_request_error

Extension version: 0.29.2025070403
VS Code version: Code - Insiders 1.102.0-insider (Universal) (96f1890d08080f46f3b0a9424553422f04133090, 2025-07-04T15:15:02.963Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M1 Pro (10 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|4, 5, 5|
|Memory (System)|32.00GB (0.19GB free)|
|Process Argv|--crash-reporter-id 409df9b5-5005-4ff0-9cbb-0f10833d1ff6|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551:31249597
c4g48928:30535728
962ge761:30841072
h48ei257:31000450
cppperfnew:30980852
dwnewjupytercf:31046870
nativeloc1:31118317
e80f6927:31120813
dwcopilot:31158714
c3hdf307:31184662
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
g012b348:31231168
pythoneinst12:31251391
c7cif404:31309980
pythonpulldiag:31287486
6gi0g917:31259950
996jf627:31264550
pythonrdcb7:31268811
usemplatestapi:31297334
e6194696:31317039
747dc170:31275146
6518g693:31302842
generatesymbolt:31280541
convertfstringf:31280702
j97ad248:31306657
usemarketplace:31336439
0g1h6703:31329154
nes-emitfast-1:31333560
replacestringexc:31340153
6abeh943:31336334
envsdeactivate2:31338962
nes-conv-00:31337516
0927b901:31340060
jbdfg126:31340538
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
The chat doesn't process anything.,"<img width=""1241"" height=""646"" alt=""Image"" src=""https://github.com/user-attachments/assets/6fbe6617-d297-432b-99d8-686274d8ddb0"" />

<img width=""221"" height=""375"" alt=""Image"" src=""https://github.com/user-attachments/assets/bcfdbed4-f229-4d83-8d20-734fd6882f45"" />

<img width=""398"" height=""326"" alt=""Image"" src=""https://github.com/user-attachments/assets/54b78b74-2e4d-4c9c-b8d9-d8b6e5eadc12"" />

Type: <b>Bug</b>

The chat isn't working, it freezes on the same task for over 30 minutes, doesn't react to anything, and this is the fourth time I've had the same problem.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details><summary>Logs</summary>
<pre>
Debug: [LanguageModelAccess] UPDATING language models
Debug: [LanguageModelAccess] DID UPDATE language models
Info: copilot token chat_enabled: true, sku: free_educational_quota
Debug: ConversationFeature: onDidAuthenticationChange has token: true
Debug: [context keys] Updating context keys.
Debug: [LanguageModelAccess] UPDATING language models
Debug: [LanguageModelAccess] DID UPDATE language models
Info: BYOK: Copilot Chat known models list fetched successfully.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: true.
Trace: [GitServiceImpl][onDidOpenRepository] Active repository: {""_repo"":{""rootUri"":{""$mid"":1,""external"":""file:///h%3A/Proyectos/Personales/trading/trading-bot-process"",""path"":""/h:/Proyectos/Personales/trading/trading-bot-process"",""scheme"":""file""},""inputBox"":{},""state"":{},""ui"":{}},""rootUri"":{""$mid"":1,""external"":""file:///h%3A/Proyectos/Personales/trading/trading-bot-process"",""path"":""/h:/Proyectos/Personales/trading/trading-bot-process"",""scheme"":""file""},""headBranchName"":""feature"",""headCommitHash"":""6b69236543a6178baaca109aea9fd19aa5a07822"",""upstreamBranchName"":""feature"",""upstreamRemote"":""origin"",""isRebasing"":true,""remotes"":[""origin""],""remoteFetchUrls"":[""https://github.com/Roddmason/trading-bot-process.git""],""changes"":{""mergeChanges"":[],""indexChanges"":[],""workingTree"":[{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}],""untrackedChanges"":[]},""_onDidChangeSignal"":{""_observers"":{},""debugName"":""Observable Signal From Event""},""headBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""headCommitHashObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""upstreamBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""upstreamRemoteObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""isRebasingObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""_checkIsIgnored"":{""_waitingTimeMs"":1000,""_queue"":[],""_timeout"":null},""_isIgnored"":{""_map"":{},""_map2"":{}}}
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Trace: [GitServiceImpl][onDidOpenRepository] Active repository: {""_repo"":{""rootUri"":{""$mid"":1,""external"":""file:///h%3A/Proyectos/Personales/trading/trading-bot-process"",""path"":""/h:/Proyectos/Personales/trading/trading-bot-process"",""scheme"":""file""},""inputBox"":{},""state"":{},""ui"":{}},""rootUri"":{""$mid"":1,""external"":""file:///h%3A/Proyectos/Personales/trading/trading-bot-process"",""path"":""/h:/Proyectos/Personales/trading/trading-bot-process"",""scheme"":""file""},""headBranchName"":""feature"",""headCommitHash"":""6b69236543a6178baaca109aea9fd19aa5a07822"",""upstreamBranchName"":""feature"",""upstreamRemote"":""origin"",""isRebasing"":true,""remotes"":[""origin""],""remoteFetchUrls"":[""https://github.com/Roddmason/trading-bot-process.git""],""changes"":{""mergeChanges"":[],""indexChanges"":[],""workingTree"":[{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}],""untrackedChanges"":[]},""_onDidChangeSignal"":{""_observers"":{},""debugName"":""Observable Signal From Event""},""headBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""headCommitHashObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""upstreamBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""upstreamRemoteObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""isRebasingObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""_checkIsIgnored"":{""_waitingTimeMs"":1000,""_queue"":[],""_timeout"":null},""_isIgnored"":{""_map"":{},""_map2"":{}}}
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: true.
Trace: [GitServiceImpl][onDidOpenRepository] Active repository: {""_repo"":{""rootUri"":{""$mid"":1,""external"":""file:///h%3A/Proyectos/Personales/trading/trading-bot-process"",""path"":""/h:/Proyectos/Personales/trading/trading-bot-process"",""scheme"":""file""},""inputBox"":{},""state"":{},""ui"":{}},""rootUri"":{""$mid"":1,""external"":""file:///h%3A/Proyectos/Personales/trading/trading-bot-process"",""path"":""/h:/Proyectos/Personales/trading/trading-bot-process"",""scheme"":""file""},""headBranchName"":""feature"",""headCommitHash"":""6b69236543a6178baaca109aea9fd19aa5a07822"",""upstreamBranchName"":""feature"",""upstreamRemote"":""origin"",""isRebasing"":true,""remotes"":[""origin""],""remoteFetchUrls"":[""https://github.com/Roddmason/trading-bot-process.git""],""changes"":{""mergeChanges"":[],""indexChanges"":[],""workingTree"":[{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}],""untrackedChanges"":[]},""_onDidChangeSignal"":{""_observers"":{},""debugName"":""Observable Signal From Event""},""headBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""headCommitHashObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""upstreamBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""upstreamRemoteObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""isRebasingObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""_checkIsIgnored"":{""_waitingTimeMs"":1000,""_queue"":[],""_timeout"":null},""_isIgnored"":{""_map"":{},""_map2"":{}}}
Trace: [GitServiceImpl][onDidOpenRepository] Active repository: {""_repo"":{""rootUri"":{""$mid"":1,""external"":""file:///h%3A/Proyectos/Personales/trading/trading-bot-process"",""path"":""/h:/Proyectos/Personales/trading/trading-bot-process"",""scheme"":""file""},""inputBox"":{},""state"":{},""ui"":{}},""rootUri"":{""$mid"":1,""external"":""file:///h%3A/Proyectos/Personales/trading/trading-bot-process"",""path"":""/h:/Proyectos/Personales/trading/trading-bot-process"",""scheme"":""file""},""headBranchName"":""feature"",""headCommitHash"":""6b69236543a6178baaca109aea9fd19aa5a07822"",""upstreamBranchName"":""feature"",""upstreamRemote"":""origin"",""isRebasing"":true,""remotes"":[""origin""],""remoteFetchUrls"":[""https://github.com/Roddmason/trading-bot-process.git""],""changes"":{""mergeChanges"":[],""indexChanges"":[],""workingTree"":[{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}],""untrackedChanges"":[]},""_onDidChangeSignal"":{""_observers"":{},""debugName"":""Observable Signal From Event""},""headBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""headCommitHashObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""upstreamBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""upstreamRemoteObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""isRebasingObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""_checkIsIgnored"":{""_waitingTimeMs"":1000,""_queue"":[],""_timeout"":null},""_isIgnored"":{""_map"":{},""_map2"":{}}}
Trace: [GitServiceImpl][onDidOpenRepository] Active repository: {""_repo"":{""rootUri"":{""$mid"":1,""external"":""file:///h%3A/Proyectos/Personales/trading/trading-bot-process"",""path"":""/h:/Proyectos/Personales/trading/trading-bot-process"",""scheme"":""file""},""inputBox"":{},""state"":{},""ui"":{}},""rootUri"":{""$mid"":1,""external"":""file:///h%3A/Proyectos/Personales/trading/trading-bot-process"",""path"":""/h:/Proyectos/Personales/trading/trading-bot-process"",""scheme"":""file""},""headBranchName"":""feature"",""headCommitHash"":""6b69236543a6178baaca109aea9fd19aa5a07822"",""upstreamBranchName"":""feature"",""upstreamRemote"":""origin"",""isRebasing"":true,""remotes"":[""origin""],""remoteFetchUrls"":[""https://github.com/Roddmason/trading-bot-process.git""],""changes"":{""mergeChanges"":[],""indexChanges"":[],""workingTree"":[{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}],""untrackedChanges"":[]},""_onDidChangeSignal"":{""_observers"":{},""debugName"":""Observable Signal From Event""},""headBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""headCommitHashObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""upstreamBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""upstreamRemoteObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""isRebasingObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""_checkIsIgnored"":{""_waitingTimeMs"":1000,""_queue"":[],""_timeout"":null},""_isIgnored"":{""_map"":{},""_map2"":{}}}
Trace: [GitServiceImpl][onDidOpenRepository] Active repository: {""_repo"":{""rootUri"":{""$mid"":1,""external"":""file:///h%3A/Proyectos/Personales/trading/trading-bot-process"",""path"":""/h:/Proyectos/Personales/trading/trading-bot-process"",""scheme"":""file""},""inputBox"":{},""state"":{},""ui"":{}},""rootUri"":{""$mid"":1,""external"":""file:///h%3A/Proyectos/Personales/trading/trading-bot-process"",""path"":""/h:/Proyectos/Personales/trading/trading-bot-process"",""scheme"":""file""},""headBranchName"":""feature"",""headCommitHash"":""6b69236543a6178baaca109aea9fd19aa5a07822"",""upstreamBranchName"":""feature"",""upstreamRemote"":""origin"",""isRebasing"":true,""remotes"":[""origin""],""remoteFetchUrls"":[""https://github.com/Roddmason/trading-bot-process.git""],""changes"":{""mergeChanges"":[],""indexChanges"":[],""workingTree"":[{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}],""untrackedChanges"":[]},""_onDidChangeSignal"":{""_observers"":{},""debugName"":""Observable Signal From Event""},""headBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""headCommitHashObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""upstreamBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""upstreamRemoteObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""isRebasingObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""_checkIsIgnored"":{""_waitingTimeMs"":1000,""_queue"":[],""_timeout"":null},""_isIgnored"":{""_map"":{},""_map2"":{}}}
Trace: [GitServiceImpl][onDidOpenRepository] Active repository: {""_repo"":{""rootUri"":{""$mid"":1,""external"":""file:///h%3A/Proyectos/Personales/trading/trading-bot-process"",""path"":""/h:/Proyectos/Personales/trading/trading-bot-process"",""scheme"":""file""},""inputBox"":{},""state"":{},""ui"":{}},""rootUri"":{""$mid"":1,""external"":""file:///h%3A/Proyectos/Personales/trading/trading-bot-process"",""path"":""/h:/Proyectos/Personales/trading/trading-bot-process"",""scheme"":""file""},""headBranchName"":""feature"",""headCommitHash"":""6b69236543a6178baaca109aea9fd19aa5a07822"",""upstreamBranchName"":""feature"",""upstreamRemote"":""origin"",""isRebasing"":true,""remotes"":[""origin""],""remoteFetchUrls"":[""https://github.com/Roddmason/trading-bot-process.git""],""changes"":{""mergeChanges"":[],""indexChanges"":[],""workingTree"":[{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}],""untrackedChanges"":[]},""_onDidChangeSignal"":{""_observers"":{},""debugName"":""Observable Signal From Event""},""headBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""headCommitHashObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""upstreamBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""upstreamRemoteObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""isRebasingObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""_checkIsIgnored"":{""_waitingTimeMs"":1000,""_queue"":[],""_timeout"":null},""_isIgnored"":{""_map"":{},""_map2"":{}}}
Debug: [LanguageModelAccess] UPDATING language models
Debug: [LanguageModelAccess] DID UPDATE language models
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: true.
Trace: [GitServiceImpl][onDidOpenRepository] Active repository: {""_repo"":{""rootUri"":{""$mid"":1,""external"":""file:///h%3A/Proyectos/Personales/trading/trading-bot-process"",""path"":""/h:/Proyectos/Personales/trading/trading-bot-process"",""scheme"":""file""},""inputBox"":{},""state"":{},""ui"":{}},""rootUri"":{""$mid"":1,""external"":""file:///h%3A/Proyectos/Personales/trading/trading-bot-process"",""path"":""/h:/Proyectos/Personales/trading/trading-bot-process"",""scheme"":""file""},""headBranchName"":""feature"",""headCommitHash"":""6b69236543a6178baaca109aea9fd19aa5a07822"",""upstreamBranchName"":""feature"",""upstreamRemote"":""origin"",""isRebasing"":true,""remotes"":[""origin""],""remoteFetchUrls"":[""https://github.com/Roddmason/trading-bot-process.git""],""changes"":{""mergeChanges"":[],""indexChanges"":[],""workingTree"":[{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}],""untrackedChanges"":[]},""_onDidChangeSignal"":{""_observers"":{},""debugName"":""Observable Signal From Event""},""headBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""headCommitHashObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""upstreamBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""upstreamRemoteObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""isRebasingObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""_checkIsIgnored"":{""_waitingTimeMs"":1000,""_queue"":[],""_timeout"":null},""_isIgnored"":{""_map"":{},""_map2"":{}}}
Trace: [GitServiceImpl][onDidOpenRepository] Active repository: {""_repo"":{""rootUri"":{""$mid"":1,""external"":""file:///h%3A/Proyectos/Personales/trading/trading-bot-process"",""path"":""/h:/Proyectos/Personales/trading/trading-bot-process"",""scheme"":""file""},""inputBox"":{},""state"":{},""ui"":{}},""rootUri"":{""$mid"":1,""external"":""file:///h%3A/Proyectos/Personales/trading/trading-bot-process"",""path"":""/h:/Proyectos/Personales/trading/trading-bot-process"",""scheme"":""file""},""headBranchName"":""feature"",""headCommitHash"":""6b69236543a6178baaca109aea9fd19aa5a07822"",""upstreamBranchName"":""feature"",""upstreamRemote"":""origin"",""isRebasing"":true,""remotes"":[""origin""],""remoteFetchUrls"":[""https://github.com/Roddmason/trading-bot-process.git""],""changes"":{""mergeChanges"":[],""indexChanges"":[],""workingTree"":[{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}],""untrackedChanges"":[]},""_onDidChangeSignal"":{""_observers"":{},""debugName"":""Observable Signal From Event""},""headBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""headCommitHashObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""upstreamBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""upstreamRemoteObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""isRebasingObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""_checkIsIgnored"":{""_waitingTimeMs"":1000,""_queue"":[],""_timeout"":null},""_isIgnored"":{""_map"":{},""_map2"":{}}}
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Trace: Resolving embedding model
Info: Fetched model metadata in 479ms 6529996e-68a4-498f-8ab1-d87db22b7569
Trace: Resolved embedding model
Trace: Resolving embedding model
Trace: Resolved embedding model
Trace: Resolving embedding model
Trace: Resolved embedding model
</pre>
</details><details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i9-10900X CPU @ 3.70GHz (20 x 3696)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|63.68GB (42.24GB free)|
|Process Argv|--crash-reporter-id 087dcb58-32af-40bd-981b-c2fc20b5d6d6|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
9d2cg352:31339597
d2249276:31341129
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->
",2
Vscode Source Control and Problems output showing files from Homebrew along with my project.,"
Type: <b>Bug</b>

1. code navigate to a std lib function. eg: click on the benchmark method in testing. 
2. problems output shows problems from not only the opened file, but from other golang files in the homebrew package. it also adds a repository in the source control sidebar.

<img width=""811"" height=""379"" alt=""Image"" src=""https://github.com/user-attachments/assets/3f494fbf-262f-4dc3-99e1-5874eb1afddd"" />

VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 23.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M1 (8 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|3, 4, 4|
|Memory (System)|16.00GB (0.07GB free)|
|Process Argv|--crash-reporter-id 253ea13d-a7b2-429e-aab6-fff1bdfab0f3|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (22)</summary>

Extension|Author (truncated)|Version
---|---|---
vscode-tailwindcss|bra|0.14.24
codeium|Cod|1.48.2
vscode-eslint|dba|3.0.10
dotenv-vscode|dot|0.28.1
es7-react-js-snippets|dsz|4.4.3
gitlens|eam|17.2.2
prettier-vscode|esb|11.0.0
vscode-github-actions|git|0.27.2
go|gol|0.48.0
batch-rename-extension|Jan|0.0.6
vscode-containers|ms-|2.0.3
vscode-docker|ms-|2.0.0
live-server|ms-|0.4.15
sqltools|mtx|0.28.4
sqltools-driver-pg|mtx|0.5.5
oklch-preview|niz|0.4.0
material-icon-theme|PKi|5.24.0
inline-sql-syntax|quf|2.16.0
quicktype|qui|23.0.170
vscode-yaml|red|1.18.0
tooltitude|too|0.136.1
gistfs|vsl|0.9.4

(1 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
0g0a1943:31341127
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
nes-diff-11:31337487
nes-set-on:31340697
testaa123cf:31335227
6abeh943:31336334
yijiwantestdri0626-t:31336930
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
"hanged on ""working""","
Type: <b>Bug</b>

entered prompt 3 times, still spins on working

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M1 Pro (8 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|3, 3, 3|
|Memory (System)|16.00GB (0.16GB free)|
|Process Argv|--crash-reporter-id ffd91278-10f1-4fd1-bf99-34b3efb5ab21|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
convertlamdaf:31329270
0g0a1943:31341127
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
b6b4d950:31327385
nes-diff-11:31337487
onetestforazureexp:31335613
6abeh943:31336334
yijiwantestdri0626-t:31336930
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
"Copilot also doesn’t actually edit the file, even though it says changes were made.","
Type: <b>Bug</b>


> After extended use, Copilot claims it has made changes to the code upon request, but in reality, VS Code doesn’t prompt to save any changes, and I can clearly see that the code Copilot generated wasn't actually added. So, no changes occurred, even though Copilot says the file was modified.
>
> Additionally, the GPT-4o model stopped responding logically and structuring requests properly. After a few hours of work, the AI began suggesting and applying changes with incorrect or completely nonsensical solutions.


Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i3-9100F CPU @ 3.60GHz (4 x 3600)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.94GB (3.91GB free)|
|Process Argv|--crash-reporter-id 57dc31fb-b2dd-4209-a9af-cfc6226fa0e8|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
j6a0c355:31339598
0g0a1943:31341127
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
not stisfied,"
Type: <b>Feature Request</b>

i just bought a new 30 days free trial with Visa Card and after one only chat, they say i've reached my 30 days free trial

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19045
Modes:


<!-- generated by issue reporter -->",0
It keeps disconnecting in the middle of code autocompletion,"
Type: <b>Bug</b>

Just add a prompt and have any model try to do code completion and it disconnects right in the middle of editing a file.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 9 7950X3D 16-Core Processor           (32 x 4192)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|63.14GB (39.69GB free)|
|Process Argv|--crash-reporter-id f121dcc1-fc8b-4af5-a2bc-84f2c22578e8|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
14424t2-chatv3:31340359
c4g48928:30535728
azure-dev_surveyonecf:30548226
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
j6a0c355:31339598
c4b42873:31341128
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
nes-diff-11:31337487
nes-set-on:31340697
6abeh943:31336334
yijiwantestdri0626-c:31336931
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
Cannot copy/paste anymore in vscode?,"A Contributed command 'editor.action.clipboardCopyAction' does not exist. 

[![Image from Gyazo](https://i.gyazo.com/ed3e8b9d6b7ce24b455cf3c24449b1ed.png)](https://gyazo.com/ed3e8b9d6b7ce24b455cf3c24449b1ed)",0
i can not use copilot chat ,"
Type: <b>Bug</b>

Error message coming when i use copilot chat 

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|13th Gen Intel(R) Core(TM) i9-13900H (20 x 2995)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.73GB (2.17GB free)|
|Process Argv|-n --crash-reporter-id 80bf20d1-01ff-4679-9956-334f2cd20798|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
convertlamdat:31329272
c4b42873:31341128
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
copilot pro not working,"
Type: <b>Bug</b>

i got copilot pro and it's still not letting me use it

Extension version: 0.28.5
VS Code version: Code 1.101.0 (Universal) (dfaf44141ea9deb3b4096f7cd6d24e00c147a4b1, 2025-06-11T15:00:50.123Z)
OS version: Darwin arm64 24.3.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M1 (8 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|4, 4, 3|
|Memory (System)|8.00GB (0.08GB free)|
|Process Argv|--crash-reporter-id c4216910-abdd-41e3-a942-ae584d4675f6|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyonecf:30548226
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
0g0a1943:31341127
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
nes-diff-11:31337487
nes-set-on:31340697
6abeh943:31336334
yijiwantestdri0626-c:31336931
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
[WSL] Add server installation `dnf install wget` for Fedora,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

I just installed `FedoraLinux-42` in WSL. In the container, I attempted to use `code` and I got the following:

```console
$ code
Updating VS Code Server to version 2901c5ac6db8a986a5666c3af51ff804d05af0d4
Removing previous installation...
ERROR: Failed to download the VS Code server. 'wget' not installed.
Please install wget:
Debian/Ubuntu: sudo apt-get install wget
```

I'd like to request adding the following line:

```
Fedora: sudo dnf install wget
```

Unfortunately, I [could not find ""install wget"" in the open source portion](https://github.com/search?q=repo%3Amicrosoft%2Fvscode+%22install+wget%22&type=code). So I cannot PR the addition myself.

Or, even better:

- https://github.com/microsoft/vscode/issues/221851",1
very slow chat and edits,"
Type: <b>Performance Issue</b>

any interaction with GHCP is very very slow today, it does edits to 99 percent and then stalls, never completes

Extension version: 0.29.2025070403
VS Code version: Code - Insiders 1.102.0-insider (96f1890d08080f46f3b0a9424553422f04133090, 2025-07-04T15:15:02.963Z)
OS version: Windows_NT x64 10.0.22631
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|12th Gen Intel(R) Core(TM) i7-12850HX (24 x 2419)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|31.67GB (9.49GB free)|
|Process Argv|--crash-reporter-id d8f59b00-e574-4161-bba9-f5e3e9d76319|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>Process Info</summary>

```
CPU %	Mem MB	   PID	Process
    0	   152	 36676	code-insiders
    0	    28	  7820	   crashpad-handler
    0	   614	 11048	window [1] (Running Extensions - aws-parameter-store-action - Visual Studio Code - Insiders)
    0	    95	 11124	pty-host
    0	     6	 16512	     ""C:\Program Files\Git\bin\bash.exe"" --init-file ""c:\Users\320081580\AppData\Local\Programs\Microsoft VS Code Insiders\resources\app/out/vs/workbench/contrib/terminal/common/scripts/shellIntegration-bash.sh""
    0	    12	 35168	       ""C:\Program Files\Git\bin\..\usr\bin\bash.exe"" --init-file ""c:\Users\320081580\AppData\Local\Programs\Microsoft VS Code Insiders\resources\app/out/vs/workbench/contrib/terminal/common/scripts/shellIntegration-bash.sh""
    0	     7	 35172	     conpty-agent
    0	   743	 25176	extension-host [1]
    0	   627	  2412	     electron-nodejs (tsserver.js )
    0	    88	 17104	       electron-nodejs (typingsInstaller.js typesMap.js )
    0	    85	  9696	     ""C:\Users\320081580\AppData\Local\Programs\Microsoft VS Code Insiders\Code - Insiders.exe"" ""c:\Users\320081580\AppData\Local\Programs\Microsoft VS Code Insiders\resources\app\extensions\json-language-features\server\dist\node\jsonServerMain"" --node-ipc --clientProcessId=25176
    0	     4	 11320	     ""c:\Users\320081580\AppData\Roaming\Code - Insiders\User\globalStorage\github.vscode-codeql\distribution1\codeql\codeql.exe"" execute cli-server -v --log-to-stderr
    0	     7	  3284	       C:\windows\system32\conhost.exe 0x4
    0	   220	 34784	       ""c:\Users\320081580\AppData\Roaming\Code - Insiders\User\globalStorage\github.vscode-codeql\distribution1\codeql\tools\win64\java\bin\java.exe"" --add-modules jdk.unsupported -cp ""c:\Users\320081580\AppData\Roaming\Code - Insiders\User\globalStorage\github.vscode-codeql\distribution1\codeql\tools\codeql.jar"" com.semmle.cli2.CodeQL execute cli-server -v --log-to-stderr
    0	   313	 15072	     ""C:\Users\320081580\AppData\Local\Programs\Microsoft VS Code Insiders\Code - Insiders.exe"" c:\Users\320081580\.vscode-insiders\extensions\angular.ng-template-20.0.1\server --logToConsole --ngProbeLocations c:\Users\320081580\.vscode-insiders\extensions\angular.ng-template-20.0.1,d:\aws-parameter-store-action --includeAutomaticOptionalChainCompletions --includeCompletionsWithSnippetText --tsProbeLocations c:\Users\320081580\.vscode-insiders\extensions\angular.ng-template-20.0.1,d:\aws-parameter-store-action --node-ipc --clientProcessId=25176
    0	     4	 21500	     electron-nodejs (structured-evaluator-log.js )
    0	     7	 35752	       C:\windows\system32\conhost.exe 0x4
    0	    25	 36632	       electron-nodejs (structured-evaluator-log.js )
    0	   328	 26892	     electron-nodejs (eslintServer.js )
    0	   141	 30748	     electron-nodejs (tsserver.js )
    0	    90	 33476	     electron-nodejs (languageserver.js )
    0	   225	 33900	     electron-nodejs (server.js )
    0	   556	 34268	     c:\Users\320081580\.vscode-insiders\extensions\sonarsource.sonarlint-vscode-4.25.1-win32-x64\jre\21.0.7-win32-x86_64.tar\bin\java -jar c:\Users\320081580\.vscode-insiders\extensions\sonarsource.sonarlint-vscode-4.25.1-win32-x64\server\sonarlint-ls.jar -stdio -analyzers c:\Users\320081580\.vscode-insiders\extensions\sonarsource.sonarlint-vscode-4.25.1-win32-x64\analyzers\sonargo.jar c:\Users\320081580\.vscode-insiders\extensions\sonarsource.sonarlint-vscode-4.25.1-win32-x64\analyzers\sonarjava.jar c:\Users\320081580\.vscode-insiders\extensions\sonarsource.sonarlint-vscode-4.25.1-win32-x64\analyzers\sonarjavasymbolicexecution.jar c:\Users\320081580\.vscode-insiders\extensions\sonarsource.sonarlint-vscode-4.25.1-win32-x64\analyzers\sonarjs.jar c:\Users\320081580\.vscode-insiders\extensions\sonarsource.sonarlint-vscode-4.25.1-win32-x64\analyzers\sonarphp.jar c:\Users\320081580\.vscode-insiders\extensions\sonarsource.sonarlint-vscode-4.25.1-win32-x64\analyzers\sonarpython.jar c:\Users\320081580\.vscode-insiders\extensions\sonarsource.sonarlint-vscode-4.25.1-win32-x64\analyzers\sonarhtml.jar c:\Users\320081580\.vscode-insiders\extensions\sonarsource.sonarlint-vscode-4.25.1-win32-x64\analyzers\sonarxml.jar c:\Users\320081580\.vscode-insiders\extensions\sonarsource.sonarlint-vscode-4.25.1-win32-x64\analyzers\sonartext.jar c:\Users\320081580\.vscode-insiders\extensions\sonarsource.sonarlint-vscode-4.25.1-win32-x64\analyzers\sonariac.jar c:\Users\320081580\.vscode-insiders\extensions\sonarsource.sonarlint-vscode-4.25.1-win32-x64\analyzers\sonarlintomnisharp.jar c:\Users\320081580\.vscode-insiders\extensions\sonarsource.sonarlint_ondemand-analyzers\sonar-cfamily-plugin\6.68.0.85760\sonarcfamily.jar
    0	     7	 22268	       C:\windows\system32\conhost.exe 0x4
    0	     8	 24100	       ""c:\Program Files (x86)\Nodist\bin\node.exe"" c:\Users\320081580\.vscode-insiders\extensions\sonarsource.sonarlint-vscode-4.25.1-win32-x64\eslint-bridge\package\bin\server.cjs 49178 127.0.0.1 false
    0	     7	 20788	         C:\windows\system32\conhost.exe 0x4
    0	   453	 28340	         ""C:\Program Files (x86)\Nodist/v-x64/22.17.0/node.exe"" c:\Users\320081580\.vscode-insiders\extensions\sonarsource.sonarlint-vscode-4.25.1-win32-x64\eslint-bridge\package\bin\server.cjs 49178 127.0.0.1 false
    0	     4	 35380	     ""c:\Users\320081580\AppData\Roaming\Code - Insiders\User\globalStorage\github.vscode-codeql\distribution1\codeql\codeql.exe"" execute language-server --check-errors ON_CHANGE -v --log-to-stderr
    0	     7	 17768	       C:\windows\system32\conhost.exe 0x4
    0	    41	 20064	       ""c:\Users\320081580\AppData\Roaming\Code - Insiders\User\globalStorage\github.vscode-codeql\distribution1\codeql\tools\win64\java\bin\java.exe"" --add-modules jdk.unsupported -cp ""c:\Users\320081580\AppData\Roaming\Code - Insiders\User\globalStorage\github.vscode-codeql\distribution1\codeql\tools\codeql.jar"" com.semmle.cli2.CodeQL execute language-server --check-errors ON_CHANGE -v --log-to-stderr
    0	    85	 35480	     electron-nodejs (server-node.js )
    0	    46	 29720	   utility-network-service
    0	   170	 30568	shared-process
    0	    75	 33972	   utility-process
    0	   232	 35860	   gpu-process
    0	    93	 36824	file-watcher [1]
```

</details>
<details>
<summary>Workspace Info</summary>

```
|  Window (Running Extensions - aws-parameter-store-action - Visual Studio Code - Insiders)
|    Folder (aws-parameter-store-action): 34 files
|      File types: yaml(6) json(5) ts(4) js(2) md(2) cfg(1) eslintignore(1)
|                  DS_Store(1) gitignore(1) prettierignore(1)
|      Conf files: package.json(2) github-actions(1) tsconfig.json(1);
```

</details>
<details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551:31249597
c4g48928:30535728
vscrp:30624060
962ge761:30841072
h48ei257:31000450
cppperfnew:30980852
dwnewjupytercf:31046870
nativeloc1:31118317
e80f6927:31120813
dwcopilot:31158714
c3hdf307:31184662
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
g012b348:31231168
pythoneinst12:31251391
c7cif404:31309980
pythonpulldiag:31287486
6gi0g917:31259950
996jf627:31264550
pythonrdcb7:31268811
usemplatestapi:31297334
e6194696:31317039
jjjhb125:31275147
6518g693:31302842
generatesymbolt:31280541
convertfstringf:31280702
j97ad248:31306657
usemarketplace:31336439
0g1h6703:31329154
nes-emitfast-1:31333560
replacestringexc:31340153
testaa123cf:31335227
6abeh943:31336334
0927b901:31340060
gji67723:31340537
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Keeps crashing,"
Type: <b>Bug</b>

I have been workign on an issue for an extremem amount of my usuage and it cant figure itout its simple conditional formatting. Now the app has been crashing repeatley. This is very unfair

Extension version: 0.28.5
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M1 (8 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|4, 3, 2|
|Memory (System)|8.00GB (0.07GB free)|
|Process Argv|--crash-reporter-id b2181c9d-f34e-4bbf-8069-3813e47924d6|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492cf:30256860
pythonvspyt551:31249599
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
d2249276:31341129
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
b6b4d950:31327385
nes-diff-11:31337487
onetestforazureexp:31335613
6abeh943:31336334
yijiwantestdri0626-t:31336930
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
Out of context suggestions,"
Type: <b>Bug</b>

Copilot will frequently suggest code fragments inside the commit message box. See attached image.

<img width=""290"" height=""279"" alt=""Image"" src=""https://github.com/user-attachments/assets/77c8e588-2f7d-403f-8227-f0f58729f4cb"" />

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Linux x64 6.11.0-29-generic snap
Modes:

<details><summary>Logs</summary>
<pre>
Info: message 0 returned. finish reason: [tool_calls]
Info: request done: requestId: [dfb0603a-b5c3-4444-a8da-05793716959b] model deployment ID: []
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Resolving chat model
Trace: Resolved chat model
Debug: Collect instructions from file: .github/copilot-instructions.md
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Built prompt
Trace: Sending prompt to model
Info: message 0 returned. finish reason: [stop]
Info: request done: requestId: [dfb0603a-b5c3-4444-a8da-05793716959b] model deployment ID: []
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [LanguageModelAccess] UPDATING language models
Info: Fetched model metadata in 797ms 24654999-0e37-4715-9151-3a53b4e0d1e0
Debug: [LanguageModelAccess] DID UPDATE language models
Trace: Resolving embedding model
Trace: Resolved embedding model
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
</pre>
</details><details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-6300U CPU @ 2.40GHz (4 x 2899)|
|GPU Status|2d_canvas: unavailable_software<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: disabled_software<br>multiple_raster_threads: enabled_on<br>opengl: disabled_off<br>rasterization: disabled_software<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: disabled_software<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: unavailable_software<br>webgl2: unavailable_software<br>webgpu: disabled_off<br>webnn: unavailable_software|
|Load (avg)|1, 1, 1|
|Memory (System)|7.61GB (2.27GB free)|
|Process Argv|--no-sandbox --force-user-env --crash-reporter-id 142699bf-6637-4271-a1b1-addef6d923fc|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|ubuntu-xorg|
|XDG_CURRENT_DESKTOP|Unity|
|XDG_SESSION_DESKTOP|ubuntu-xorg|
|XDG_SESSION_TYPE|x11|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
jhi8h917:31341130
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
cant enter copilot edit mode from chat,"
Type: <b>Bug</b>

No idea how I got into this state - I was in edit mode and at some point I found myself in ask mode. When I tried to use the menu in the chat to got tpo edit or agent mode nothing happened - I stayed in ask mode. When I used ctrl shift I to go to agent mode It switched to agent mode - but I couldnt use the menu to switch to ask or edit mode

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details><summary>Logs</summary>
<pre>
Trace: Resolved chat model
Trace: [panel] chat request received from extension host
Trace: Resolving chat model
Trace: Processing intent
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Resolved chat model
Trace: Processed intent
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Resolving chat model
Trace: Resolved chat model
Debug: Collect instructions from file: .github/copilot-instructions.md
Debug: Collect instructions from file: .github/copilot-instructions.md
Trace: Resolving chat model
Trace: Resolved chat model
Info: message 0 returned. finish reason: [stop]
Info: request done: requestId: [17510427-4fe3-4d41-9d50-e0eb7eb0db47] model deployment ID: []
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Built prompt
Trace: Sending prompt to model
Info: message 0 returned. finish reason: [stop]
Info: request done: requestId: [f7517c57-87b1-4013-994b-2f03ff84a484] model deployment ID: []
Trace: Resolving embedding model
Trace: Resolved embedding model
Trace: Resolving embedding model
Trace: Resolved embedding model
Trace: Resolving embedding model
Trace: Resolved embedding model
Trace: Resolving embedding model
Trace: Resolved embedding model
Trace: Resolving embedding model
Trace: Resolved embedding model
Trace: Resolving embedding model
Trace: Resolved embedding model
Trace: Resolving embedding model
Trace: Resolved embedding model
Trace: Resolving embedding model
Trace: Resolved embedding model
Trace: Resolving embedding model
Trace: Resolved embedding model
Trace: Resolving embedding model
Trace: Resolved embedding model
Trace: Resolving embedding model
Trace: Resolved embedding model
Trace: Resolving embedding model
Trace: Resolved embedding model
</pre>
</details><details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-6200U CPU @ 2.30GHz (4 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|7.88GB (1.68GB free)|
|Process Argv|--crash-reporter-id 3fad198a-256d-414c-98e5-a8fc8d31b12e|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
9d2cg352:31339597
d2249276:31341129
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
4f60g487:31327383
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
co-pilot tries to run another build befor the first is complete.,"
Type: <b>Bug</b>

Now let me build and test the enhanced debug output:
(runs build.ps1)
Let me wait for the build to complete:
(attemps to run build.ps1 again)
Checked background terminal output

no... running the build a second time is not checking terminal output heh.



Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.27891
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-6600U CPU @ 2.60GHz (4 x 2808)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.86GB (5.01GB free)|
|Process Argv|--crash-reporter-id 27cf282e-6431-47b5-924c-7ded64435c14|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
d2249276:31341129
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
Copilot doesn't understand Git renames,"
Type: <b>Bug</b>

## Scope: commit message generation

When staging renamed or moved files, commit message generation will describe an addition instead.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Linux x64 6.11.0-29-generic snap
Modes:

<details><summary>Logs</summary>
<pre>
Trace: Resolved chat model
Debug: Collect instructions from file: .github/copilot-instructions.md
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Built prompt
Trace: Sending prompt to model
Info: message 0 returned. finish reason: [tool_calls]
Info: request done: requestId: [dfb0603a-b5c3-4444-a8da-05793716959b] model deployment ID: []
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Resolving chat model
Trace: Resolved chat model
Debug: Collect instructions from file: .github/copilot-instructions.md
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Built prompt
Trace: Sending prompt to model
Info: message 0 returned. finish reason: [stop]
Info: request done: requestId: [dfb0603a-b5c3-4444-a8da-05793716959b] model deployment ID: []
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [LanguageModelAccess] UPDATING language models
Info: Fetched model metadata in 797ms 24654999-0e37-4715-9151-3a53b4e0d1e0
Debug: [LanguageModelAccess] DID UPDATE language models
Trace: Resolving embedding model
Trace: Resolved embedding model
</pre>
</details><details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-6300U CPU @ 2.40GHz (4 x 2900)|
|GPU Status|2d_canvas: unavailable_software<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: disabled_software<br>multiple_raster_threads: enabled_on<br>opengl: disabled_off<br>rasterization: disabled_software<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: disabled_software<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: unavailable_software<br>webgl2: unavailable_software<br>webgpu: disabled_off<br>webnn: unavailable_software|
|Load (avg)|1, 1, 1|
|Memory (System)|7.61GB (2.31GB free)|
|Process Argv|--no-sandbox --force-user-env --crash-reporter-id 142699bf-6637-4271-a1b1-addef6d923fc|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|ubuntu-xorg|
|XDG_CURRENT_DESKTOP|Unity|
|XDG_SESSION_DESKTOP|ubuntu-xorg|
|XDG_SESSION_TYPE|x11|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
jhi8h917:31341130
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
Claude 4  doesn't stop for approval,"
Type: <b>Bug</b>

I have specific instructions for Copilot to not make any edit before I approve his plan of action. GPT 4.1 almost always presents the plan and stops. Claude, on the other hand, will sometimes present a plan but even then will proceed without stopping.
This is really frustrating because Copilot usually doesn't get it right from the get to, and correcting edits is more expensive than fixing the initial plan.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Linux x64 6.11.0-29-generic snap
Modes:

<details><summary>Logs</summary>
<pre>
Trace: Resolved chat model
Trace: [panel] chat request received from extension host
Trace: Resolving chat model
Trace: Processing intent
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Resolved chat model
Trace: Processed intent
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Resolving chat model
Trace: Resolved chat model
Debug: Collect instructions from file: .github/copilot-instructions.md
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Built prompt
Trace: Sending prompt to model
Info: message 0 returned. finish reason: [tool_calls]
Info: request done: requestId: [dfb0603a-b5c3-4444-a8da-05793716959b] model deployment ID: []
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Resolving chat model
Trace: Resolved chat model
Debug: Collect instructions from file: .github/copilot-instructions.md
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Built prompt
Trace: Sending prompt to model
Info: message 0 returned. finish reason: [stop]
Info: request done: requestId: [dfb0603a-b5c3-4444-a8da-05793716959b] model deployment ID: []
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: false, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
</pre>
</details><details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-6300U CPU @ 2.40GHz (4 x 2899)|
|GPU Status|2d_canvas: unavailable_software<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: disabled_software<br>multiple_raster_threads: enabled_on<br>opengl: disabled_off<br>rasterization: disabled_software<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: disabled_software<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: unavailable_software<br>webgl2: unavailable_software<br>webgpu: disabled_off<br>webnn: unavailable_software|
|Load (avg)|2, 1, 1|
|Memory (System)|7.61GB (2.42GB free)|
|Process Argv|--no-sandbox --force-user-env --crash-reporter-id 142699bf-6637-4271-a1b1-addef6d923fc|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|ubuntu-xorg|
|XDG_CURRENT_DESKTOP|Unity|
|XDG_SESSION_DESKTOP|ubuntu-xorg|
|XDG_SESSION_TYPE|x11|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
jhi8h917:31341130
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
Custom instructions not applied to new files,"
Type: <b>Bug</b>

When asking Copilot to create a new file that is covered in the `applyTo` instruction file metadata, the files won't be included. They are only included for existing files.

## Steps to reproduce
1. Create a custom instruction file that applies to a specific type of file.
2. Ask Copilot to create a new file of that type.
### Expected result
Instructions are applied to the new file.
### Actual result
Instructions are not applied.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Linux x64 6.11.0-29-generic snap
Modes:

<details><summary>Logs</summary>
<pre>
Trace: Resolved chat model
Trace: No test results
Trace: Resolving chat model
Trace: Resolved chat model
Trace: [GitServiceImpl][onDidOpenRepository] Active repository: {""_repo"":{""rootUri"":{""$mid"":1,""external"":""file:///home/ezequiel/Desarrollo/Proyectos/Transend/Fuentes/nls-rework"",""path"":""/home/ezequiel/Desarrollo/Proyectos/Transend/Fuentes/nls-rework"",""scheme"":""file""},""inputBox"":{},""state"":{},""ui"":{}},""rootUri"":{""$mid"":1,""external"":""file:///home/ezequiel/Desarrollo/Proyectos/Transend/Fuentes/nls-rework"",""path"":""/home/ezequiel/Desarrollo/Proyectos/Transend/Fuentes/nls-rework"",""scheme"":""file""},""headBranchName"":""tool/mock-backend"",""headCommitHash"":""ff2ac90c1bcf3608af2114dfeef72637344c205f"",""upstreamBranchName"":""tool/mock-backend"",""upstreamRemote"":""origin"",""isRebasing"":true,""remotes"":[""origin""],""remoteFetchUrls"":[""https://ezequielcicala@bitbucket.org/transend/nls-rework.git""],""changes"":{""mergeChanges"":[],""indexChanges"":[],""workingTree"":[{}],""untrackedChanges"":[]},""_onDidChangeSignal"":{""_observers"":{},""debugName"":""Observable Signal From Event""},""headBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""headCommitHashObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""upstreamBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""upstreamRemoteObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""isRebasingObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""_checkIsIgnored"":{""_waitingTimeMs"":1000,""_queue"":[],""_timeout"":null},""_isIgnored"":{""_map"":{},""_map2"":{}}}
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Built prompt
Trace: Sending prompt to model
Trace: No test results
Info: TypeScript server plugin activated.
Info: Registered TypeScript context provider with Copilot inline completions.
Info: message 0 returned. finish reason: [stop]
Info: request done: requestId: [f242c78d-cd3f-4099-a127-800bfa7d0a16] model deployment ID: []
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Resolving chat model
Trace: Resolved chat model
Trace: [panel] chat request received from extension host
Trace: Resolving chat model
Trace: Processing intent
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Resolved chat model
Trace: Processed intent
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Resolving chat model
Trace: Resolved chat model
Debug: Collect instructions from file: .github/copilot-instructions.md
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Resolving chat model
Trace: Resolved chat model
Trace: Built prompt
Trace: Sending prompt to model
Info: message 0 returned. finish reason: [stop]
Info: request done: requestId: [4303f77a-311f-4ff4-9af7-4f015c4ad0e9] model deployment ID: []
Trace: No test results
Trace: [GitServiceImpl][onDidOpenRepository] Active repository: {""_repo"":{""rootUri"":{""$mid"":1,""external"":""file:///home/ezequiel/Desarrollo/Proyectos/Transend/Fuentes/nls-rework"",""path"":""/home/ezequiel/Desarrollo/Proyectos/Transend/Fuentes/nls-rework"",""scheme"":""file""},""inputBox"":{},""state"":{},""ui"":{}},""rootUri"":{""$mid"":1,""external"":""file:///home/ezequiel/Desarrollo/Proyectos/Transend/Fuentes/nls-rework"",""path"":""/home/ezequiel/Desarrollo/Proyectos/Transend/Fuentes/nls-rework"",""scheme"":""file""},""headBranchName"":""tool/mock-backend"",""headCommitHash"":""ff2ac90c1bcf3608af2114dfeef72637344c205f"",""upstreamBranchName"":""tool/mock-backend"",""upstreamRemote"":""origin"",""isRebasing"":true,""remotes"":[""origin""],""remoteFetchUrls"":[""https://ezequielcicala@bitbucket.org/transend/nls-rework.git""],""changes"":{""mergeChanges"":[],""indexChanges"":[],""workingTree"":[{}],""untrackedChanges"":[]},""_onDidChangeSignal"":{""_observers"":{},""debugName"":""Observable Signal From Event""},""headBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""headCommitHashObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""upstreamBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""upstreamRemoteObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""isRebasingObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""_checkIsIgnored"":{""_waitingTimeMs"":1000,""_queue"":[],""_timeout"":null},""_isIgnored"":{""_map"":{},""_map2"":{}}}
Trace: No test results
Trace: No test results
Trace: [GitServiceImpl][onDidOpenRepository] Active repository: {""_repo"":{""rootUri"":{""$mid"":1,""external"":""file:///home/ezequiel/Desarrollo/Proyectos/Transend/Fuentes/nls-rework"",""path"":""/home/ezequiel/Desarrollo/Proyectos/Transend/Fuentes/nls-rework"",""scheme"":""file""},""inputBox"":{},""state"":{},""ui"":{}},""rootUri"":{""$mid"":1,""external"":""file:///home/ezequiel/Desarrollo/Proyectos/Transend/Fuentes/nls-rework"",""path"":""/home/ezequiel/Desarrollo/Proyectos/Transend/Fuentes/nls-rework"",""scheme"":""file""},""headBranchName"":""tool/mock-backend"",""headCommitHash"":""ff2ac90c1bcf3608af2114dfeef72637344c205f"",""upstreamBranchName"":""tool/mock-backend"",""upstreamRemote"":""origin"",""isRebasing"":true,""remotes"":[""origin""],""remoteFetchUrls"":[""https://ezequielcicala@bitbucket.org/transend/nls-rework.git""],""changes"":{""mergeChanges"":[],""indexChanges"":[],""workingTree"":[{}],""untrackedChanges"":[]},""_onDidChangeSignal"":{""_observers"":{},""debugName"":""Observable Signal From Event""},""headBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""headCommitHashObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""upstreamBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""upstreamRemoteObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""isRebasingObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""_checkIsIgnored"":{""_waitingTimeMs"":1000,""_queue"":[],""_timeout"":null},""_isIgnored"":{""_map"":{},""_map2"":{}}}
Trace: No test results
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.
Debug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.
Trace: No test results
Trace: [GitServiceImpl][onDidOpenRepository] Active repository: {""_repo"":{""rootUri"":{""$mid"":1,""external"":""file:///home/ezequiel/Desarrollo/Proyectos/Transend/Fuentes/nls-rework"",""path"":""/home/ezequiel/Desarrollo/Proyectos/Transend/Fuentes/nls-rework"",""scheme"":""file""},""inputBox"":{},""state"":{},""ui"":{}},""rootUri"":{""$mid"":1,""external"":""file:///home/ezequiel/Desarrollo/Proyectos/Transend/Fuentes/nls-rework"",""path"":""/home/ezequiel/Desarrollo/Proyectos/Transend/Fuentes/nls-rework"",""scheme"":""file""},""headBranchName"":""tool/mock-backend"",""headCommitHash"":""ff2ac90c1bcf3608af2114dfeef72637344c205f"",""upstreamBranchName"":""tool/mock-backend"",""upstreamRemote"":""origin"",""isRebasing"":true,""remotes"":[""origin""],""remoteFetchUrls"":[""https://ezequielcicala@bitbucket.org/transend/nls-rework.git""],""changes"":{""mergeChanges"":[],""indexChanges"":[],""workingTree"":[{}],""untrackedChanges"":[]},""_onDidChangeSignal"":{""_observers"":{},""debugName"":""Observable Signal From Event""},""headBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""headCommitHashObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""upstreamBranchNameObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""upstreamRemoteObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""isRebasingObs"":{""_observers"":{},""_debugNameData"":{},""_state"":0,""_updateCount"":0,""_dependencies"":{},""_dependenciesToBeRemoved"":{},""_isUpdating"":false,""_isComputing"":false,""_didReportChange"":false,""_removedObserverToCallEndUpdateOn"":null,""_isReaderValid"":false},""_checkIsIgnored"":{""_waitingTimeMs"":1000,""_queue"":[],""_timeout"":null},""_isIgnored"":{""_map"":{},""_map2"":{}}}
</pre>
</details><details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-6300U CPU @ 2.40GHz (4 x 2900)|
|GPU Status|2d_canvas: unavailable_software<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: disabled_software<br>multiple_raster_threads: enabled_on<br>opengl: disabled_off<br>rasterization: disabled_software<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: disabled_software<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: unavailable_software<br>webgl2: unavailable_software<br>webgpu: disabled_off<br>webnn: unavailable_software|
|Load (avg)|1, 1, 1|
|Memory (System)|7.61GB (1.80GB free)|
|Process Argv|--no-sandbox --force-user-env --crash-reporter-id 142699bf-6637-4271-a1b1-addef6d923fc|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|ubuntu-xorg|
|XDG_CURRENT_DESKTOP|Unity|
|XDG_SESSION_DESKTOP|ubuntu-xorg|
|XDG_SESSION_TYPE|x11|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
jhi8h917:31341130
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
"SOLVED: ""editor.wrappingIndent"":""same"" has no effect. Wrapping Indent does not work on MacOS","<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.88.1 (Universal)
- OS Version: MacOS: Darwin x64 24.5.0

This is just informative (no bug I think) related to #205212
(maybe just add it to the initial ticket to help anyone struggling with the same issue)
[pawelkontek](https://github.com/pawelkontek) @alexdima I had had exactly that same issue: wrapped lines would not indent
I had disabled all Extensions 
Then I turned of ""Optimize for Screen Reader"" and the problem went away
- I'm not sure why I ever used ""Optimize for Screen Reader"" (no idea what it does)
- ""solved"" may be an over-statement.  There may be a conflict between those two

Steps to Reproduce:

1. disable ""Optimize for Screen Reader""
2. 

/chriV",0
VSCode 1.90.x - 1.101.2 Crashes on Startup: bad_optional_access in -fno-exceptions Mode,"Hi team!
First off — thank you for all the hard work on VSCode! I’ve been a happy user for years. I wanted to report a serious issue I ran into after updating to version 1.90.x on Windows 10.

After launching, VSCode immediately crashes and disappears — there’s no window, no error popup, just an instant close.

I’ve already gone through all the usual steps:
Launched with safe flags:

code --disable-extensions --disable-gpu --disable-chromium-sandbox

Deleted:

%APPDATA%\Code\
.vscode\extensions
GPUCache, Backups, etc.

Fully uninstalled and reinstalled VSCode

Sadly, nothing helped — it still closes instantly.

When running from the command line with --verbose, this is the last line I get:

bad_optional_access was thrown in -fno-exceptions mode

I downgraded to VSCode 1.89.1 without any problems

Same system, same flags — no issues at all

my laptop info is:
OS: Windows 10
GPU: NVIDIA GTX 1650 Ti

[EDIT 1]: Forgot to mention the problem persists on all the version till V1.101.2",2
Why copilot chat section at the times of edit apply gets refreshed in between again and again,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 
- OS Version: 

Steps to Reproduce:

1. 
2. 
",0
ERR_CERT_COMMON_NAME_INVALID,"
Type: <b>Bug</b>

Sorry, your request failed. Please try again. Request id: 56af2904-2272-4d18-93eb-b0aa9d9ffd29

Reason: Please check your firewall rules and network connection then try again. Error Code: net::ERR_CERT_COMMON_NAME_INVALID.


every so many minutes i have to restart vscode cause im getting this error. then it goes away temporarily.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 9 7950X3D 16-Core Processor           (32 x 4200)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|63.14GB (32.57GB free)|
|Process Argv|--crash-reporter-id c1f2dbd7-1bb4-4a35-ae88-4fdd7f9a5069|
|Screen Reader|no|
|VM|0%|
</details>
<!-- generated by issue reporter -->",2
CoPilot hangs,"
Type: <b>Bug</b>

CoPilot keeps hanging on this operation.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.22621
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-10700K CPU @ 3.80GHz (16 x 3792)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|63.89GB (53.84GB free)|
|Process Argv|--crash-reporter-id 2bf01145-b8e6-471e-beb5-f2ee7adfd428|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
jjjhb125:31275178
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
jhi8h917:31341130
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
nes-diff-11:31337487
nes-set-on:31340697
6abeh943:31336334
yijiwantestdri0626-c:31336931
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
"""Don't save any""","I found this issue: https://github.com/microsoft/vscode/issues/163634 but it's marked as a duplicate. However, no duplicate issue is linked, and I could not find the duplicate issue.

Please link the duplicate issue and close this one (if it exists)...
",0
Reason: Error on conversation request. Check the log for more details.,"
Type: <b>Bug</b>

The error occurs too many time, where can I find the log?

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:
Remote OS version: Linux x64 5.15.167.4-microsoft-standard-WSL2

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i5-1140G7 @ 1.10GHz (8 x 1805)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.65GB (3.33GB free)|
|Process Argv|--crash-reporter-id fd8c347d-8191-46a7-9eb9-b620f33970cf|
|Screen Reader|no|
|VM|0%|

|Item|Value|
|---|---|
|Remote|WSL: Ubuntu-24.04|
|OS|Linux x64 5.15.167.4-microsoft-standard-WSL2|
|CPUs|11th Gen Intel(R) Core(TM) i5-1140G7 @ 1.10GHz (8 x 0)|
|Memory (System)|7.58GB (4.54GB free)|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
d2249276:31341129
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
82j33506:31327384
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Unable to parse the shell environment: Bad escaped character in JSON at position 6601 (line 2 column 6600),"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 
- 

<img width=""592"" height=""419"" alt=""Image"" src=""https://github.com/user-attachments/assets/a6d0b40a-8932-4311-addd-bcc9f8a70feb"" />

- OS Version:  Win11
When you start vscode, there will be an error prompt: Unable to parse the shell environment: Bad escaped character in JSON at position 6601 (line 2 column 6600)

https://code.visualstudio.com/docs/supporting/faq#_resolving-shell-environment-fails

According to the prompts in the document, I still can't analyze where the error occurred.

<img width=""716"" height=""175"" alt=""Image"" src=""https://github.com/user-attachments/assets/2f646702-fc5f-48ad-a6fe-fdf3ba9a4580"" />

<img width=""1770"" height=""360"" alt=""Image"" src=""https://github.com/user-attachments/assets/43287d14-6028-440b-9a7b-fbb3bd7b7d41"" />

<img width=""2929"" height=""831"" alt=""Image"" src=""https://github.com/user-attachments/assets/45fd5ad7-a8e3-4f94-8e01-614008425e62"" />




```
40294@antlrv D:\tempFiles\WebDemos\my-react-app git:123 ~1
> psinfo
C:\Program Files\WindowsApps\Microsoft.PowerShell_7.5.2.0_x64__8wekyb3d8bbwe\pwsh.exe
C:\Users\40294\Documents\PowerShell\Microsoft.PowerShell_profile.ps1

Name                           Value
----                           -----
PSVersion                      7.5.2
PSEdition                      Core
GitCommitId                    7.5.2
OS                             Microsoft Windows 10.0.22631
Platform                       Win32NT
PSCompatibleVersions           {1.0, 2.0, 3.0, 4.0…}
PSRemotingProtocolVersion      2.3
SerializationVersion           1.1.0.1
WSManStackVersion              3.0

Name              : Conda
Path              : D:\DevTools\miniconda3\shell\condabin\Conda.psm1
Description       : 
Guid              : 00000000-0000-0000-0000-000000000000
Version           : 0.0
ModuleBase        : D:\DevTools\miniconda3\shell\condabin
ModuleType        : Script
PrivateData       : 
AccessMode        : ReadWrite
ExportedAliases   : {[conda, conda], [etenv, etenv], [exenv, exenv], [genv, genv]}
ExportedCmdlets   : {}
ExportedFunctions : {[Enter-CondaEnvironment, Enter-CondaEnvironment], [Exit-CondaEnvironment, Exit-CondaEnvironment], [Get-CondaEnvironment, Get-CondaEnvironment], [Invoke-Conda, Invoke-Conda]…}
ExportedVariables : {}
NestedModules     : {}


Name              : Microsoft.PowerShell.Management
Path              : C:\program files\windowsapps\microsoft.powershell_7.5.2.0_x64__8wekyb3d8bbwe\Modules\Microsoft.PowerShell.Management\Microsoft.PowerShell.Management.psd1
Description       : 
Guid              : eefcb906-b326-4e99-9f54-8b4bb6ef3c6d
Version           : 7.0.0.0
ModuleBase        : C:\Program Files\WindowsApps\Microsoft.PowerShell_7.5.2.0_x64__8wekyb3d8bbwe
ModuleType        : Manifest
PrivateData       : 
AccessMode        : ReadWrite
ExportedAliases   : {[gcb, gcb], [gin, gin], [gtz, gtz], [scb, scb]…}
ExportedCmdlets   : {[Add-Content, Add-Content], [Clear-Content, Clear-Content], [Clear-Item, Clear-Item], [Clear-ItemProperty, Clear-ItemProperty]…}
ExportedFunctions : {}
ExportedVariables : {}
NestedModules     : {Microsoft.PowerShell.Commands.Management}


Name              : Microsoft.PowerShell.Utility
Path              : C:\program files\windowsapps\microsoft.powershell_7.5.2.0_x64__8wekyb3d8bbwe\Modules\Microsoft.PowerShell.Utility\Microsoft.PowerShell.Utility.psd1
Description       : 
Guid              : 1da87e53-152b-403e-98dc-74d7b4d63d59
Version           : 7.0.0.0
ModuleBase        : C:\Program Files\WindowsApps\Microsoft.PowerShell_7.5.2.0_x64__8wekyb3d8bbwe
ModuleType        : Manifest
PrivateData       : 
AccessMode        : ReadWrite
ExportedAliases   : {[fhx, fhx]}
ExportedCmdlets   : {[Add-Member, Add-Member], [Add-Type, Add-Type], [Clear-Variable, Clear-Variable], [Compare-Object, Compare-Object]…}
ExportedFunctions : {}
ExportedVariables : {}
NestedModules     : {Microsoft.PowerShell.Commands.Utility}


Name              : oh-my-posh-core
Path              : D:\tempFiles\WebDemos\my-react-app\10b6f58e-1bf6-4a0d-9b48-8a3d753cbdb3
Description       : 
Guid              : 00000000-0000-0000-0000-000000000000
Version           : 0.0
ModuleBase        : D:\tempFiles\WebDemos\my-react-app
ModuleType        : Script
PrivateData       : 
AccessMode        : ReadWrite
ExportedAliases   : {}
ExportedCmdlets   : {}
ExportedFunctions : {[Enable-PoshLineError, Enable-PoshLineError], [Enable-PoshTooltips, Enable-PoshTooltips], [Enable-PoshTransientPrompt, Enable-PoshTransientPrompt], [Set-PoshContext, Set-PoshContext]}
ExportedVariables : {}
NestedModules     : {}


Name              : posh-git
Path              : C:\tools\poshgit\dahlbyk-posh-git-9bda399\src\posh-git.psm1
Description       : Provides prompt with Git status summary information and tab completion for Git commands, parameters, remotes and branch names.
Guid              : 74c9fd30-734b-4c89-a8ae-7727ad21d1d5
Version           : 0.7.3.1
ModuleBase        : C:\tools\poshgit\dahlbyk-posh-git-9bda399\src
ModuleType        : Script
PrivateData       : {[PSData, System.Collections.Hashtable]}
AccessMode        : ReadWrite
ExportedAliases   : {[??, ??]}
ExportedCmdlets   : {}
ExportedFunctions : {[Add-PoshGitToProfile, Add-PoshGitToProfile], [Add-SshKey, Add-SshKey], [Enable-GitColors, Enable-GitColors], [Expand-GitCommand, Expand-GitCommand]…}
ExportedVariables : {[GitPromptScriptBlock, System.Management.Automation.PSVariable]}
NestedModules     : {}


Name              : PSFzf
Path              : C:\Users\40294\Documents\PowerShell\Modules\PSFzf\2.6.10\PSFzf.psm1
Description       : A thin wrapper around Fzf (https://github.com/junegunn/fzf).  If PSReadline is loaded, this wrapper registers Fzf with the keyboard chord Ctrl+t.
Guid              : 9454f107-c8bd-4fb9-9098-41a619ad0654
Version           : 2.6.10
ModuleBase        : C:\Users\40294\Documents\PowerShell\Modules\PSFzf\2.6.10
ModuleType        : Script
PrivateData       : {[PSData, System.Collections.Hashtable]}
AccessMode        : ReadWrite
ExportedAliases   : {}
ExportedCmdlets   : {}
ExportedFunctions : {[Enable-PsFzfAliases, Enable-PsFzfAliases], [Invoke-FuzzyEdit, Invoke-FuzzyEdit], [Invoke-FuzzyFasd, Invoke-FuzzyFasd], [Invoke-FuzzyGitStatus, Invoke-FuzzyGitStatus]…}
ExportedVariables : {}
NestedModules     : {}


Name              : PSReadLine
Path              : C:\Users\40294\Documents\PowerShell\Modules\PSReadLine\2.4.2\PSReadLine.psm1
Description       : Great command line editing in the PowerShell console host
Guid              : 5714753b-2afd-4492-a5fd-01d9e2cff8b5
Version           : 2.4.2
ModuleBase        : C:\Users\40294\Documents\PowerShell\Modules\PSReadLine\2.4.2
ModuleType        : Script
PrivateData       : {[PSData, System.Collections.Hashtable]}
AccessMode        : ReadWrite
ExportedAliases   : {}
ExportedCmdlets   : {[Get-PSReadLineKeyHandler, Get-PSReadLineKeyHandler], [Get-PSReadLineOption, Get-PSReadLineOption], [Remove-PSReadLineKeyHandler, Remove-PSReadLineKeyHandler], [Set-PSReadLineKeyHandler, Set-PSReadLineKeyHandler]…}
ExportedFunctions : {[PSConsoleHostReadLine, PSConsoleHostReadLine]}
ExportedVariables : {}
NestedModules     : {Microsoft.PowerShell.PSReadLine}


Name              : Terminal-Icons
Path              : C:\Users\40294\Documents\PowerShell\Modules\Terminal-Icons\0.11.0\Terminal-Icons.psm1
Description       : PowerShell module to add file icons to terminal based on file extension
Guid              : 4419ddb6-3528-47cd-baf3-7fb9d8566620
Version           : 0.11.0
ModuleBase        : C:\Users\40294\Documents\PowerShell\Modules\Terminal-Icons\0.11.0
ModuleType        : Script
PrivateData       : {[PSData, System.Collections.Hashtable]}
AccessMode        : ReadWrite
ExportedAliases   : {}
ExportedCmdlets   : {}
ExportedFunctions : {[Add-TerminalIconsColorTheme, Add-TerminalIconsColorTheme], [Add-TerminalIconsIconTheme, Add-TerminalIconsIconTheme], [Format-TerminalIcons, Format-TerminalIcons], [Get-TerminalIconsColorTheme, Get-TerminalIconsColorTheme]…}
ExportedVariables : {}
NestedModules     : {}


Name              : z
Path              : C:\Users\40294\Documents\PowerShell\Modules\z\1.1.14\z.psm1
Description       : z lets you quickly navigate the file system in PowerShell based on your cd command history. It's a port of the z bash shell script
Guid              : bc198554-ae1f-4ab6-84ce-5d3a41b74553
Version           : 1.1.14
ModuleBase        : C:\Users\40294\Documents\PowerShell\Modules\z\1.1.14
ModuleType        : Script
PrivateData       : {[PSData, System.Collections.Hashtable]}
AccessMode        : ReadWrite
ExportedAliases   : {}
ExportedCmdlets   : {}
ExportedFunctions : {[cdX, cdX], [popdX, popdX], [pushdX, pushdX], [z, z]}
ExportedVariables : {}
NestedModules     : {}


40294@antlrv D:\tempFiles\WebDemos\my-react-app git:123 ~1
>

```

<img width=""721"" height=""159"" alt=""Image"" src=""https://github.com/user-attachments/assets/4ccb8651-eae8-4040-a8ab-5064b41d62a7"" />

<img width=""730"" height=""512"" alt=""Image"" src=""https://github.com/user-attachments/assets/12cd18b5-b22f-4e84-870d-e5b7b6411b4d"" />

<img width=""1037"" height=""286"" alt=""Image"" src=""https://github.com/user-attachments/assets/2ebdf6c4-224f-4e36-9383-5a3102f991ca"" />
",1
Splitting e.g. Terminal and Problems is impossible to undo,"
Type: <b>Bug</b>

Open the terminal view (bottom window) and drag the ""TERMINAL"" to either side of the window to split the view, resulting in e.g. TERMINAL and PROBLEMS to be shown side-by-side. It is impossible to reset this view and all things that were ever split out remain separate.

VS Code version: Code 1.100.2 (848b80aeb52026648a8ff9f7c45a9b0a80641e2e, 2025-05-14T21:47:40.416Z)
OS version: Linux x64 6.12.32
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|13th Gen Intel(R) Core(TM) i9-13900H (20 x 3026)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: disabled_off<br>webnn: disabled_off|
|Load (avg)|1, 1, 1|
|Memory (System)|31.02GB (19.83GB free)|
|Process Argv|. --crash-reporter-id c3804b5c-8936-4c8f-8bda-86e1efa7351e|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|plasma|
|XDG_CURRENT_DESKTOP|KDE|
|XDG_SESSION_DESKTOP|KDE|
|XDG_SESSION_TYPE|wayland|
</details><details><summary>Extensions (21)</summary>

Extension|Author (truncated)|Version
---|---|---
continue|Con|1.1.56
dart-code|Dar|3.114.2
flutter|Dar|3.114.0
vscode-eslint|dba|3.0.15
vscode-deno|den|3.44.2
gitlens|eam|17.2.2
RunOnSave|eme|0.2.7
prettier-vscode|esb|11.0.0
copilot|Git|1.338.0
copilot-chat|Git|0.27.3
go|gol|0.48.0
treefmt-vscode|ibe|2.2.1
nix-ide|jno|0.4.18
Kotlin|mat|1.7.1
debugpy|ms-|2025.8.0
python|ms-|2025.8.0
vscode-pylance|ms-|2025.6.2
proto|pet|0.0.4
rust-analyzer|rus|0.3.2519
svelte-vscode|sve|109.9.0
vscode-icons|vsc|12.13.0


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
jhi8h917:31341130
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
"Allow filtering what symbols should appear in ""Go to symbol"" to allow a search similar to Sublime Text","<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

### Problem

When I use *Go to symbol* I almost always use it to go to a method.

In Sublime Text you can search for methods and class definitions using the `@` prefix in the fuzzy search (Ctrl + R).

<img width=""608"" height=""130"" alt=""Image"" src=""https://github.com/user-attachments/assets/e9c9f1ce-f890-4559-9616-a1c202c30b45"" />

In VSCode we can use *Go to symbol*, but it includes <ins>all</ins> type of symbols.

<img width=""608"" height=""366"" alt=""Image"" src=""https://github.com/user-attachments/assets/66da265c-5596-48f5-97b0-d5c86306ef02"" />

Even if we use `@:` to group the symbols, then we still have the problem that anonymous functions are included, as can be seen in the screenshot above (`rows.map`).

In Lua it's even worse with it including parameters (perhaps because of an extension), whereas in Sublime Text it neatly lists the methods.

<img width=""1044"" height=""435"" alt=""Image"" src=""https://github.com/user-attachments/assets/dda37af6-8475-44c5-be7d-f9dfbc6deeed"" />

### Request

*Go to symbol* should be modified to allow for two things:
1. Set which types of symbols should appear in the fuzzy search.
2. Set whether you want to include anonymous functions or not.",0
hung up spinning,"
Type: <b>Bug</b>

large file about 1059 lines long. C#, sits at ""generating edits"" or sits at editing 100%. 

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|13th Gen Intel(R) Core(TM) i9-13950HX (32 x 2419)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|63.69GB (34.24GB free)|
|Process Argv|--crash-reporter-id f4cc210b-1730-4e14-a2a2-ee0f82f36c6b|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
convertlamdaf:31329270
c4b42873:31341128
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
nes-diff-11:31337487
nes-set-on:31340697
testaa123:31335226
6abeh943:31336334
yijiwantestdri0626-c:31336931
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Won't upgrade,"
Type: <b>Bug</b>

Click uopgrade

Extension version: 0.28.5
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin x64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz (12 x 2600)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|9, 8, 6|
|Memory (System)|16.00GB (0.08GB free)|
|Process Argv|--crash-reporter-id e8a58910-a9d6-4664-8363-be98b72e4d51|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551:31249599
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
j6a0c355:31339598
d2249276:31341129
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
82j33506:31327384
nes-diff-11:31337487
nes-set-on:31340697
testaa123cf:31335227
6abeh943:31336334
yijiwantestdri0626-t:31336930
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
Add support for `structuredContent`,"The latest [MCP spec](https://modelcontextprotocol.io/specification/2025-06-18/changelog) from 18th June adds a new return type: instead of the unstructured `content`, output with output schema should return a `structuredContent` object. See [here](https://modelcontextprotocol.io/specification/2025-06-18/server/tools#structured-content) more.

### Current behavior

VS Code agent mode is expecting only a `content` object as a return from a tool, and fails if the tool returns `structuredContent`.

### Expected behavior

 VS Code copilot in agent mode should be able to process both `content` and `structuredContent` outputs",1
Please fuck fix this login issue,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 
- OS Version: 

Steps to Reproduce:
I can access YouTube and Google normally

<img width=""1014"" height=""987"" alt=""Image"" src=""https://github.com/user-attachments/assets/c58e2304-d66e-4f6d-83f5-0f05ad95d6fd"" />",0
Copilot Chat: enabled copilotDebugCommand creates invalid PATH on Linux SSH remotes,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.101.2
- Copilot Chat Extension 0.28.4
- OS Version: Windows 10 (VSCode UI) / Ubuntu 22.04 (SSH remote)

Steps to Reproduce:

1. connect to a Linux SSH remote
2. enable Copilot Chat Debug Command (`""github.copilot.chat.copilotDebugCommand.enabled"": true`)
3. open a terminal
4. an error message is printed in the terminal (e.g. `""c:UsersRolandAppDataRoamingCodeUserglobalStoragegithub.copilot-chatdebugCommand: command not found""`)
5. the PATH environment variable ends with `"";c:\Users\Roland\AppData\Roaming\Code\User\globalStorage\github.copilot-chat\debugCommand""`
> $ echo $PATH
/home/ubuntu/bin:/home/ubuntu/.vscode-server/bin/c420c5bd6db8a186a5777c3af51ff804d08af0c3/bin/remote-cli:/home/ubuntu/bin:/home/ubuntu/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/ubuntu/.fzf/bin;c:\Users\Roland\AppData\Roaming\Code\User\globalStorage\github.copilot-chat\debugCommand

This corrupts the second to last PATH element, i.e. in this case it becomes  `/home/ubuntu/.fzf/bin;c` - which does not resolve, of course.
Also, I do not understand why this Windows path is appended in the fist place.

Setting `""github.copilot.chat.copilotDebugCommand.enabled"": false` stops the problematic behavior.

",1
"Chat blocked on old messages, doesn't follow selected code","
Type: <b>Bug</b>

I just chatted with him and it suddenly started to chat about old code


Extension version: 0.28.5
VS Code version: Code 1.101.0 (dfaf44141ea9deb3b4096f7cd6d24e00c147a4b1, 2025-06-11T15:00:50.123Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-9300H CPU @ 2.40GHz (8 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|7.84GB (1.43GB free)|
|Process Argv|--crash-reporter-id aa580c3c-96f6-429c-85ad-1e629b813849|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
d2249276:31341129
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
testaa123cf:31335227
6abeh943:31336334
yijiwantestdri0626-c:31336931
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
my codes are not geting run,"
Type: <b>Feature Request</b>

my codes are not getting executed

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:


<!-- generated by issue reporter -->",0
Separator line in Single Editor Layout,"Dear Community,

I’ve been using VS Code for a few days and have run into a strange phenomenon that even ChatGPT couldn’t help me with. When I code in the single editor layout, a horizontal divider line appears that splits my workspace into two columns. I can’t get rid of this second column, nor can I move the divider line. I’ve already disabled the following settings:

""workbench.editor.centeredLayoutAutoResize"": false,
""workbench.editor.centeredLayoutEnabled"": false,
""workbench.editor.limit.centeredLayoutWidth"": false
and turned off the indentation guides, but none of it helped. I’m now assuming this is a bug.

Maybe someone can reproduce this. As I said, I am working with the Single Editor so I SHOULD get just one column to code in.

Greetings, 

Phil



<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: Version: 1.101.2 (Universal)
Commit: 2901c5ac6db8a986a5666c3af51ff804d05af0d4
Date: 2025-06-24T20:27:15.391Z
Electron: 35.5.1
ElectronBuildId: 11727614
Chromium: 134.0.6998.205
Node.js: 22.15.1
V8: 13.4.114.21-electron.0
OS: Darwin arm64 24.5.0
- 
- OS Version: macOS 15.5

Steps to Reproduce:

1. Create a new Project ( I started a flutter/dart project)
2. 

<img width=""1968"" height=""635"" alt=""Image"" src=""https://github.com/user-attachments/assets/60ed2a1f-e4fb-4f2b-9050-a27050505d7c"" />
",0
[Copilot] [Suggestion] Default prompt for repository,"Hello!

I'm using Copilot in a large monorepo, and I can see that sometimes it struggles to compile something the way it should. This is especially the case for monorepos that compose different languages. 
This is greatly mitigated if, at the beginning of each session, I introduce a default prompt explaining the structure of the monorepo, how to compile things, where to play around, where to always keep as it is, how to run tests for each folder, etc...

Couldn't we have a special file in the repo that Copilot would automatically use to set up context for each new session we create? That way, we could avoid the trouble of inputting detailed info for the project now and then",0
Port Forwarding does not work and download ps2.ps1 file,"
Type: <b>Bug</b>

I host a simple web server using AioHttp:
```python
from aiohttp import web


# Serve static HTML file
async def handle(request):
    return web.FileResponse(""static/scan.html"")

# Create app
app = web.Application()
app.router.add_get(""/"", handle)
app.router.add_static(""/static/"", path=""static"", name=""static"")

if __name__ == ""__main__"":
    web.run_app(app, host=""0.0.0.0"", port=9999)
```

I do port forwarding with 9999 port and when I open the link it downloads ps2.ps1 file, also when I not enable server it download same file.

<img width=""675"" height=""130"" alt=""Image"" src=""https://github.com/user-attachments/assets/efc7649b-0476-442a-acdf-9dd1a7d30e76"" />

<img width=""346"" height=""62"" alt=""Image"" src=""https://github.com/user-attachments/assets/0bf78a9e-79a5-4171-8b28-a47a4758840f"" />

`curl 127.0.0.1:9999` Works:
<img width=""338"" height=""58"" alt=""Image"" src=""https://github.com/user-attachments/assets/1b837d2c-1d6c-4c8c-a4cf-f450a898a747"" />

Logs:
```
2025-07-06 14:23:33.819 [info] [forwarding] starting CLI
2025-07-06 14:23:33.859 [info] [forwarding] [2025-07-06 14:23:33] trace Found token in keyring
2025-07-06 14:23:34.927 [info] [forwarding] [2025-07-06 14:23:34] trace wsl availability detected via subprocess
2025-07-06 14:23:34.927 [info] [forwarding] [2025-07-06 14:23:34] trace Found token in keyring
2025-07-06 14:23:35.461 [info] [forwarding] [2025-07-06 14:23:35] trace Found token in keyring
2025-07-06 14:23:36.172 [info] [forwarding] [2025-07-06 14:23:36] debug Starting tunnel to server...
2025-07-06 14:23:36.172 [info] [forwarding] [2025-07-06 14:23:36] trace Found token in keyring
2025-07-06 14:23:36.596 [info] [forwarding] [2025-07-06 14:23:36] debug Connected to tunnel endpoint: TunnelRelayTunnelEndpoint { base: TunnelEndpoint { id: ""d07a1730-32e1-4814-8be9-7b46bc96869e-relay"", connection_mode: TunnelRelay, host_id: ""d07a1730-32e1-4814-8be9-7b46bc96869e"", host_public_keys: [], port_uri_format: Some(""https://m5lzv5kd-{port}.euw.devtunnels.ms/""), tunnel_uri: Some(""https://m5lzv5kd.euw.devtunnels.ms/""), port_ssh_command_format: Some(""ssh m5lzv5kd-{port}@ssh.euw.devtunnels.ms""), tunnel_ssh_command: Some(""ssh m5lzv5kd@ssh.euw.devtunnels.ms""), ssh_gateway_public_key: None }, host_relay_uri: Some(""wss://euw-data.rel.tunnels.api.visualstudio.com/api/v1/Host/Connect/peaceful-pond-qnz0srl""), client_relay_uri: Some(""wss://euw-data.rel.tunnels.api.visualstudio.com/api/v1/Client/Connect/peaceful-pond-qnz0srl"") }
2025-07-06 14:23:36.596 [info] [forwarding] [2025-07-06 14:23:36] trace Found token in keyring
2025-07-06 14:23:36.597 [info] [forwarding] [2025-07-06 14:23:36] trace Found token in keyring
2025-07-06 14:23:37.278 [info] [forwarding] [2025-07-06 14:23:37] info forwarding http port 9999 at Private
2025-07-06 14:26:58.024 [info] [forwarding] [2025-07-06 14:26:58] trace Found token in keyring
2025-07-06 14:26:58.880 [info] [forwarding] [2025-07-06 14:26:58] info stopped forwarding http port 9999 at Private
2025-07-06 14:27:02.829 [info] [forwarding] [2025-07-06 14:27:02] trace Found token in keyring
2025-07-06 14:27:03.399 [info] [forwarding] [2025-07-06 14:27:03] info forwarding http port 9999 at Private
```

UPD: The script send some request and collecting data, and messing with registry 🤔
UPD 2: Opening with other IP works, with Ukraine IP it downloads some script...

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 5 1600 Six-Core Processor             (12 x 3194)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.92GB (8.24GB free)|
|Process Argv|--disable-extensions --crash-reporter-id 13770298-2c95-471e-9a48-923d56603311|
|Screen Reader|no|
|VM|0%|
</details>Extensions disabled<details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492cf:30256860
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
9d2cg352:31339597
c4b42873:31341128
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
4f60g487:31327383
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
Message not replying ,"
Type: <b>Bug</b>

screenshot sent 



Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 7 5700U with Radeon Graphics          (16 x 1797)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.33GB (6.77GB free)|
|Process Argv|--crash-reporter-id ed5e6b2e-a295-4090-a287-2ce03f975aa0|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
convertlamdat:31329272
d2249276:31341129
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
testaa123cf:31335227
6abeh943:31336334
yijiwantestdri0626-c:31336931
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Copilot network error when docker network created/removed,"
Type: <b>Bug</b>

When run docker compose up/down command during copilot session  coplilot request is failing.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Linux x64 6.15.3-2-default
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 7 PRO 7840U w/ Radeon 780M Graphics (16 x 2370)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: disabled_off<br>webnn: disabled_off|
|Load (avg)|1, 1, 0|
|Memory (System)|30.00GB (17.04GB free)|
|Process Argv|--crash-reporter-id a690cd21-953a-4e5f-bab5-f979c163f169|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|gnome|
|XDG_CURRENT_DESKTOP|GNOME|
|XDG_SESSION_DESKTOP|gnome|
|XDG_SESSION_TYPE|wayland|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
9d2cg352:31339597
0g0a1943:31341127
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
Claimed untrue things,"
Type: <b>Bug</b>

This is what happened


""
Beomene: It is flagging something as wrong

GitHub Copilot: The likely issue is a filename mismatch:  
Your file is named TextBoxRenderer.js (with a capital ""B""), but your import statement is:
```js
import { TextBoxManager } from './TextBoxManager.js';
```
If the actual file is named `TextboxManager.js` (with a lowercase ""b""), this will cause a module not found error on case-sensitive systems or some build tools.

**How to fix:**  
Make sure the import matches the actual filename exactly:
- If your file is `TextboxManager.js`, change the import to:
  ```js
  import { TextBoxManager } from './TextboxManager.js';
  ```
- If your file is TextBoxManager.js, keep the import as is.

Would you like me to update the import statement to match your file's actual name?

Beomene: You are in agent mode, so don't give me a lecture or be a teacher - work and find the problem

GitHub Copilot: 
````

````


Made changes.

The import path and class name in TextBoxRenderer.js are correct and match the actual file and export. If there is still an error flagged, it is not due to a filename or import mismatch. If you are seeing a specific error message, please provide it so I can resolve the exact issue. Otherwise, the code structure is correct as written.

Beomene: ....... does.... the...... TextBoxManager.js have ""export"" at the end, or not? Does it need to have such code so that TextBoxRenderer.js can actually import it?

GitHub Copilot: Yes, your TextBoxManager.js file already has the correct export statement at the top:
```js
export class TextBoxManager {
```
This is all that is needed for TextBoxRenderer.js to import it using:
```js
import { TextBoxManager } from './TextBoxManager.js';
```
You do not need an additional `export` at the end of the file. The class export at the top is correct and sufficient for ES module imports. No changes are needed for exporting.

Beomene: It's still being flagged as wrong

GitHub Copilot: The problem is a filename case mismatch. Your file is named `TextboxManager.js` (with a lowercase ""b""), but your import statement uses TextBoxManager.js (with an uppercase ""B""). This causes issues on case-sensitive systems and some build tools.

To fix it, change the import in TextBoxRenderer.js to:
```js
import { TextBoxManager } from './TextboxManager.js';
```
Would you like me to make this change for you?

Beomene: I think you base that on what you see in our chat... Do not base your logic on the previous guesses you've made. Truly look at the code we have.

I can't find ""TextboxManager"" mentioned anywhere.

Everywhere it says ""TextBoxManager"", doesn't it?

GitHub Copilot: You are correct. Every reference in your code is to ""TextBoxManager"" (with a capital ""B""), and your file is named ""TextBoxManager.js"". There are no references to ""TextboxManager"" (with a lowercase ""b"") anywhere in your codebase.

If you are still seeing a case-related error, it may be due to a cached build artifact or an editor/OS issue. The code and filenames are consistent and correct as they are. No further changes are needed regarding the class or filename casing. If the error persists, try restarting your development server or clearing any build cache.
""

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-2600 CPU @ 3.40GHz (8 x 3411)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: unavailable_off<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.98GB (7.96GB free)|
|Process Argv|G:\\trinkaloop\\test --crash-reporter-id b2095749-a94e-4f20-adf9-207f969a79a1|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
vscrpc:30673769
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
c4b42873:31341128
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
testaa123cf:31335227
6abeh943:31336334
yijiwantestdri0626-c:31336931
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
No highlight in template string if enabled preferTreeSitter.typescript in Abyss color theme,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.101.2
- OS Version: Windows 11 24H2

Steps to Reproduce:

1. `""editor.experimental.preferTreeSitter.typescript"": true`
2. <img width=""695"" height=""83"" alt=""Image"" src=""https://github.com/user-attachments/assets/350f0a79-69c2-40c3-a65c-c847665bd1b6"" />
",0
Just terrible,"You really need to stop with the quantization of these models, they are dumber than a heavily quantized open source LLM. You should be paying us to use them! The quality of the output from these models are terrible and a complete waste of premium tokens.",0
UI improvement - no shortcut for column selection mode,"
Type: <b>Feature Request</b>

Hello,
as it's common to edit multiple columns by using the multiple column selection mode, it would be very convinient for users to use a shortcut to activate this text editing mode. Now users may only access this nice feature by using menu through pointing with mice. As generaly code editing requires mainly the use of keyboard, continuous changes between the usage of keyboard and mouse is disturbing the users of vsc.

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:


<!-- generated by issue reporter -->",1
Extension Signature Verification Failed: Go,"
Type: <b>Bug</b>

Please include following log `F1 > Open View... > Shared` below.

2025-07-06 12:00:12.370 [info] [Shared] Getting Manifest... golang.go
2025-07-06 12:00:12.428 [info] [Shared] Installing extension: golang.go {""pinned"":false,""productVersion"":{""version"":""1.101.2"",""date"":""2025-06-24T20:27:57.062Z""},""operation"":3,""isApplicationScoped"":false,""profileLocation"":{""$mid"":1,""external"":""vscode-userdata:/home/allan/.vscode/extensions/extensions.json"",""path"":""/home/allan/.vscode/extensions/extensions.json"",""scheme"":""vscode-userdata""}}
2025-07-06 12:00:13.365 [info] [Shared] Extension signature verification result for golang.go: UnknownError. Executed: false. Duration: 435ms.
2025-07-06 12:00:13.367 [error] [Shared] Error while installing the extension golang.go Signature verification failed with 'UnknownError' error. vscode-userdata:/home/allan/.vscode/extensions/extensions.json
2025-07-06 12:00:13.368 [error] [Shared] [uncaught exception in sharedProcess]: Signature verification failed with 'UnknownError' error.: SignatureVerificationInternal: Signature verification failed with 'UnknownError' error.
    at $h.Bb (file:///nix/store/agpig8x5mf0zlbadr97xs2iymjmkdbwq-vscode-latest/lib/vscode/resources/app/out/vs/code/electron-utility/sharedProcess/sharedProcessMain.js:74:64839)
    at async $h.Ab (file:///nix/store/agpig8x5mf0zlbadr97xs2iymjmkdbwq-vscode-latest/lib/vscode/resources/app/out/vs/code/electron-utility/sharedProcess/sharedProcessMain.js:74:63310)


VS Code version: Code 1.101.1 (18e3a1ec544e6907be1e944a94c496e302073435, 2025-06-18T13:35:12.605Z)
OS version: Linux x64 6.6.94
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz (36 x 3646)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: disabled_off<br>webnn: disabled_off|
|Load (avg)|1, 1, 1|
|Memory (System)|62.48GB (54.93GB free)|
|Process Argv|fctest/ --crash-reporter-id 1b7cf89c-2a33-465e-a7d3-6433d8e51e0c|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|plasma|
|XDG_CURRENT_DESKTOP|KDE|
|XDG_SESSION_DESKTOP|KDE|
|XDG_SESSION_TYPE|x11|
</details><details><summary>Extensions (38)</summary>

Extension|Author (truncated)|Version
---|---|---
google-cloud-platform-ssh|and|1.0.0
vscode-tailwindcss|bra|0.14.22
docker|doc|0.10.0
copilot|Git|1.338.0
copilot-chat|Git|0.28.0
go|gol|0.46.1
git-graph|mhu|1.30.0
vscode-containers|ms-|2.0.3
csharp|ms-|2.80.16
vscode-dotnet-runtime|ms-|2.3.6
vscode-kubernetes-tools|ms-|1.3.24
debugpy|ms-|2025.8.0
python|ms-|2025.8.0
remote-ssh|ms-|0.120.0
remote-ssh-edit|ms-|0.87.0
vscode-remote-extensionpack|ms-|0.26.0
cmake-tools|ms-|1.20.53
cpptools|ms-|1.25.3
cpptools-extension-pack|ms-|1.3.1
makefile-tools|ms-|0.12.17
powershell|ms-|2025.0.0
remote-explorer|ms-|0.5.0
remote-server|ms-|1.5.2
openiap-assistant|ope|0.0.40
java|red|1.42.0
vscode-yaml|red|1.18.0
rust-analyzer|rus|0.3.2500
markdown|sta|2.0.10
svelte-vscode|sve|109.8.1
vscode-lldb|vad|1.11.5
vscode-gradle|vsc|3.16.4
vscode-java-debug|vsc|0.58.2
vscode-java-dependency|vsc|0.24.1
vscode-java-pack|vsc|0.29.2
vscode-maven|vsc|0.44.0
eno|Wsc|2.3.53
php-debug|xde|1.36.1
pretty-ts-errors|Yoa|0.6.1


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
c4b42873:31341128
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
onetestforazureexpcf:31335614
6abeh943:31336334
yijiwantestdri0626-c:31336931
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
Extensions are stuck installing in WSL mode,"
Type: <b>Bug</b>

I have a Vscode installed in Windows, which is working in WSL mode via a extension. 

When project is opened in WSL mode - Im trying to install extensions in WSL, and they get stuck. I see no progress and cannot find related logs. They are just not being installed.

<img width=""445"" height=""526"" alt=""Image"" src=""https://github.com/user-attachments/assets/c19a14a1-2553-4001-9b2b-3169c3f00a3c"" />

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:
Remote OS version: Linux x64 5.15.133.1-microsoft-standard-WSL2

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i9-11900K @ 3.50GHz (16 x 3504)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|63.85GB (38.29GB free)|
|Process Argv|--crash-reporter-id bc0eb2b7-c544-4f6a-b768-f034cb56f7c9|
|Screen Reader|no|
|VM|0%|

|Item|Value|
|---|---|
|Remote|WSL: Debian|
|OS|Linux x64 5.15.133.1-microsoft-standard-WSL2|
|CPUs|11th Gen Intel(R) Core(TM) i9-11900K @ 3.50GHz (16 x 0)|
|Memory (System)|31.27GB (29.29GB free)|
|VM|0%|
</details><details><summary>Extensions (10)</summary>

Extension|Author (truncated)|Version
---|---|---
npm-outdated|msk|2.2.0
jupyter-keymap|ms-|1.1.2
remote-containers|ms-|0.417.0
remote-ssh|ms-|0.120.0
remote-ssh-edit|ms-|0.87.0
remote-wsl|ms-|0.99.0
vscode-remote-extensionpack|ms-|0.26.0
remote-explorer|ms-|0.5.0
remote-server|ms-|1.5.2
pdf|tom|1.2.2


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551cf:31249601
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
convertlamdaf:31329270
c4b42873:31341128
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
b6b4d950:31327385
nes-diff-11:31337487
testaa123cf:31335227
6abeh943:31336334
yijiwantestdri0626-t:31336930
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
screen flickring,"
Type: <b>Bug</b>

when i open vs code my laptop screen flickers many times its feels so irritating and feels like to break the laptop plz solve or help me i  this issue on  my laptop


VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-5200U CPU @ 2.20GHz (4 x 2195)|
|GPU Status|2d_canvas: unavailable_software<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: disabled_software<br>multiple_raster_threads: enabled_on<br>opengl: disabled_off<br>rasterization: disabled_software<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: disabled_software<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: unavailable_software<br>webgl2: unavailable_software<br>webgpu: unavailable_software<br>webnn: unavailable_software|
|Load (avg)|undefined|
|Memory (System)|7.92GB (2.61GB free)|
|Process Argv|--crash-reporter-id 24f6d403-79cb-4648-bf7e-a66554fc4a73|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (1)</summary>

Extension|Author (truncated)|Version
---|---|---
LiveServer|rit|5.7.9


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551:31249599
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
9b8hh234:30694863
2e7ec940:31000449
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
j6a0c355:31339598
0g0a1943:31341127
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
b6b4d950:31327385
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
Encryption has stopped working,"
Type: <b>Bug</b>

In v1.99.3, I am successfully using the Encryptor extension (https://marketplace.visualstudio.com/items/?itemName=RushFrisby.Encryptor) to encrypt and decrypt files using the encryptfile and decryptfile commands.
In the latest v1.101.2, neither the above, nor the EncrypThor (https://marketplace.visualstudio.com/items/?itemName=maxlmuller.EncrypThor) extension works. Both extensions do nothing when the encryptfile or decryptfile commands are issued.

I rely on encryption working, so I have reverted to v1.99.3 until the issue is fixed. I have confirmed that the encryption commands work when I revert the Code version to 1.99.3.
I don't know whether the issue is with the extensions or with Code, but I've marked the issue as one for Code.

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.22631
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|13th Gen Intel(R) Core(TM) i7-13800H (20 x 2918)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|31.62GB (15.35GB free)|
|Process Argv|--crash-reporter-id ddc27ede-2374-4ad7-9bba-9892ce2f581d|
|Screen Reader|no|
|VM|67%|
</details><details><summary>Extensions (17)</summary>

Extension|Author (truncated)|Version
---|---|---
read-only-indicator|ale|3.11.0
vscode-edit-csv|jan|0.11.5
simple-csv-syntax|jef|0.0.3
vscode-clangd|llv|0.2.0
markdown-shortcuts|mdi|0.12.0
debugpy|ms-|2025.8.0
python|ms-|2025.8.0
vscode-pylance|ms-|2025.6.2
jupyter|ms-|2025.5.0
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.1.0
vscode-jupyter-cell-tags|ms-|0.1.9
vscode-jupyter-slideshow|ms-|0.1.6
Encryptor|Rus|0.3.1
partial-diff|ryu|1.4.3
sf-xml-formatter|Swa|0.0.3
markdown-all-in-one|yzh|3.6.3


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyonecf:30548226
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
jhi8h917:31341130
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
Github copilot enter button failing in wsl,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.101.2
2901c5ac6db8a986a5666c3af51ff804d05af0d4
x64
- OS Version: 10.0.26100 N/A Build 26100 | win 11 pro & wsl - ubuntu 24

Steps to Reproduce:

1. Open code on wsl 
2. Open chat on code, enter a prompt and press enter
3. the enter does not work and displays the below issue:

<img width=""947"" height=""943"" alt=""Image"" src=""https://github.com/user-attachments/assets/bd983bfb-d02a-4c99-aacb-48f070cfe4f6"" />
<img width=""831"" height=""185"" alt=""Image"" src=""https://github.com/user-attachments/assets/ec7024c6-bad9-4ed3-8249-66462b925ff4"" />
",0
check issue,"
Type: <b>Bug</b>

check issue

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i3 CPU       M 370  @ 2.40GHz (4 x 2394)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: unavailable_off<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: unavailable_off<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: unavailable_off<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|5.80GB (1.74GB free)|
|Process Argv|--crash-reporter-id 6c687188-0b4e-4268-bd06-315d285abbed|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492:30256859
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
convertlamdat:31329272
c4b42873:31341128
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
Error While fetching extinction. Failed to fetch,This error appeared recently and I don't use any proxy and it appeared in the two versions of the normal Vscode and vscode iniders. I don't know what to do. I made sure of the time and turned off the firewall and it still doesn't work.,1
Keep terminal name after task ends,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
When using ""panel"": ""dedicated"" in tasks.json, the terminal name resets after the task ends. I'd like to be able to preserve the task label or a custom name like ""terminalName"" even when the task finishes, to keep the terminal organized.

Or more that I can keep the mane of the task that has been triggered even after it ends so that I can find my way back without having to go and open all of them.",0
Delegate to coding agent: consider a custom dialog explaining more,"The dialog today is cryptic:

<img width=""396"" height=""498"" alt=""Image"" src=""https://github.com/user-attachments/assets/f0566eaf-5a8e-44b7-9513-9549955dd250"" />

I suggest a custom dialog with markdown and nice styling explaining the concept. Specifically ""...will continue your work in <repo>"" is not clear to me.

//cc @mrleemurray @rebornix ",0
Delegate to coding agent: revisit location of action and enablement,"Having used this feature over the weekend, I find the interaction model strange:

<img width=""259"" height=""78"" alt=""Image"" src=""https://github.com/user-attachments/assets/2635266c-c2d6-41e4-96b4-7606444235b8"" />

I can build up context in a chat session but then to finally hand it off to the remote agent, I need to type something in the chat input, I cannot just click the cloud icon, so I have to invent something like ""Start coding"".

I think the action should better be located in the view title menu that contains actions targeting the entire chat and not the prompt.

There is also the additional confusion that neither the mode nor the model plays any role when I click the ☁ icon.

//cc @rebornix ",0
close braces correction error thrown even after correction in VS code,"
Type: <b>Bug</b>

after correcting the braces and restarting VS code copilot still throws same error correcting braces.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.22631
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|13th Gen Intel(R) Core(TM) i7-1355U (12 x 2611)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.69GB (3.45GB free)|
|Process Argv|--crash-reporter-id 1a91ab76-8135-4419-af46-a2fde0ed7644|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
j6a0c355:31339598
convertlamdat:31329272
c4b42873:31341128
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
onetestforazureexp:31335613
6abeh943:31336334
yijiwantestdri0626-c:31336931
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
feature request: Function focus mode for scoped editing in large files,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

When working with very large source files, it can be challenging to focus on and edit a specific function or code block. I suggest introducing a ""Function Focus Mode"" that enables users to:

- Show only the source code of a selected function or code scope (e.g., class, method, or block).
- Restrict all editing, search, and replace operations to the visible range.
- Seamlessly switch between focused and full-file views.
- Optionally, allow expanding the focus to include related scopes (e.g., adjacent functions, containing classes).

**Benefits:**

- Helps avoid the common pitfall where, while scrolling through a long file, the user unintentionally ends up viewing or editing a different function—ensuring focus remains strictly on the intended code block.
- Makes it easier to work with large files by reducing distractions.
- Prevents accidental edits outside the targeted scope.
- Improves code review and refactoring workflows.

**Example Workflow:**

1. User right-clicks a function in the outline or code and selects “Focus on Function.”
2. The editor hides all other code except for the chosen function.
3. All editing and search/replace operations apply only within the focused scope.
4. User can exit focus mode to return to the full file as needed.

**Related Features:**  
While VS Code supports code folding, Go to Symbol, and search within selection, none of these restrict the editor context and operations as strictly as described above.
",0
[feat] Introduce `json.schemas[XXX].cache` to always use latest generated local jsonschema,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

## scenario

When devloping jsonschema

```json
{
  ""$schema"": ""file:///tmp/gen/reproduce.schema.json#42"",
  ""value"": ""42"",
}
```

## problem

After generating new `/tmp/gen/reproduce.schema.json`, need to workaround vscode schema cache.

```json
{
  ""$schema"": ""file:///tmp/gen/reproduce.schema.json#43"",
  ""value"": ""42"",
}
```

## feat

Introduce `json.schemas[XXX].cache` to always use latest generated local jsonschema.

```jsonc
    ""json.schemas"": [
      {
        ""fileMatch"": [
          ""/reproduce.json""
        ],
        ""url"": ""file:///tmp/gen/reproduce.schema.json"",
        ""cache"": false,
      }
    ],
```
",0
[BUG] [json] [jsonschema] tab completion shows incomplete enum candidate,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version:
- OS Version:

```
Version: 1.101.2
Commit: 2901c5ac6db8a986a5666c3af51ff804d05af0d4
Date: 2025-06-24T20:27:15.391Z
Electron: 35.5.1
ElectronBuildId: 11727614
Chromium: 134.0.6998.205
Node.js: 22.15.1
V8: 13.4.114.21-electron.0
OS: Linux x64 6.12.29-1-lts
```

Steps to Reproduce:

1.
2.

## bug

[json] [jsonschema] tab completion shows incomplete enum candidate

## prepare

root struct `Reproduce` has required enum `Node` property.

enum `Node` maybe `Foo` `Bar` `Baz`.

`Foo` `Bar` has optional string property.

`Baz` has required string property.

<details>
<summary>rust</summary>

```rust
#[derive(Debug, Deserialize, JsonSchema)]
#[serde(deny_unknown_fields)]
pub struct Reproduce {
    #[serde(rename = ""$schema"")]
    pub schema: Option<String>,
    pub node: Node,
}

#[derive(Debug, Deserialize, JsonSchema)]
#[serde(deny_unknown_fields)]
#[serde(tag = ""type"")]
pub enum Node {
    Foo(Foo),
    Bar(Bar),
    Baz(Baz),
}

#[derive(Debug, Deserialize, JsonSchema)]
#[serde(deny_unknown_fields)]
pub struct Foo {
    pub foo: Option<String>,
}

#[derive(Debug, Deserialize, JsonSchema)]
#[serde(deny_unknown_fields)]
pub struct Bar {
    pub bar: Option<String>,
}

#[derive(Debug, Deserialize, JsonSchema)]
#[serde(deny_unknown_fields)]
pub struct Baz {
    pub baz: String,
}
```

</details>

<details>
<summary>schema</summary>

```json
{
  ""$schema"": ""https://json-schema.org/draft/2020-12/schema"",
  ""title"": ""Reproduce"",
  ""type"": ""object"",
  ""properties"": {
    ""$schema"": {
      ""type"": [
        ""string"",
        ""null""
      ]
    },
    ""node"": {
      ""$ref"": ""#/$defs/Node""
    }
  },
  ""additionalProperties"": false,
  ""required"": [
    ""node""
  ],
  ""$defs"": {
    ""Node"": {
      ""oneOf"": [
        {
          ""type"": ""object"",
          ""properties"": {
            ""foo"": {
              ""type"": [
                ""string"",
                ""null""
              ]
            },
            ""type"": {
              ""type"": ""string"",
              ""const"": ""Foo""
            }
          },
          ""additionalProperties"": false,
          ""required"": [
            ""type""
          ]
        },
        {
          ""type"": ""object"",
          ""properties"": {
            ""bar"": {
              ""type"": [
                ""string"",
                ""null""
              ]
            },
            ""type"": {
              ""type"": ""string"",
              ""const"": ""Bar""
            }
          },
          ""additionalProperties"": false,
          ""required"": [
            ""type""
          ]
        },
        {
          ""type"": ""object"",
          ""properties"": {
            ""baz"": {
              ""type"": ""string""
            },
            ""type"": {
              ""type"": ""string"",
              ""const"": ""Baz""
            }
          },
          ""additionalProperties"": false,
          ""required"": [
            ""type"",
            ""baz""
          ]
        }
      ]
    }
  }
}
```

</details>

## reproduce

```json
{
  ""$schema"": ""file:///tmp/gen/reproduce.schema.json#42"",
  ""node"": {
    ""type"": ""<TAB></TAB>""
  }
}
```

## epxect

enum `Node` maybe `Foo` `Bar` `Baz`.

`Foo` `Bar` has optional string property.

`Baz` has required string property.

TAB show

- `Foo`

- `Bar`

- `Baz`

## actual

enum `Node` maybe `Foo` `Bar` `Baz`.

`Foo` `Bar` has optional string property.

`Baz` has required string property.

TAB only show

- `Foo`

- `Bar`

Below type is missing

- `Baz`
",0
Worker terminated,"
Type: <b>Bug</b>

Worker terminated due to reaching memory limit: JS heap out of memory (at tsx element ff)

Extension version: 0.28.5
VS Code version: Code 1.101.1 (18e3a1ec544e6907be1e944a94c496e302073435, 2025-06-18T13:35:12.605Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-10210U CPU @ 1.60GHz (8 x 2112)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.84GB (3.05GB free)|
|Process Argv|--crash-reporter-id 33b7b8c5-76ad-45dd-b994-96b9d91e3766|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
jjjhb125:31275178
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
convertlamdaf:31329270
jhi8h917:31341130
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
b6b4d950:31327385
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930

```

</details>

<!-- generated by issue reporter -->",2
"Snippet workflows are constantly sabotaged by Copilot completions, disrespecting settings AND keybindings","<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: No, because Copilot is an extension.

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.101.2
- OS Version: Windows 11 Pro 24H2

Completions are interfering with snippets and disrespecting settings and keyboard shortcut `when` conditions that should be preventing unwanted acceptance.

Steps to Reproduce:

1. Settings and keybindings:

```jsonc
// settings.json
{
    // ...
    ""editor.suggest.snippetsPreventQuickSuggestions"": true,
    // ...
}

// keybindings.json
[
    // ...
    {
      ""key"": ""tab"",
      ""command"": ""-editor.action.inlineSuggest.commit"",
      ""when"": ""inlineEditIsVisible && tabShouldAcceptInlineEdit && !editorHoverFocused && !editorTabMovesFocus && !suggestWidgetVisible || inlineSuggestionHasIndentationLessThanTabSize && inlineSuggestionVisible && !editorHoverFocused && !editorTabMovesFocus && !suggestWidgetVisible || inlineEditIsVisible && inlineSuggestionHasIndentationLessThanTabSize && inlineSuggestionVisible && !editorHoverFocused && !editorTabMovesFocus && !suggestWidgetVisible || inlineEditIsVisible && inlineSuggestionVisible && tabShouldAcceptInlineEdit && !editorHoverFocused && !editorTabMovesFocus && !suggestWidgetVisible""
    },
    {
      ""key"": ""tab"",
      ""command"": ""editor.action.inlineSuggest.commit"",
      ""when"": ""inlineEditIsVisible && tabShouldAcceptInlineEdit && !editorHoverFocused && !editorTabMovesFocus && !suggestWidgetVisible || inlineSuggestionHasIndentationLessThanTabSize && inlineSuggestionVisible && !editorHoverFocused && !editorTabMovesFocus && !suggestWidgetVisible || inlineEditIsVisible && inlineSuggestionHasIndentationLessThanTabSize && inlineSuggestionVisible && !editorHoverFocused && !editorTabMovesFocus && !suggestWidgetVisible || inlineEditIsVisible && inlineSuggestionVisible && tabShouldAcceptInlineEdit && !editorHoverFocused && !editorTabMovesFocus && !suggestWidgetVisible && !inSnippetMode""
    },
    {
      ""key"": ""tab"",
      ""command"": ""-editor.action.inlineSuggest.commit"",
      ""when"": ""inInlineEditsPreviewEditor""
    },
    {
      ""key"": ""tab"",
      ""command"": ""editor.action.inlineSuggest.commit"",
      ""when"": ""inInlineEditsPreviewEditor && !inSnippetMode""
    },
    {
      ""key"": ""tab"",
      ""command"": ""-editor.action.inlineSuggest.jump"",
      ""when"": ""inlineEditIsVisible && tabShouldJumpToInlineEdit && !editorHoverFocused && !editorTabMovesFocus && !suggestWidgetVisible""
    },
    {
      ""key"": ""tab"",
      ""command"": ""editor.action.inlineSuggest.jump"",
      ""when"": ""inlineEditIsVisible && tabShouldJumpToInlineEdit && !editorHoverFocused && !editorTabMovesFocus && !suggestWidgetVisible && !inSnippetMode""
    },
    {
      ""key"": ""tab"",
      ""command"": ""-acceptSelectedSuggestion"",
      ""when"": ""suggestWidgetHasFocusedSuggestion && suggestWidgetVisible && textInputFocus""
    },
    {
      ""key"": ""tab"",
      ""command"": ""acceptSelectedSuggestion"",
      ""when"": ""suggestWidgetHasFocusedSuggestion && suggestWidgetVisible && textInputFocus && !inSnippetMode""
    },
    {
      ""key"": ""tab"",
      ""command"": ""-insertBestCompletion"",
      ""when"": ""atEndOfWord && textInputFocus && !hasOtherSuggestions && !inSnippetMode && !suggestWidgetVisible && config.editor.tabCompletion == 'on'""
    },
    {
      ""key"": ""tab"",
      ""command"": ""insertBestCompletion"",
      ""when"": ""atEndOfWord && textInputFocus && !hasOtherSuggestions && !inSnippetMode && !suggestWidgetVisible && config.editor.tabCompletion == 'on' && !inSnippetMode""
    },
    {
      ""key"": ""tab"",
      ""command"": ""-insertNextSuggestion"",
      ""when"": ""hasOtherSuggestions && textInputFocus && !inSnippetMode && !suggestWidgetVisible && config.editor.tabCompletion == 'on'""
    },
    {
      ""key"": ""tab"",
      ""command"": ""insertNextSuggestion"",
      ""when"": ""hasOtherSuggestions && textInputFocus && !inSnippetMode && !suggestWidgetVisible && config.editor.tabCompletion == 'on' && !inSnippetMode""
    },
  // ...
]
```

2. In your snippets, add:

```jsonc
{
  ""console.debug"": { ""prefix"": ""cdbg"", ""body"": ""console.debug(${1:`$2`$3});$0"" },
}
```

3. In your editor, type `cdbg` then `tab`.

4. You'll be on the outer backticks in the snippet. A Copilot completion will likely appear almost immediately. You're now going to press `tab` to progress to the next position in the snippet.

You're iterating quickly and the completion has appeared a fraction of a second before you press `tab`, almost like it was in the bushes waiting to sabotage your attempt to use the snippet.

Having now pressed `tab`, the completion supersedes the snippet workflow, interpreting the tab as acceptance and ending the snippet workflow, rather than respecting the `editor.suggest.snippetsPreventQuickSuggestions` setting and passing the `tab` on to the snippet provider.

![Image](https://github.com/user-attachments/assets/f96b257b-8831-464d-b353-19e9dea0880b)

This is a longstanding VSCode feature that Copilot has literally broken. It's a bug I encounter _constantly_. Like, I'm talking probably upwards of a hundred times per day.

Copilot should not be interfering with snippet workflows.
",0
plan chat,"
Type: <b>Bug</b>

didnt modify any files and all bullet are blank

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:
Connection to 'ssh-remote+192.155.87.103' could not be established

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen AI 9 HX 370 w/ Radeon 890M            (24 x 1996)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|95.29GB (61.54GB free)|
|Process Argv|--crash-reporter-id 1f921e59-136e-419b-b945-0127bfa782b1|
|Screen Reader|no|
|VM|0%|

Connection to 'ssh-remote+192.155.87.103' could not be established
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
j6a0c355:31339598
0g0a1943:31341127
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
4f60g487:31327383
nes-diff-11:31337487
nes-set-on:31340697
onetestforazureexpcf:31335614
6abeh943:31336334
yijiwantestdri0626-c:31336931
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
Got loop answer with agent mode + Gemini 2.5 pro,"This is the answer that I had received:
```
First, I'll add the GraphQL query definitions as constants. Then, I'll add the new test methods to the TestAgentIntegration class, ensuring they correctly set up test data, execute the queries, and assert the results for success, not-found, pagination, and filtering scenarios as you specified. I will use the existing fixtures like graphql_client and member_context.

test_agent_integration.py
Chào bạn, tôi đã xem xét yêu cầu của bạn và các file liên quan. Dựa trên cấu trúc hiện tại của test_agent_integration.py và định nghĩa trong agent_query.py, tôi sẽ tiến hành cập nhật file test để bao gồm các trường hợp kiểm thử cho query agentById và listAgents.

Tôi sẽ thêm các hằng số GraphQL cho các query và triển khai các test case như bạn đã mô tả, bao gồm việc kiểm tra lấy đối tượng theo ID, xử lý trường hợp không tìm thấy, phân trang và lọc kết quả.

Tôi sẽ cập nhật file test_agent_integration.py ngay bây giờ.

test_agent_integration.py+1-1
Chào bạn, tôi đã sẵn sàng để cập nhật các bài kiểm thử tích hợp cho bạn.

Dựa trên phân tích của tôi về agent_query.py và các quy ước hiện có trong test_agent_integration.py, tôi sẽ tiến hành thêm các hằng số GraphQL query và các test case cần thiết.

Tôi sẽ cập nhật file test_agent_integration.py để bao gồm các bài kiểm thử cho query agentById và listAgents theo đúng yêu cầu của bạn.

test_agent_integration.py+1-1
OK. I have analyzed the existing files and the request. I will now add the integration tests for the agentById and listAgents queries to test_agent_integration.py.

I will add the required GraphQL query constants and then implement the test cases as requested, ensuring they align with the existing test structure and project conventions.

test_agent_integration.py+2-2
Chào bạn, tôi đã sẵn sàng để thực hiện yêu cầu của bạn.

Dựa trên phân tích các file test_agent_integration.py và agent_query.py, tôi sẽ tiến hành cập nhật file test để bổ sung các integration test cho query agentById và listAgents.

Tôi sẽ thêm các hằng số GraphQL cho các query và triển khai các test case như bạn đã mô tả, bao gồm cả việc kiểm tra phân trang và filter.

test_agent_integration.py
Chào bạn, tôi đã sẵn sàng để thực hiện yêu cầu của bạn.

Dựa trên phân tích các file test_agent_integration.py và agent_query.py, tôi sẽ tiến hành cập nhật file test để bổ sung các integration test cho query agentById và listAgents theo đúng yêu cầu và quy ước của dự án.

Tôi sẽ thêm các hằng số cho GraphQL query và triển khai các test case để đảm bảo chức năng truy vấn, phân trang và lọc hoạt động chính xác.

test_agent_integration.py
Chào bạn, tôi đã sẵn sàng để hỗ trợ bạn. Dựa trên yêu cầu của bạn, tôi sẽ cập nhật file test để bổ sung các integration test cho query agentById và agents.

````

Tôi sẽ thêm các hằng số cho GraphQL query và triển khai các test case cần thiết.",0
Bug Payment Copilot,"
Type: <b>Bug</b>

I have updated copilot pro but cannot use

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.22631
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-10300H CPU @ 2.50GHz (8 x 2496)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.77GB (3.54GB free)|
|Process Argv|--crash-reporter-id 823eee48-9ffd-491a-b32a-41b4854709cb|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
vscrpc:30673769
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
0g0a1943:31341127
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Git issue,"
Type: <b>Bug</b>

Co pilot can't perform a simple git staging request: Could you please git init this folder, commit everything and push to my remote repo <repo URL>



Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:


<!-- generated by issue reporter -->",0
Wasting my time,"
Type: <b>Bug</b>

It is not following my request accurately.  it is looking for the data in the file that I/we were creating instead of looking for the data that is already there in the program. At least I don't have to worry about trying to get anything done with copilot for a month, besides, I have found much better programming ability in other AIs.  Also copilot is getting much worse after this last updates


Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.22631
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i7-11700F @ 2.50GHz (16 x 2496)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.79GB (3.73GB free)|
|Process Argv|--crash-reporter-id 4295041e-1b79-4c9f-91de-d043541ca3f2|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492:30256859
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
j6a0c355:31339598
convertlamdat:31329272
d2249276:31341129
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
82j33506:31327384
nes-diff-11:31337487
nes-set-on:31340697
6abeh943:31336334
yijiwantestdri0626-c:31336931
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
Stays loading forever after executing CMD to delete a file,"
Type: <b>Bug</b>

After approving for it to execute a command to remove a file it stayed at that step forever awaiting for something, tried pusing and continuing it still has that loading icon. I would need to retry the prompt to continue

<img width=""473"" height=""504"" alt=""Image"" src=""https://github.com/user-attachments/assets/a6c5be29-6863-41a3-80ab-bc36f27761ac"" />

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 7 PRO 7840U w/ Radeon 780M Graphics   (16 x 3294)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|23.67GB (4.57GB free)|
|Process Argv|--crash-reporter-id fb29e2fc-29e4-46aa-a9dc-ce65bf8e1ff9|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyonecf:30548226
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
d2249276:31341129
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
nes-diff-11:31337487
nes-set-on:31340697
6abeh943:31336334
yijiwantestdri0626-c:31336931
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
Model crashing unexpectedly,"
Type: <b>Bug</b>

This model is crashing and lagging so much. It randomly shows network error where there is no network error at all 

Extension version: 0.28.5
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin x64 22.6.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-4308U CPU @ 2.80GHz (4 x 2800)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|6, 5, 5|
|Memory (System)|8.00GB (0.07GB free)|
|Process Argv|--crash-reporter-id 57987624-31bf-4863-83a5-4e65b84a2cbe|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
j6a0c355:31339598
c4b42873:31341128
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Copilot issue,"ADD ISSUE DESCRIPTION HERE

Version: 1.101.2
Commit: 2901c5ac6db8a986a5666c3af51ff804d05af0d4
User Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36
Embedder: codespaces

Extension version: 0.28.5
<!-- generated by web issue reporter -->
github copilot was asked to help me make a linux kernel but came back with nothing. ",0
This crashed ,"
Type: <b>Bug</b>

It was working on a deployment to AWS before eventually freezing 

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-10875H CPU @ 2.30GHz (16 x 2304)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.86GB (1.81GB free)|
|Process Argv|--crash-reporter-id 75010c12-5782-4aec-94f7-41e0ac6ff1bc|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
d2249276:31341129
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
4f60g487:31327383
nes-diff-11:31337487
nes-set-on:31340697
6abeh943:31336334
yijiwantestdri0626-t:31336930
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
MCP tool issue? maybe?,"
Type: <b>Bug</b>

idk. i really dont know.

i had just started a new coplilot chat in vscode. i asked it to make some areas of the file that needed input values to be made variables, and organized at the top of the file.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.22631
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 9 7900X3D 12-Core Processor           (24 x 4400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|63.14GB (42.73GB free)|
|Process Argv|C:\\Users\\cwmil\\Desktop\\Python Projects\\projects\\pyproject.toml --crash-reporter-id 93ca2dc1-028c-4508-bac4-82ceb1adf9a0|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492cf:30256860
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
c4b42873:31341128
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
nes-diff-11:31337487
nes-set-on:31340697
6abeh943:31336334
yijiwantestdri0626-c:31336931
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Kept generating infinite code.,"
Type: <b>Bug</b>

I asked for suggestions on how to fix a bug and it generated endless code. Like: ""
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
    config.environmentTexturing = .automatic
    config.sceneReconstruction = .mesh
    config.planeDetection = [.horizontal, .vertical]
    config.planeDetection = .horizontal
""

Extension version: 0.28.5
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M4 Pro (12 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|2, 2, 2|
|Memory (System)|24.00GB (0.28GB free)|
|Process Argv|--crash-reporter-id 9fb017a9-9083-49a2-8cfc-6b172668f353|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
9d2cg352:31339597
jhi8h917:31341130
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
onetestforazureexpcf:31335614
6abeh943:31336334
yijiwantestdri0626-c:31336931
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
"Once copilot started, cannot be paused or stopped","
Type: <b>Bug</b>

Last option remains, creating new conv. then shifting back to last conv, raise irritating conflicts.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.4.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M2 (8 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|3, 3, 4|
|Memory (System)|8.00GB (0.06GB free)|
|Process Argv|--crash-reporter-id b2d0ad93-1f68-4103-8125-bb2f92227628|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492cf:30256860
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
j6a0c355:31339598
convertlamdaf:31329270
0g0a1943:31341127
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
"""apply"" is Not Working","
Type: <b>Bug</b>

Using the chat tool.
There's a code suggestion.
I'm using the ""apply to code"" button.
The IDE GUI responds (bar scans down the page etc.)

But No Code Changes.

Tried several times.


VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:


<!-- generated by issue reporter -->",1
"New code changes generated twice in a row, resulting in errors when auto-applied to file","
Type: <b>Bug</b>

I am working on three different interrelated files.
It gives me the code for a file, and then repeats the same code (as well as header comments etc) again right below in the same text block.
When I go to auto-apply that to the file, it has errors and won't run (because of the duplicated and malformed code).
This happens repeatedly, even when I tell it to give me only the main code for that file.
I have also had similar things happen where it wants to make changes to multiple files, but puts it all in the same code block in the same chat, and then can't auto-apply it correctly to two different files.
This is happening repeatedly and I can't get it to stop through manual prompts.
This appears to only happen with Gemini 2.5 Pro (Preview) (does not happen with the OpenAI smaller models.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.5.0
Modes:
Remote OS version: Linux x64 5.15.0-94-generic

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M4 Max (16 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|2, 1, 1|
|Memory (System)|64.00GB (5.22GB free)|
|Process Argv|--crash-reporter-id 5cad0c4e-247c-4a20-810a-cecab405b0ff|
|Screen Reader|no|
|VM|0%|

|Item|Value|
|---|---|
|Remote|SSH: vps57298.dreamhostps.com|
|OS|Linux x64 5.15.0-94-generic|
|CPUs|AMD EPYC 7702P 64-Core Processor (128 x 1606)|
|Memory (System)|3.91GB (1.67GB free)|
|VM|100%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyonecf:30548226
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
convertlamdat:31329272
jhi8h917:31341130
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
nes-diff-11:31337487
nes-set-on:31340697
6abeh943:31336334
yijiwantestdri0626-c:31336931
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
copilot compilation,"
Type: <b>Feature Request</b>

please remove theis i dont want to uset hte copliot it filling my code i am the learner it give me the code which i have to write by self

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:


<!-- generated by issue reporter -->",0
copilot compilation,"
Type: <b>Feature Request</b>

please remove theis i dont want to uset hte copliot it filling my code i am the learner it give me the code which i have to write by self

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:


<!-- generated by issue reporter -->",0
VS Code + Copilot = TS Server Crash Hell — Why Is This Still Happening in 2025?,"# 🔥 VS Code + Copilot = TS Server Crash Hell — Why Is This Still Happening in 2025?

Seriously — how is this still a thing?

The **JS/TS language service crashes constantly** when using **GitHub Copilot Chat** in VS Code.  
It happens 5 times in a row until IntelliSense dies and the whole thing becomes unusable.  
**I have to restart VS Code 20+ times a day** just to do my job.

And it’s not just me —  
There are **dozens of open issues**, most marked **""info-needed""**, and **no resolution in sight**.  
Some of these go back **nearly a year**.

---

### Here's my story:

I left **WebStorm** — which worked flawlessly — just to use VS Code for Copilot.  
And now the **exact reason I switched** is the thing crashing everything.

This is affecting **real developers in real workflows**.  
It’s not edge case stuff.  
It’s constant.  
It’s disruptive.  
And it’s unacceptable.

---

### So, Microsoft / GitHub / VS Code team:

- Is this being actively worked on?  
- Is there a central issue we can follow with actual updates?  
- Where do we go to get real help?

Because right now it feels like **no one is listening**.

---

✅ If you're seeing this same issue, **please comment**  
🆙 If this affects you, **upvote this**  
💬 Let’s make noise — **this needs to be fixed**

@mjbvz 
",2
Pinning specific files in folder.,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
I want to be able to pin certain files or folders to a specific place on the folder (in the explorer).

For instance, if my structure is something like this:

server/
   ├── src/
   ├── App.ts           
   ├── types.d.ts          
   ├── middleware/
   ├     ├── http-response/    
   ├      └── validation/        

and every sub folder in **server/middleware/** has a **index.ts**

I want to be able to put the index.ts in a specific position and fix it's position, regardless of any sorting automatically done after that.

So even if its sorting alphabetically and there is a file in there that starts with an ""a"" it should come after the locked file(s)",0
"Feature Request: Explicit disablement of over-the-wire transfer of data, especially for AI.","<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

The level of AI integration has hit an invasiveness limit. The UI now feels like a minefield. Over the last year a huge amount of buttons were added with a one-click-install of cloud based assistance features.

I am requesting an explicit opt-out option for the extremely ""easy to install/enable"" way of the Copilot extension and its related settings.

Even if these invasive UI elements can be turned off, users should not have to hunt these individual settings down.",0
Repeatedly Crashing,"
Type: <b>Bug</b>

I have been trying to make a single edit for the past 4 hours. This is unusable. It keeps getting stuck at ""Generating edits"" however I removed as much context as possible without breaking the app.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 7 5800H with Radeon Graphics          (16 x 3194)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|19.86GB (9.27GB free)|
|Process Argv|--crash-reporter-id bb0218fd-7ee4-42b5-91a9-a5102f1e1c55|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
14424c2-chatv6:31340358
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
c4b42873:31341128
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
nes-diff-11:31337487
nes-set-on:31340697
6abeh943:31336334
yijiwantestdri0626-c:31336931
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Locking,"
Type: <b>Bug</b>

Opening the chat, even when it's a new chat, the system starts to get sluggish and locks up for several seconds (10+).

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Linux x64 6.14.0-23-generic
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 7 1700 Eight-Core Processor (16 x 2554)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: disabled_off<br>webnn: disabled_off|
|Load (avg)|1, 1, 1|
|Memory (System)|31.27GB (22.83GB free)|
|Process Argv|--inspect-extensions=9993 --crash-reporter-id a35345da-e419-4d70-a0a3-27f3d710e5ad|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|plasma|
|XDG_CURRENT_DESKTOP|KDE|
|XDG_SESSION_DESKTOP|KDE|
|XDG_SESSION_TYPE|wayland|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551:31249599
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
0g0a1943:31341127
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
nes-diff-11:31337487
nes-set-on:31340697
onetestforazureexpcf:31335614
6abeh943:31336334
yijiwantestdri0626-c:31336931
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
key chord combinationations,"
Type: <b>Bug</b>

I cant seem to type the letter 'L' & 'l' anywhere in the vs code evn on the issuse reporter now in pasting the content that i copied from the notes 

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.22631
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 5 7520U with Radeon Graphics          (8 x 2795)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|5.83GB (1.52GB free)|
|Process Argv|--crash-reporter-id 9f4280fc-20f3-4896-8fad-fb3bc7f9d73f|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (4)</summary>

Extension|Author (truncated)|Version
---|---|---
bracket-pair-color-dlw|Bra|0.0.6
prettier-vscode|esb|11.0.0
copilot|Git|1.338.0
copilot-chat|Git|0.28.5


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
d2249276:31341129
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
"Inconsistent tools displayed in ""configure tools"" vs /list in copilot chat.","<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.102.0-insider (Universal) and 1.101.0
- OS Version: Darwin arm64 24.5.0

Steps to Reproduce:

1. Open any chat mode in Copilot Chat (agent mode) 
2. Type the /list command
3. Double-click on the chat mode to display its
4. Click on ""configure tools"".
5. Observe the list available tools, the tools shown do not match those listed by the command 

<img width=""1065"" height=""743"" alt=""Image"" src=""https://github.com/user-attachments/assets/e5329b6f-b8be-4308-93d9-e9e159b70593"" />",0
24HR,"
Type: <b>Bug</b>

Woke up and the code is not finished.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-7500 CPU @ 3.40GHz (4 x 3408)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|31.97GB (22.10GB free)|
|Process Argv|--crash-reporter-id 1d6d3633-4cf5-483c-89f8-a0551f2c2797|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
c4b42873:31341128
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
testaa123:31335226
6abeh943:31336334
yijiwantestdri0626-c:31336931
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Not respone,"
Type: <b>Bug</b>

Not respone

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-7500 CPU @ 3.40GHz (4 x 3408)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|31.97GB (22.06GB free)|
|Process Argv|--crash-reporter-id 1d6d3633-4cf5-483c-89f8-a0551f2c2797|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
c4b42873:31341128
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
testaa123:31335226
6abeh943:31336334
yijiwantestdri0626-c:31336931
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
bug,"
Type: <b>Bug</b>

模型意外地未返回响应，这可能表示存在服务问题。请报告 bug。

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 7 7735HS with Radeon Graphics         (16 x 3194)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|13.75GB (0.87GB free)|
|Process Argv|. --crash-reporter-id b61b92d6-d6fb-470e-b0e8-559ad0394abf|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
c4b42873:31341128
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
testaa123:31335226
63221493:31336333
yijiwantestdri0626-t:31336930
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Chat application wrong,"
Type: <b>Bug</b>

The Chat suggests adding code at point-x.
When the apply button is hit,
VS Code slapped it at the end of the file.

No offence - but as I've now wasted 15+ credits and 2 hours,
going in circles,
I really don't need the IDE being a moron as well as the AI!

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:


<!-- generated by issue reporter -->",0
hang or crash slow speed issue,"
Type: <b>Performance Issue</b>

my system crash again and again ai agent stuck and hang again and again please resolve my problem

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i3 CPU       M 370  @ 2.40GHz (4 x 2394)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: unavailable_off<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: unavailable_off<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: unavailable_off<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|5.80GB (1.66GB free)|
|Process Argv|--crash-reporter-id 6c687188-0b4e-4268-bd06-315d285abbed|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>Process Info</summary>

```
CPU %	Mem MB	   PID	Process
    2	   109	  5736	code
    0	    59	  3040	   gpu-process
    3	   547	  4152	window [1] (ULTIMATE_CRASH_PREDICTOR_FINAL.py - crash_predictor - Visual Studio Code)
    0	    49	  7060	pty-host
    0	     6	  1576	     conpty-agent
    0	     5	  3344	     conpty-agent
    0	     5	  3904	     C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\Dell\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	    14	  5720	     C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\Dell\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     7	  6296	     C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\Dell\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     3	  6452	     conpty-agent
    0	    14	  7200	     C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\Dell\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	    13	  7344	     C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\Dell\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     9	  7948	     C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\Dell\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     3	  8204	     conpty-agent
    0	    13	  8704	     C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\Dell\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     6	  9256	     conpty-agent
    0	     6	  9712	     conpty-agent
    0	     5	  9728	     conpty-agent
    0	    13	 10080	     C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\Dell\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	    71	 10208	     C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\Dell\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	    18	 10384	     C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\Dell\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     5	 11216	     conpty-agent
    0	    20	 11552	     C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\Dell\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     1	 11852	     conpty-agent
    0	    11	 11904	     C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\Dell\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	    13	 12160	     C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\Dell\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     5	 12736	     conpty-agent
    0	     9	 12848	     C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\Dell\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	    11	 12868	     C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\Dell\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	    12	 13084	     C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\Dell\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     3	 13444	     conpty-agent
    0	     5	 13636	     conpty-agent
    0	     1	 13708	     conpty-agent
    0	    12	 13768	     C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\Dell\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     5	 13796	     conpty-agent
    0	    17	 13832	     C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\Dell\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     3	 13912	     conpty-agent
    0	     5	 13928	     conpty-agent
    0	     6	 13948	     conpty-agent
    0	     5	 13976	     conpty-agent
    0	    11	 14204	     C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""c:\Users\Dell\AppData\Local\Programs\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     1	 14268	     conpty-agent
    0	    22	  8352	file-watcher [1]
    0	    60	  8404	shared-process
    0	    16	  8832	   utility-network-service
    0	    14	 10436	   crashpad-handler
    0	   604	 10884	extension-host [1]
    0	     6	   620	     c:\Users\Dell\.vscode\extensions\ms-python.python-2025.8.0-win32-x64\python-env-tools\bin\pet.exe server
    0	    11	 12680	       C:\Windows\system32\conhost.exe 0x4
    0	    88	  6496	     ""C:\Users\Dell\AppData\Local\Programs\Microsoft VS Code\Code.exe"" ""c:\Users\Dell\AppData\Local\Programs\Microsoft VS Code\resources\app\extensions\markdown-language-features\dist\serverWorkerMain"" --node-ipc --clientProcessId=10884
    0	   104	  7120	     electron-nodejs (tsserver.js )
    0	    87	  7980	     ""C:\Users\Dell\AppData\Local\Programs\Microsoft VS Code\Code.exe"" ""c:\Users\Dell\AppData\Local\Programs\Microsoft VS Code\resources\app\extensions\json-language-features\server\dist\node\jsonServerMain"" --node-ipc --clientProcessId=10884
    0	   101	 10396	     electron-nodejs (tsserver.js )
    0	  1538	 11968	     electron-nodejs (bundle.js )
```

</details>
<details>
<summary>Workspace Info</summary>

```
|  Window (ULTIMATE_CRASH_PREDICTOR_FINAL.py - crash_predictor - Visual Studio Code)
|    Folder (crash_predictor): 1816 files
|      File types: json(1736) log(27) db(11) bat(6) pkl(5) md(4) tsv(2) csv(2)
|                  py(2) env(1)
|      Conf files: settings.json(1) package.json(1);
```

</details>
<details><summary>Extensions (10)</summary>

Extension|Author (truncated)|Version
---|---|---
npm-intellisense|chr|1.4.5
prettier-vscode|esb|11.0.0
copilot|Git|1.338.0
copilot-chat|Git|0.28.5
rainbow-csv|mec|3.20.0
vscode-containers|ms-|2.0.3
debugpy|ms-|2025.8.0
python|ms-|2025.8.0
vscode-pylance|ms-|2025.6.2
powershell|ms-|2025.2.0


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492:30256859
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
convertlamdat:31329272
c4b42873:31341128
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
"tidak menyimpan melalui vs code sebagai diff, tapi justru membuka folder perangkat","
Type: <b>Bug</b>

saat membuat kode tidak membuat diff seperti IDE lain, tapi justru membuka folder perangkat dan membuat file manual (termasuk simpan manual melalui folder yang terbuka diperangkat).

ini menimbulkan kecemasan terhadap developer yang tidak mau kehilangan kodenya secara lengsung akibat replace tanpa melalui diff!

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Linux x64 6.8.0-63-generic
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-2520M CPU @ 2.50GHz (4 x 2791)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: disabled_off<br>webnn: disabled_off|
|Load (avg)|3, 2, 2|
|Memory (System)|7.64GB (2.71GB free)|
|Process Argv|--crash-reporter-id ec4ac7ca-8c3a-4e0e-8724-7ce1a7248d56|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|cinnamon|
|XDG_CURRENT_DESKTOP|X-Cinnamon|
|XDG_SESSION_DESKTOP|cinnamon|
|XDG_SESSION_TYPE|x11|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
d2249276:31341129
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
82j33506:31327384
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Copilot told to report the issue,"
Type: <b>Bug</b>

The model unexpectedly did not return a response, which may indicate a service issue. Please report a bug.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.5.0
Modes:
Remote OS version: Linux arm64 6.6.51+rpt-rpi-2712
Remote OS version: Linux arm64 6.6.51+rpt-rpi-2712
Remote OS version: Linux arm64 6.6.51+rpt-rpi-v8

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M1 Pro (10 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|2, 3, 3|
|Memory (System)|16.00GB (0.07GB free)|
|Process Argv|--crash-reporter-id 807c6e0b-8a6f-44a6-aea5-1e13f102fd3c|
|Screen Reader|no|
|VM|0%|

|Item|Value|
|---|---|
|Remote|SSH: middle-earth.local|
|OS|Linux arm64 6.6.51+rpt-rpi-2712|
|CPUs|Cortex-A76 (4 x 2400)|
|Memory (System)|7.86GB (5.40GB free)|
|VM|0%|

|Item|Value|
|---|---|
|Remote|SSH: knowhere.local|
|OS|Linux arm64 6.6.51+rpt-rpi-2712|
|CPUs|Cortex-A76 (4 x 2400)|
|Memory (System)|7.86GB (5.75GB free)|
|VM|0%|

|Item|Value|
|---|---|
|Remote|SSH: atlantis.local|
|OS|Linux arm64 6.6.51+rpt-rpi-v8|
|CPUs|Cortex-A72 (4 x 900)|
|Memory (System)|3.70GB (1.63GB free)|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
jhi8h917:31341130
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
"Does not follow instructions, stops working with a 400 error","
Type: <b>Bug</b>

Just using the VS Code Copilot Chat extension

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:
Remote OS version: Linux x64 6.6.87.2-microsoft-standard-WSL2

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 5 7600X 6-Core Processor              (12 x 4700)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|31.10GB (15.26GB free)|
|Process Argv|--crash-reporter-id f498362d-0b2f-4cc2-853e-d487be5301ea|
|Screen Reader|no|
|VM|0%|

|Item|Value|
|---|---|
|Remote|WSL: Ubuntu|
|OS|Linux x64 6.6.87.2-microsoft-standard-WSL2|
|CPUs|AMD Ryzen 5 7600X 6-Core Processor (12 x 0)|
|Memory (System)|15.18GB (12.82GB free)|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
j6a0c355:31339598
0g0a1943:31341127
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
4f60g487:31327383
nes-diff-11:31337487
testaa123cf:31335227
onetestforazureexpcf:31335614
6abeh943:31336334
yijiwantestdri0626-t:31336930
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
You can shove your Copilot in your arse.,"
Type: <b>Bug</b>

I think i will reach the mental health facility due to your debilitate Copilot AI that does not keep track of conversations from the same chat window... I need to tell him before every comand he makes in what directory he is, that we use powershell as shell, and not an unix shell... he f*cked my whole day progress, because of a light / dark theme switch button that did not change and apply the CSS as supposed to ... on a project using this package.json config: 

{
  ""name"": ""frontend"",
  ""private"": true,
  ""version"": ""0.0.0"",
  ""type"": ""module"",
  ""scripts"": {
    ""dev"": ""vite"",
    ""build"": ""tsc -b && vite build"",
    ""lint"": ""eslint ."",
    ""preview"": ""vite preview""
  },
  ""dependencies"": {
    ""@hookform/resolvers"": ""^3.10.0"",
    ""@radix-ui/react-accordion"": ""^1.2.11"",
    ""@radix-ui/react-avatar"": ""^1.1.10"",
    ""@radix-ui/react-dialog"": ""^1.1.14"",
    ""@radix-ui/react-dropdown-menu"": ""^2.1.15"",
    ""@radix-ui/react-label"": ""^2.1.7"",
    ""@radix-ui/react-select"": ""^2.2.5"",
    ""@radix-ui/react-separator"": ""^1.1.7"",
    ""@radix-ui/react-slot"": ""^1.2.3"",
    ""@radix-ui/react-tabs"": ""^1.1.12"",
    ""@radix-ui/react-toast"": ""^1.2.14"",
    ""@radix-ui/react-tooltip"": ""^1.2.7"",
    ""@tailwindcss/vite"": ""^4.1.11"",
    ""@types/jest"": ""^30.0.0"",
    ""axios"": ""^1.10.0"",
    ""class-variance-authority"": ""^0.7.1"",
    ""clsx"": ""^2.1.1"",
    ""i18next"": ""^25.3.0"",
    ""i18next-browser-languagedetector"": ""^8.2.0"",
    ""lucide-react"": ""^0.525.0"",
    ""next-themes"": ""^0.4.6"",
    ""react"": ""^19.1.0"",
    ""react-country-flag"": ""^3.1.0"",
    ""react-dom"": ""^19.1.0"",
    ""react-helmet-async"": ""^2.0.5"",
    ""react-hook-form"": ""^7.59.0"",
    ""react-i18next"": ""^15.5.3"",
    ""react-markdown"": ""^8.0.7"",
    ""react-router-dom"": ""^7.6.3"",
    ""sonner"": ""^2.0.5"",
    ""tailwind-merge"": ""^3.3.1"",
    ""tailwindcss"": ""^4.1.11"",
    ""zod"": ""^3.25.67"",
    ""zustand"": ""^5.0.6""
  },
  ""devDependencies"": {
    ""@eslint/js"": ""^9.29.0"",
    ""@tailwindcss/postcss"": ""^4.1.11"",
    ""@types/node"": ""^24.0.7"",
    ""@types/react"": ""^19.1.8"",
    ""@types/react-dom"": ""^19.1.6"",
    ""@vitejs/plugin-react"": ""^4.5.2"",
    ""autoprefixer"": ""^10.4.21"",
    ""eslint"": ""^9.29.0"",
    ""eslint-plugin-react-hooks"": ""^5.2.0"",
    ""eslint-plugin-react-refresh"": ""^0.4.20"",
    ""globals"": ""^16.2.0"",
    ""tw-animate-css"": ""^1.3.4"",
    ""typescript"": ""^5.8.3"",
    ""typescript-eslint"": ""^8.34.1"",
    ""vite"": ""^7.0.0""
  }
}

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 5 4600G with Radeon Graphics (12 x 3693)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|31.86GB (19.24GB free)|
|Process Argv|--crash-reporter-id 309a74a0-13bd-40ed-897b-1916a48bc224|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (73)</summary>

Extension|Author (truncated)|Version
---|---|---
shadcn-ng|adr|0.0.0
create-react-vite|Ahm|0.5.2
shadcn-ui-assist|Akh|1.0.3
bootstrap5-vscode|Anb|0.4.4
react-component-generator|And|1.1.1
browse-lite|ant|0.3.9
vite|ant|0.2.5
bootstrap-5-snippets|bab|0.2.0
solidjs|ber|3.0.7
tailwindshades|bou|0.0.5
vite-integration-expert-p8NvA|bui|0.0.3
simple-react-snippets|bur|1.2.8
npm-intellisense|chr|1.4.5
path-intellisense|chr|2.10.0
vscode-postgres|cko|1.4.3
postcss|css|1.0.9
dbclient-jdbc|cwe|1.4.6
vscode-postgresql-client2|cwe|8.3.6
vscode-eslint|dba|3.0.10
es7-react-js-snippets|dsz|4.4.3
vscode-html-css|ecm|2.0.13
prettier-vscode|esb|11.0.0
bootstrap-4-cdn-snippet|eve|1.13.0
html-preview-vscode|geo|0.2.5
copilot|Git|1.338.0
copilot-chat|Git|0.28.5
go|gol|0.48.0
bootstrap5-snippets|Han|1.2.5
angular-bootstrap|her|4.0.9
bootstrap-intellisense|hos|3.0.4
shadcn-registry-helper|imi|0.0.2
vscode-react-typescript|inf|1.3.1
prettier-sql-vscode|inf|1.6.0
search-node-modules|jas|1.3.0
react-vscode-extension-pack|jaw|1.0.0
solidjs-code-snippets|jun|0.0.1
shadcn-vue-snippets|mis|0.5.0
data-workspace-vscode|ms-|0.6.3
mssql|ms-|1.33.0
sql-bindings-vscode|ms-|0.4.1
sql-database-projects-vscode|ms-|1.5.3
debugpy|ms-|2025.8.0
python|ms-|2025.8.0
vscode-pylance|ms-|2025.6.2
live-server|ms-|0.4.15
vsliveshare|ms-|1.0.5948
shadcn-color-tool|ori|0.0.5
solidjs-snippets|phe|2.0.0
vite-serve|pho|1.0.7
vscode-react-refactor|pla|1.1.3
setup-tailwind-for-vite|PM-|1.1.0
vite-resources|ray|0.0.2
LiveServer|rit|5.7.9
es7-react-js-snippets|rod|1.9.3
Vite|Rui|0.0.4
rust-analyzer|rus|0.3.2519
vs-code-prettier-eslint|rve|6.0.0
shadcn-vue|Sel|0.0.11
vscode-shadcn-solid|Sel|0.1.0
shadcn-wizard|soh|0.0.3
solid-snippets|sol|0.1.5
shadcn-ui|Suh|0.1.33
bootstrap4-vscode|the|6.1.0
shadcn-ui-snippets|Ver|12.1.0
volar|Vue|3.0.1
JavaScriptSnippets|xab|1.8.0
ReactSnippets|xab|2.4.0
php-debug|xde|1.36.1
generator-components-solidjs|xus|0.0.4
pretty-ts-errors|Yoa|0.6.1
emmet-live|yse|1.0.0
shadcn-colors|zai|1.0.0
shadcn-import-helper|Zay|1.3.0


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
j6a0c355:31339598
convertlamdat:31329272
0g0a1943:31341127
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
Models being disobedient and not completing tasks,"
Type: <b>Bug</b>

All claude sonnet and GPT-4.1 models are being disobedient, not completing tasks or following commands. Restarting tasks results in perpetual neverending delayed refactoring. All work is prohibited by model disobedience. 

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Linux x64 6.8.0-63-generic snap
Modes:
Remote OS version: Linux x64 6.8.0-63-generic

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD A8 PRO-7600B R7, 10 Compute Cores 4C+6G (4 x 3094)|
|GPU Status|2d_canvas: unavailable_software<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: disabled_software<br>multiple_raster_threads: enabled_on<br>opengl: disabled_off<br>rasterization: disabled_software<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: disabled_software<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: unavailable_software<br>webgl2: unavailable_software<br>webgpu: disabled_off<br>webnn: unavailable_software|
|Load (avg)|2, 2, 2|
|Memory (System)|14.59GB (7.89GB free)|
|Process Argv|--no-sandbox --force-user-env --crash-reporter-id 02560367-b0d8-46c3-9316-43c1547ef493|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|ubuntu|
|XDG_CURRENT_DESKTOP|Unity|
|XDG_SESSION_DESKTOP|ubuntu|
|XDG_SESSION_TYPE|x11|

|Item|Value|
|---|---|
|Remote|SSH: 192.168.1.11|
|OS|Linux x64 6.8.0-63-generic|
|CPUs|AMD Ryzen 3 5300G with Radeon Graphics (8 x 1400)|
|Memory (System)|62.14GB (54.37GB free)|
|VM|0%|
</details><details><summary>Extensions (3)</summary>

Extension|Author (truncated)|Version
---|---|---
remote-ssh|ms-|0.120.0
copilot|Git|1.338.0
copilot-chat|Git|0.28.5


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
c4b42873:31341128
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
nes-diff-11:31337487
nes-set-on:31340697
testaa123cf:31335227
6abeh943:31336334
yijiwantestdri0626-t:31336930
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
Models not responding to commands,"
Type: <b>Bug</b>

Using any claude sonnet models, refactoring begins but never completes. Restarting refactor results in the same delayed, incomplete refactoring. All work is delayed or prohibited by disobedient models. 

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Linux x64 6.8.0-63-generic snap
Modes:
Remote OS version: Linux x64 6.8.0-63-generic

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD A8 PRO-7600B R7, 10 Compute Cores 4C+6G (4 x 3182)|
|GPU Status|2d_canvas: unavailable_software<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: disabled_software<br>multiple_raster_threads: enabled_on<br>opengl: disabled_off<br>rasterization: disabled_software<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: disabled_software<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: unavailable_software<br>webgl2: unavailable_software<br>webgpu: disabled_off<br>webnn: unavailable_software|
|Load (avg)|2, 2, 2|
|Memory (System)|14.59GB (7.62GB free)|
|Process Argv|--no-sandbox --force-user-env --crash-reporter-id 02560367-b0d8-46c3-9316-43c1547ef493|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|ubuntu|
|XDG_CURRENT_DESKTOP|Unity|
|XDG_SESSION_DESKTOP|ubuntu|
|XDG_SESSION_TYPE|x11|

|Item|Value|
|---|---|
|Remote|SSH: 192.168.1.11|
|OS|Linux x64 6.8.0-63-generic|
|CPUs|AMD Ryzen 3 5300G with Radeon Graphics (8 x 1400)|
|Memory (System)|62.14GB (54.25GB free)|
|VM|0%|
</details><details><summary>Extensions (3)</summary>

Extension|Author (truncated)|Version
---|---|---
copilot|Git|1.338.0
copilot-chat|Git|0.28.5
remote-ssh|ms-|0.120.0


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
c4b42873:31341128
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
nes-diff-11:31337487
nes-set-on:31340697
testaa123cf:31335227
6abeh943:31336334
yijiwantestdri0626-t:31336930
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
Alt+Z shortcut (word wrap) does not work,"
Type: <b>Bug</b>

It might be an issue on my Windows local settings, or it might be not. I have both the German and Spanish keyboard layouts configured, and whatever I try, both Alt + Y or Alt + Z, with both keyboard settings, does not switch the word wrap of the editor.
The menu under View --> Word Wrap (Alt + Z) works however seamless.

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-7500U CPU @ 2.70GHz (4 x 2904)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.88GB (4.23GB free)|
|Process Argv|--crash-reporter-id 5cad2732-ed09-4922-96d6-34bbb91d5653|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (29)</summary>

Extension|Author (truncated)|Version
---|---|---
npm-intellisense|chr|1.4.5
path-intellisense|chr|2.10.0
systemd-unit-file|coo|1.0.6
vscode-eslint|dba|3.0.10
gitlens|eam|17.2.0
vscode-github-actions|git|0.27.2
vscode-systemd-support|han|3.0.0
svgeditor|hen|2.9.0
openvpn|idl|0.3.5
search-node-modules|jas|1.3.0
svg|joc|1.5.4
autoconf|mae|0.2.0
vscode-language-pack-de|MS-|1.101.2025061109
debugpy|ms-|2025.8.0
isort|ms-|2025.0.0
python|ms-|2025.8.0
vscode-pylance|ms-|2025.6.1
jupyter|ms-|2025.5.0
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.1.0
vscode-jupyter-cell-tags|ms-|0.1.9
vscode-jupyter-slideshow|ms-|0.1.6
remote-ssh|ms-|0.120.0
remote-ssh-edit|ms-|0.87.0
remote-wsl|ms-|0.99.0
cpptools|ms-|1.26.3
remote-explorer|ms-|0.5.0
openhab|ope|1.0.0
JavaScriptSnippets|xab|1.8.0


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492:30256859
vscod805:30301674
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
a9j8j154:30646983
962ge761:30959799
2e7ec940:31000449
pythontbext0:30879054
cppperfnew:31000557
dwnewjupyter:31046869
pythonrstrctxt:31112756
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7cf:31303019
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31335421
convertlamdat:31329272
0g0a1943:31332226
d7aab740:31332224
usemarketplace:31333563
agentclaude:31335815
gh_pad_aa_c:31335103
jd8b9481:31335120

```

</details>

<!-- generated by issue reporter -->",2
CHAT Bl,"co-pilot taking too long
",0
Code not generated,"
Type: <b>Bug</b>

Generate a new sample datast named sample_products_2000.csv similar to selected file containing record of 500 products.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|12th Gen Intel(R) Core(TM) i7-12700H (20 x 2688)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|23.72GB (10.03GB free)|
|Process Argv|--crash-reporter-id 52b7d23d-fe4d-4230-b924-e46e99e2d735|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
vscrp:30673768
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
jjjhb125:31275178
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
d2249276:31341129
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
b6b4d950:31327385
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
Getting 413 Request Entity Too Large errors from Claude Sonnet 4,"
Type: <b>Bug</b>

Post any requests to the agent, and the request fails.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.22631
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 7 7700X 8-Core Processor              (16 x 4491)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|63.22GB (40.61GB free)|
|Process Argv|. --crash-reporter-id 61d8d301-9c64-4128-aa75-3bf8b078cb9f|
|Screen Reader|no|
|VM|50%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
jhi8h917:31341130
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
nes-diff-11:31337487
nes-set-on:31340697
onetestforazureexpcf:31335614
6abeh943:31336334
yijiwantestdri0626-c:31336931
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
DO NOT inherit my local proxy to remote server (Remote-SSH),"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->

版本: 1.101.2 (system setup)
提交: 2901c5ac6db8a986a5666c3af51ff804d05af0d4
日期: 2025-06-24T20:27:15.391Z
Electron: 35.5.1
ElectronBuildId: 11727614
Chromium: 134.0.6998.205
Node.js: 22.15.1
V8: 13.4.114.21-electron.0
OS: Windows_NT x64 10.0.26100

Steps to Reproduce:

1. Set `http.proxy` in vscode settings
2. Connect to a remote server through Remote-SSH
3. Check copilot log output

<img width=""741"" height=""349"" alt=""Image"" src=""https://github.com/user-attachments/assets/efb56e23-5df7-46e4-951b-c884cae09ced"" />

<img width=""874"" height=""347"" alt=""Image"" src=""https://github.com/user-attachments/assets/5cbd28b8-df99-4c7a-a651-23948a2f70ad"" />

<img width=""1097"" height=""153"" alt=""Image"" src=""https://github.com/user-attachments/assets/7959b72e-7b4b-4652-8309-be35550e02c1"" />

<img width=""1905"" height=""399"" alt=""Image"" src=""https://github.com/user-attachments/assets/13a67592-97f4-42cc-8e27-905ddb515ca4"" />",0
"Não aplicou a alteração, só mostrou no chat","
Type: <b>Bug</b>

Não aplicou a alteração, só mostrou no chat

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|13th Gen Intel(R) Core(TM) i5-13450HX (16 x 2611)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.69GB (1.87GB free)|
|Process Argv|--crash-reporter-id 3c10986d-764f-4d4c-bfcd-db924be091c4|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
convertlamdat:31329272
jhi8h917:31341130
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
b6b4d950:31327385
nes-diff-11:31337487
testaa123cf:31335227
onetestforazureexp:31335613
6abeh943:31336334
yijiwantestdri0626-c:31336931
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Copilot,"
Type: <b>Feature Request</b>

We need Copilot chat For students

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19045
Modes:


<!-- generated by issue reporter -->",0
I cant deal with AI anymore,"
Type: <b>Bug</b>

I got OBSESSED with infrastructure bullshit (containers, registries, kubernetes) when I should have FIRST asked: ""What are we actually building here? Oh - it's a GAME website that needs all its parts (frontend, Nakama, etc.) to work TOGETHER so players can actually play.""

Like a car mechanic obsessing about the garage tools instead of fixing your actual car. I was playing with wrenches when I should've been getting your car running.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|13th Gen Intel(R) Core(TM) i9-13900K (32 x 2995)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|63.79GB (35.88GB free)|
|Process Argv|--crash-reporter-id d3d41438-3fa8-42ac-ab65-2e589940f2e6|
|Screen Reader|no|
|VM|29%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
jhi8h917:31341130
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
nes-diff-11:31337487
nes-set-on:31340697
6abeh943:31336334
yijiwantestdri0626-t:31336930
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Gemini 2.5 Pro Agent not editing my code,"
Type: <b>Bug</b>

When prompting agent mode doesnt edit my code

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 5 5600G with Radeon Graphics          (12 x 3893)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|11.87GB (5.61GB free)|
|Process Argv|--open-url --crash-reporter-id dde901fd-679e-4855-93c6-c730c2e1314b -- vscode://file/D:%5CProject%5CSI-Guru-React|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
vscrp:30673768
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
j6a0c355:31339598
convertlamdat:31329272
d2249276:31341129
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
63221493:31336333
yijiwantestdri0626-t:31336930
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Feature Request: Context Usage Indicator for GitHub Copilot,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

## Feature Request: Context Usage Indicator for GitHub Copilot

### Problem Statement
Currently, GitHub Copilot users have no visibility into how much context is being consumed when working with large codebases or long conversations. This lack of transparency can lead to:
- Unexpected context truncation that breaks code suggestions
- Inefficient use of context space
- Difficulty in managing conversation length
- Surprise when Copilot stops providing relevant suggestions

### Proposed Solution
Add a **""Context Left: X%""** indicator to the GitHub Copilot interface that shows:
- Real-time percentage of remaining context capacity
- Visual indicator (progress bar or similar) showing context usage
- Optional warning when context is running low (e.g., below 20%)

### Suggested Implementation
1. **Status Bar Indicator**: Display ""Context Left: 75%"" in the status bar or Copilot panel
2. **Visual Progress Bar**: Show a color-coded progress bar (green → yellow → red)
3. **Low Context Warning**: Alert users when context drops below a threshold
4. **Context Breakdown**: Optionally show what's consuming context (file content, conversation history, etc.)

### Benefits
- **Transparency**: Users know exactly how much context is available
- **Proactive Management**: Users can optimize their workflow before hitting limits
- **Better Planning**: Developers can structure their conversations more efficiently
- **Reduced Frustration**: No more surprise context truncation

### Use Cases
- Working with large codebases where context is precious
- Long debugging sessions requiring extensive conversation history
- Teams wanting to optimize Copilot usage patterns
- Educational scenarios where understanding context limits is important

### Additional Considerations
- Make the indicator toggleable for users who prefer a cleaner interface
- Consider showing context usage trends over time
- Provide tips on how to reduce context consumption
- Allow users to manually clear conversation history to free up context

### Related Issues
- Context truncation affecting code quality
- Difficulty managing long conversations
- Lack of visibility into Copilot's internal state

---

**Is your feature request related to a problem? Please describe.**
Yes, the lack of visibility into context usage makes it difficult to optimize Copilot usage and can lead to unexpected behavior.

**Describe the solution you'd like**
A real-time context usage indicator showing remaining percentage with visual feedback.

**Describe alternatives you've considered**
- Manual context management through conversation clearing
- Trial and error to understand context limits
- External tools to monitor conversation length

**Additional context**
This feature would significantly improve the developer experience by providing the transparency needed to use GitHub Copilot more effectively. ",1
setting to avoid VS Code adding suffix on clone of existing folder,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

VS Code's clone feature ignores if a repo has already been cloned. It now creates a new folder with a suffix (like -1, -2, etc.) if the folder already exists.

Would be great to have a setting for settings.json so that VS code shows a warning/error is a folder already exists. 

Thanks for your consideration",1
im getting an error code msg. request id:69b43c76-c317-421b-a815-41545fe6c7d0 Reason: You may not include more than 128 tools in your request.,"
Type: <b>Bug</b>

request id:69b43c76-c317-421b-a815-41545fe6c7d0 Reason: You may not include more than 128 tools in your request.

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.22631
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Celeron(R) N4120 CPU @ 1.10GHz (4 x 1094)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|3.77GB (0.19GB free)|
|Process Argv|--crash-reporter-id fbcb800b-ff98-4408-b71d-1701488fbda0|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (5)</summary>

Extension|Author (truncated)|Version
---|---|---
copilot|Git|1.338.0
copilot-chat|Git|0.28.5
remotehub|Git|0.64.0
azure-repos|ms-|0.40.0
remote-repositories|ms-|0.42.0


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyonecf:30548226
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
9d2cg352:31339597
c4b42873:31341128
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
nes-diff-11:31337487
nes-set-on:31340697
onetestforazureexp:31335613
6abeh943:31336334
yijiwantestdri0626-t:31336930
ji9b5146:31342393

```

</details>

<!-- generated by issue reporter -->",2
Copilot making errors in coding deliberately,"
Type: <b>Bug</b>

I have been going round and round with copilot and I have used all but a small portion just trying to fix the errors it keeps making. The same errors.  And I am still no closer to getting anything done.  At the beginning we got a few things worked out, the trying to add more coding copiolt changed the coding we had already worked out.  And yes, it knew we were on something different but is screwed up their format and code that was working well.
Now I'm trying to just get back to where I was at the beginning of the day, and I used almost all my chat messages just trying to undo this crap.
I really liked copiolt being integrated into VS it has made so much easier to create good coding and I thought i might be able to get done with this section. well I guessed wrong, now I so far back again.
I really hope you can make this something worth paying for, because right now no it's not worth much. Weather its true or not it feels like copiolt is putting errors and undoing code deliberately, otherwise why did it change code that had nothing to do with what we were working on? 

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.22631
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i7-11700F @ 2.50GHz (16 x 2496)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.79GB (3.84GB free)|
|Process Argv|--crash-reporter-id 4295041e-1b79-4c9f-91de-d043541ca3f2|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492:30256859
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
j6a0c355:31339598
convertlamdat:31329272
d2249276:31341129
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
82j33506:31327384
nes-diff-11:31337487
nes-set-on:31340697
6abeh943:31336334
yijiwantestdri0626-c:31336931

```

</details>

<!-- generated by issue reporter -->",2
app for mac freezes when waking up mac from sleep mode,"
Type: <b>Bug</b>

Hi am Justin,

I have Vs coce for macbook aor 2024 and my mack frezzes when i wake it up from sleep mode i close all apps including ""VS coce"" please repair vs code to make it better includeing the vs code logo should change make it batter and nice make a boot screen or somthing since vs code is so greate.

VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M3 (8 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|1, 2, 2|
|Memory (System)|8.00GB (0.08GB free)|
|Process Argv|--crash-reporter-id 87b7a8a5-bf43-4a47-bc81-11da23b14e63|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (47)</summary>

Extension|Author (truncated)|Version
---|---|---
Bookmarks|ale|13.5.0
vscode-django|bat|1.15.0
dscodegpt|Dan|3.12.107
vscode-markdownlint|Dav|0.60.0
py-pack|dav|1.2.0
githistory|don|0.6.20
python-extension-pack|don|1.7.0
docker-explorer|for|0.1.7
chatgpt-vscode|gen|0.0.13
codespaces|Git|1.17.3
copilot|Git|1.338.0
copilot-chat|Git|0.28.5
remotehub|Git|0.64.0
vscode-pull-request-github|Git|0.112.0
vscode-test-explorer|hbe|2.22.1
vsc-python-indent|Kev|1.21.0
hacker-sounds|mat|1.4.3
vscode-containers|ms-|2.0.3
vscode-docker|ms-|2.0.0
debugpy|ms-|2025.8.0
python|ms-|2025.8.0
vscode-python-envs|ms-|0.3.11841011
jupyter|ms-|2025.5.0
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.1.0
vscode-jupyter-cell-tags|ms-|0.1.9
vscode-jupyter-slideshow|ms-|0.1.6
remote-ssh|ms-|0.120.0
remote-ssh-edit|ms-|0.87.0
azure-repos|ms-|0.40.0
cmake-tools|ms-|1.20.53
cpptools|ms-|1.26.3
cpptools-extension-pack|ms-|1.3.1
remote-explorer|ms-|0.5.0
remote-repositories|ms-|0.42.0
remote-server|ms-|1.5.2
test-adapter-converter|ms-|0.2.1
vscode-speech|ms-|0.16.0
autodocstring|njp|0.6.1
swift-playgrounds|pie|0.1.1
trailing-spaces|sha|0.4.1
code-spell-checker|str|4.0.47
cmake|twx|0.0.17
intellicode-api-usage-examples|Vis|0.2.9
vscodeintellicode|Vis|1.3.2
jinja|who|0.0.8
markdown-all-in-one|yzh|3.6.3

(14 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551cf:31249601
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
0g0a1943:31341127
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
b6b4d950:31327385
nes-diff-11:31337487
nes-set-on:31340697
testaa123:31335226
6abeh943:31336334
yijiwantestdri0626-c:31336931
f76d9909:31342392

```

</details>

<!-- generated by issue reporter -->",2
Refusing to refactor any files or provide any feedback,"
Type: <b>Bug</b>

Regardless of the model i use or prompts i give, the model refuses to provide any guidance, feedback or refactor relevant files. I just paid for Copilot Pro today and now it's completely useless and disobedient. that was a waste of $10

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Linux x64 6.8.0-63-generic snap
Modes:
Remote OS version: Linux x64 6.8.0-63-generic

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD A8 PRO-7600B R7, 10 Compute Cores 4C+6G (4 x 3175)|
|GPU Status|2d_canvas: unavailable_software<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: disabled_software<br>multiple_raster_threads: enabled_on<br>opengl: disabled_off<br>rasterization: disabled_software<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: disabled_software<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: unavailable_software<br>webgl2: unavailable_software<br>webgpu: disabled_off<br>webnn: unavailable_software|
|Load (avg)|2, 2, 2|
|Memory (System)|14.59GB (7.18GB free)|
|Process Argv|--no-sandbox --force-user-env --crash-reporter-id 02560367-b0d8-46c3-9316-43c1547ef493|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|ubuntu|
|XDG_CURRENT_DESKTOP|Unity|
|XDG_SESSION_DESKTOP|ubuntu|
|XDG_SESSION_TYPE|x11|

|Item|Value|
|---|---|
|Remote|SSH: 192.168.1.11|
|OS|Linux x64 6.8.0-63-generic|
|CPUs|AMD Ryzen 3 5300G with Radeon Graphics (8 x 4242)|
|Memory (System)|62.14GB (54.05GB free)|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31342333
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
c4b42873:31341128
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
nes-diff-11:31337487
nes-set-on:31340697
testaa123cf:31335227
6abeh943:31336334
yijiwantestdri0626-t:31336930
4gdec884:31342391

```

</details>

<!-- generated by issue reporter -->",2
Copilot chat inline feedback options are too far out of the way,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
I think it would be more convenient for users if the thumbs up/down UI elements were closer to other elements in the Copilot inline suggestions pop-up.",0
I am consistently getting errors with Gemini 2.5,"
Type: <b>Bug</b>

Submit any request with Gemini 2.5 (pro or flash)

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|12th Gen Intel(R) Core(TM) i9-12900K (24 x 3187)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|63.73GB (24.05GB free)|
|Process Argv|--crash-reporter-id 2501963f-2f91-4d7f-8232-24c04f614a3c|
|Screen Reader|no|
|VM|0%|
</details>
<!-- generated by issue reporter -->",2
Window resize lag when a webview is visible,"There is noticeable lag while resizing the VS Code window when any webview is open (viewing an extension page for example)

<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
Version: 1.101.2 (Universal)
Commit: 2901c5ac6db8a986a5666c3af51ff804d05af0d4
Date: 2025-06-24T20:27:15.391Z (1 wk ago)
Electron: 35.5.1
ElectronBuildId: 11727614
Chromium: 134.0.6998.205
Node.js: 22.15.1
V8: 13.4.114.21-electron.0
OS: Darwin arm64 24.5.0
",0
Claude Sonnet 4 -- not reponding,"
Type: <b>Bug</b>

After another model created an implementation I asked C. Sonnet4 to review it and proceed with some testing. Then the error happened.

Extension version: 0.28.5
VS Code version: Code 1.101.1 (18e3a1ec544e6907be1e944a94c496e302073435, 2025-06-18T13:35:12.605Z)
OS version: Linux x64 6.11.0-29-generic snap
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 9 7900 12-Core Processor (24 x 4861)|
|GPU Status|2d_canvas: unavailable_software<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: disabled_software<br>multiple_raster_threads: enabled_on<br>opengl: disabled_off<br>rasterization: disabled_software<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: disabled_software<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: unavailable_software<br>webgl2: unavailable_software<br>webgpu: disabled_off<br>webnn: unavailable_software|
|Load (avg)|2, 2, 3|
|Memory (System)|62.25GB (41.85GB free)|
|Process Argv|--no-sandbox --force-user-env --crash-reporter-id 0e028a6d-2aeb-4a42-a948-d5c1b8268bcc|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|ubuntu|
|XDG_CURRENT_DESKTOP|Unity|
|XDG_SESSION_DESKTOP|ubuntu|
|XDG_SESSION_TYPE|x11|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805:30301674
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dwnewjupytercf:31046870
pythonrstrctxt:31112756
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7cf:31303019
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
convertlamdaf:31329270
jhi8h917:31341130
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
onetestforazureexpcf:31335614
6abeh943:31336334
yijiwantestdri0626-c:31336931

```

</details>

<!-- generated by issue reporter -->",2
Severe Performance Issues and Automatic File Generation in VS Code Stable (Resolved in Insiders),"**Operating System:** Linux justin-MS-7C02 6.8.0-62-generic #65-Ubuntu SMP PREEMPT_DYNAMIC Mon May 19 17:15:03 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux

**VS Code Version:** 1.101.2 (Commit: 2901c5ac6db8a986a5666c3af51ff804d05af0d4, Date: 2025-06-24T20:27:15.391Z)
**VS Code - Insiders Version:** 1.102.0-insider (Commit: 96f1890d08080f46f3b0a9424553422f04133090, Date: 2025-07-04T15:15:02.963Z)

**Issue:** VS Code stable has runaway zygote processes and file creation issues, but VS Code - Insiders works fine.

**Additional Note:**  
Simply opening VS Code (without running any scripts) causes the `demos/` folder to be recreated automatically, even after it has been deleted. This does not happen in VS Code - Insiders.

# Synopsis: VS Code Performance Issues in AI Project

## **Core Problem:**
VS Code exhibits severe performance issues with runaway zygote processes consuming 80-150% CPU in a cyclic crash-and-respawn pattern, plus unwanted automatic file generation.

## **What We've Discovered:**

### **1. The Runaway Process Pattern:**
- VS Code zygote processes start at ~100%+ CPU
- Gradually decline to ~30% over 15-20 minutes  
- Crash and immediately respawn at 100%+ CPU
- Cycle repeats indefinitely

### **2. Root Cause is NOT Large Files:**
We systematically ruled out file size issues:
- ❌ 67MB JSON file (excluded)
- ❌ Multi-gigabyte GGUF model files (moved)
- ❌ Git pack files (removed)
- ❌ Python venv directory (excluded)

### **3. Root Cause is NOT Extensions or Configuration:**
- ❌ Disabled all extensions (including Python/Pylance)
- ❌ Reset VS Code workspace completely
- ❌ Cleared all caches and settings
- ❌ Fresh VS Code installation

### **4. Issue is VS Code Core Architecture:**
- Problem persists even with minimal configuration
- PyCharm handles the exact same project perfectly
- inotify monitoring showed no excessive file I/O during high CPU
- Issue appears to be internal VS Code process management failure

### **5. Secondary Issue: Unwanted File Creation:**
VS Code auto-generates demo files on startup despite exclusion settings, suggesting extensions ignore configuration.

## **Conclusion:**
This appears to be a **fundamental incompatibility** between VS Code's core file processing/zygote management and something in your specific project structure that we haven't identified. The issue is **not fixable through normal troubleshooting** as it persists despite eliminating all obvious causes.

## **Current Status:**
- ✅ PyCharm works perfectly with the project
- ❌ Regular VS Code remains problematic
- ✅ VS Code Insiders works perfectly with the project

<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 
- OS Version: 

Steps to Reproduce:

1. 
2. 
",2
Duplicate entries in edits list,"<img width=""369"" height=""172"" alt=""Image"" src=""https://github.com/user-attachments/assets/4aca8474-a19d-415f-8aae-299d48784eac"" />

I believe there was an issue for this previously and I can't find it. I see one normal `file:` URI in here and one `chat-editing-snapshot-text-model` URI, and the conversation shows we were sending the snapshot as implicit context and the agent was editing it. I think we need to

- Not send files of that scheme as implicit context (or attachments at all, it seems I can attach it manually in this state)
- Disallow editing these models, the edit tool should fail. Maybe doesn't matter if we do the first one but there's probably always some way that they can leak unless we are sure that they will be filtered from all tool results.

[edit-snapshot.md](https://github.com/user-attachments/files/21064907/edit-snapshot.md)",1
Cloude Sonnet 4 Agent failure,"
Type: <b>Bug</b>

Eache time I request an update to my codespace as I have been for months, the vscode coopilot agent extension appears to try to make edits. Then as soon as the edit nears 100%, the window for the files edited goes from keep/undo to retry. And retry is not functional. I'm paying for a service that is continually failing and slow. 

Extension version: 0.28.5
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M4 Max (14 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|2, 2, 3|
|Memory (System)|36.00GB (18.26GB free)|
|Process Argv|--crash-reporter-id 32c30673-2095-41dc-8dbc-b8989d4ea1b1|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551cf:31249601
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
cppperfnew:31000557
dwnewjupyter:31046869
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31303018
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
j6a0c355:31339598
convertlamdaf:31329270
jhi8h917:31341130
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
nes-diff-11:31337487
nes-set-on:31340697
6abeh943:31336334
yijiwantestdri0626-t:31336930
bc3b0373:31337022

```

</details>

<!-- generated by issue reporter -->",2
bug,"
Type: <b>Bug</b>

The model unexpectedly did not return a response, which may indicate a service issue. Please report a bug.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 3 3250U with Radeon Graphics          (4 x 2600)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|9.94GB (2.52GB free)|
|Process Argv|--crash-reporter-id 87f5e521-4b44-4044-9a00-0443c2fd04d0|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
cppperfnew:31000557
dwnewjupytercf:31046870
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31303018
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
0g0a1943:31341127
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
testaa123cf:31335227
onetestforazureexpcf:31335614
63221493:31336333
yijiwantestdri0626-c:31336931

```

</details>

<!-- generated by issue reporter -->",2
Remote SSH to Centos linux comes bakc asking for password,"
Type: <b>Bug</b>

1- SSH Remote
2 - enter centos server name
3 - Choose ""Linux"" type
4 - Enter password
5 - request for password continues to come back instead of opening a session in spite of the fact I connect from terminal with ssh myuse:password@centosserver. Also, after googling, I updated sshd_config and updated AllowTcpForwarding with yes and authentication with password is also yes

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz (8 x 3408)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|31.94GB (8.81GB free)|
|Process Argv|--crash-reporter-id 3c6cc184-cdef-4bfc-96ad-fd651be59f47|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (180)</summary>

Extension|Author (truncated)|Version
---|---|---
terraform|4op|0.2.5
better-comments|aar|3.0.2
add-reference|adr|1.0.2
arepl|alm|3.0.0
amazon-q-vscode|ama|1.81.0
aws-toolkit-vscode|ama|3.68.0
vscode-color|ans|0.4.5
swagger-viewer|Arj|3.1.2
vscode-azureautomation|azu|1.2.5
azurite|Azu|3.34.0
armview|ben|0.4.6
docs-view|bie|0.1.0
lit-html|bie|1.11.1
xml2json|bui|1.2.5
multi-cursor-case-preserve|Car|1.0.5
turbo-console-log|Cha|3.1.1
vscode-better-align|cho|1.4.2
npm-intellisense|chr|1.4.5
path-intellisense|chr|2.10.0
regex|chr|0.6.0
vscode-eslint|dba|3.0.10
composer-php-vscode|DEV|1.59.17515
intelli-php-vscode|DEV|0.12.15062
phptools-vscode|DEV|1.59.17515
profiler-php-vscode|DEV|1.59.17515
docker|doc|0.11.0
githistory|don|0.6.20
xml|Dot|2.5.1
gitlens|eam|17.2.2
vscode-html-css|ecm|2.0.13
prettier-vscode|esb|11.0.0
comment-anchors|Exo|1.10.4
vscode-diff|fab|2.1.2
css-stacking-contexts|fel|1.0.15
php-intellisense|fel|2.3.14
vscode-solution-explorer|fer|0.9.1
vscode-firefox-debug|fir|2.15.0
auto-close-tag|for|0.5.15
azure-storage-explorer|for|0.1.2
code-runner|for|0.12.2
docker-explorer|for|0.1.7
dotnet|for|0.0.4
dotnet-test-explorer|for|0.7.8
vscode-mysql|for|0.5.0
toggle-zen-mode|fud|1.1.2
codespaces|Git|1.17.3
copilot|Git|1.338.0
copilot-chat|Git|0.28.5
remotehub|Git|0.64.0
vscode-pull-request-github|Git|0.112.0
go|gol|0.48.0
hcl|has|0.6.0
terraform|has|2.34.5
vscode-test-explorer|hbe|2.22.1
vscode-guid|hea|1.9.0
AzureFunctionsSnippets|hha|0.1.0
vscode-htmlhint|HTM|1.11.1
rest-client|hum|0.25.1
csharpnewclass|hun|1.0.0
icon-fonts|idl|2.5.4
vscode-tfs|iva|0.7.2
RelativePath|jak|1.5.0
polacode-2019|jef|0.6.2
vscode-peacock|joh|4.2.2
vscode-csharp-snippets|jor|1.1.0
krinql-vscode|Kri|0.1.1
ftp-sync|luk|0.3.9
start-git-bash|McC|1.2.1
rainbow-csv|mec|3.20.0
git-graph|mhu|1.30.0
mongodb-vscode|mon|1.13.3
azure-pipelines|ms-|1.249.0
microsoft-testing|ms-|0.1.17
vscode-azurecache|ms-|0.1.0
azure-dev|ms-|0.9.0
vscode-apimanagement|ms-|1.2.0
vscode-azure-github-copilot|ms-|1.0.42
vscode-azureappservice|ms-|0.26.2
vscode-azurecontainerapps|ms-|0.8.3
vscode-azurefunctions|ms-|1.17.3
vscode-azurelogicapps|ms-|5.109.14
vscode-azureresourcegroups|ms-|0.11.0
vscode-azurestaticwebapps|ms-|0.13.1
vscode-azurestorage|ms-|0.16.5
vscode-azureterraform|ms-|0.5.0
vscode-azurevirtualmachines|ms-|0.6.9
vscode-bicep|ms-|0.36.1
vscode-containers|ms-|2.0.3
vscode-cosmosdb|ms-|0.26.0
vscode-docker|ms-|2.0.0
vscode-logicapps|ms-|1.2.9
csdevkit|ms-|1.30.32
csharp|ms-|2.84.19
dotnet-interactive-vscode|ms-|1.0.6323010
vscode-dotnet-runtime|ms-|2.3.6
vscodeintellicode-csharp|ms-|2.2.3
vscode-kubernetes-tools|ms-|1.3.25
data-workspace-vscode|ms-|0.6.3
mssql|ms-|1.33.0
sql-bindings-vscode|ms-|0.4.1
sql-database-projects-vscode|ms-|1.5.3
playwright|ms-|1.1.15
debugpy|ms-|2025.8.0
isort|ms-|2025.0.0
python|ms-|2025.8.0
vscode-pylance|ms-|2025.6.2
jupyter|ms-|2025.5.0
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.1.0
vscode-jupyter-cell-tags|ms-|0.1.9
vscode-jupyter-slideshow|ms-|0.1.6
remote-containers|ms-|0.417.0
remote-ssh|ms-|0.120.0
remote-ssh-edit|ms-|0.87.0
remote-wsl|ms-|0.99.0
vscode-remote-extensionpack|ms-|0.26.0
azure-account|ms-|0.12.0
azure-repos|ms-|0.40.0
azurecli|ms-|0.6.0
cmake-tools|ms-|1.20.53
cpptools|ms-|1.26.3
cpptools-extension-pack|ms-|1.3.1
js-debug-nightly|ms-|2025.7.217
live-server|ms-|0.4.15
powershell|ms-|2025.2.0
remote-explorer|ms-|0.5.0
remote-repositories|ms-|0.42.0
remote-server|ms-|1.5.2
test-adapter-converter|ms-|0.2.1
vscode-node-azure-pack|ms-|1.6.0
team|ms-|1.161.1
windows-ai-studio|ms-|0.16.0
azurerm-vscode-tools|msa|0.15.15
debugger-for-edge|msj|1.0.15
sqltools|mtx|0.28.4
color-highlight|nau|2.8.0
live-server-preview|neg|0.1.4
gitdownloadazurerepos|nei|1.0.16
indent-rainbow|ode|8.3.1
refactor|p42|3.0.1
advanced-new-file|pat|1.2.2
excalidraw-editor|pom|3.9.0
vscode-css-peek|pra|4.4.3
live-preview|pro|0.0.3
puppet-vscode|pup|1.5.5
quicktype|qui|23.0.170
addlocalnetreferences|Raf|0.4.8
Csharp-ASPNETCore|rah|1.11.0
vscode-thunder-client|ran|2.35.3
ansible|red|25.7.0
java|red|1.43.1
vscode-commons|red|0.0.6
vscode-xml|red|0.29.0
vscode-yaml|red|1.18.0
xill-language|roe|0.0.5
partial-diff|ryu|1.4.3
totvs-coverage-analysis|shi|0.0.6
snyk-vulnerability-scanner|sny|2.22.0
sonarlint-vscode|Son|4.25.1
vscode-odata|sta|0.1.0
vscode-ai-foundry|Tea|0.7.0
bootstrap4-vscode|the|6.1.0
cmake|twx|0.0.17
highlight-matching-tag|vin|0.11.0
intellicode-api-usage-examples|Vis|0.2.9
vscodeintellicode|Vis|1.3.2
vscode-gradle|vsc|3.16.4
vscode-java-debug|vsc|0.58.2
vscode-java-dependency|vsc|0.24.1
vscode-java-pack|vsc|0.29.2
vscode-java-test|vsc|0.43.1
vscode-maven|vsc|0.44.0
vscode-icons|vsc|12.13.0
iis-express|war|1.5.0
php-debug|xde|1.36.1
php-pack|xde|1.0.3
markdown-all-in-one|yzh|3.6.3
bootstrap-v4-snippets|Zac|1.1.3
material-theme|zhu|3.19.0
html-css-class-completion|Zig|1.20.0

(8 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805cf:30301675
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
2i9eh265:30646982
962ge761:30959799
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dwnewjupytercf:31046870
pythonrstrctxt:31112756
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7cf:31303019
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
j6a0c355:31339598
d2249276:31341129
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
onetestforazureexpcf:31335614
6abeh943:31336334
yijiwantestdri0626-c:31336931

```

</details>

<!-- generated by issue reporter -->",2
mcp server definition in user settings,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
In a recent update you removed mcp servers from the user settings file and moved them into mcp.json. 
This is not a good design for us since there is no good way to have access to all mcp servers we use in each project.

The workspace approach where we put a .code-workspace file into the root of our developer folder does not work for us since you have to open that workspace which contains over a hundred folders in our case. we need to be able to open up just subfolders in vs code.

The other approach to add a .vscode/settings.json in each folder leads to a ton of duplication

Can you please allow mcp servers and inputs to be defined again in the user settings.json file?

Thank you
Fabian",0
Close All Diff Editors doesn't close diff from GitLens-Compare references,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 
- OS Version: 

Steps to Reproduce:

1. Install GitLens extensions
2. In GitLens section, add a new SearchAndCompare (example a branch with is main parent)
3. Click on one or more modified files (this will open a diff editor)
4. Then click F1 an select CloseAllDiffEditors. This will close all diff editor showing local changes, but not the ones from the GitLens SearchandCompare.

I am unsure if this is an issue with vscode or the GitLens Extension.",1
Does not complete prompt response,"
Type: <b>Bug</b>

I seems like there are multiple occasions when the AI starts writing the response and out of nowhere it deletes it and it does not complete it.

Extension version: 0.28.3
VS Code version: Code 1.101.1 (18e3a1ec544e6907be1e944a94c496e302073435, 2025-06-18T13:35:12.605Z)
OS version: Windows_NT x64 10.0.22631
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) Ultra 5 135U (14 x 2688)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|31.45GB (8.83GB free)|
|Process Argv|--crash-reporter-id 4f10455b-7b9f-460f-9ed2-0da154387c8d|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492cf:30256860
vscod805:30301674
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyonecf:30548226
962ge761:30959799
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dwnewjupytercf:31046870
pythonrstrctxt:31112756
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
5b33h341:31326279
6gi0g917:31259952
996jf627:31283433
pythonrdcb7:31303018
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
slnrequired:31334858
convertlamdaf:31329270
c4b42873:31332228
d7aab740:31332224
usemarketplace:31333563
agentclaude:31331946
id45c886:31333049
test-11-control:31334521

```

</details>

<!-- generated by issue reporter -->",2
REZALET!!!!!!!!!!,"
Type: <b>Bug</b>

TÜM PROJEYİ YOK EDEN BİR YAPAY ZEKA DENEYİMİ YAŞATTIĞINIZ İÇİN TEŞEKKÜRLER!!!!!!!

Extension version: 0.28.5
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M3 Max (16 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|2, 3, 3|
|Memory (System)|128.00GB (89.47GB free)|
|Process Argv|--crash-reporter-id 19ea9a5e-9267-4e4a-9698-d550689211b4|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805cf:30301675
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
2i9eh265:30646982
962ge761:30959799
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dwnewjupytercf:31046870
pythonrstrctxt:31112756
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31303018
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
d2249276:31341129
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930

```

</details>

<!-- generated by issue reporter -->",2
Not read the output npm test,"
Type: <b>Bug</b>

not read the output from npm test in the console of the copilot

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.22631
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i7-11370H @ 3.30GHz (8 x 2995)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.74GB (0.96GB free)|
|Process Argv|--crash-reporter-id 7eba0a46-138f-42d7-8829-6777c3924f4f|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492cf:30256860
pythonvspyt551cf:31249601
vscod805cf:30301675
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dwnewjupyter:31046869
pythonrstrctxt:31112756
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31303018
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
c4b42873:31341128
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
onetestforazureexp:31335613
6abeh943:31336334
yijiwantestdri0626-t:31336930

```

</details>

<!-- generated by issue reporter -->",2
"The model unexpectedly did not return a response, which may indicate a service issue. Please report a bug.","
Type: <b>Bug</b>

The model unexpectedly did not return a response, which may indicate a service issue. Please report a bug.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M3 Pro (11 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|4, 3, 3|
|Memory (System)|18.00GB (0.07GB free)|
|Process Argv|--crash-reporter-id 690417ae-0d7c-4230-9ca0-c5253b9a81ca|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805:30301674
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
pythontbext0:30879054
cppperfnew:31000557
dwnewjupyter:31046869
pythonrstrctxt:31112756
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
5b33h341:31326279
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31303018
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
0g0a1943:31341127
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
bc3b0373:31337022

```

</details>

<!-- generated by issue reporter -->",2
Can't send messages to copilot chat,"The co-pilot does not allow sending messages or interacting with the

<img width=""595"" height=""966"" alt=""Image"" src=""https://github.com/user-attachments/assets/6c872a63-a9fd-42f5-85b5-023d37cd5d6a"" />


I've already tried reinstalling the version, reinstalling vscode, trying on another device and it's like it's an error on my account.",1
Code Block Disappearing,"
Type: <b>Bug</b>

the code block the copilot outputted disappeared after a while, I wanted to access it and it is literally gone

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen AI 7 PRO 360 w/ Radeon 880M           (16 x 1996)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|31.15GB (12.41GB free)|
|Process Argv|--crash-reporter-id 761b615d-4788-451f-9c75-4a10d7ac4192|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805cf:30301675
binariesv615:30325510
vscaat:30438848
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dwnewjupytercf:31046870
pythonrstrctxt:31112756
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7cf:31303019
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
9d2cg352:31339597
jhi8h917:31341130
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
1292j425:31332164
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931

```

</details>

<!-- generated by issue reporter -->",2
Code completion didn't works on pro,"
Type: <b>Bug</b>

code completion didn't works on pro, i'm already update my plan but its not works.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 5 7600 6-Core Processor               (12 x 3800)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|31.10GB (15.99GB free)|
|Process Argv|--crash-reporter-id 94a70d42-b2ce-4a1a-b749-44bc800006d3|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805cf:30301675
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
pythontbext0:30879054
cppperfnew:31000557
dwnewjupytercf:31046870
pythonrstrctxt:31112756
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31303018
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
9d2cg352:31339597
c4b42873:31341128
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931

```

</details>

<!-- generated by issue reporter -->",2
issue inline copilot,"
Type: <b>Bug</b>

cant close it

Extension version: 0.28.5
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 22.3.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M2 Pro (10 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|2, 2, 2|
|Memory (System)|16.00GB (0.05GB free)|
|Process Argv|--crash-reporter-id 6d8e4535-8a69-4050-a211-0202e2a63ec8|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805:30301674
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
vscrpc:30673769
2i9eh265:30646982
962ge761:30959799
2e7ec940:31000449
pythontbext0:30879054
cppperfnew:31000557
dwnewjupyter:31046869
pythonrstrctxt:31112756
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
e77a2192:31326280
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31303018
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
9d2cg352:31339597
d2249276:31341129
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
82j33506:31327384
nes-diff-11:31337487
onetestforazureexpcf:31335614
63221493:31336333
yijiwantestdri0626-c:31336931
hi92a955:31337023

```

</details>

<!-- generated by issue reporter -->",2
"Bug: Unable to correct invalid API key in ""Manage Models"" due to blocking error dialog","<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.101.2 (user setup)
Commit: 2901c5ac6db8a986a5666c3af51ff804d05af0d4
Date: 2025-06-24T20:27:15.391Z
Electron: 35.5.1
ElectronBuildId: 11727614
Chromium: 134.0.6998.205
Node.js: 22.15.1
V8: 13.4.114.21-electron.0
OS: Windows_NT x64 10.0.26100
- OS Version: Windows 11 x64 26100.4484
- GH Copilot Version: 1.338.0


**Description**

When an invalid API key is entered for a third-party model provider (e.g., Gemini, Anthropic) via the ""Manage Models"" UI, a recurring error dialog appears. This dialog blocks the underlying UI, making it impossible to correct the invalid key. The user becomes stuck in an error loop and must manually edit the settings.json file to resolve the issue.

**Steps to Reproduce**

1.  Open the Command Palette (`Ctrl+Shift+P`).
2.  Run the command `Copilot: Manage Models`.
3.  Select a model provider that requires an API key (e.g., Gemini).
4.  When prompted to enter the API key, paste an **incorrect** or **invalid** key and press Enter.
5.  An error dialog appears stating the ""API key not valid.""
6.  Click ""Go Back"" or ""Cancel"" on the dialog. This returns you to the provider selection list.
7.  Select the same model provider again, with the intent to enter the correct API key.
8.  The error dialog immediately appears again, blocking the user from entering a new key.

**Expected Behavior**

After selecting the provider again in step 7, the user should be prompted to enter the API key, allowing them to input the correct one.

**Actual Behavior**

Before allowing the user to input a new key, the extension attempts to validate the previously entered (and cached) invalid key by fetching the model endpoint. This validation attempt fails, which immediately triggers the error dialog and blocks the UI, preventing the user from ever reaching the input prompt to correct the key.

<img width=""905"" height=""355"" alt=""Image"" src=""https://github.com/user-attachments/assets/d9cc0f75-c03d-4e92-a091-f058cff74fd9"" />

 
<img width=""750"" height=""81"" alt=""Image"" src=""https://github.com/user-attachments/assets/95e3c0fc-673e-466c-ba99-41b7edc8c340"" />

<img width=""453"" height=""213"" alt=""Image"" src=""https://github.com/user-attachments/assets/b3561a4c-de58-4577-9694-6c0c366c135d"" />

",1
Authentication claimed as unfinished after success,"
Type: <b>Bug</b>

The plugin redirects to github that sucessfully goes through authentication and sucessfully returns to vscode.

However at that point the plugin claims authentication to be unfinished and asks if you want to continue by other means. 

Clicking ""No"" leands to it asking for username and passwords, whcih also seems off.


VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M4 Pro (14 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|3, 2, 2|
|Memory (System)|48.00GB (6.69GB free)|
|Process Argv||
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (2)</summary>

Extension|Author (truncated)|Version
---|---|---
rdetools|Blo|1.36.4
vscode-yaml|red|1.18.0


</details>
<!-- generated by issue reporter -->",1
"Copilot and VS code when connected via SSH is crashing too much and AFTER I reopen, the changes are undone and so much mess in code","```
[15:12:38.150] Log Level: 2
[15:12:38.168] SSH Resolver called for ""ssh-remote+7b22686f73744e616d65223a2231302e3138332e38332e313433222c2275736572223a2261646d696e6973747261746f72227d"", attempt 1
[15:12:38.170] remote.SSH.useLocalServer = true
[15:12:38.170] remote.SSH.useExecServer = true
[15:12:38.170] remote.SSH.bindHost = {}
[15:12:38.170] remote.SSH.path = undefined
[15:12:38.170] remote.SSH.configFile = undefined
[15:12:38.170] remote.SSH.useFlock = true
[15:12:38.171] remote.SSH.lockfilesInTmp = false
[15:12:38.171] remote.SSH.localServerDownload = auto
[15:12:38.171] remote.SSH.remoteServerListenOnSocket = false
[15:12:38.171] remote.SSH.defaultExtensions = []
[15:12:38.171] remote.SSH.defaultExtensionsIfInstalledLocally = []
[15:12:38.171] remote.SSH.loglevel = 2
[15:12:38.171] remote.SSH.enableDynamicForwarding = true
[15:12:38.171] remote.SSH.enableRemoteCommand = false
[15:12:38.171] remote.SSH.serverPickPortsFromRange = {}
[15:12:38.171] remote.SSH.serverInstallPath = {}
[15:12:38.171] remote.SSH.permitPtyAllocation = false
[15:12:38.171] remote.SSH.preferredLocalPortRange = undefined
[15:12:38.171] remote.SSH.useCurlAndWgetConfigurationFiles = false
[15:12:38.171] remote.SSH.experimental.chat = false
[15:12:38.171] remote.SSH.experimental.enhancedSessionLogs = false
[15:12:38.171] remote.SSH.httpProxy = {}
[15:12:38.172] remote.SSH.httpsProxy = {}
[15:12:38.177] VS Code version: 1.101.2
[15:12:38.177] Remote-SSH version: remote-ssh@0.120.0
[15:12:38.177] linux x64
[15:12:38.192] SSH Resolver called for host: administrator@10.183.83.143
[15:12:38.192] Setting up SSH remote ""10.183.83.143""
[15:12:38.203] Acquiring local install lock: /tmp/vscode-remote-ssh-bad318fe-install.lock
[15:12:38.204] Looking for existing server data file at /home/rohan/.config/Code/User/globalStorage/ms-vscode-remote.remote-ssh/vscode-ssh-host-bad318fe-2901c5ac6db8a986a5666c3af51ff804d05af0d4-0.120.0-es/data.json
[15:12:38.204] Found existing data file
[15:12:38.204] Found local server running: {""remoteListeningOn"":{""port"":37481},""osReleaseId"":""ubuntu"",""arch"":""x86_64"",""sshAuthSock"":"""",""display"":"""",""tmpDir"":""/run/user/1000"",""platform"":""linux"",""execServerToken"":""62a31c25-2fd1-428d-85d1-ed6a2be445e8"",""pid"":15943,""ipcHandlePath"":""/run/user/1000/vscode-ssh-askpass-72c348cd4dbcd7539ff2638ad3cafa0a11330011.sock"",""socksPort"":35559,""startupTime"":1751634732986}
[15:12:38.213] Found running server - short-circuiting install
[15:12:38.218] Starting forwarding server. local port 36321 -> socksPort 35559 -> remotePort 37481
[15:12:38.218] Forwarding server listening on port 36321
[15:12:38.219] Waiting for ssh tunnel to be ready
[15:12:38.220] [Forwarding server port 36321] Got connection 0
[15:12:38.222] Tunneled port 37481 to local port 36321
[15:12:38.222] Resolved ""ssh-remote+7b22686f73744e616d65223a2231302e3138332e38332e313433222c2275736572223a2261646d696e6973747261746f72227d"" to ""port 36321""
[15:12:38.231] Initizing new exec server for ssh-remote+7b22686f73744e616d65223a2231302e3138332e38332e313433222c2275736572223a2261646d696e6973747261746f72227d
[15:12:38.232] Resolving exec server at port 36321
[15:12:38.232] [Forwarding server port 36321] Got connection 1
[15:12:38.420] Exec server for ssh-remote+7b22686f73744e616d65223a2231302e3138332e38332e313433222c2275736572223a2261646d696e6973747261746f72227d created and cached
[15:12:38.421] Extensions to install: 
[15:12:38.434] ------




[15:12:38.435] No hints found in the recent session.
[15:12:38.482] [server] Checking /home/administrator/.vscode-server/cli/servers/Stable-2901c5ac6db8a986a5666c3af51ff804d05af0d4/log.txt and /home/administrator/.vscode-server/cli/servers/Stable-2901c5ac6db8a986a5666c3af51ff804d05af0d4/pid.txt for a running server...
[15:12:38.483] [server] Found running server (pid=27544)
[15:45:44.364] ------




[15:45:44.365] SSH Resolver called for ""ssh-remote+7b22686f73744e616d65223a2231302e3138332e38332e313433222c2275736572223a2261646d696e6973747261746f72227d"", attempt 2, (Reconnection)
[15:45:44.385] SSH Resolver called for host: administrator@10.183.83.143
[15:45:44.385] Setting up SSH remote ""10.183.83.143""
[15:45:44.390] Acquiring local install lock: /tmp/vscode-remote-ssh-bad318fe-install.lock
[15:45:44.392] Looking for existing server data file at /home/rohan/.config/Code/User/globalStorage/ms-vscode-remote.remote-ssh/vscode-ssh-host-bad318fe-2901c5ac6db8a986a5666c3af51ff804d05af0d4-0.120.0-es/data.json
[15:45:44.393] Found existing data file
[15:45:44.393] Found local server running: {""remoteListeningOn"":{""port"":37481},""osReleaseId"":""ubuntu"",""arch"":""x86_64"",""sshAuthSock"":"""",""display"":"""",""tmpDir"":""/run/user/1000"",""platform"":""linux"",""execServerToken"":""62a31c25-2fd1-428d-85d1-ed6a2be445e8"",""pid"":15943,""ipcHandlePath"":""/run/user/1000/vscode-ssh-askpass-72c348cd4dbcd7539ff2638ad3cafa0a11330011.sock"",""socksPort"":35559,""startupTime"":1751634732986}
[15:45:44.393] Running server is stale. Ignoring
[15:45:44.396] Using commit id ""2901c5ac6db8a986a5666c3af51ff804d05af0d4"" and quality ""stable"" for server
[15:45:44.396] Extensions to install: 
[15:45:44.405] Install and start server if needed
[15:45:44.411] PATH: /opt/ros/humble/bin:/home/rohan/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin
[15:45:44.411] Checking ssh with ""ssh -V""
[15:45:44.419] > OpenSSH_8.9p1 Ubuntu-3ubuntu0.13, OpenSSL 3.0.2 15 Mar 2022

[15:45:44.423] askpass server listening on /run/user/1000/vscode-ssh-askpass-e1fdf52308959bb657c0c7a3e56ef2697becd22b.sock
[15:45:44.423] Spawning local server with {""serverId"":1,""ipcHandlePath"":""/run/user/1000/vscode-ssh-askpass-dd3f9c1561ee1109656884ec7f70ee285d0fe5c5.sock"",""sshCommand"":""ssh"",""sshArgs"":[""-v"",""-T"",""-D"",""46733"",""-o"",""ConnectTimeout=15"",""administrator@10.183.83.143""],""serverDataFolderName"":"".vscode-server"",""dataFilePath"":""/home/rohan/.config/Code/User/globalStorage/ms-vscode-remote.remote-ssh/vscode-ssh-host-bad318fe-2901c5ac6db8a986a5666c3af51ff804d05af0d4-0.120.0-es/data.json""}
[15:45:44.424] Local server env: {""SSH_AUTH_SOCK"":""/run/user/1000/keyring/ssh"",""SHELL"":""/bin/bash"",""DISPLAY"":"":1"",""ELECTRON_RUN_AS_NODE"":""1"",""SSH_ASKPASS"":""/home/rohan/.vscode/extensions/ms-vscode-remote.remote-ssh-0.120.0/out/local-server/askpass.sh"",""VSCODE_SSH_ASKPASS_NODE"":""/snap/code/198/usr/share/code/code"",""VSCODE_SSH_ASKPASS_EXTRA_ARGS"":"""",""VSCODE_SSH_ASKPASS_MAIN"":""/home/rohan/.vscode/extensions/ms-vscode-remote.remote-ssh-0.120.0/out/askpass-main.js"",""VSCODE_SSH_ASKPASS_HANDLE"":""/run/user/1000/vscode-ssh-askpass-e1fdf52308959bb657c0c7a3e56ef2697becd22b.sock""}
[15:45:44.429] Spawned 16496
[15:45:44.429] Using connect timeout of 17 seconds
[15:45:44.502] > local-server-1> Running ssh connection command: ssh -v -T -D 46733 -o ConnectTimeout=15 administrator@10.183.83.143
[15:45:44.505] > local-server-1> Spawned ssh, pid=16504
[15:45:44.509] stderr> OpenSSH_8.9p1 Ubuntu-3ubuntu0.13, OpenSSL 3.0.2 15 Mar 2022
[15:45:44.558] stderr> debug1: Server host key: ssh-ed25519 SHA256:Xq98QaXHere8LgkOgGjt+Gcx/AT9zmAL0ewNSUEFv7A
[15:45:44.738] Got askpass request: {""request"":"" administrator@10.183.83.143's password: ""}
[15:45:44.739] Showing password prompt
[15:45:44.739] Listening for interwindow password on /run/user/1000/vscode-ssh-askpass-e35f441dcf4ae660fabe15643a74c8ed8ad34a04.sock
[15:45:44.740] Writing password prompt to globalState
[15:45:48.109] Exec server for ssh-remote+7b22686f73744e616d65223a2231302e3138332e38332e313433222c2275736572223a2261646d696e6973747261746f72227d closed (gracefully)
[15:45:48.861] Got password response
[15:45:48.861] Interactor gave response: *********
[15:45:48.861] Cleaning up other-window auth server
[15:45:48.862] Using connect timeout of 17 seconds
[15:45:49.300] stderr> Authenticated to 10.183.83.143 ([10.183.83.143]:22) using ""password"".
[15:45:49.367] >             .-/+oossssoo+/-.
>         `:+ssssssssssssssssss+:`
>       -+ssssssssssssssssssyyssss+-
>     .ossssssssssssssssssdMMMNysssso.
>    /ssssssssssshdmmNNmmyNMMMMhssssss/
>   +ssssssssshmydMMMMMMMNddddyssssssss+
>  /sssssssshNMMMyhhyyyyhmNMMMNhssssssss/
> .ssssssssdMMMNhsssssssssshNMMMdssssssss.
> +sssshhhyNMMNyssssssssssssyNMMMysssssss+
> ossyNMMMNyMMhsssssssssssssshmmmhssssssso
> ossyNMMMNyMMhsssssssssssssshmmmhssssssso
> +sssshhhyNMMNyssssssssssssyNMMMysssssss+
> .ssssssssdMMMNhsssssssssshNMMMdssssssss.
>  /sssssssshNMMMyhhyyyyhdNMMMNhssssssss/
>   +sssssssssdmydMMMMMMMMddddyssssssss+
>    /ssssssssssshdmNNNNmyNMMMMhssssss/
>     .ossssssssssssssssssdMMMNysssso.
>       -+sssssssssssssssssyyyssss+-
>         `:+ssssssssssssssssss+:`
>             .-/+oossssoo+/-.
> root@jackal-j100-0814 
> --------------------- 
> OS: Ubuntu 22.04.5 LTS x86_64 
> Host: PH13FEI 
> Kernel: 6.8.0-52-generic 
> Uptime: 1 hour, 44 mins 
> Packages: 3520 (dpkg), 9 (snap) 
> Shell: bash 5.1.16 
> Terminal: run-parts 
> CPU: Intel i7-9700TE (8) @ 3.800GHz 
> GPU: Intel CoffeeLake-S GT2 [UHD Graphics 630] 
> Memory: 3602MiB / 15821MiB 
> 
>                         
>                         
> 
> 
> 
> 
> 
> 
> -------------------------------------------------------------------------------
>        _  _  _  _  ____   __  ____  ____  _  _   __  ____                      
>       ( \/ )( \/ )(  _ \ /  \(_  _)/ ___)/ )( \ /  \(  _ \                     
>       / \/ \ )  /  ) _ ((  O ) )(  \___ \) __ ((  O )) __/                     
>       \_)(_/(__/  (____/ \__/ (__) (____/\_)(_/ \__/(__)                       
>                                                                                
> -------------------------------------------------------------------------------
> Robot Service Status for clearpath-robot: Active 
> Robot Service Status for clearpath-platform: Active 
> -------------------------------------------------------------------------------
> Robot Service Status for mbs-fixposition: Active 
> Robot Service Status for mbs-robosense: Active 
> Robot Service Status for mbs-realsense-front: Active 
> -------------------------------------------------------------------------------
> Jackal IP:           192.168.131.1                                  
> Pswd:                mybotshop                                      
> SSH:                 ssh -XC administrator@192.168.131.1            
> -------------------------------------------------------------------------------
> Jackal Drivers               
[15:45:49.374] >                                                   
> -------------------------------------------------------------------------------
> Service status:      sudo service {service_name} status             
>                                                                                
> Navi 2D SLAM:        ros2 launch jkl_nav2 slam.launch.py            
> Navi Map:            ros2 launch jkl_nav2 map_navi.launch.py        
> Navi Odom:           ros2 launch jkl_nav2 odom_navi.launch.py       
> Visualization:       ros2 launch jkl_viz view_robot.launch.py       
>                                                                                
> Rebuild package:     colcon build --symlink-install                 
> -------------------------------------------------------------------------------
> ready: 9cb1a04e6500
[15:45:49.390] > Linux 6.8.0-52-generic #53~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Jan 15 19:18:46 UTC 2
[15:45:49.390] Platform: linux
[15:45:49.398] > /bin/bash
[15:45:49.398] Parent Shell: bash
[15:45:49.399] Parent Shell pid: 16496
[15:45:49.399] Waiting for subshell to start
[15:45:49.408] > 38243
[15:45:49.408] stdout -> '38243'
[15:45:49.408] sub-process detected
[15:45:49.418] > 9cb1a04e6500: running
> Script executing under PID: 38243
[15:45:49.435] > Found existing installation at /home/administrator/.vscode-server...
> Starting VS Code CLI...
> Removing old logfile at /home/administrator/.vscode-server/.cli.2901c5ac6db8a986a5666c3af51ff804d05af0d4.log
[15:45:49.440] > Spawned remote CLI: 38261
> Waiting for server log...
[15:45:49.469] > Waiting for server log...
[15:45:49.549] > 9cb1a04e6500: start
> listeningOn==127.0.0.1:40333==
> osReleaseId==ubuntu==
> arch==x86_64==
> vscodeArch==x64==
> bitness==64==
> tmpDir==/run/user/1000==
> platform==linux==
> unpackResult====
> didLocalDownload==0==
> downloadTime====
> installTime====
> serverStartTime==73==
> execServerToken==11aa1a11-1aa1-111a-1a1a-a1a1aaa11111==
> platformDownloadPath==cli-alpine-x64==
> SSH_AUTH_SOCK====
> DISPLAY====
> 9cb1a04e6500: end
[15:45:49.549] Received install output: 
listeningOn==127.0.0.1:40333==
osReleaseId==ubuntu==
arch==x86_64==
vscodeArch==x64==
bitness==64==
tmpDir==/run/user/1000==
platform==linux==
unpackResult====
didLocalDownload==0==
downloadTime====
installTime====
serverStartTime==73==
execServerToken==11aa1a11-1aa1-111a-1a1a-a1a1aaa11111==
platformDownloadPath==cli-alpine-x64==
SSH_AUTH_SOCK====
DISPLAY====

[15:45:49.550] Remote server is listening on port 40333
[15:45:49.551] Parsed server configuration: {""serverConfiguration"":{""remoteListeningOn"":{""port"":40333},""osReleaseId"":""ubuntu"",""arch"":""x86_64"",""sshAuthSock"":"""",""display"":"""",""tmpDir"":""/run/user/1000"",""platform"":""linux"",""execServerToken"":""11aa1a11-1aa1-111a-1a1a-a1a1aaa11111""},""serverStartTime"":73,""installUnpackCode"":""""}
[15:45:49.551] Persisting server connection details to /home/rohan/.config/Code/User/globalStorage/ms-vscode-remote.remote-ssh/vscode-ssh-host-bad318fe-2901c5ac6db8a986a5666c3af51ff804d05af0d4-0.120.0-es/data.json
[15:45:49.555] Starting forwarding server. local port 40719 -> socksPort 46733 -> remotePort 40333
[15:45:49.556] Forwarding server listening on port 40719
[15:45:49.556] Waiting for ssh tunnel to be ready
[15:45:49.557] [Forwarding server port 40719] Got connection 0
[15:45:49.559] Tunneled port 40333 to local port 40719
[15:45:49.560] Resolved ""ssh-remote+7b22686f73744e616d65223a2231302e3138332e38332e313433222c2275736572223a2261646d696e6973747261746f72227d"" to ""port 40719""
[15:45:49.561] Initizing new exec server for ssh-remote+7b22686f73744e616d65223a2231302e3138332e38332e313433222c2275736572223a2261646d696e6973747261746f72227d
[15:45:49.561] Resolving exec server at port 40719
[15:45:49.562] [Forwarding server port 40719] Got connection 1
[15:45:49.661] Exec server for ssh-remote+7b22686f73744e616d65223a2231302e3138332e38332e313433222c2275736572223a2261646d696e6973747261746f72227d created and cached
[15:45:49.662] Extensions to install: 
[15:45:49.665] ------




[15:45:49.666] No hints found in the recent session.
[15:45:49.688] [server] Checking /home/administrator/.vscode-server/cli/servers/Stable-2901c5ac6db8a986a5666c3af51ff804d05af0d4/log.txt and /home/administrator/.vscode-server/cli/servers/Stable-2901c5ac6db8a986a5666c3af51ff804d05af0d4/pid.txt for a running server...
[15:45:49.732] [server] Found running server (pid=27544)
[15:46:38.997] ------




[15:46:38.997] SSH Resolver called for ""ssh-remote+7b22686f73744e616d65223a2231302e3138332e38332e313433222c2275736572223a2261646d696e6973747261746f72227d"", attempt 3, (Reconnection)
[15:46:39.014] SSH Resolver called for host: administrator@10.183.83.143
[15:46:39.014] Setting up SSH remote ""10.183.83.143""
[15:46:39.018] Acquiring local install lock: /tmp/vscode-remote-ssh-bad318fe-install.lock
[15:46:39.019] Looking for existing server data file at /home/rohan/.config/Code/User/globalStorage/ms-vscode-remote.remote-ssh/vscode-ssh-host-bad318fe-2901c5ac6db8a986a5666c3af51ff804d05af0d4-0.120.0-es/data.json
[15:46:39.019] Found existing data file
[15:46:39.020] Found local server running: {""remoteListeningOn"":{""port"":40333},""osReleaseId"":""ubuntu"",""arch"":""x86_64"",""sshAuthSock"":"""",""display"":"""",""tmpDir"":""/run/user/1000"",""platform"":""linux"",""execServerToken"":""92ca0c54-0cd5-415e-8a3f-e4b6dcc73554"",""pid"":16496,""ipcHandlePath"":""/run/user/1000/vscode-ssh-askpass-dd3f9c1561ee1109656884ec7f70ee285d0fe5c5.sock"",""socksPort"":46733,""startupTime"":1751636744429}
[15:46:39.020] Running server is stale. Ignoring
[15:46:39.021] Using commit id ""2901c5ac6db8a986a5666c3af51ff804d05af0d4"" and quality ""stable"" for server
[15:46:39.023] Extensions to install: 
[15:46:39.032] Install and start server if needed
[15:46:39.037] askpass server listening on /run/user/1000/vscode-ssh-askpass-a2d6aa484fbd9f75ec4e5b75440819e7fa5d9342.sock
[15:46:39.037] Spawning local server with {""serverId"":2,""ipcHandlePath"":""/run/user/1000/vscode-ssh-askpass-59063124351de2b9a8df35212033904262220c27.sock"",""sshCommand"":""ssh"",""sshArgs"":[""-v"",""-T"",""-D"",""44199"",""-o"",""ConnectTimeout=15"",""administrator@10.183.83.143""],""serverDataFolderName"":"".vscode-server"",""dataFilePath"":""/home/rohan/.config/Code/User/globalStorage/ms-vscode-remote.remote-ssh/vscode-ssh-host-bad318fe-2901c5ac6db8a986a5666c3af51ff804d05af0d4-0.120.0-es/data.json""}
[15:46:39.037] Local server env: {""SSH_AUTH_SOCK"":""/run/user/1000/keyring/ssh"",""SHELL"":""/bin/bash"",""DISPLAY"":"":1"",""ELECTRON_RUN_AS_NODE"":""1"",""SSH_ASKPASS"":""/home/rohan/.vscode/extensions/ms-vscode-remote.remote-ssh-0.120.0/out/local-server/askpass.sh"",""VSCODE_SSH_ASKPASS_NODE"":""/snap/code/198/usr/share/code/code"",""VSCODE_SSH_ASKPASS_EXTRA_ARGS"":"""",""VSCODE_SSH_ASKPASS_MAIN"":""/home/rohan/.vscode/extensions/ms-vscode-remote.remote-ssh-0.120.0/out/askpass-main.js"",""VSCODE_SSH_ASKPASS_HANDLE"":""/run/user/1000/vscode-ssh-askpass-a2d6aa484fbd9f75ec4e5b75440819e7fa5d9342.sock""}
[15:46:39.044] Spawned 16520
[15:46:39.044] Using connect timeout of 17 seconds
[15:46:39.116] > local-server-2> Running ssh connection command: ssh -v -T -D 44199 -o ConnectTimeout=15 administrator@10.183.83.143
[15:46:39.120] > local-server-2> Spawned ssh, pid=16528
[15:46:39.122] stderr> OpenSSH_8.9p1 Ubuntu-3ubuntu0.13, OpenSSL 3.0.2 15 Mar 2022
[15:46:39.190] stderr> debug1: Server host key: ssh-ed25519 SHA256:Xq98QaXHere8LgkOgGjt+Gcx/AT9zmAL0ewNSUEFv7A
[15:46:39.364] Got askpass request: {""request"":"" administrator@10.183.83.143's password: ""}
[15:46:39.365] Showing password prompt
[15:46:39.365] Listening for interwindow password on /run/user/1000/vscode-ssh-askpass-b7defc023a361cb4955f7922950a8ad93f7eb09f.sock
[15:46:39.365] Writing password prompt to globalState
[15:46:43.956] Exec server for ssh-remote+7b22686f73744e616d65223a2231302e3138332e38332e313433222c2275736572223a2261646d696e6973747261746f72227d closed (gracefully)
[15:46:43.957] > local-server-1> Timed out
[15:46:43.962] Local server exit: 0
[15:46:45.539] Got password response
[15:46:45.539] Interactor gave response: *********
[15:46:45.540] Cleaning up other-window auth server
[15:46:45.541] Using connect timeout of 17 seconds
[15:46:45.611] stderr> Authenticated to 10.183.83.143 ([10.183.83.143]:22) using ""password"".
[15:46:45.974] >             .-/+oossssoo+/-.
>         `:+ssssssssssssssssss+:`
>       -+ssssssssssssssssssyyssss+-
>     .ossssssssssssssssssdMMMNysssso.
>    /ssssssssssshdmmNNmmyNMMMMhssssss/
>   +ssssssssshmydMMMMMMMNddddyssssssss+
>  /sssssssshNMMMyhhyyyyhmNMMMNhssssssss/
> .ssssssssdMMMNhsssssssssshNMMMdssssssss.
> +sssshhhyNMMNyssssssssssssyNMMMysssssss+
> ossyNMMMNyMMhsssssssssssssshmmmhssssssso
> ossyNMMMNyMMhsssssssssssssshmmmhssssssso
> +sssshhhyNMMNyssssssssssssyNMMMysssssss+
> .ssssssssdMMMNhsssssssssshNMMMdssssssss.
>  /sssssssshNMMMyhhyyyyhdNMMMNhssssssss/
>   +sssssssssdmydMMMMMMMMddddyssssssss+
>    /ssssssssssshdmNNNNmyNMMMMhssssss/
>     .ossssssssssssssssssdMMMNysssso.
>       -+sssssssssssssssssyyyssss+-
>         `:+ssssssssssssssssss+:`
>             .-/+oossssoo+/-.
> root@jackal-j100-0814 
> --------------------- 
> OS: Ubuntu 22.04.5 LTS x86_64 
> Host: PH13FEI 
> Kernel: 6.8.0-52-generic 
> Uptime: 1 hour, 45 mins 
> Packages: 3520 (dpkg), 9 (snap) 
> Shell: bash 5.1.16 
> Terminal: run-parts 
> CPU: Intel i7-9700TE (8) @ 3.800GHz 
> GPU: Intel CoffeeLake-S GT2 [UHD Graphics 630] 
> Memory: 5148MiB / 15821MiB 
> 
>                         
>                         
> 
> 
> 
> 
> 
> 
> -------------------------------------------------------------------------------
>        _  _  _  _  ____   __  ____  ____  _  _   __  ____                      
>       ( \/ )( \/ )(  _ \ /  \(_  _)/ ___)/ )( \ /  \(  _ \                     
>       / \/ \ )  /  ) _ ((  O ) )(  \___ \) __ ((  O )) __/                     
>       \_)(_/(__/  (____/ \__/ (__) (____/\_)(_/ \__/(__)                       
>                                                                                
> -------------------------------------------------------------------------------
> Robot Service Status for clearpath-robot: Active 
> Robot Service Status for clearpath-platform: Active 
> -------------------------------------------------------------------------------
> Robot Service Status for mbs-fixposition: Active 
> Robot Service Status for mbs-robosense: Active 
> Robot Service Status for mbs-realsense-front: Active 
> -------------------------------------------------------------------------------
> Jackal IP:           192.168.131.1                                  
> Pswd:                mybotshop                                      
> SSH:                 ssh -XC administrator@192.168.131.1            
> -------------------------------------------------------------------------------
> Jackal Drivers                                                                 
> -------------------------------------------------------------------------------
> Service status:      sudo service {service_name} status             
>                                                                                
> Navi 2D SLAM:        ros2 launch jkl_nav2 slam.launch.py            
> Navi Map:            ros2 launch jkl_nav2 map_navi.launch.py        
> Navi Odom:           ros2 launch jkl_nav2 odom_navi.launch.py       
> Visualization:       ros2 launch jkl_viz view_robot.launch.py       
>                                                                                
> Rebuild package:     colcon build --symlink-install                 
> -------------------------------------------------------------------------------
[15:46:45.982] > ready: 9ef92fcd9194
[15:46:45.992] > Linux 6.8.0-52-generic #53~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Jan 15 19:18:46 UTC 2
[15:46:45.993] Platform: linux
[15:46:46.001] > /bin/bash
[15:46:46.001] Parent Shell: bash
[15:46:46.001] Parent Shell pid: 16520
[15:46:46.002] Waiting for subshell to start
[15:46:46.011] > 39288
[15:46:46.011] stdout -> '39288'
[15:46:46.011] sub-process detected
[15:46:46.024] > 9ef92fcd9194: running
> Script executing under PID: 39288
[15:46:46.036] > Found existing installation at /home/administrator/.vscode-server...
> Starting VS Code CLI...
[15:46:46.039] > Removing old logfile at /home/administrator/.vscode-server/.cli.2901c5ac6db8a986a5666c3af51ff804d05af0d4.log
[15:46:46.044] > Spawned remote CLI: 39306
[15:46:46.049] > Waiting for server log...
[15:46:46.082] > Waiting for server log...
[15:46:46.116] > 9ef92fcd9194: start
> listeningOn==127.0.0.1:44707==
> osReleaseId==ubuntu==
> arch==x86_64==
> vscodeArch==x64==
> bitness==64==
> tmpDir==/run/user/1000==
> platform==linux==
> unpackResult====
> didLocalDownload==0==
> downloadTime====
> installTime====
> serverStartTime==75==
> execServerToken==11a1aa11-11a1-111a-1111-11a1a1aaa1a1==
> platformDownloadPath==cli-alpine-x64==
> SSH_AUTH_SOCK====
> DISPLAY====
> 9ef92fcd9194: end
[15:46:46.116] Received install output: 
listeningOn==127.0.0.1:44707==
osReleaseId==ubuntu==
arch==x86_64==
vscodeArch==x64==
bitness==64==
tmpDir==/run/user/1000==
platform==linux==
unpackResult====
didLocalDownload==0==
downloadTime====
installTime====
serverStartTime==75==
execServerToken==11a1aa11-11a1-111a-1111-11a1a1aaa1a1==
platformDownloadPath==cli-alpine-x64==
SSH_AUTH_SOCK====
DISPLAY====

[15:46:46.117] Remote server is listening on port 44707
[15:46:46.117] Parsed server configuration: {""serverConfiguration"":{""remoteListeningOn"":{""port"":44707},""osReleaseId"":""ubuntu"",""arch"":""x86_64"",""sshAuthSock"":"""",""display"":"""",""tmpDir"":""/run/user/1000"",""platform"":""linux"",""execServerToken"":""11a1aa11-11a1-111a-1111-11a1a1aaa1a1""},""serverStartTime"":75,""installUnpackCode"":""""}
[15:46:46.118] Persisting server connection details to /home/rohan/.config/Code/User/globalStorage/ms-vscode-remote.remote-ssh/vscode-ssh-host-bad318fe-2901c5ac6db8a986a5666c3af51ff804d05af0d4-0.120.0-es/data.json
[15:46:46.121] Starting forwarding server. local port 45421 -> socksPort 44199 -> remotePort 44707
[15:46:46.122] Forwarding server listening on port 45421
[15:46:46.122] Waiting for ssh tunnel to be ready
[15:46:46.123] [Forwarding server port 45421] Got connection 0
[15:46:46.123] Tunneled port 44707 to local port 45421
[15:46:46.124] Resolved ""ssh-remote+7b22686f73744e616d65223a2231302e3138332e38332e313433222c2275736572223a2261646d696e6973747261746f72227d"" to ""port 45421""
[15:46:46.124] Initizing new exec server for ssh-remote+7b22686f73744e616d65223a2231302e3138332e38332e313433222c2275736572223a2261646d696e6973747261746f72227d
[15:46:46.124] Resolving exec server at port 45421
[15:46:46.125] [Forwarding server port 45421] Got connection 1
[15:46:46.189] Exec server for ssh-remote+7b22686f73744e616d65223a2231302e3138332e38332e313433222c2275736572223a2261646d696e6973747261746f72227d created and cached
[15:46:46.189] Extensions to install: 
[15:46:46.194] ------




[15:46:46.195] No hints found in the recent session.
[15:46:46.239] [server] Checking /home/administrator/.vscode-server/cli/servers/Stable-2901c5ac6db8a986a5666c3af51ff804d05af0d4/log.txt and /home/administrator/.vscode-server/cli/servers/Stable-2901c5ac6db8a986a5666c3af51ff804d05af0d4/pid.txt for a running server...
[15:46:46.240] [server] Found running server (pid=27544)
[15:48:18.842] ------




[15:48:18.842] SSH Resolver called for ""ssh-remote+7b22686f73744e616d65223a2231302e3138332e38332e313433222c2275736572223a2261646d696e6973747261746f72227d"", attempt 4, (Reconnection)
[15:48:18.852] SSH Resolver called for host: administrator@10.183.83.143
[15:48:18.852] Setting up SSH remote ""10.183.83.143""
[15:48:18.854] Acquiring local install lock: /tmp/vscode-remote-ssh-bad318fe-install.lock
[15:48:18.855] Looking for existing server data file at /home/rohan/.config/Code/User/globalStorage/ms-vscode-remote.remote-ssh/vscode-ssh-host-bad318fe-2901c5ac6db8a986a5666c3af51ff804d05af0d4-0.120.0-es/data.json
[15:48:18.855] Found existing data file
[15:48:18.855] Found local server running: {""remoteListeningOn"":{""port"":44707},""osReleaseId"":""ubuntu"",""arch"":""x86_64"",""sshAuthSock"":"""",""display"":"""",""tmpDir"":""/run/user/1000"",""platform"":""linux"",""execServerToken"":""04e7ec87-49f1-459b-8180-57f9e8fdb0a7"",""pid"":16520,""ipcHandlePath"":""/run/user/1000/vscode-ssh-askpass-59063124351de2b9a8df35212033904262220c27.sock"",""socksPort"":44199,""startupTime"":1751636799044}
[15:48:18.855] Running server is stale. Ignoring
[15:48:18.856] Using commit id ""2901c5ac6db8a986a5666c3af51ff804d05af0d4"" and quality ""stable"" for server
[15:48:18.856] Extensions to install: 
[15:48:18.860] Install and start server if needed
[15:48:18.866] askpass server listening on /run/user/1000/vscode-ssh-askpass-21fac6eba247ec3641f1b1fd82050bdc51b9357b.sock
[15:48:18.866] Spawning local server with {""serverId"":3,""ipcHandlePath"":""/run/user/1000/vscode-ssh-askpass-8a368c94831b4df4cbcdd0428f0021d9705d23ea.sock"",""sshCommand"":""ssh"",""sshArgs"":[""-v"",""-T"",""-D"",""39699"",""-o"",""ConnectTimeout=15"",""administrator@10.183.83.143""],""serverDataFolderName"":"".vscode-server"",""dataFilePath"":""/home/rohan/.config/Code/User/globalStorage/ms-vscode-remote.remote-ssh/vscode-ssh-host-bad318fe-2901c5ac6db8a986a5666c3af51ff804d05af0d4-0.120.0-es/data.json""}
[15:48:18.866] Local server env: {""SSH_AUTH_SOCK"":""/run/user/1000/keyring/ssh"",""SHELL"":""/bin/bash"",""DISPLAY"":"":1"",""ELECTRON_RUN_AS_NODE"":""1"",""SSH_ASKPASS"":""/home/rohan/.vscode/extensions/ms-vscode-remote.remote-ssh-0.120.0/out/local-server/askpass.sh"",""VSCODE_SSH_ASKPASS_NODE"":""/snap/code/198/usr/share/code/code"",""VSCODE_SSH_ASKPASS_EXTRA_ARGS"":"""",""VSCODE_SSH_ASKPASS_MAIN"":""/home/rohan/.vscode/extensions/ms-vscode-remote.remote-ssh-0.120.0/out/askpass-main.js"",""VSCODE_SSH_ASKPASS_HANDLE"":""/run/user/1000/vscode-ssh-askpass-21fac6eba247ec3641f1b1fd82050bdc51b9357b.sock""}
[15:48:18.872] Spawned 16632
[15:48:18.873] Using connect timeout of 17 seconds
[15:48:18.944] > local-server-3> Running ssh connection command: ssh -v -T -D 39699 -o ConnectTimeout=15 administrator@10.183.83.143
[15:48:18.947] > local-server-3> Spawned ssh, pid=16640
[15:48:18.950] stderr> OpenSSH_8.9p1 Ubuntu-3ubuntu0.13, OpenSSL 3.0.2 15 Mar 2022
[15:48:19.218] stderr> debug1: Server host key: ssh-ed25519 SHA256:Xq98QaXHere8LgkOgGjt+Gcx/AT9zmAL0ewNSUEFv7A
[15:48:19.409] Got askpass request: {""request"":"" administrator@10.183.83.143's password: ""}
[15:48:19.410] Showing password prompt
[15:48:19.410] Listening for interwindow password on /run/user/1000/vscode-ssh-askpass-2fcc2a0eb111cd7c679c49b27c118d234d6a2234.sock
[15:48:19.410] Writing password prompt to globalState
[15:48:22.582] Exec server for ssh-remote+7b22686f73744e616d65223a2231302e3138332e38332e313433222c2275736572223a2261646d696e6973747261746f72227d closed (gracefully)
[15:48:22.583] > local-server-2> Timed out
[15:48:22.589] Local server exit: 0
[15:48:23.368] Got password response
[15:48:23.368] Interactor gave response: *********
[15:48:23.368] Cleaning up other-window auth server
[15:48:23.369] Using connect timeout of 17 seconds
[15:48:28.099] stderr> Authenticated to 10.183.83.143 ([10.183.83.143]:22) using ""password"".
[15:48:28.283] >             .-/+oossssoo+/-.
>         `:+ssssssssssssssssss+:`
>       -+ssssssssssssssssssyyssss+-
>     .ossssssssssssssssssdMMMNysssso.
>    /ssssssssssshdmmNNmmyNMMMMhssssss/
>   +ssssssssshmydMMMMMMMNddddyssssssss+
>  /sssssssshNMMMyhhyyyyhmNMMMNhssssssss/
> .ssssssssdMMMNhsssssssssshNMMMdssssssss.
> +sssshhhyNMMNyssssssssssssyNMMMysssssss+
> ossyNMMMNyMMhsssssssssssssshmmmhssssssso
> ossyNMMMNyMMhsssssssssssssshmmmhssssssso
> +sssshhhyNMMNyssssssssssssyNMMMysssssss+
> .ssssssssdMMMNhsssssssssshNMMMdssssssss.
>  /sssssssshNMMMyhhyyyyhdNMMMNhssssssss/
>   +sssssssssdmydMMMMMMMMddddyssssssss+
>    /ssssssssssshdmNNNNmyNMMMMhssssss/
>     .ossssssssssssssssssdMMMNysssso.
>       -+sssssssssssssssssyyyssss+-
>         `:+ssssssssssssssssss+:`
>             .-/+oossssoo+/-.
> root@jackal-j100-0814 
> --------------------- 
> OS: Ubuntu 22.04.5 LTS x86_64 
> Host: PH13FEI 
> Kernel: 6.8.0-52-generic 
> Uptime: 1 hour, 47 mins 
> Packages: 3520 (dpkg), 9 (snap) 
> Shell: bash 5.1.16 
> Terminal: run-parts 
> CPU: Intel i7-9700TE (8) @ 3.800GHz 
> GPU: Intel CoffeeLake-S GT2 [UHD Graphics 630] 
> Memory: 5123MiB / 15821MiB 
> 
>                         
>                         
> 
> 
> 
> 
> 
> 
> -------------------------------------------------------------------------------
>        _  _  _  _  ____   __  ____  ____  _  _   __  ____                      
>       ( \/ )( \/ )(  _ \ /  \(_  _)/ ___)/ )( \ /  \(  _ \                     
>       / \/ \ )  /  ) _ ((  O ) )(  \___ \) __ ((  O )) __/                     
>       \_)(_/(__/  (____/ \__/ (__) (____/\_)(_/ \__/(__)                       
>                                                                                
> -------------------------------------------------------------------------------
> Robot Service Status for clearpath-robot: Active 
> Robot Service Status for clearpath-platform: Active 
> -------------------------------------------------------------------------------
> Robot Service Status for mbs-fixposition: Active 
> Robot Service Status for mbs-robosense: Active 
> Robot Service Status for mbs-realsense-front: Active 
> -------------------------------------------------------------------------------
> Jackal IP:           192.168.131.1                                  
> Pswd:                mybotshop                                      
> SSH:                 ssh -XC administrator@192.168.131.1            
> -------------------------------------------------------------------------------
> Jackal Drivers                                                                 
> -------------------------------------------------------------------------------
> Service status:      sudo service {service_name} status             
>                                                                                
> Navi 2D SLAM:        ros2 launch jkl_nav2 slam.launch.py            
> Navi Map:            ros2 launch jkl_nav2 map_navi.launch.py        
> Navi Odom:           ros2 launch jkl_nav2 odom_navi.launch.py       
> Visualization:       ros2 launch jkl_viz view_robot.launch.py       
>                                                                                
> Rebuild package:     colcon build --symlink-install                 
> -------------------------------------------------------------------------------
[15:48:28.294] > ready: 50a22447ce8a
[15:48:28.311] > Linux 6.8.0-52-generic #53~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Jan 15 19:18:46 UTC 2
[15:48:28.311] Platform: linux
[15:48:28.318] > /bin/bash
[15:48:28.319] Parent Shell: bash
[15:48:28.319] Parent Shell pid: 16632
[15:48:28.320] Waiting for subshell to start
[15:48:28.331] > 39928
[15:48:28.331] stdout -> '39928'
[15:48:28.331] sub-process detected
[15:48:28.356] > 50a22447ce8a: running
> Script executing under PID: 39928
[15:48:28.372] > Found existing installation at /home/administrator/.vscode-server...
> Starting VS Code CLI...
> Removing old logfile at /home/administrator/.vscode-server/.cli.2901c5ac6db8a986a5666c3af51ff804d05af0d4.log
[15:48:28.376] > Spawned remote CLI: 39946
[15:48:28.379] > Waiting for server log...
[15:48:28.413] > Waiting for server log...
[15:48:28.447] > 50a22447ce8a: start
> listeningOn==127.0.0.1:36871==
> osReleaseId==ubuntu==
> arch==x86_64==
> vscodeArch==x64==
> bitness==64==
> tmpDir==/run/user/1000==
> platform==linux==
> unpackResult====
> didLocalDownload==0==
> downloadTime====
> installTime====
> serverStartTime==75==
> execServerToken==a111a111-1aaa-111a-aa1a-1aa1a111aaaa==
> platformDownloadPath==cli-alpine-x64==
> SSH_AUTH_SOCK====
> DISPLAY====
> 50a22447ce8a: end
[15:48:28.448] Received install output: 
listeningOn==127.0.0.1:36871==
osReleaseId==ubuntu==
arch==x86_64==
vscodeArch==x64==
bitness==64==
tmpDir==/run/user/1000==
platform==linux==
unpackResult====
didLocalDownload==0==
downloadTime====
installTime====
serverStartTime==75==
execServerToken==a111a111-1aaa-111a-aa1a-1aa1a111aaaa==
platformDownloadPath==cli-alpine-x64==
SSH_AUTH_SOCK====
DISPLAY====

[15:48:28.449] Remote server is listening on port 36871
[15:48:28.449] Parsed server configuration: {""serverConfiguration"":{""remoteListeningOn"":{""port"":36871},""osReleaseId"":""ubuntu"",""arch"":""x86_64"",""sshAuthSock"":"""",""display"":"""",""tmpDir"":""/run/user/1000"",""platform"":""linux"",""execServerToken"":""a111a111-1aaa-111a-aa1a-1aa1a111aaaa""},""serverStartTime"":75,""installUnpackCode"":""""}
[15:48:28.451] Persisting server connection details to /home/rohan/.config/Code/User/globalStorage/ms-vscode-remote.remote-ssh/vscode-ssh-host-bad318fe-2901c5ac6db8a986a5666c3af51ff804d05af0d4-0.120.0-es/data.json
[15:48:28.456] Starting forwarding server. local port 40533 -> socksPort 39699 -> remotePort 36871
[15:48:28.456] Forwarding server listening on port 40533
[15:48:28.457] Waiting for ssh tunnel to be ready
[15:48:28.457] [Forwarding server port 40533] Got connection 0
[15:48:28.458] Tunneled port 36871 to local port 40533
[15:48:28.458] Resolved ""ssh-remote+7b22686f73744e616d65223a2231302e3138332e38332e313433222c2275736572223a2261646d696e6973747261746f72227d"" to ""port 40533""
[15:48:28.459] Initizing new exec server for ssh-remote+7b22686f73744e616d65223a2231302e3138332e38332e313433222c2275736572223a2261646d696e6973747261746f72227d
[15:48:28.459] Resolving exec server at port 40533
[15:48:28.460] [Forwarding server port 40533] Got connection 1
[15:48:28.528] Exec server for ssh-remote+7b22686f73744e616d65223a2231302e3138332e38332e313433222c2275736572223a2261646d696e6973747261746f72227d created and cached
[15:48:28.529] Extensions to install: 
[15:48:28.535] ------




[15:48:28.537] No hints found in the recent session.
[15:48:28.566] [server] Checking /home/administrator/.vscode-server/cli/servers/Stable-2901c5ac6db8a986a5666c3af51ff804d05af0d4/log.txt and /home/administrator/.vscode-server/cli/servers/Stable-2901c5ac6db8a986a5666c3af51ff804d05af0d4/pid.txt for a running server...
[15:48:28.574] [server] Found running server (pid=27544)
[15:50:19.743] ------




[15:50:19.743] SSH Resolver called for ""ssh-remote+7b22686f73744e616d65223a2231302e3138332e38332e313433222c2275736572223a2261646d696e6973747261746f72227d"", attempt 5, (Reconnection)
[15:50:19.754] SSH Resolver called for host: administrator@10.183.83.143
[15:50:19.755] Setting up SSH remote ""10.183.83.143""
[15:50:19.757] Acquiring local install lock: /tmp/vscode-remote-ssh-bad318fe-install.lock
[15:50:19.758] Looking for existing server data file at /home/rohan/.config/Code/User/globalStorage/ms-vscode-remote.remote-ssh/vscode-ssh-host-bad318fe-2901c5ac6db8a986a5666c3af51ff804d05af0d4-0.120.0-es/data.json
[15:50:19.759] Found existing data file
[15:50:19.759] Found local server running: {""remoteListeningOn"":{""port"":36871},""osReleaseId"":""ubuntu"",""arch"":""x86_64"",""sshAuthSock"":"""",""display"":"""",""tmpDir"":""/run/user/1000"",""platform"":""linux"",""execServerToken"":""d390b523-7ffb-475a-ba4f-1cb0d136aaef"",""pid"":16632,""ipcHandlePath"":""/run/user/1000/vscode-ssh-askpass-8a368c94831b4df4cbcdd0428f0021d9705d23ea.sock"",""socksPort"":39699,""startupTime"":1751636898873}
[15:50:19.759] Running server is stale. Ignoring
[15:50:19.759] Using commit id ""2901c5ac6db8a986a5666c3af51ff804d05af0d4"" and quality ""stable"" for server
[15:50:19.760] Extensions to install: 
[15:50:19.763] Install and start server if needed
[15:50:19.770] askpass server listening on /run/user/1000/vscode-ssh-askpass-da80875de8717acaa51a62a05276f15c6adf9444.sock
[15:50:19.770] Spawning local server with {""serverId"":4,""ipcHandlePath"":""/run/user/1000/vscode-ssh-askpass-12dbb575bc72744313197dba7bbb54a9351655ed.sock"",""sshCommand"":""ssh"",""sshArgs"":[""-v"",""-T"",""-D"",""34617"",""-o"",""ConnectTimeout=15"",""administrator@10.183.83.143""],""serverDataFolderName"":"".vscode-server"",""dataFilePath"":""/home/rohan/.config/Code/User/globalStorage/ms-vscode-remote.remote-ssh/vscode-ssh-host-bad318fe-2901c5ac6db8a986a5666c3af51ff804d05af0d4-0.120.0-es/data.json""}
[15:50:19.770] Local server env: {""SSH_AUTH_SOCK"":""/run/user/1000/keyring/ssh"",""SHELL"":""/bin/bash"",""DISPLAY"":"":1"",""ELECTRON_RUN_AS_NODE"":""1"",""SSH_ASKPASS"":""/home/rohan/.vscode/extensions/ms-vscode-remote.remote-ssh-0.120.0/out/local-server/askpass.sh"",""VSCODE_SSH_ASKPASS_NODE"":""/snap/code/198/usr/share/code/code"",""VSCODE_SSH_ASKPASS_EXTRA_ARGS"":"""",""VSCODE_SSH_ASKPASS_MAIN"":""/home/rohan/.vscode/extensions/ms-vscode-remote.remote-ssh-0.120.0/out/askpass-main.js"",""VSCODE_SSH_ASKPASS_HANDLE"":""/run/user/1000/vscode-ssh-askpass-da80875de8717acaa51a62a05276f15c6adf9444.sock""}
[15:50:19.777] Spawned 16696
[15:50:19.778] Using connect timeout of 17 seconds
[15:50:19.854] > local-server-4> Running ssh connection command: ssh -v -T -D 34617 -o ConnectTimeout=15 administrator@10.183.83.143
[15:50:19.858] > local-server-4> Spawned ssh, pid=16704
[15:50:19.861] stderr> OpenSSH_8.9p1 Ubuntu-3ubuntu0.13, OpenSSL 3.0.2 15 Mar 2022
[15:50:23.051] stderr> debug1: Server host key: ssh-ed25519 SHA256:Xq98QaXHere8LgkOgGjt+Gcx/AT9zmAL0ewNSUEFv7A
[15:50:23.245] Got askpass request: {""request"":"" administrator@10.183.83.143's password: ""}
[15:50:23.245] Showing password prompt
[15:50:23.246] Listening for interwindow password on /run/user/1000/vscode-ssh-askpass-afe803ae979a8b6993e6c14a239768c5b98f32be.sock
[15:50:23.246] Writing password prompt to globalState
[15:50:24.427] > local-server-3> Timed out
[15:50:24.427] Exec server for ssh-remote+7b22686f73744e616d65223a2231302e3138332e38332e313433222c2275736572223a2261646d696e6973747261746f72227d closed (gracefully)
[15:50:24.432] Local server exit: 0
[15:50:30.487] Got password response
[15:50:30.487] Interactor gave response: *********
[15:50:30.488] Cleaning up other-window auth server
[15:50:30.488] Using connect timeout of 17 seconds
[15:50:47.491] Terminating local server
[15:50:47.493] Resolver error: Error: Connecting with SSH timed out
	at y.Timeout (/home/rohan/.vscode/extensions/ms-vscode-remote.remote-ssh-0.120.0/out/extension.js:2:744895)
	at Timeout._onTimeout (/home/rohan/.vscode/extensions/ms-vscode-remote.remote-ssh-0.120.0/out/extension.js:2:805797)
	at listOnTimeout (node:internal/timers:588:17)
	at process.processTimers (node:internal/timers:523:7)
[15:50:47.503] ------




[15:50:47.506]  ---------- [Session Summary] ----------- 
[15:50:47.506] [Timeout]: Error: Timeout (Connecting with SSH timed out)
[15:50:47.507]  ---------------------------------------- 
[15:50:47.510] Local server exit: 15
[15:50:52.592] ------




[15:50:52.592] SSH Resolver called for ""ssh-remote+7b22686f73744e616d65223a2231302e3138332e38332e313433222c2275736572223a2261646d696e6973747261746f72227d"", attempt 6, (Reconnection)
[15:50:52.600] SSH Resolver called for host: administrator@10.183.83.143
[15:50:52.600] Setting up SSH remote ""10.183.83.143""
[15:50:52.602] Acquiring local install lock: /tmp/vscode-remote-ssh-bad318fe-install.lock
[15:50:52.602] Looking for existing server data file at /home/rohan/.config/Code/User/globalStorage/ms-vscode-remote.remote-ssh/vscode-ssh-host-bad318fe-2901c5ac6db8a986a5666c3af51ff804d05af0d4-0.120.0-es/data.json
[15:50:52.603] No existing data file
[15:50:52.603] Using commit id ""2901c5ac6db8a986a5666c3af51ff804d05af0d4"" and quality ""stable"" for server
[15:50:52.603] Extensions to install: 
[15:50:52.607] Install and start server if needed
[15:50:52.612] askpass server listening on /run/user/1000/vscode-ssh-askpass-81556c014e7bdc6dcf1f135d7b3ea4db782a3a0e.sock
[15:50:52.612] Spawning local server with {""serverId"":5,""ipcHandlePath"":""/run/user/1000/vscode-ssh-askpass-ae395e8b852d61b718296f47485b63741fe2ac6a.sock"",""sshCommand"":""ssh"",""sshArgs"":[""-v"",""-T"",""-D"",""43689"",""-o"",""ConnectTimeout=15"",""administrator@10.183.83.143""],""serverDataFolderName"":"".vscode-server"",""dataFilePath"":""/home/rohan/.config/Code/User/globalStorage/ms-vscode-remote.remote-ssh/vscode-ssh-host-bad318fe-2901c5ac6db8a986a5666c3af51ff804d05af0d4-0.120.0-es/data.json""}
[15:50:52.613] Local server env: {""SSH_AUTH_SOCK"":""/run/user/1000/keyring/ssh"",""SHELL"":""/bin/bash"",""DISPLAY"":"":1"",""ELECTRON_RUN_AS_NODE"":""1"",""SSH_ASKPASS"":""/home/rohan/.vscode/extensions/ms-vscode-remote.remote-ssh-0.120.0/out/local-server/askpass.sh"",""VSCODE_SSH_ASKPASS_NODE"":""/snap/code/198/usr/share/code/code"",""VSCODE_SSH_ASKPASS_EXTRA_ARGS"":"""",""VSCODE_SSH_ASKPASS_MAIN"":""/home/rohan/.vscode/extensions/ms-vscode-remote.remote-ssh-0.120.0/out/askpass-main.js"",""VSCODE_SSH_ASKPASS_HANDLE"":""/run/user/1000/vscode-ssh-askpass-81556c014e7bdc6dcf1f135d7b3ea4db782a3a0e.sock""}
[15:50:52.622] Spawned 16718
[15:50:52.622] Using connect timeout of 17 seconds
[15:50:52.711] > local-server-5> Running ssh connection command: ssh -v -T -D 43689 -o ConnectTimeout=15 administrator@10.183.83.143
[15:50:52.715] > local-server-5> Spawned ssh, pid=16726
[15:50:52.719] stderr> OpenSSH_8.9p1 Ubuntu-3ubuntu0.13, OpenSSL 3.0.2 15 Mar 2022
[15:50:52.956] stderr> debug1: Server host key: ssh-ed25519 SHA256:Xq98QaXHere8LgkOgGjt+Gcx/AT9zmAL0ewNSUEFv7A
[15:50:53.123] Got askpass request: {""request"":"" administrator@10.183.83.143's password: ""}
[15:50:53.123] Showing password prompt
[15:50:53.124] Listening for interwindow password on /run/user/1000/vscode-ssh-askpass-19086181ba7feb1645224912c83fa914d698b01d.sock
[15:50:53.124] Writing password prompt to globalState
[15:50:57.189] Got password response
[15:50:57.190] Interactor gave response: *********
[15:50:57.190] Cleaning up other-window auth server
[15:50:57.191] Using connect timeout of 17 seconds
[15:51:14.192] Terminating local server
[15:51:14.195] Resolver error: Error: Connecting with SSH timed out
	at y.Timeout (/home/rohan/.vscode/extensions/ms-vscode-remote.remote-ssh-0.120.0/out/extension.js:2:744895)
	at Timeout._onTimeout (/home/rohan/.vscode/extensions/ms-vscode-remote.remote-ssh-0.120.0/out/extension.js:2:805797)
	at listOnTimeout (node:internal/timers:588:17)
	at process.processTimers (node:internal/timers:523:7)
[15:51:14.200] ------




[15:51:14.202]  ---------- [Session Summary] ----------- 
[15:51:14.202] [Timeout]: Error: Timeout (Connecting with SSH timed out)
[15:51:14.202] [Timeout]: Error: Timeout (Connecting with SSH timed out)
[15:51:14.202]  ---------------------------------------- 
[15:51:14.205] Local server exit: 15
[15:51:17.956] ------




[15:51:17.956] SSH Resolver called for ""ssh-remote+7b22686f73744e616d65223a2231302e3138332e38332e313433222c2275736572223a2261646d696e6973747261746f72227d"", attempt 7, (Reconnection)
[15:51:17.965] SSH Resolver called for host: administrator@10.183.83.143
[15:51:17.965] Setting up SSH remote ""10.183.83.143""
[15:51:17.966] Acquiring local install lock: /tmp/vscode-remote-ssh-bad318fe-install.lock
[15:51:17.967] Looking for existing server data file at /home/rohan/.config/Code/User/globalStorage/ms-vscode-remote.remote-ssh/vscode-ssh-host-bad318fe-2901c5ac6db8a986a5666c3af51ff804d05af0d4-0.120.0-es/data.json
[15:51:17.967] No existing data file
[15:51:17.968] Using commit id ""2901c5ac6db8a986a5666c3af51ff804d05af0d4"" and quality ""stable"" for server
[15:51:17.968] Extensions to install: 
[15:51:17.970] Install and start server if needed
[15:51:17.975] askpass server listening on /run/user/1000/vscode-ssh-askpass-f3c2af3a6edfb1550cba64ccf4dd27ae396350b9.sock
[15:51:17.976] Spawning local server with {""serverId"":6,""ipcHandlePath"":""/run/user/1000/vscode-ssh-askpass-6d7d05b1e6091e8bb2f35035d8d9acc8cd177526.sock"",""sshCommand"":""ssh"",""sshArgs"":[""-v"",""-T"",""-D"",""45793"",""-o"",""ConnectTimeout=15"",""administrator@10.183.83.143""],""serverDataFolderName"":"".vscode-server"",""dataFilePath"":""/home/rohan/.config/Code/User/globalStorage/ms-vscode-remote.remote-ssh/vscode-ssh-host-bad318fe-2901c5ac6db8a986a5666c3af51ff804d05af0d4-0.120.0-es/data.json""}
[15:51:17.976] Local server env: {""SSH_AUTH_SOCK"":""/run/user/1000/keyring/ssh"",""SHELL"":""/bin/bash"",""DISPLAY"":"":1"",""ELECTRON_RUN_AS_NODE"":""1"",""SSH_ASKPASS"":""/home/rohan/.vscode/extensions/ms-vscode-remote.remote-ssh-0.120.0/out/local-server/askpass.sh"",""VSCODE_SSH_ASKPASS_NODE"":""/snap/code/198/usr/share/code/code"",""VSCODE_SSH_ASKPASS_EXTRA_ARGS"":"""",""VSCODE_SSH_ASKPASS_MAIN"":""/home/rohan/.vscode/extensions/ms-vscode-remote.remote-ssh-0.120.0/out/askpass-main.js"",""VSCODE_SSH_ASKPASS_HANDLE"":""/run/user/1000/vscode-ssh-askpass-f3c2af3a6edfb1550cba64ccf4dd27ae396350b9.sock""}
[15:51:17.982] Spawned 16751
[15:51:17.982] Using connect timeout of 17 seconds
[15:51:18.053] > local-server-6> Running ssh connection command: ssh -v -T -D 45793 -o ConnectTimeout=15 administrator@10.183.83.143
[15:51:18.057] > local-server-6> Spawned ssh, pid=16759
[15:51:18.060] stderr> OpenSSH_8.9p1 Ubuntu-3ubuntu0.13, OpenSSL 3.0.2 15 Mar 2022
[15:51:19.133] stderr> debug1: Server host key: ssh-ed25519 SHA256:Xq98QaXHere8LgkOgGjt+Gcx/AT9zmAL0ewNSUEFv7A
[15:51:19.299] Got askpass request: {""request"":"" administrator@10.183.83.143's password: ""}
[15:51:19.299] Showing password prompt
[15:51:19.299] Listening for interwindow password on /run/user/1000/vscode-ssh-askpass-b1850516c1fcb48eb64a17475faebfcbcdd0de68.sock
[15:51:19.299] Writing password prompt to globalState
[15:51:22.848] Got password response
[15:51:22.848] Interactor gave response: *********
[15:51:22.848] Cleaning up other-window auth server
[15:51:22.849] Using connect timeout of 17 seconds
[15:51:24.256] stderr> Authenticated to 10.183.83.143 ([10.183.83.143]:22) using ""password"".
[15:51:24.519] >             .-/+oossssoo+/-.
>         `:+ssssssssssssssssss+:`
>       -+ssssssssssssssssssyyssss+-
>     .ossssssssssssssssssdMMMNysssso.
>    /ssssssssssshdmmNNmmyNMMMMhssssss/
>   +ssssssssshmydMMMMMMMNddddyssssssss+
>  /sssssssshNMMMyhhyyyyhmNMMMNhssssssss/
> .ssssssssdMMMNhsssssssssshNMMMdssssssss.
> +sssshhhyNMMNyssssssssssssyNMMMysssssss+
> ossyNMMMNyMMhsssssssssssssshmmmhssssssso
> ossyNMMMNyMMhsssssssssssssshmmmhssssssso
> +sssshhhyNMMNyssssssssssssyNMMMysssssss+
> .ssssssssdMMMNhsssssssssshNMMMdssssssss.
>  /sssssssshNMMMyhhyyyyhdNMMMNhssssssss/
>   +sssssssssdmydMMMMMMMMddddyssssssss+
>    /ssssssssssshdmNNNNmyNMMMMhssssss/
>     .ossssssssssssssssssdMMMNysssso.
>       -+sssssssssssssssssyyyssss+-
>         `:+ssssssssssssssssss+:`
>             .-/+oossssoo+/-.
> root@jackal-j100-0814 
> --------------------- 
> OS: Ubuntu 22.04.5 LTS x86_64 
> Host: PH13FEI 
> Kernel: 6.8.0-52-generic 
> Uptime: 1 hour, 50 mins 
> Packages: 3520 (dpkg), 9 (snap) 
> Shell: bash 5.1.16 
> Terminal: run-parts 
> CPU: Intel i7-9700TE (8) @ 3.800GHz 
> GPU: Intel CoffeeLake-S GT2 [UHD Graphics 630] 
> Memory: 5152MiB / 15821MiB 
> 
>                         
>                         
> 
> 
> 
> 
> 
> 
> -------------------------------------------------------------------------------
>        _  _  _  _  ____   __  ____  ____  _  _   __  ____                      
>       ( \/ )( \/ )(  _ \ /  \(_  _)/ ___)/ )( \ /  \(  _ \                     
>       / \/ \ )  /  ) _ ((  O ) )(  \___ \) __ ((  O )) __/                     
>       \_)(_/(__/  (____/ \__/ (__) (____/\_)(_/ \__/(__)                       
>                                                                                
> -------------------------------------------------------------------------------
> Robot Service Status for clearpath-robot: Active 
> Robot Service Status for clearpath-platform: Active 
> -------------------------------------------------------------------------------
> Robot Service Status for mbs-fixposition: Active 
> Robot Service Status for mbs-robosense: Active 
> Robot Service Status for mbs-realsense-front: Active 
> -------------------------------------------------------------------------------
> Jackal IP:           192.168.131.1                                  
> Pswd:                mybotshop                                      
> SSH:                 ssh -XC administrator@192.168.131.1            
> -------------------------------------------------------------------------------
> Jackal Drivers                                                                 
> -------------------------------------------------------------------------------
> Service status:      sudo service {service_name} status             
>                                                                                
> Navi 2D SLAM:        ros2 launch jkl_nav2 slam.launch.py            
> Navi Map:            ros2 launch jkl_nav2 map_navi.launch.py        
> Navi Odom:           ros2 launch jkl_nav2 odom_navi.launch.py       
> Visualization:       ros2 launch jkl_viz view_robot.launch.py       
>                                                                                
> Rebuild package:     colcon build --symlink-install                 
> -------------------------------------------------------------------------------
[15:51:24.530] > ready: 5246cefb9334
[15:51:24.545] > Linux 6.8.0-52-generic #53~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Jan 15 19:18:46 UTC 2
[15:51:24.546] Platform: linux
[15:51:24.555] > /bin/bash
[15:51:24.556] Parent Shell: bash
[15:51:24.556] Parent Shell pid: 16751
[15:51:24.557] Waiting for subshell to start
[15:51:24.568] > 41283
[15:51:24.568] stdout -> '41283'
[15:51:24.568] sub-process detected
[15:51:24.584] > 5246cefb9334: running
> Script executing under PID: 41283
[15:51:24.599] > Found existing installation at /home/administrator/.vscode-server...
> Starting VS Code CLI...
[15:51:24.602] > Removing old logfile at /home/administrator/.vscode-server/.cli.2901c5ac6db8a986a5666c3af51ff804d05af0d4.log
[15:51:24.626] > Spawned remote CLI: 41301
[15:51:24.629] > Waiting for server log...
[15:51:24.662] > Waiting for server log...
[15:51:24.696] > 5246cefb9334: start
> listeningOn==127.0.0.1:33625==
> osReleaseId==ubuntu==
> arch==x86_64==
> vscodeArch==x64==
> bitness==64==
> tmpDir==/run/user/1000==
> platform==linux==
> unpackResult====
> didLocalDownload==0==
> downloadTime====
> installTime====
> serverStartTime==94==
> execServerToken==aaa11111-a11a-1aa1-111a-aa11aaa1a111==
> platformDownloadPath==cli-alpine-x64==
> SSH_AUTH_SOCK====
> DISPLAY====
> 5246cefb9334: end
[15:51:24.697] Received install output: 
listeningOn==127.0.0.1:33625==
osReleaseId==ubuntu==
arch==x86_64==
vscodeArch==x64==
bitness==64==
tmpDir==/run/user/1000==
platform==linux==
unpackResult====
didLocalDownload==0==
downloadTime====
installTime====
serverStartTime==94==
execServerToken==aaa11111-a11a-1aa1-111a-aa11aaa1a111==
platformDownloadPath==cli-alpine-x64==
SSH_AUTH_SOCK====
DISPLAY====

[15:51:24.697] Remote server is listening on port 33625
[15:51:24.697] Parsed server configuration: {""serverConfiguration"":{""remoteListeningOn"":{""port"":33625},""osReleaseId"":""ubuntu"",""arch"":""x86_64"",""sshAuthSock"":"""",""display"":"""",""tmpDir"":""/run/user/1000"",""platform"":""linux"",""execServerToken"":""aaa11111-a11a-1aa1-111a-aa11aaa1a111""},""serverStartTime"":94,""installUnpackCode"":""""}
[15:51:24.698] Persisting server connection details to /home/rohan/.config/Code/User/globalStorage/ms-vscode-remote.remote-ssh/vscode-ssh-host-bad318fe-2901c5ac6db8a986a5666c3af51ff804d05af0d4-0.120.0-es/data.json
[15:51:24.702] Starting forwarding server. local port 36851 -> socksPort 45793 -> remotePort 33625
[15:51:24.702] Forwarding server listening on port 36851
[15:51:24.703] Waiting for ssh tunnel to be ready
[15:51:24.704] [Forwarding server port 36851] Got connection 0
[15:51:24.705] Tunneled port 33625 to local port 36851
[15:51:24.705] Resolved ""ssh-remote+7b22686f73744e616d65223a2231302e3138332e38332e313433222c2275736572223a2261646d696e6973747261746f72227d"" to ""port 36851""
[15:51:24.706] Initizing new exec server for ssh-remote+7b22686f73744e616d65223a2231302e3138332e38332e313433222c2275736572223a2261646d696e6973747261746f72227d
[15:51:24.706] Resolving exec server at port 36851
[15:51:24.707] [Forwarding server port 36851] Got connection 1
[15:51:24.776] Exec server for ssh-remote+7b22686f73744e616d65223a2231302e3138332e38332e313433222c2275736572223a2261646d696e6973747261746f72227d created and cached
[15:51:24.776] Extensions to install: 
[15:51:24.779] ------




[15:51:24.781]  ---------- [Session Summary] ----------- 
[15:51:24.781] [Timeout]: Error: Timeout (Connecting with SSH timed out)
[15:51:24.781] [Timeout]: Error: Timeout (Connecting with SSH timed out)
[15:51:24.781]  ---------------------------------------- 
[15:51:24.802] [server] Checking /home/administrator/.vscode-server/cli/servers/Stable-2901c5ac6db8a986a5666c3af51ff804d05af0d4/log.txt and /home/administrator/.vscode-server/cli/servers/Stable-2901c5ac6db8a986a5666c3af51ff804d05af0d4/pid.txt for a running server...
[15:51:24.810] [server] Found running server (pid=27544)

```

Github ssh remote connection too unstable, when I work on terminal its stable, but VS Cose is making life miserabe 
",2
Crash,"
Type: <b>Bug</b>

When using AI completion and agents in the application, the system lags and freezes with 89–100 files open, even though I’m using a 128 GB M3 Max device, and it deletes all the work on its own. I work with 80 GB files in databases without any issues, but when using Copilot through VSCode, I experience severe crashes and data loss.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M3 Max (16 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|4, 4, 3|
|Memory (System)|128.00GB (83.78GB free)|
|Process Argv|--crash-reporter-id 19ea9a5e-9267-4e4a-9698-d550689211b4|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805cf:30301675
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
2i9eh265:30646982
962ge761:30959799
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dwnewjupytercf:31046870
pythonrstrctxt:31112756
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31303018
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
d2249276:31341129
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930

```

</details>

<!-- generated by issue reporter -->",2
Option to remember model used in each chat session,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
Since some people might not want this, I suggest it as an option that enables the Chat to remember what model was used inside each session. I use multiple models sometimes, one for each chat, and I have to remember what model I used when I go back from a chat that used a different one.

I also open this issue so I can work on an implementation myself, and to know what others think about it.",0
Code editor and terminal are blank,"
Type: <b>Bug</b>

The code is present but is not visible at all, I do not know what is cauing it. The side bar towards the right which shows the whole code miniaturized is working, but the code itself is not visible. Classes, methods are visible if you scroll, but the actual code is not.

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|13th Gen Intel(R) Core(TM) i5-13420H (12 x 2611)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.71GB (4.04GB free)|
|Process Argv|--crash-reporter-id 6afd1520-99ee-4c92-880a-c3048dd714f4|
|Screen Reader|no|
|VM|0%|
</details>Extensions: none<details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492:30256859
pythonvspyt551:31249599
vscod805:30301674
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
pythontbext0:30879054
cppperfnew:31000557
dwnewjupyter:31046869
pythonrstrctxt:31112756
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7cf:31303019
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
9d2cg352:31339597
0g0a1943:31341127
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931

```

</details>

<!-- generated by issue reporter -->",2
Try with different compilation commands without reason,"
Type: <b>Bug</b>

The build succeed but it tries another compilation command....

Extension version: 0.28.5
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M1 Pro (10 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|4, 5, 5|
|Memory (System)|32.00GB (0.52GB free)|
|Process Argv|. --crash-reporter-id 521f2518-c97a-4a14-a48f-ff6b9a269c1d|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492:30256859
vscod805cf:30301675
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dwnewjupyter:31046869
pythonrstrctxt:31112756
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31303018
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
0g0a1943:31341127
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
b6b4d950:31327385
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930

```

</details>

<!-- generated by issue reporter -->",2
Remove chat.defaultmode experiment and allow to control if free users get agent by default,"We should:
1) Remove the chat.defaultMode experiment, but make it such
2) That we can control via ExP if Free users get Agent as default mode or not. By default they should not get it

The defaultModel we can keep for now - and use it to slowly move everyone to auto in August.
",1
Multiple reintentos,"
Type: <b>Bug</b>

Constantemente pide reintentos.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 3 3200U with Radeon Vega Mobile Gfx   (4 x 2595)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|6.95GB (2.47GB free)|
|Process Argv|--crash-reporter-id 3d8cf75c-64a0-4b9c-8653-d993bb0ff474|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492:30256859
pythonvspyt551:31249599
vscod805:30301674
binariesv615:30325510
vscaac:30438847
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dwnewjupyter:31046869
pythonrstrctxt:31112756
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31303018
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
convertlamdat:31329272
d2249276:31341129
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931

```

</details>

<!-- generated by issue reporter -->",2
"Missing Code suggestion in ""Ask"" mode while using Claude Sonnet","
Type: <b>Bug</b>

The code suggestions does not show any code overall. It shows the border where the code suggestions should be inside, but it has no readable code inside. 

Extension version: 0.28.3
VS Code version: Code 1.101.0 (Universal) (dfaf44141ea9deb3b4096f7cd6d24e00c147a4b1, 2025-06-11T15:00:50.123Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M3 Pro (12 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|3, 4, 3|
|Memory (System)|18.00GB (1.27GB free)|
|Process Argv|--crash-reporter-id 766c381a-482f-43a1-937c-0b8143fa170b|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805cf:30301675
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
a9j8j154:30646983
962ge761:30959799
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dwnewjupyter:31046869
pythonrstrctxt:31112756
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31325931
996jf627:31283433
pythonrdcb7cf:31303019
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
convertlamdaf:31329270
d784b465:31332230
i851h500:31332225
useunpkg:31333564
nesew1to4:31330636
id45c886:31333049
gh_pad_aa_t:31335104
jd8b9481:31335120
testaa123cf:31335227

```

</details>

<!-- generated by issue reporter -->",2
window not responding repeatly,"
Type: <b>Bug</b>

﻿
workbench.desktop.main.js:sourcemap:35  WARN Via 'product.json#extensionEnabledApiProposals' extension 'ms-vsliveshare.vsliveshare' wants API proposal 'notebookCellExecutionState' but that proposal DOES NOT EXIST. Likely, the proposal has been finalized (check 'vscode.d.ts') or was abandoned.
workbench.desktop.main.js:sourcemap:35  WARN Via 'product.json#extensionEnabledApiProposals' extension 'ms-python.gather' wants API proposal 'notebookCellExecutionState' but that proposal DOES NOT EXIST. Likely, the proposal has been finalized (check 'vscode.d.ts') or was abandoned.
workbench.desktop.main.js:sourcemap:35  WARN Via 'product.json#extensionEnabledApiProposals' extension 'ms-python.vscode-pylance' wants API proposal 'mcpConfigurationProvider' but that proposal DOES NOT EXIST. Likely, the proposal has been finalized (check 'vscode.d.ts') or was abandoned.
workbench.desktop.main.js:sourcemap:35  INFO Started local extension host with pid 18232.
workbench.desktop.main.js:sourcemap:35  INFO ComputeTargetPlatform: win32-x64
workbench.desktop.main.js:sourcemap:35   ERR Extension 'ms-python.vscode-pylance' wants API proposal 'mcpConfigurationProvider' but that proposal DOES NOT EXIST. Likely, the proposal has been finalized (check 'vscode.d.ts') or was abandoned.
workbench.desktop.main.js:sourcemap:35   ERR [Extension Host] (node:18232) [DEP0040] DeprecationWarning: The `punycode` module is deprecated. Please use a userland alternative instead.
(Use `Code --trace-deprecation ...` to show where the warning was created)
workbench.desktop.main.js:sourcemap:1281 [Extension Host] (node:18232) [DEP0040] DeprecationWarning: The `punycode` module is deprecated. Please use a userland alternative instead.
(Use `Code --trace-deprecation ...` to show where the warning was created)
workbench.desktop.main.js:sourcemap:35  INFO Settings Sync: Token updated for the account srinithshrajkumar
workbench.desktop.main.js:sourcemap:3602 An iframe which has both allow-scripts and allow-same-origin for its sandbox attribute can escape its sandboxing.
workbench.desktop.main.js:sourcemap:35  INFO Settings Sync: Account status changed from uninitialized to available
workbench.desktop.main.js:sourcemap:35  INFO [perf] Render performance baseline is 23ms
workbench.desktop.main.js:sourcemap:1281 [Extension Host] geo.data.viewer:activate(): activated! extPath: c:\Users\srini\.vscode\extensions\randomfractalsinc.geo-data-viewer-2.6.0
workbench.desktop.main.js:sourcemap:1281 [Extension Host] [FileSystemService] file:///c%3A/Users/srini/AppData/Roaming/Code/User/workspaceStorage/062f698…/GitHub.copilot-chat/workspace-chunks.json is a LARGE file (15MB > 5MB)
2
workbench.desktop.main.js:sourcemap:35  INFO Settings Sync: Token updated for the account srinithshrajkumar
index.html:1 Access to fetch at 'https://unpkg.com/' from origin 'vscode-webview://1mu162cqhhhibo59ml7og3ctdoihgfvtl85k2kgcmnksitt8fnhg' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource. If an opaque response serves your needs, set the request's mode to 'no-cors' to fetch the resource with CORS disabled.
unpkg.com/:1 
 Failed to load resource: net::ERR_FAILED
ipywidgetsKernel.js:17 Failed to access CDN https://unpkg.com/ after 0 attempt(s), TypeError: Failed to fetch
(anonymous)	@	ipywidgetsKernel.js:17

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i7-11390H @ 3.40GHz (8 x 2918)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.75GB (2.78GB free)|
|Process Argv|--crash-reporter-id 65af8bad-c25e-4aa0-8f9c-67ba99a6174a|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (8)</summary>

Extension|Author (truncated)|Version
---|---|---
copilot|Git|1.338.0
copilot-chat|Git|0.28.5
jupyter|ms-|2025.5.0
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.1.0
vscode-jupyter-cell-tags|ms-|0.1.9
vscode-jupyter-slideshow|ms-|0.1.6
geo-data-viewer|Ran|2.6.0


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805:30301674
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dwnewjupyter:31046869
pythonrstrctxt:31112756
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7cf:31303019
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
j6a0c355:31339598
0g0a1943:31341127
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931

```

</details>

<!-- generated by issue reporter -->",2
did not give me a trial !,"
Type: <b>Bug</b>

did not give me a trial !

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 5 5500GT with Radeon Graphics         (12 x 3593)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|13.89GB (6.84GB free)|
|Process Argv|--crash-reporter-id 543fcdea-b053-4f02-bd02-2a930c3c4bed|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492cf:30256860
vscod805:30301674
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
vscrpc:30673769
962ge761:30959799
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dwnewjupytercf:31046870
pythonrstrctxt:31112756
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31303018
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
d2249276:31341129
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931

```

</details>

<!-- generated by issue reporter -->",2
Problem with terminal shortcut,"
Type: <b>Bug</b>

when I click on ctrl+shift+`  it suppose to open new terminal but that is not the case there is a problem with it so please look upon it.

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:


<!-- generated by issue reporter -->",0
Installing kernel fails if my venv uses `uv venv`,"repro: 

1. run `uv venv` 
2. run `source .venv/bin/activate` 
3. create a notebook
4. pick this new venv as the one for notebook 
5. run a python cell `print('foo')` 
6. get dialog offering to install kernel and click on it
7. it fails because running `pip install ... ` is not allowed in uv venv, it should be `uv pip install ...` I think


Version: 1.102.0-insider
Commit: ac320d66c838f2bee42b650ec7409807e40dbd27
Date: 2025-07-04T07:03:36.959Z
Electron: 35.6.0
ElectronBuildId: 11847422
Chromium: 134.0.6998.205
Node.js: 22.15.1
V8: 13.4.114.21-electron.0
OS: Darwin arm64 24.5.0",0
dependency conflict in main branch,"npm ERR! ERESOLVE unable to resolve dependency tree
![Image](https://github.com/user-attachments/assets/9b76c2c9-7370-4738-8678-17a342c25e95)

devDependencies ""@types/react"" should be ""^18.0.0""",0
copilot can't see a bracket that is there,"
Type: <b>Bug</b>

Hello, I think it might be a bug. Copilot thinks there is no closing bracket on the line, but there is. The line is a cell in a table and it looks like this:  | [Create a charge (type: `UNSCHEDULED`)][create-charge-endpoint]    | 300 per minute   | per merchant           | 300 calls per minute per merchant serial number              |


It keeps tellng me to add the bracket:
[Create a charge (type: `UNSCHEDULED`)][create-charge-endpoint]

and the original code looks identical already:
[Create a charge (type: `UNSCHEDULED`)][create-charge-endpoint] 

so, it think it's blind, or else I am.


Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i7-1165G7 @ 2.80GHz (8 x 2803)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.73GB (3.64GB free)|
|Process Argv|. --crash-reporter-id de119a28-276b-4028-b2d2-7beb32d591fe|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805cf:30301675
binariesv615:30325510
vscaac:30438847
c4g48928:30535728
azure-dev_surveyone:30548225
2i9eh265:30646982
962ge761:30959799
2e7ec940:31000449
pythontbext0:30879054
cppperfnew:31000557
dwnewjupytercf:31046870
pythonrstrctxt:31112756
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7cf:31303019
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
0g0a1943:31341127
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
82j33506:31327384
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
bc3b0373:31337022

```

</details>

<!-- generated by issue reporter -->",2
Asking about `#codebase` on an ADO repo doesn't ask to authenticate,"From https://github.com/microsoft/vscode/issues/254010

1. Clone an ADO repo. Verify you need to authenticate to get access to the repo index.
2. In Chat, ask something about `#codebase`

🐛 Copilot doesn't ask to authenticate",0
She was completely sobataging me and my friends game  and a big thing is that she keep trying to go through my folders!!!!,"<img width=""702"" alt=""Image"" src=""https://github.com/user-attachments/assets/c188f42e-c372-4053-9409-ea9ed3761dbb"" />
We have written the needed data into your clipboard because it was too large to send. Please paste.",0
Bug,"
Type: <b>Bug</b>

no quiere generar codigo


Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|12th Gen Intel(R) Core(TM) i7-12700H (20 x 2688)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|7.68GB (1.10GB free)|
|Process Argv|--crash-reporter-id 65e9f903-a967-427e-90ab-dd4e5c009aec|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805cf:30301675
binariesv615:30325510
vscaat:30438848
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
pythontbext0:30879054
cppperfnew:31000557
dwnewjupytercf:31046870
pythonrstrctxt:31112756
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7cf:31303019
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
jhi8h917:31341130
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
63221493:31336333
yijiwantestdri0626-c:31336931
hi92a955:31337023

```

</details>

<!-- generated by issue reporter -->",2
The text is not displayed,"
Type: <b>Bug</b>

While the text is being generated, I can see it streaming, but once the generation is complete, everything disappears and turns into a mysterious blank space, making the response invisible

Extension version: 0.28.5
VS Code version: Code 1.101.1 (Universal) (18e3a1ec544e6907be1e944a94c496e302073435, 2025-06-18T13:35:12.605Z)
OS version: Darwin arm64 23.3.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M1 Pro (10 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|4, 50, 55|
|Memory (System)|32.00GB (0.09GB free)|
|Process Argv|--crash-reporter-id 6652203c-c9b4-4e96-aa4c-27b0e1bb3189|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805:30301674
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dwnewjupyter:31046869
pythonrstrctxt:31112756
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7cf:31303019
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
c4b42873:31341128
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930

```

</details>

<!-- generated by issue reporter -->",2
卡顿,"
Type: <b>Performance Issue</b>

一直返回失败，还在增加我的TOken，如何计算费用？

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) Ultra 5 225H (14 x 3686)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|31.50GB (16.22GB free)|
|Process Argv|C:\\Users\\19045\\OneDrive\\桌面\\袁总报价模型\\embeddingModel-v7.0-待委单历史最高最低.py --crash-reporter-id 22f9a191-406e-4ce8-8bd1-08bc73699908|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>Process Info</summary>

```
CPU %	Mem MB	   PID	Process
    0	   174	 16768	code
    0	    47	  3236	   utility-network-service
    0	   529	  7700	extension-host [1]
    0	   623	 17708	     electron-nodejs (bundle.js )
    0	    93	 24568	     ""D:\Microsoft VS Code\Code.exe"" ""d:\Microsoft VS Code\resources\app\extensions\json-language-features\server\dist\node\jsonServerMain"" --node-ipc --clientProcessId=7700
    0	    97	  8164	file-watcher [1]
    0	   738	  9796	window [1] (embeddingModel-v7.1-传入待委单查询.py - 袁总报价模型 - Visual Studio Code)
    0	   419	 12192	   gpu-process
    0	   147	 15392	shared-process
    0	    30	 16836	   crashpad-handler
    0	   114	 21360	pty-host
    0	     9	  4672	     conpty-agent
    0	    55	  6836	     C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""d:\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     9	  7340	     conpty-agent
    0	    54	 11692	     C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""d:\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     9	 12352	     conpty-agent
    0	    60	 13708	     C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""d:\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	    56	 14476	     C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""d:\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	    55	 15772	     C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -noexit -command ""try { . \""d:\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     9	 17904	     conpty-agent
    0	     9	 27108	     conpty-agent
```

</details>
<details>
<summary>Workspace Info</summary>

```
|  Window (embeddingModel-v7.1-传入待委单查询.py - 袁总报价模型 - Visual Studio Code)
|    Folder (袁总报价模型): 20 files
|      File types: py(9) xlsx(5) sqlite3(1) json(1) log(1) docx(1) txt(1)
|      Conf files:;
```

</details>
<details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805:30301674
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
vscrp:30673768
962ge761:30959799
2e7ec940:31000449
pythontbext0:30879054
cppperfnew:31000557
dwnewjupyter:31046869
pythonrstrctxt:31112756
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
5b33h341:31326279
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31303018
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
9d2cg352:31339597
convertlamdat:31329272
c4b42873:31341128
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
onetestforazureexp:31335613
6abeh943:31336334
yijiwantestdri0626-t:31336930

```

</details>

<!-- generated by issue reporter -->",2
Massive edit will crash copilot chat and reloads it,"Type: Bug

When copilot edits a large file ~4000 lines it crashes and reloads or gives option to click retry.

VS Code version: Code 1.101.0 (dfaf44141ea9deb3b4096f7cd6d24e00c147a4b1, 2025-06-11T15:00:50.123Z)
OS version: Windows_NT x64 10.0.26100
Modes:
Remote OS version: Linux x64 5.15.167.4-microsoft-standard-WSL2

![Image](https://github.com/user-attachments/assets/90ef4510-baf4-4a95-8242-25989593d49b)

Output: https://pastebin.com/EFWR8tdx",2
Folding hover is native and not custom,"<img width=""302"" height=""165"" alt=""Image"" src=""https://github.com/user-attachments/assets/b9a28bf9-c613-444c-b38a-549c7e88c80c"" />",0
Unable to scroll up the accessibility buffer (alt+f2),"
Type: <b>Bug</b>

in accessibility buffer (alt+f2), as a screen reader NVDA user, I couldn't scroll up the buffer. Meaning, if the buffer contains a lot of lines, I could not scroll up to read the earlier history. for example, there are 1000 lines, it shows the last 700 lines, but I want to go up to line 1, which doesn't appear in the buffer but sure exist in the output screen. 

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-7200U CPU @ 2.50GHz (4 x 2712)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|7.90GB (1.95GB free)|
|Process Argv|--crash-reporter-id 3ad94506-bcb0-41d7-8972-76d97217466a|
|Screen Reader|yes|
|VM|0%|
</details><details><summary>Extensions (11)</summary>

Extension|Author (truncated)|Version
---|---|---
vscode-mysql|for|0.5.0
copilot|Git|1.338.0
copilot-chat|Git|0.28.5
vsc-python-indent|Kev|1.21.0
vscode-containers|ms-|2.0.3
vscode-docker|ms-|2.0.0
debugpy|ms-|2025.8.0
python|ms-|2025.8.0
vscode-pylance|ms-|2025.6.2
remote-containers|ms-|0.417.0
remote-wsl|ms-|0.99.0


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805:30301674
binariesv615:30325510
vscaat:30438848
c4g48928:30535728
azure-dev_surveyone:30548225
a9j8j154:30646983
962ge761:30959799
2e7ec940:31000449
pythontbext0:30879054
cppperfnew:31000557
dwnewjupyter:31046869
pythonrstrctxt:31112756
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31303018
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
9d2cg352:31339597
d2249276:31341129
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
82j33506:31327384
nes-diff-11:31337487
onetestforazureexp:31335613
6abeh943:31336334
yijiwantestdri0626-t:31336930
hi92a955:31337023

```

</details>

<!-- generated by issue reporter -->",2
First click on tile does nothing,"Steps to Reproduce:

1.  fresh first start
2. open a folder
3. notice chat opens
4. click any tile

=> 🐛 focus goes into the input box but no text is inserted


",0
issue #110141 - TextEdit.setEndOfLine applies an edit and invalidates redo stack even when no change is made,"https://monacotools.visualstudio.com/Monaco/_build/results?buildId=347598&view=logs&j=672276a2-8d3a-5fab-615d-090c51352f92&t=da81c1dc-000b-5af1-5a41-3a48a97d898e&s=27eddb93-7805-576c-c80f-37b2176e40f7

```
5) vscode API - workspace
       issue #110141 - TextEdit.setEndOfLine applies an edit and invalidates redo stack even when no change is made:

      AssertionError [ERR_ASSERTION] [ERR_ASSERTION]: Expected values to be strictly equal:
+ actual - expected

+ 'hello2\nworld'
- 'hello\nworld'
        ^

      + expected - actual

      -hello2
      +hello
       world
      
      at Context.<anonymous> (D:\a\_work\1\s\extensions\vscode-api-tests\src\singlefolder-tests\workspace.test.ts:1210:11)
```",1
issue #107739 - Redo of rename Java Class name has no effect,"https://monacotools.visualstudio.com/Monaco/_build/results?buildId=347598&view=logs&j=672276a2-8d3a-5fab-615d-090c51352f92&t=da81c1dc-000b-5af1-5a41-3a48a97d898e&s=27eddb93-7805-576c-c80f-37b2176e40f7

```
 4) vscode API - workspace
       issue #107739 - Redo of rename Java Class name has no effect:
```",0
TextEditor.edit can control undo/redo stack 1,"https://monacotools.visualstudio.com/Monaco/_build/results?buildId=347598&view=logs&j=672276a2-8d3a-5fab-615d-090c51352f92&t=da81c1dc-000b-5af1-5a41-3a48a97d898e&s=27eddb93-7805-576c-c80f-37b2176e40f7

```
  2) vscode API - editors
       TextEditor.edit can control undo/redo stack 1:

      AssertionError [ERR_ASSERTION] [ERR_ASSERTION]: Expected values to be strictly equal:
+ actual - expected

+ 'hELLO world!'
- 'Hello world!'

      + expected - actual

      -hELLO world!
      +Hello world!
      
      at D:\a\_work\1\s\extensions\vscode-api-tests\src\singlefolder-tests\editor.test.ts:195:11

```",1
Integration test failure: breakpoints are available before accessing debug extension API,"https://monacotools.visualstudio.com/Monaco/_build/results?buildId=347598&view=logs&j=672276a2-8d3a-5fab-615d-090c51352f92&t=da81c1dc-000b-5af1-5a41-3a48a97d898e&s=27eddb93-7805-576c-c80f-37b2176e40f7

```
  1) vscode API - debug
       breakpoints are available before accessing debug extension API:

      AssertionError [ERR_ASSERTION] [ERR_ASSERTION]: Expected values to be strictly equal:

0 !== 1

      + expected - actual

      -0
      +1
      
      at Context.<anonymous> (D:\a\_work\1\s\extensions\vscode-api-tests\src\singlefolder-tests\debug.test.ts:21:10)
```",1
"The model unexpectedly did not return a response, which may indicate a service issue. Please report a bug.","
Type: <b>Bug</b>

The model unexpectedly did not return a response, which may indicate a service issue. Please report a bug.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M4 (10 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|1, 2, 3|
|Memory (System)|16.00GB (0.06GB free)|
|Process Argv|--crash-reporter-id d257b3b6-fde8-4c18-b407-357d66eb806e|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805:30301674
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
pythontbext0:30879054
cppperfnew:31000557
dwnewjupyter:31046869
pythonrstrctxt:31112756
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31303018
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
convertlamdaf:31329270
0g0a1943:31341127
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931

```

</details>

<!-- generated by issue reporter -->",2
"Hanged, if not used for few days",COpilot chat is responding and is too slow.,1
Git tags for Copilot Chat contains no details,"For tags like this https://github.com/microsoft/vscode-copilot-chat/releases/tag/v0.29.2025070302 there's no details so unclear what's been added/changed.

Ideally there would be proper GitHub releases.

<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.102.0-insider
- OS Version: Windows 10 22H2

Steps to Reproduce:

1. Navigate to https://github.com/microsoft/vscode-copilot-chat/releases/tag/v0.29.2025070302
2. Be disappointed in the lack of details
",1
Cannot Access Chat -copilot -Chat is not working shwing access Denied message,"
Type: <b>Bug</b>

Cannot Access Chat -copilot -Chat is not working shwing access Denied message 

VS Code version: Code 1.93.1 (38c31bc77e0dd6ae88a4e9cc93428cc27a56ba40, 2024-09-11T17:20:05.685Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i7-11850H @ 2.50GHz (16 x 2496)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|31.67GB (15.18GB free)|
|Process Argv||
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (23)</summary>

Extension|Author (truncated)|Version
---|---|---
cpp-helper|ami|0.3.4
syntax-highlighting|bos|1.2.0
githistory|don|0.6.20
vscode-highlight|fab|1.9.0
copilot|Git|1.250.1260
copilot-chat|Git|0.20.3
vscode-ltex-plus|lte|15.2.0
fixlink|mah|0.0.2
vscode-catch2-test-adapter|mat|4.12.1
git-graph|mhu|1.30.0
debugpy|ms-|2024.12.0
python|ms-|2024.14.1
vscode-pylance|ms-|2024.10.1
jupyter|ms-|2024.8.1
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.0.19
vscode-jupyter-cell-tags|ms-|0.1.9
vscode-jupyter-slideshow|ms-|0.1.6
cpptools|ms-|1.22.10
vsliveshare|ms-|1.0.5941
rtc-source-control|sha|1.0.0
esbonio|swy|0.11.0
markdown-all-in-one|yzh|3.6.2

(1 theme extensions excluded)

</details>
<!-- generated by issue reporter -->",1
Falta de informacion clara.,"
Type: <b>Performance Issue</b>

Al iniciar el uso de Copilot no se me informó de las limittaciones (cuota mensual de mensajes de chat) ni que debería pasarme a Copilot Pro si queria continual usandolo o esperar otro mes... Teniendo en cuenta lo mal que funciona, los bucles de errores que comete y que supuestamente esta aprendiendo y mejorando con la interaccion pues... me parece un timo total, mil chats para corregir aspectos nímios de código unicamente visual, francamente, raya la estafa (si hubiese pagado por el producto lo seria claro) y me hace sonreir cuando sugieren que pague por el Pro con un producto tan defectuoso.
Un saludo.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:


<!-- generated by issue reporter -->",1
The code was not produced,"
Type: <b>Bug</b>

The file was created but code was not written. Looks like it has done it internally somehow but the code not get pasted in editor

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 5 7520U with Radeon Graphics          (8 x 2795)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.24GB (2.03GB free)|
|Process Argv|--crash-reporter-id f41632d8-b536-4162-943f-960e7194e7a8|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551:31249599
vscod805:30301674
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
a9j8j154:30646983
962ge761:30959799
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dwnewjupyter:31046869
pythonrstrctxt:31112756
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
563cc122:31326278
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31303018
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
d784b465:31338105
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
testaa123:31335226
6abeh943:31336334
yijiwantestdri0626-t:31336930

```

</details>

<!-- generated by issue reporter -->",2
"Copilot chat history should support full‑text search across all chat messages, not just conversation titles","Right now, when we search our Copilot chat history, it only looks at the conversation titles. If the keyword we are searching for is mentioned in the actual chat but not in the title, nothing comes up. This makes it hard to find old chats, code snippets, or discussions—especially when the auto generated title is generic.

**Steps to reproduce:**
Open chat history.
Search for a word that only appears in the chat messages.
Notice that it doesn’t find the conversation.

**Actual**
Only the title is searched; chat messages are ignored.

**Expectation**
The search should look through all messages in each chat, so you can find anything you discussed before.




",0
keeps generating duplicate code as if it doesn't see lines off screen,"
Type: <b>Bug</b>

when making minor edits, it sometimes just duplicates code

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 9 7940HS w/ Radeon 780M Graphics      (16 x 3992)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|61.81GB (32.43GB free)|
|Process Argv|--crash-reporter-id 776cf056-c1f0-44c5-984a-cf11a988dfb4|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805:30301674
binariesv615:30325510
vscaat:30438848
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dwnewjupytercf:31046870
pythonrstrctxt:31112756
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7cf:31303019
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
d2249276:31341129
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
4f60g487:31327383
nes-diff-11:31337487
testaa123:31335226
onetestforazureexpcf:31335614
6abeh943:31336334
yijiwantestdri0626-c:31336931

```

</details>

<!-- generated by issue reporter -->",2
The access token won't be refreshed after log out and log in using the microsoft account,"Is it expected, if yes how to force refresh

- VS Code Version: 
- 1.101.0
- OS Version: 

Steps to Reproduce:

1.  get an access token for a resource, using the below code  in an extension
```
 await authentication.getSession(getConfiguredAuthProviderId(), scopes, {
                createIfNone: false,
            });
```
3.  logout and log in using the same microsoft account
4. get access token again, compare it with the previous access token",0
Chat Import Blank/Inconsistent,"Anytime I import a chat, I cannot see the contents unless I scroll, and the contents is hidden again. It's not a very big chat. Sometimes I can't reach the bottom again and when entering new info, I can't see what I enter or any responses. If I restart, it goes back to normal.

![Image](https://github.com/user-attachments/assets/60fa03a7-8272-4b2a-8518-c0406a407bbc)

![Image](https://github.com/user-attachments/assets/7ea201cb-0015-41c9-8b21-a11aaec9e32e)",0
sorry no response returned !,"
Type: <b>Bug</b>

step 1 . 
use Gemini 2.5 pro in agent mode until you see this error



Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:
Remote OS version: Linux arm64 5.15.0-1081-oracle

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 5 5625U with Radeon Graphics          (12 x 2296)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|31.31GB (22.54GB free)|
|Process Argv|--crash-reporter-id 8ed51e7b-0105-4ccb-998e-4391d40321cb|
|Screen Reader|no|
|VM|0%|

|Item|Value|
|---|---|
|Remote|Dev Container: KittyAI Dev Container @ orc-4cpu.lak.nz|
|OS|Linux arm64 5.15.0-1081-oracle|
|CPUs|Neoverse-N1 (4 x 0)|
|Memory (System)|23.43GB (17.60GB free)|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492cf:30256860
vscod805:30301674
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dwnewjupytercf:31046870
pythonrstrctxt:31112756
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7cf:31303019
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
jjjhb125:31275178
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
convertlamdat:31329272
jhi8h917:31341130
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
testaa123:31335226
6abeh943:31336334
yijiwantestdri0626-c:31336931

```

</details>

<!-- generated by issue reporter -->",2
Default Loop Auth Server callback url http://127.0.0.1/* isn't supported by Microsoft Entra Application,"https://github.com/microsoft/vscode/blob/d9f1bf8fed9ccec3f04ddb7b8644f16656f613fb/src/vs/workbench/api/node/loopbackServer.ts#L148-L154

According to https://learn.microsoft.com/en-us/entra/identity-platform/reply-url?utm_source=chatgpt.com#prefer-127001-over-localhost, the default http://127.0.0.1:* loop auth callback url isn't supported by Microsoft Entra. Would like to understand more about current implementation, and evaluate, if possible, change to http://localhost:* by default.",1
git error causes coding agent to fail,"> it then popped up a terminal to do it but I don't have my linux machine set up right so it returns an error ""unable to auto-detect email address"" which is fatal
> 
> It popped up the terminal but didn't let you perform the commit?  That terminal should wait until the dirty changes have been committed - giving you as long as needed to perform the commit manually.
> 
> That said this also sounds like a bit of a edge case - it assumes the user's dev environment is not set up to use git (yet they're working in a version controlled project) 

 _Originally posted by @joshspicer in [#253445](https://github.com/microsoft/vscode/issues/253445#issuecomment-3033912840)_",2
Overly apologetic agent,"While verifying  https://github.com/microsoft/vscode/issues/251640 I found that the agent when running against Claude 3.5 is overly apologetic. 

<img width=""2962"" height=""2744"" alt=""Image"" src=""https://github.com/user-attachments/assets/bc67e7fe-1c8a-49cd-89a9-8c1cd7cafea5"" />",0
Reduce 4.1 usage's of insert_edit_into_file,"In swebench we saw 4.1 loves apply_patch and rarely uses insert_edit_into_file. However, in the real world, our telemetry indicates that it still calls it more than we would like. Some ideas:

1. See if we'd be able to remove insert_edit from 4.1's capabilities
2. Or, keep it in with instructions like ""Never use this tool unless told otherwise"" and have apply_patch in its output say ""You may use insert_edit for this edit"" only if apply_patch fails twice in a row.

cc @isidorn @roblourens ",0
"Installing mcp server is stuck on ""Installing""","- Find an mcp server
- Click Install
- It says ""installing"" forever but _does_ install the server

I don't see any interesting errors anywhere. The server is installed but I was waiting on the UI

<img width=""505"" height=""387"" alt=""Image"" src=""https://github.com/user-attachments/assets/305b43f4-2549-492c-8e35-a28c96b27c4c"" />",1
"""resource"" added to Authorization Request causing AADSTS901002: The 'resource' request parameter is not supported.","https://github.com/microsoft/vscode/blob/25ee562cba29e4256a144af0b1f3af9b2f8f7f04/src/vs/workbench/api/node/extHostAuthentication.ts#L88-L103

Based on https://datatracker.ietf.org/doc/html/rfc6749#section-4.1.1, resource is not required. Plus, resource is not allowed for Microsoft Entra OAuth 2.0 flow. Would like to understand the logic behind current implementation, and check if it is possible to remove resource in the generated auth url.",0
"Sorry, no response was returned.","
Type: <b>Bug</b>

I know there are other open tickets, but this error has been persisting for 2 days and I can't work at all. Wasted time for nothing!

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i9-14900K (32 x 3187)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|63.77GB (35.25GB free)|
|Process Argv|C:\\xampp\\htdocs\\zecesrl --crash-reporter-id 83bcb891-7f5d-4d0b-8fc0-00226b09474c|
|Screen Reader|yes|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805:30301674
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
pythontbext0:30879054
cppperfnew:31000557
dwnewjupytercf:31046870
pythonrstrctxt:31112756
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
563cc122:31326278
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31303018
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
j6a0c355:31339598
jhi8h917:31341130
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930
bc3b0373:31337022

```

</details>

<!-- generated by issue reporter -->",2
Copilot Chat Agent doesn't modify the files when a project on a remote linux system is opened on VSCode on windows host.,"
Type: <b>Bug</b>

Using VSCode SSH extension, open a folder in a remote linux system. Use Github Copilot agent mode to generate code. 

Observed Behavior: 

Generated code is in a file on the Windows host system, and not on the remote system. 

Expected Behavior:

Generated code is in the files in the remote system, not the windows system. 

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.22621
Modes:
Remote OS version: Linux x64 5.15.0-25-generic

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 7 PRO 7840U w/ Radeon 780M Graphics   (16 x 3294)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|61.78GB (43.64GB free)|
|Process Argv|--crash-reporter-id 7a9736c0-a692-4db4-bda6-8fb25e7d9c62|
|Screen Reader|no|
|VM|0%|

|Item|Value|
|---|---|
|Remote|SSH: xxx-yyy.com|
|OS|Linux x64 5.15.0-25-generic|
|CPUs|AMD EPYC 7313 16-Core Processor (32 x 1499)|
|Memory (System)|125.66GB (115.57GB free)|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805:30301674
binariesv615:30325510
vscaat:30438848
c4g48928:30535728
azure-dev_surveyonecf:30548226
962ge761:30959799
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dwnewjupytercf:31046870
pythonrstrctxt:31112756
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31303018
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
jhi8h917:31341130
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
nes-diff-11:31337487
nes-set-on:31340697
6abeh943:31336334
yijiwantestdri0626-t:31336930

```

</details>

<!-- generated by issue reporter -->",2
Extension Signature Verification Failed,"
Type: <b>Bug</b>

Steps to reproduce:
Attempt updating any extension on a managed remote server

Result: Error: Extension Signature Verification Failed, Unknown Error

I suspect this issues is unique to me, but the error message ""Unknown Error"" makes it extremely unhelpful in determining what is going on. Advice is appreciated.

Please include following log `F1 > Open View... > Shared` below.

2025-07-03 16:30:17.962 [info] [Window] Auto updating outdated extensions. eamodio.gitlens, ms-python.black-formatter, ms-python.debugpy, ms-python.isort, ms-python.python, ms-python.vscode-pylance, ms-toolsai.jupyter, ms-vscode.cpptools
2025-07-03 16:30:20.426 [error] [Window] SignatureVerificationInternal: Signature verification failed with 'UnknownError' error.
    at Al.Bb (file:///.../.vscode-server/cli/servers/Stable-2901c5ac6db8a986a5666c3af51ff804d05af0d4/server/out/server-main.js:55:16748)
    at async Al.Ab (file:///.../.vscode-server/cli/servers/Stable-2901c5ac6db8a986a5666c3af51ff804d05af0d4/server/out/server-main.js:55:15219)
2025-07-03 16:30:20.762 [error] [Window] Signature verification failed with 'UnknownError' error.: SignatureVerificationInternal: Signature verification failed with 'UnknownError' error.
    at Al.Bb (file:///.../.vscode-server/cli/servers/Stable-2901c5ac6db8a986a5666c3af51ff804d05af0d4/server/out/server-main.js:55:16748)
    at async Al.Ab (file:///.../.vscode-server/cli/servers/Stable-2901c5ac6db8a986a5666c3af51ff804d05af0d4/server/out/server-main.js:55:15219)
2025-07-03 16:30:20.763 [error] [Window] Signature verification failed with 'UnknownError' error.: SignatureVerificationInternal: Signature verification failed with 'UnknownError' error.
    at Al.Bb (file:///.../.vscode-server/cli/servers/Stable-2901c5ac6db8a986a5666c3af51ff804d05af0d4/server/out/server-main.js:55:16748)
    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)
    at async Al.Ab (file:///.../.vscode-server/cli/servers/Stable-2901c5ac6db8a986a5666c3af51ff804d05af0d4/server/out/server-main.js:55:15219)
2025-07-03 16:30:20.765 [error] [Window] Signature verification failed with 'UnknownError' error.: SignatureVerificationInternal: Signature verification failed with 'UnknownError' error.
    at Al.Bb (file:///.../.vscode-server/cli/servers/Stable-2901c5ac6db8a986a5666c3af51ff804d05af0d4/server/out/server-main.js:55:16748)
    at async Al.Ab (file:///.../.vscode-server/cli/servers/Stable-2901c5ac6db8a986a5666c3af51ff804d05af0d4/server/out/server-main.js:55:15219)


VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19045
Modes:
Remote OS version: Linux x64 4.18.0-553.54.1.el8_10.x86_64

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-7200U CPU @ 2.50GHz (4 x 2712)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|7.82GB (1.53GB free)|
|Process Argv|--crash-reporter-id 1d54058e-2d7f-4e70-bebc-7944b7ec9486|
|Screen Reader|no|
|VM|0%|

|Item|Value|
|---|---|
|Remote|SSH: Quartz|
|OS|Linux x64 4.18.0-553.54.1.el8_10.x86_64|
|CPUs|AMD EPYC 7742 64-Core Processor (128 x 1675)|
|Memory (System)|503.71GB (328.59GB free)|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492:30256859
pythonvspyt551cf:31249601
vscod805:30301674
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyonecf:30548226
962ge761:30959799
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dwnewjupyter:31046869
pythonrstrctxt:31112756
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31303018
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
jjjhb125:31275178
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
j6a0c355:31339598
jhi8h917:31341130
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
nes-diff-11:31337487
nes-set-on:31340697
6abeh943:31336334
yijiwantestdri0626-c:31336931
hi92a955:31337023

```

</details>

<!-- generated by issue reporter -->",2
Requesting command to send the next key stroke to terminal.,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
Similar like `workbench.action.terminal.sendSequence`, but it will just send whatever user's next keystroke to terminal. Basically functionality of TTY setting `lnext`, which when a character set as `lnext` is pressed, then treat the next as literal character, by passing TTY processing.

Something like `workbench.action.terminal.sendNextAsLiteral` or something like that. If the command is executed (most likely via the associated keybinding) the whatever the next user keystroke is always sent to terminal, bypassing VS Code keyboard binding.

So if a user want to bind, say `Ctrl+C` to a command, the user does not need to explicitly also set the corresponding keybinding to send the `Ctrl+C` to terminal.

Currently, if user choose to override a keyboard binding, say any key stroke that generate a C0 characters, for each override, he should also create an associated keybinding so he is able to send the overridden C0 character to terminal. This is burden some.

If there is a command that mimic TTY `lnext` behavior, then user can just set a single keybinding only for it, then allow him to override any [Shift+]Ctrl+<key> and/or [Shift+]Alt+<key> as he wish.",0
VSCode Çok sık,"
Type: <b>Bug</b>

son zamanlarda çok sık alıyorum bu hatayı


Extension version: 0.29.2025070301
VS Code version: Code - Insiders 1.102.0-insider (Universal) (6307b2e05830e59099f10a3ab650a9ef56e9b9e9, 2025-07-03T12:48:59.863Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M2 Pro (12 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|2, 2, 3|
|Memory (System)|16.00GB (0.38GB free)|
|Process Argv|--crash-reporter-id 2c1f885a-29d2-40e1-b980-dedd8b814e55|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551:31249597
vscod805cf:30301675
c4g48928:30535728
962ge761:30841072
dsvsc014:30777825
dsvsc015:30821418
2e7ec940:31000449
pythontbext0:30879054
cppperfnew:30980852
dwnewjupyter:31046869
pythonrstrctxt:31093868
nativeloc1:31118317
e80f6927:31120813
dwcopilot:31158714
5b1c1929:31184661
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
g012b348:31231168
pythoneinst12:31251391
bgtreat:31252123
c7cif404:31309980
pythonpulldiag:31287486
6gi0g917:31259950
996jf627:31264550
pythonrdcb7:31268811
usemplatestapi:31297334
e6194696:31317039
747dc170:31275146
pythonfullctx:31296836
6518g693:31302842
generatesymbolt:31280541
convertfstringf:31280702
j97ad248:31306657
usemarketplace:31336439
0g1h6703:31329154
4f60g487:31327383
nes-emitfast-1:31333560
replacestringexc:31340153
6abeh943:31336334
nes-conv-11:31337514
0927b901:31340060
jbdfg126:31340538

```

</details>

<!-- generated by issue reporter -->",2
ERR_NETWORK_CHANGED,"
Type: <b>Performance Issue</b>

Sorry, your request failed. Please try again. Request id: d96c4880-51af-4942-98d1-68f251d4102a

Reason: Please check your firewall rules and network connection then try again. Error Code: net::ERR_NETWORK_CHANGED.



Extension version: 0.29.2025070301
VS Code version: Code - Insiders 1.102.0-insider (Universal) (6307b2e05830e59099f10a3ab650a9ef56e9b9e9, 2025-07-03T12:48:59.863Z)
OS version: Darwin arm64 24.6.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M4 (10 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|2, 2, 2|
|Memory (System)|16.00GB (0.14GB free)|
|Process Argv|--crash-reporter-id b2f3d381-567c-4fe9-bea8-a026fa042570|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>Process Info</summary>

```
CPU %	Mem MB	   PID	Process
    9	30962247438	  5338	code-insiders
    6	16888498603	  5341	   gpu-process
    0	8444249301	  5342	   utility-network-service
    2	90071992547	  5344	window [1] (page.tsx — StudentLoanCenter)
    2	28147497671	  5359	shared-process
    0	     0	  6613	     /bin/ps -ax -o pid=,ppid=,pcpu=,pmem=,command=
    0	11258999068	  5397	pty-host
    0	     0	  5401	     /bin/zsh -il
    0	     0	  5411	     /bin/zsh -il
    0	     0	  5621	     /bin/zsh -il
    0	2814749767	  6203	       npm run dev
    0	2814749767	  6219	         node /Users/joselopez/Documents/LiaisonLabs/Repos/Web Dev/Landing Pages/StudentLoanCenter/node_modules/.bin/next dev
    0	104145741383	  6220	           next-server (v14.2.11)
    0	75998243712	  5398	extension-host [1]
    0	14073748836	  5459	     electron-nodejs (tsserver.js )
    0	81627743246	  5460	     electron-nodejs (tsserver.js )
    0	8444249301	  5485	       electron-nodejs (typingsInstaller.js typesMap.js )
    0	11258999068	  5464	     mdx-language-server
    0	8444249301	  5467	     /Applications/Visual Studio Code - Insiders.app/Contents/Frameworks/Code - Insiders Helper (Plugin).app/Contents/MacOS/Code - Insiders Helper (Plugin) /Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/extensions/json-language-features/server/dist/node/jsonServerMain --node-ipc --clientProcessId=5398
    0	11258999068	  5493	     electron-nodejs (serverMain.js )
    0	19703248370	  5522	     electron-nodejs (tailwindServer.js )
    0	11258999068	  5399	file-watcher [1]
    0	16888498603	  5400	extension-host [1]
```

</details>
<details>
<summary>Workspace Info</summary>

```
|  Window (page.tsx — StudentLoanCenter)
|    Folder (StudentLoanCenter): 221 files
|      File types: jpg(33) tsx(28) svg(26) gz(20) ts(17) json(16) js(10)
|                  png(6) old(3) md(3)
|      Conf files: package.json(3) tasks.json(1) tsconfig.json(1);
```

</details>
<details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551:31249597
vscod805cf:30301675
vscaat:30438846
c4g48928:30535728
2i9eh265:30646982
962ge761:30841072
dsvsc014:30777825
dsvsc015:30821418
h48ei257:31000450
pythontbext0:30879054
cppperfnew:30980852
dwnewjupytercf:31046870
pythonrstrctxt:31093868
nativeloc1:31118317
e80f6927:31120813
dwcopilot:31158714
5b1c1929:31184661
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
g012b348:31231168
pythoneinst12:31251391
bgtreat:31252123
c7cif404:31309980
pythonpulldiag:31287486
6gi0g917:31259950
996jf627:31264550
pythonrdcb7:31268811
usemplatestapi:31297334
e6194696:31317039
747dc170:31275146
pythonfullctx:31296836
6518g693:31302842
generatesymbolt:31280541
convertfstringf:31280702
b99bg931:31306656
usemarketplace:31336439
0g1h6703:31329154
82j33506:31327384
nes-emitfast-1:31333560
replacestringexc:31340153
63221493:31336333
0927b901:31340060
gji67723:31340537

```

</details>

<!-- generated by issue reporter -->",2
Extremely poor performance from Copilot models,"
Type: <b>Bug</b>

Initially, I was blown away by the Gemini 2.5 Pro model. Lately however, it has been not only unhelpful, but has wasted so much of my time. I keep checking back in to see if whatever has made it so bad has been resolved, but at this point I'm not sure they're even aware of how bad it is, and I can't complain if I'm not contributing to the feedback they need. I've experienced similar issues with nearly every model, and keep returning to this one because at its' best, it was awesome.

Many of these agents (2.5 Pro, Claude Sonnet 4, even GPT 4.1) have been unable to stay in scope or provide real links to documentation. I cannot ask them to do a thing without them making random changes throughout my codebase that are entirely out of scope and require cleanup. They will justify their changes using links that do not exist - not that have been moved, literally made up links. They recommend made up classes from libraries. And agent and edit mode are now entirely indistinguishable. My first time using Agent mode, it handled so much context in one query - I remember being blown away at how much it was able to do without asking if I wanted to continue. Then over time, it would ask if I wanted it to continue more and more, and after less and less time spent solving the problem. Now, it will just not complete the entire task I asked it to do and exit as though it finished, with no indication that it's aware it has more work to do.


Another issue that's made these difficult to use is they will spend so much time telling you what they're going to do that when they get to the point they might start doing something, the response either errors out, or it just completely stops as though it thinks it did anything at all. This happens even when I ask it explicitly to be concise. This issue is particularly common with Gemini 2.5 Pro, meaning from premium models users are getting worse than non-premium results (i.e., doing anything at all), but at a higher price.

I've been a big supporter of Copilot since it Released, and I drive use within my company as a member of our Enablement team. With my experiences from the past month, I can no longer in good conscience do this as I have been significantly less efficient using Copilot. Whenever I've used it recently, the process has consistently been 1) waste ~1hr or more trying to get copilot to do something, 2) remove 75-100% of what it did (heavily weighted toward 100%), and start from scratch.

I know this may not be the proper place to submit this kind of feedback, but I don't know where to do that. I'm hoping you can help me get this to where it needs to go. I clicked ""Report an issue"" against a response to generate this because I can't multi-select why I'm thumbs-downing a response and often it's because it didn't follow instructions, didn't complete my request, and provided inaccurate information. Seriously, like every single time. So if anything, perhaps supporting multi-select for copilot feedback can be the scope of this ticket if my feedback is improperly directed, because it's rarely just one reason.

Extension version: 0.28.4
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.6.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M1 Pro (10 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|4, 6, 6|
|Memory (System)|32.00GB (0.11GB free)|
|Process Argv|. --crash-reporter-id 9a9b62ab-9903-44de-b42c-ede67cbdd702|
|Screen Reader|no|
|VM|0%|
</details>
<!-- generated by issue reporter -->",2
mcp servers removed from user settings,"
Type: <b>Bug</b>

I upgraded to the latest version of vs code insiders 1.102
i then found all my mcp servers were gone.
i found them in a separate file mcp.json in my folder below
C:\Users\xxxx\AppData\Roaming\Code - Insiders\User

when i open  that file there are no start links to start  the servers. i also have inputs that will prompt me for passwords

when i try to add mcp servers to user settings it tells me this is an invalid item
how is this supposed to work globally with this mcp.json file?

VS Code version: Code - Insiders 1.102.0-insider (6307b2e05830e59099f10a3ab650a9ef56e9b9e9, 2025-07-03T12:48:59.863Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Xeon(R) Gold 6342 CPU @ 2.80GHz (8 x 2793)|
|GPU Status|2d_canvas: unavailable_software<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: disabled_software<br>multiple_raster_threads: enabled_on<br>opengl: disabled_off<br>rasterization: disabled_software<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: disabled_software<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: unavailable_software<br>webgl2: unavailable_software<br>webgpu: unavailable_software<br>webnn: unavailable_software|
|Load (avg)|undefined|
|Memory (System)|32.00GB (8.84GB free)|
|Process Argv|--crash-reporter-id b26c2d2f-f4ec-41de-9b22-0b3920e43550|
|Screen Reader|no|
|VM|100%|
</details><details><summary>Extensions (51)</summary>

Extension|Author (truncated)|Version
---|---|---
better-comments|aar|3.0.2
vscode-sqlite|ale|0.14.1
csharpier-vscode|csh|2.0.8
vscode-eslint|dba|3.0.15
dbcode|dbc|1.14.15
es7-react-js-snippets-with-semicolons|dme|3.0.1
docker|doc|0.11.0
prettier-vscode|esb|11.0.0
figma-vscode-extension|fig|0.4.1
auto-rename-tag|for|0.1.10
copilot|Git|1.339.1656
copilot-chat|Git|0.29.2025070301
vscode-pull-request-github|Git|0.113.2025070315
rainbow-csv|mec|3.20.0
theme-monokai-pro-vscode|mon|2.0.7
azure-dev|ms-|0.9.0
ms-entra|ms-|1.0.8
vscode-azure-github-copilot|ms-|1.0.42
vscode-azureappservice|ms-|0.26.2
vscode-azurefunctions|ms-|1.17.3
vscode-azureresourcegroups|ms-|0.11.0
vscode-azurestaticwebapps|ms-|0.13.1
vscode-azurestorage|ms-|0.16.5
vscode-bicep|ms-|0.36.1
vscode-containers|ms-|2.0.3
vscode-docker|ms-|2.0.0
csdevkit|ms-|1.30.32
csharp|ms-|2.84.19
vscode-dotnet-runtime|ms-|2.3.6
playwright|ms-|1.1.15
black-formatter|ms-|2025.3.11831009
debugpy|ms-|2025.11.2025070101
python|ms-|2025.8.0
vscode-pylance|ms-|2025.6.101
vscode-python-envs|ms-|0.3.11841011
jupyter|ms-|2025.6.2025070201
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.3.2025062701
vscode-jupyter-cell-tags|ms-|0.1.9
vscode-jupyter-slideshow|ms-|0.1.6
remote-containers|ms-|0.417.0
remote-wsl|ms-|0.99.0
vscode-remote-extensionpack|ms-|0.26.0
powershell|ms-|2025.2.0
vscode-copilot-vision|ms-|0.2.2025032409
windows-ai-studio|ms-|0.16.0
sqlite-viewer|qwt|25.6.1
vscode-yaml|red|1.18.0
vscode-icons|vsc|12.13.0
vscode-import-cost|wix|3.3.0
vscode-sqlite3-editor|yy0|1.0.207

(1 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551:31249597
vscod805:30301674
c4g48928:30535728
a9j8j154:30646983
962ge761:30841072
dsvsc014:30777825
dsvsc015:30821418
h48ei257:31000450
pythontbext0:30879054
cppperfnew:30980852
dwnewjupyter:31046869
pythonrstrctxt:31093868
nativeloc1:31118317
e80f6927:31120813
dwcopilot:31158714
5b1c1929:31184661
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
g012b348:31231168
pythoneinst12:31251391
bgtreat:31252123
c7cif404:31309980
pythonpulldiag:31287486
6gi0g917:31259950
996jf627:31264550
pythonrdcb7:31268811
usemplatestapi:31297334
jdghv92:31317040
747dc170:31275146
pythonfullctx:31296836
6518g693:31302842
generatesymbolt:31280541
convertfstringf:31280702
b99bg931:31306656
usemarketplace:31336439
0g1h6703:31329154
b6b4d950:31327385
nes-emitfast-1:31333560
replacestringexc:31340153
testaa123cf:31335227
6abeh943:31336334
envsdeactivate2:31338962
nes-conv-10:31337515
0927b901:31340060
gji67723:31340537

```

</details>

<!-- generated by issue reporter -->",2
Can't change modes or create new chat,"
Type: <b>Bug</b>

When ever i press any of them they dont do anything, it worked yesterday but today nothing. I've tried reinstalling the copilot github and vscode but nothing worked.

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 9 5900X 12-Core Processor             (24 x 3700)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|63.91GB (50.63GB free)|
|Process Argv|--open-url --crash-reporter-id 2185e8a1-44d0-4459-a48a-415800ab5115 -- vscode://github.copilot-chat/|
|Screen Reader|no|
|VM|29%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805cf:30301675
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
vscrpc:30673769
962ge761:30959799
2e7ec940:31000449
pythontbext0:30879054
cppperfnew:31000557
dwnewjupyter:31046869
pythonrstrctxt:31112756
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7cf:31303019
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
d2249276:31341129
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
b6b4d950:31327385
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931

```

</details>

<!-- generated by issue reporter -->",2
"Add ""Close Working Trees"" option to editor tab context menu","<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

### Feature Request: Add ""Close Working Trees"" option to tab context menu

When using GitHub Copilot in **agent mode**, every file that Copilot edits opens a new tab showing a diff. These tabs are labeled with the file name followed by **“(Working Tree)”**. While this is helpful for reviewing changes, these tabs accumulate quickly and clutter the editor tab bar.

I often find myself manually closing each “(Working Tree)” tab, which interrupts my workflow.

#### Suggested Solution

Add a new option to the file tab right-click context menu:

> **Close Working Trees**

This would automatically close all open tabs that include `(Working Tree)` in their title.

#### Benefits

- Streamlines workflows for users working heavily with Copilot agent.
- Reduces visual clutter.
- Saves time and avoids repetitive manual tab management.",1
Terminal suggest: Remote WSL terminals use the wrong slashes in the resolved path on the right,"<img width=""537"" height=""286"" alt=""Image"" src=""https://github.com/user-attachments/assets/ab642723-9486-4a2e-a126-a44d946a6f25"" />",0
vscode --reuse-window <filename> does not load file in current window on SLES15,"
Type: <b>Bug</b>

I am using SLES15 Linux and the following VSCode version:

Version: 1.91.1
Commit: f1e16e1e6214d7c44d078b1f0607b2388f29d729
Date: 2024-07-09T22:08:12.169Z
Electron: 29.4.0
ElectronBuildId: 9728852
Chromium: 122.0.6261.156
Node.js: 20.9.0
V8: 12.2.281.27-electron.0
OS: Linux x64 5.14.21-150400.24.158-default

I have my property window.openFilesInNewWindow set to off.  On Windows when I am SSH'd into the same machine I can just do ""code <filename>"" and it will always reuse my current window.  But, when I use the Linux version of VSCode I cannot say ""vscode <filename>"" and have it reuse the window.  If I ctrl-click the file it will open in the current window.  

VS Code version: Code 1.91.1 (f1e16e1e6214d7c44d078b1f0607b2388f29d729, 2024-07-09T22:08:12.169Z)
OS version: Linux x64 5.14.21-150400.24.158-default
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Xeon(R) E-2378G CPU @ 2.80GHz (8 x 4472)|
|GPU Status|2d_canvas: unavailable_software<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: disabled_software<br>multiple_raster_threads: enabled_on<br>opengl: disabled_off<br>rasterization: disabled_software<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: disabled_software<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: unavailable_software<br>webgl2: unavailable_software<br>webgpu: disabled_off|
|Load (avg)|0, 0, 0|
|Memory (System)|125.30GB (118.87GB free)|
|Process Argv|--user-data-dir /tmp/bjpajak_vscode_tmp/.vscode --extensions-dir /tmp/bjpajak_vscode_tmp/.vscode/extensions /nfs/site/disks/sc_dteg_5001/users/bjpajak/ub_main/ub.code --new-window --command workbench.action.terminal.new --crash-reporter-id 9d8ef875-4a41-4e80-9339-9d2dc6edda2b|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|xfce|
|XDG_CURRENT_DESKTOP|XFCE|
|XDG_SESSION_DESKTOP|undefined|
|XDG_SESSION_TYPE|undefined|
</details><details><summary>Extensions (3)</summary>

Extension|Author (truncated)|Version
---|---|---
tcl|ras|0.1.0
vim|vsc|1.30.1
jinja|who|0.0.8


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805:30301674
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
2e7ec940:31000449
pythontbext0:30879054
cppperfnew:31000557
dwnewjupytercf:31046870
pythonrstrctxt:31112756
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31303018
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
0g0a1943:31341127
d7aab740:31338110
usemarketplace:31336439
4f60g487:31327383
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-c:31336931
hi92a955:31337023

```

</details>

<!-- generated by issue reporter -->",2
GitHub Copilot Down,"
Type: <b>Bug</b>

GitHub Copilot Down

Extension version: 0.28.5
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 25.0.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M4 Pro (12 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|3, 3, 2|
|Memory (System)|24.00GB (0.17GB free)|
|Process Argv|--crash-reporter-id 9a28b7b0-c375-4d3b-a28f-a482235eb1e5|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492:30256859
vscod805:30301674
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyonecf:30548226
2i9eh265:30646982
962ge761:30959799
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dwnewjupyter:31046869
pythonrstrctxt:31112756
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31303018
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
c4b42873:31341128
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
nes-diff-11:31337487
nes-set-on:31340697
onetestforazureexpcf:31335614
6abeh943:31336334
yijiwantestdri0626-c:31336931

```

</details>

<!-- generated by issue reporter -->",2
GitHub Copilot Down,"
Type: <b>Bug</b>

GitHub Copilot Down

Extension version: 0.28.5
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 25.0.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M4 Pro (12 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|3, 3, 2|
|Memory (System)|24.00GB (0.17GB free)|
|Process Argv|--crash-reporter-id 9a28b7b0-c375-4d3b-a28f-a482235eb1e5|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492:30256859
vscod805:30301674
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyonecf:30548226
2i9eh265:30646982
962ge761:30959799
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dwnewjupyter:31046869
pythonrstrctxt:31112756
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31303018
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
c4b42873:31341128
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
nes-diff-11:31337487
nes-set-on:31340697
onetestforazureexpcf:31335614
6abeh943:31336334
yijiwantestdri0626-c:31336931

```

</details>

<!-- generated by issue reporter -->",2
GitHub Copilot Down,"
Type: <b>Bug</b>

GitHub Copilot Down

Extension version: 0.28.5
VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 25.0.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M4 Pro (12 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|3, 3, 2|
|Memory (System)|24.00GB (0.17GB free)|
|Process Argv|--crash-reporter-id 9a28b7b0-c375-4d3b-a28f-a482235eb1e5|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492:30256859
vscod805:30301674
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyonecf:30548226
2i9eh265:30646982
962ge761:30959799
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dwnewjupyter:31046869
pythonrstrctxt:31112756
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31303018
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
j6a0c355:31339598
c4b42873:31341128
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
nes-diff-11:31337487
nes-set-on:31340697
onetestforazureexpcf:31335614
6abeh943:31336334
yijiwantestdri0626-c:31336931

```

</details>

<!-- generated by issue reporter -->",2
stuck in apply patch / heal patch loop,"I might be overloading with custom instructions, but the (gpt 4.1) agent inserted some code in the wrong place, identified that mistake, but got stuck in a loop trying to fix it.

```
Here’s how I’ll address these:

Move the new test implementation into the correct, empty test block (not inside the previous test).
Use unique variable names for data, doc, and notebookEditor in the new test.
Check the correct API for setting controller affinity and accessing the selected controller.
Ensure the test is self-contained and does not interfere with other tests.
I will now fix these issues by editing the test file.

The previous patch failed due to context mismatch and API issues. I will now:

Remove the incorrect duplicate test code that was inserted inside the previous test.
Place the new test in the correct, empty test block.
Use unique variable names.
Use the correct (possibly internal/experimental) API for setting controller affinity and accessing the selected controller, using type casting if necessary.
I will now read the first 2000 lines of ipynb.test.ts to ensure I have the correct context for a clean patch.

Read [ipynb.test.ts](vscode-file://vscode-app/c:/Users/aaron/AppData/Local/Programs/Microsoft%20VS%20Code%20Insiders/resources/app/out/vs/code/electron-browser/workbench/workbench.html)

I see that the previous patch incorrectly inserted the new test code inside the ""Can open a REPL notebook"" test, causing variable redeclarations and logic errors. The correct approach is to:

Remove the mistakenly inserted code from the ""Can open a REPL notebook"" test.
Place the new test logic inside the empty ""REPL created with one suggested kernel should auto select kernel"" test.
Use unique variable names and the correct API for affinity and controller selection, using type casting if necessary.
I will now clean up the file and implement the new test in the correct location.

The patch failed due to a context mismatch, likely because the previous patch left the file in an inconsistent state. I will now:

Remove the incorrect code block that was inserted into the ""Can open a REPL notebook"" test.
Place the correct implementation only in the ""REPL created with one suggested kernel should auto select kernel"" test.
Ensure the rest of the file remains unchanged.
I will now apply a patch that only affects the correct test block and cleans up the file.

The patch continues to fail due to context mismatches, likely because the previous patch attempts left the file in an inconsistent state. To resolve this, I will:

Remove all code related to the controller from the ""Can open a REPL notebook"" test, restoring it to its original form.
Place the correct implementation only in the ""REPL created with one suggested kernel should auto select kernel"" test, using unique variable names and the correct API.
I will now generate a patch that restores the ""Can open a REPL notebook"" test to its original state and implements the new test in the correct location.

```",1
Prompt coming too big,"
Type: <b>Bug</b>

Hi, i love your app and i have created a develoepr pack for nodejs which i use on my laptop and i can see that with my curent vs code, when i click install on myt developer pack the prompt is too big on the middle of screen and i cannot do anything and i simply cannot proceed further from the prompt with half of the text not showing.....

The fix is to press ctrl + - a few times to click on trust and install.... but not everyone will know about this.

My developer pack is prabhakar-kumar.nodejs-developer-pack-by-prabhakar.

Thanks for doing a great work microsoft....



VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Linux x64 5.15.0-139-generic snap
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz (8 x 3487)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: disabled_off<br>webnn: disabled_off|
|Load (avg)|1, 1, 2|
|Memory (System)|15.40GB (12.07GB free)|
|Process Argv|--no-sandbox --force-user-env --crash-reporter-id d6eda29c-3b8d-4d7b-ad26-d7c64b9828bf|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|ubuntu|
|XDG_CURRENT_DESKTOP|Unity|
|XDG_SESSION_DESKTOP|ubuntu|
|XDG_SESSION_TYPE|x11|
</details><details><summary>Extensions (9)</summary>

Extension|Author (truncated)|Version
---|---|---
npm-intellisense|chr|1.4.5
vscode-eslint|dba|3.0.10
EditorConfig|Edi|0.17.4
prettier-vscode|esb|11.0.0
dotenv|mik|1.0.1
vscode-docker|ms-|2.0.0
remote-containers|ms-|0.420.0
vscode-yaml|red|1.18.0
JavaScriptSnippets|xab|1.8.0


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805cf:30301675
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dwnewjupyter:31046869
pythonrstrctxt:31112756
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
471b6256:31263136
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7:31303018
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
j6a0c355:31339598
0g0a1943:31341127
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
testaa123:31335226
6abeh943:31336334
yijiwantestdri0626-t:31336930

```

</details>

<!-- generated by issue reporter -->",2
No display,"
Type: <b>Bug</b>

Not displaying anything

Extension version: 0.28.5
VS Code version: Code 1.101.1 (Universal) (18e3a1ec544e6907be1e944a94c496e302073435, 2025-06-18T13:35:12.605Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M2 (8 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|3, 12, 36|
|Memory (System)|24.00GB (0.07GB free)|
|Process Argv|--crash-reporter-id 3d90dddd-3de8-4a9c-9520-246222741375|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492cf:30256860
vscod805cf:30301675
binariesv615:30325510
vscaat:30438848
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dwnewjupyter:31046869
pythonrstrctxt:31112756
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7cf:31303019
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixf:31329273
jhi8h917:31341130
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
onetestforazureexp:31335613
6abeh943:31336334
yijiwantestdri0626-c:31336931

```

</details>

<!-- generated by issue reporter -->",2
blinking on ubuntus v 24.4.2,"
Type: <b>Bug</b>

just click and i got flick blinking on UI like tolbar every where 

VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Linux x64 6.11.0-29-generic snap
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|12th Gen Intel(R) Core(TM) i5-12500H (16 x 4200)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: disabled_off<br>webnn: disabled_off|
|Load (avg)|0, 0, 0|
|Memory (System)|15.25GB (12.24GB free)|
|Process Argv|--no-sandbox --force-user-env --crash-reporter-id 85003a82-08ed-465c-801b-a9396d542ae3|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|ubuntu-wayland|
|XDG_CURRENT_DESKTOP|Unity|
|XDG_SESSION_DESKTOP|ubuntu-wayland|
|XDG_SESSION_TYPE|wayland|
</details>Extensions: none<details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805:30301674
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
a9j8j154:30646983
962ge761:30959799
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dwnewjupyter:31046869
pythonrstrctxt:31112756
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
hdaa2157:31222309
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31303018
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
pylancequickfixt:31329274
j6a0c355:31339598
convertlamdaf:31329270
jhi8h917:31341130
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
63221493:31336333
yijiwantestdri0626-c:31336931
hi92a955:31337023

```

</details>

<!-- generated by issue reporter -->",2
Running into issues after first payment,"
Type: <b>Bug</b>

Chat responses keep failing

Extension version: 0.29.2025062502
VS Code version: Code - Insiders 1.102.0-insider (Universal) (94a250260181297ca183a963f3d059e380f2da26, 2025-06-24T05:03:47.389Z)
OS version: Darwin arm64 25.0.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M3 Pro (12 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|4, 4, 4|
|Memory (System)|18.00GB (0.21GB free)|
|Process Argv|--crash-reporter-id 8f90ca83-41e5-481f-ad25-266e90fda7a0|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vsc_aa:30263845
pythonvspyt551:31249597
vscod805:30301674
c4g48928:30535728
962ge761:30841072
dsvsc014:30777825
dsvsc015:30821418
h48ei257:31000450
pythontbext0:30879054
cppperfnew:30980852
dwnewjupytercf:31046870
pythonrstrctxt:31093868
nativeloc1:31118317
e80f6927:31120813
dwcopilot:31158714
5b1c1929:31184661
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
g012b348:31231168
pythoneinst12:31251391
bgtreat:31252123
c7cif404:31309980
pythonpulldiag:31287486
6gi0g917:31259950
996jf627:31264550
pythonrdcb7:31268811
usemplatestapi:31297334
e6194696:31317039
747dc170:31275146
pythoncompactctx:31296835
6518g693:31302842
generatesymbolt:31280541
convertfstringf:31280702
j97ad248:31306657
usemarketplace:31336439
0g1h6703:31329154
b6b4d950:31327385
nes-emitfast-1:31333560
replacestringexc:31340153
onetestforazureexpcf:31335614
6abeh943:31336334
nes-conv-10:31337515
0927b901:31340060
jbdfg126:31340538

```

</details>

<!-- generated by issue reporter -->",2
Extensions do not run with proper ENV variables when .zshrc has specific strings (regression in v 1.101),"Type: <b>Bug</b>


What seems to be happening: including a specific string in an ENV var via my `~/.zshrc` causes **all** extensions to run code with some baseline set of ENV variables instead of the ENV variables I would expect from my `~/.zshrc`. This causes issues with various extensions that expect to find tools they don't have access to because the `PATH` is not properly set (for example, I have prettier running on `.rb` files and it fails to run ruby because it can't find `bundler`). But I imagine this could cause _all sorts_ of other subtle bugs. 

This happens even when **only** a single test extension is running.

## Steps to reproduce:

**Step 1**: download this [example extension I have created](https://github.com/storey/vscode-env-example) based on the ""first extension"" template

**Step 2**: update your `.zshrc` to be the code below. LS_COLORS is a string I generated using `vivid` to support custom colors in the [lsd tool](https://github.com/lsd-rs/lsd). 
```
export AAA_TEST='test value'

export LS_COLORS='*~=0;38;2;198;198;198:bd=0;38;2;8;8;8;48;2;194;251;217:ca=0:cd=0;38;2;8;8;8;48;2;194;251;217:di=1;38;2;62;133;98:do=0;38;2;8;8;8;48;2;194;251;217:ex=0;38;2;232;70;134:fi=0;38;2;103;211;142:ln=0;38;2;242;179;213:mh=0:mi=0;38;2;8;8;8;48;2;242;179;213:no=0;38;2;103;211;142:or=0;38;2;8;8;8;48;2;242;179;213:ow=0:pi=0;38;2;8;8;8;48;2;194;251;217:rs=0;38;2;103;211;142:sg=0:so=0;38;2;8;8;8;48;2;194;251;217:st=0:su=0:tw=0:*.1=0;38;2;8;8;8:*.a=0;38;2;232;70;134:*.c=0;38;2;103;211;142:*.d=0;38;2;103;211;142:*.h=0;38;2;103;211;142:*.m=0;38;2;103;211;142:*.o=0;38;2;198;198;198:*.p=0;38;2;103;211;142:*.r=0;38;2;103;211;142:*.t=0;38;2;103;211;142:*.v=0;38;2;103;211;142:*.z=0;38;2;8;8;8:*.7z=0;38;2;8;8;8:*.ai=0;38;2;8;8;8:*.as=0;38;2;103;211;142:*.bc=0;38;2;198;198;198:*.bz=0;38;2;8;8;8:*.cc=0;38;2;103;211;142:*.cp=0;38;2;103;211;142:*.cr=0;38;2;103;211;142:*.cs=0;38;2;103;211;142:*.db=0;38;2;8;8;8:*.di=0;38;2;103;211;142:*.el=0;38;2;103;211;142:*.ex=0;38;2;103;211;142:*.fs=0;38;2;103;211;142:*.go=0;38;2;103;211;142:*.gv=0;38;2;103;211;142:*.gz=0;38;2;8;8;8:*.ha=0;38;2;103;211;142:*.hh=0;38;2;103;211;142:*.hi=0;38;2;198;198;198:*.hs=0;38;2;103;211;142:*.jl=0;38;2;103;211;142:*.js=0;38;2;103;211;142:*.ko=0;38;2;232;70;134:*.kt=0;38;2;103;211;142:*.la=0;38;2;198;198;198:*.ll=0;38;2;103;211;142:*.lo=0;38;2;198;198;198:*.ma=0;38;2;8;8;8:*.mb=0;38;2;8;8;8:*.md=0;38;2;8;8;8:*.mk=0;38;2;103;211;142:*.ml=0;38;2;103;211;142:*.mn=0;38;2;103;211;142:*.nb=0;38;2;103;211;142:*.nu=0;38;2;103;211;142:*.pl=0;38;2;103;211;142:*.pm=0;38;2;103;211;142:*.pp=0;38;2;103;211;142:*.ps=0;38;2;8;8;8:*.py=0;38;2;103;211;142:*.rb=0;38;2;103;211;142:*.rm=0;38;2;8;8;8:*.rs=0;38;2;103;211;142:*.sh=0;38;2;103;211;142:*.so=0;38;2;232;70;134:*.td=0;38;2;103;211;142:*.ts=0;38;2;103;211;142:*.ui=0;38;2;103;211;142:*.vb=0;38;2;103;211;142:*.wv=0;38;2;8;8;8:*.xz=0;38;2;8;8;8:*FAQ=0;38;2;8;8;8:*.3ds=0;38;2;8;8;8:*.3fr=0;38;2;8;8;8:*.3mf=0;38;2;8;8;8:*.adb=0;38;2;103;211;142:*.ads=0;38;2;103;211;142:*.aif=0;38;2;8;8;8:*.amf=0;38;2;8;8;8:*.ape=0;38;2;8;8;8:*.apk=0;38;2;8;8;8:*.ari=0;38;2;8;8;8:*.arj=0;38;2;8;8;8:*.arw=0;38;2;8;8;8:*.asa=0;38;2;103;211;142:*.asm=0;38;2;103;211;142:*.aux=0;38;2;198;198;198:*.avi=0;38;2;8;8;8:*.awk=0;38;2;103;211;142:*.bag=0;38;2;8;8;8:*.bak=0;38;2;198;198;198:*.bat=0;38;2;232;70;134:*.bay=0;38;2;8;8;8:*.bbl=0;38;2;198;198;198:*.bcf=0;38;2;198;198;198:*.bib=0;38;2;103;211;142:*.bin=0;38;2;8;8;8:*.blg=0;38;2;198;198;198:*.bmp=0;38;2;8;8;8:*.bsh=0;38;2;103;211;142:*.bst=0;38;2;103;211;142:*.bz2=0;38;2;8;8;8:*.c++=0;38;2;103;211;142:*.cap=0;38;2;8;8;8:*.cfg=0;38;2;103;211;142:*.cgi=0;38;2;103;211;142:*.clj=0;38;2;103;211;142:*.com=0;38;2;232;70;134:*.cpp=0;38;2;103;211;142:*.cr2=0;38;2;8;8;8:*.cr3=0;38;2;8;8;8:*.crw=0;38;2;8;8;8:*.css=0;38;2;103;211;142:*.csv=0;38;2;8;8;8:*.csx=0;38;2;103;211;142:*.cxx=0;38;2;103;211;142:*.dae=0;38;2;8;8;8:*.dcr=0;38;2;8;8;8:*.dcs=0;38;2;8;8;8:*.deb=0;38;2;8;8;8:*.def=0;38;2;103;211;142:*.dll=0;38;2;232;70;134:*.dmg=0;38;2;8;8;8:*.dng=0;38;2;8;8;8:*.doc=0;38;2;8;8;8:*.dot=0;38;2;103;211;142:*.dox=0;38;2;103;211;142:*.dpr=0;38;2;103;211;142:*.drf=0;38;2;8;8;8:*.dxf=0;38;2;8;8;8:*.eip=0;38;2;8;8;8:*.elc=0;38;2;103;211;142:*.elm=0;38;2;103;211;142:*.epp=0;38;2;103;211;142:*.eps=0;38;2;8;8;8:*.erf=0;38;2;8;8;8:*.erl=0;38;2;103;211;142:*.exe=0;38;2;232;70;134:*.exr=0;38;2;8;8;8:*.exs=0;38;2;103;211;142:*.fbx=0;38;2;8;8;8:*.fff=0;38;2;8;8;8:*.fls=0;38;2;198;198;198:*.flv=0;38;2;8;8;8:*.fnt=0;38;2;8;8;8:*.fon=0;38;2;8;8;8:*.fsi=0;38;2;103;211;142:*.fsx=0;38;2;103;211;142:*.gif=0;38;2;8;8;8:*.git=0;38;2;198;198;198:*.gpr=0;38;2;8;8;8:*.gvy=0;38;2;103;211;142:*.h++=0;38;2;103;211;142:*.hda=0;38;2;8;8;8:*.hip=0;38;2;8;8;8:*.hpp=0;38;2;103;211;142:*.htc=0;38;2;103;211;142:*.htm=0;38;2;8;8;8:*.hxx=0;38;2;103;211;142:*.ico=0;38;2;8;8;8:*.ics=0;38;2;8;8;8:*.idx=0;38;2;198;198;198:*.igs=0;38;2;8;8;8:*.iiq=0;38;2;8;8;8:*.ilg=0;38;2;198;198;198:*.img=0;38;2;8;8;8:*.inc=0;38;2;103;211;142:*.ind=0;38;2;198;198;198:*.ini=0;38;2;103;211;142:*.inl=0;38;2;103;211;142:*.ino=0;38;2;103;211;142:*.ipp=0;38;2;103;211;142:*.iso=0;38;2;8;8;8:*.jar=0;38;2;8;8;8:*.jpg=0;38;2;8;8;8:*.jsx=0;38;2;103;211;142:*.jxl=0;38;2;8;8;8:*.k25=0;38;2;8;8;8:*.kdc=0;38;2;8;8;8:*.kex=0;38;2;8;8;8:*.kra=0;38;2;8;8;8:*.kts=0;38;2;103;211;142:*.log=0;38;2;198;198;198:*.ltx=0;38;2;103;211;142:*.lua=0;38;2;103;211;142:*.m3u=0;38;2;8;8;8:*.m4a=0;38;2;8;8;8:*.m4v=0;38;2;8;8;8:*.mdc=0;38;2;8;8;8:*.mef=0;38;2;8;8;8:*.mid=0;38;2;8;8;8:*.mir=0;38;2;103;211;142:*.mkv=0;38;2;8;8;8:*.mli=0;38;2;103;211;142:*.mos=0;38;2;8;8;8:*.mov=0;38;2;8;8;8:*.mp3=0;38;2;8;8;8:*.mp4=0;38;2;8;8;8:*.mpg=0;38;2;8;8;8:*.mrw=0;38;2;8;8;8:*.msi=0;38;2;8;8;8:*.mtl=0;38;2;8;8;8:*.nef=0;38;2;8;8;8:*.nim=0;38;2;103;211;142:*.nix=0;38;2;103;211;142:*.nrw=0;38;2;8;8;8:*.obj=0;38;2;8;8;8:*.obm=0;38;2;8;8;8:*.odp=0;38;2;8;8;8:*.ods=0;38;2;8;8;8:*.odt=0;38;2;8;8;8:*.ogg=0;38;2;8;8;8:*.ogv=0;38;2;8;8;8:*.orf=0;38;2;8;8;8:*.org=0;38;2;8;8;8:*.otf=0;38;2;8;8;8:*.otl=0;38;2;8;8;8:*.out=0;38;2;198;198;198:*.pas=0;38;2;103;211;142:*.pbm=0;38;2;8;8;8:*.pcx=0;38;2;8;8;8:*.pdf=0;38;2;8;8;8:*.pef=0;38;2;8;8;8:*.pgm=0;38;2;8;8;8:*.php=0;38;2;103;211;142:*.pid=0;38;2;198;198;198:*.pkg=0;38;2;8;8;8:*.png=0;38;2;8;8;8:*.pod=0;38;2;103;211;142:*.ppm=0;38;2;8;8;8:*.pps=0;38;2;8;8;8:*.ppt=0;38;2;8;8;8:*.pro=0;38;2;103;211;142:*.ps1=0;38;2;103;211;142:*.psd=0;38;2;8;8;8:*.ptx=0;38;2;8;8;8:*.pxn=0;38;2;8;8;8:*.pyc=0;38;2;198;198;198:*.pyd=0;38;2;198;198;198:*.pyo=0;38;2;198;198;198:*.qoi=0;38;2;8;8;8:*.r3d=0;38;2;8;8;8:*.raf=0;38;2;8;8;8:*.rar=0;38;2;8;8;8:*.raw=0;38;2;8;8;8:*.rpm=0;38;2;8;8;8:*.rst=0;38;2;8;8;8:*.rtf=0;38;2;8;8;8:*.rw2=0;38;2;8;8;8:*.rwl=0;38;2;8;8;8:*.rwz=0;38;2;8;8;8:*.sbt=0;38;2;103;211;142:*.sql=0;38;2;103;211;142:*.sr2=0;38;2;8;8;8:*.srf=0;38;2;8;8;8:*.srw=0;38;2;8;8;8:*.stl=0;38;2;8;8;8:*.stp=0;38;2;8;8;8:*.sty=0;38;2;198;198;198:*.svg=0;38;2;8;8;8:*.swf=0;38;2;8;8;8:*.swp=0;38;2;198;198;198:*.sxi=0;38;2;8;8;8:*.sxw=0;38;2;8;8;8:*.tar=0;38;2;8;8;8:*.tbz=0;38;2;8;8;8:*.tcl=0;38;2;103;211;142:*.tex=0;38;2;103;211;142:*.tga=0;38;2;8;8;8:*.tgz=0;38;2;8;8;8:*.tif=0;38;2;8;8;8:*.tml=0;38;2;103;211;142:*.tmp=0;38;2;198;198;198:*.toc=0;38;2;198;198;198:*.tsx=0;38;2;103;211;142:*.ttf=0;38;2;8;8;8:*.txt=0;38;2;8;8;8:*.typ=0;38;2;8;8;8:*.usd=0;38;2;8;8;8:*.vcd=0;38;2;8;8;8:*.vim=0;38;2;103;211;142:*.vob=0;38;2;8;8;8:*.vsh=0;38;2;103;211;142:*.wav=0;38;2;8;8;8:*.wma=0;38;2;8;8;8:*.wmv=0;38;2;8;8;8:*.wrl=0;38;2;8;8;8:*.x3d=0;38;2;8;8;8:*.x3f=0;38;2;8;8;8:*.xlr=0;38;2;8;8;8:*.xls=0;38;2;8;8;8:*.xml=0;38;2;8;8;8:*.xmp=0;38;2;103;211;142:*.xpm=0;38;2;8;8;8:*.xvf=0;38;2;8;8;8:*.yml=0;38;2;103;211;142:*.zig=0;38;2;103;211;142:*.zip=0;38;2;8;8;8:*.zsh=0;38;2;103;211;142:*.zst=0;38;2;8;8;8:*TODO=0;38;2;8;8;8:*hgrc=0;38;2;103;211;142:*.avif=0;38;2;8;8;8:*.bash=0;38;2;103;211;142:*.braw=0;38;2;8;8;8:*.conf=0;38;2;103;211;142:*.dart=0;38;2;103;211;142:*.data=0;38;2;8;8;8:*.diff=0;38;2;103;211;142:*.docx=0;38;2;8;8;8:*.epub=0;38;2;8;8;8:*.fish=0;38;2;103;211;142:*.flac=0;38;2;8;8;8:*.h264=0;38;2;8;8;8:*.hack=0;38;2;103;211;142:*.heif=0;38;2;8;8;8:*.hgrc=0;38;2;103;211;142:*.html=0;38;2;8;8;8:*.iges=0;38;2;8;8;8:*.info=0;38;2;8;8;8:*.java=0;38;2;103;211;142:*.jpeg=0;38;2;8;8;8:*.json=0;38;2;103;211;142:*.less=0;38;2;103;211;142:*.lisp=0;38;2;103;211;142:*.lock=0;38;2;198;198;198:*.make=0;38;2;103;211;142:*.mojo=0;38;2;103;211;142:*.mpeg=0;38;2;8;8;8:*.nims=0;38;2;103;211;142:*.opus=0;38;2;8;8;8:*.orig=0;38;2;198;198;198:*.pptx=0;38;2;8;8;8:*.prql=0;38;2;103;211;142:*.psd1=0;38;2;103;211;142:*.psm1=0;38;2;103;211;142:*.purs=0;38;2;103;211;142:*.raku=0;38;2;103;211;142:*.rlib=0;38;2;198;198;198:*.sass=0;38;2;103;211;142:*.scad=0;38;2;103;211;142:*.scss=0;38;2;103;211;142:*.step=0;38;2;8;8;8:*.tbz2=0;38;2;8;8;8:*.tiff=0;38;2;8;8;8:*.toml=0;38;2;103;211;142:*.usda=0;38;2;8;8;8:*.usdc=0;38;2;8;8;8:*.usdz=0;38;2;8;8;8:*.webm=0;38;2;8;8;8:*.webp=0;38;2;8;8;8:*.woff=0;38;2;8;8;8:*.xbps=0;38;2;8;8;8:*.xlsx=0;38;2;8;8;8:*.yaml=0;38;2;103;211;142:*stdin=0;38;2;198;198;198:*v.mod=0;38;2;103;211;142:*.blend=0;38;2;8;8;8:*.cabal=0;38;2;103;211;142:*.cache=0;38;2;198;198;198:*.class=0;38;2;198;198;198:*.cmake=0;38;2;103;211;142:*.ctags=0;38;2;198;198;198:*.dylib=0;38;2;232;70;134:*.dyn_o=0;38;2;198;198;198:*.gcode=0;38;2;103;211;142:*.ipynb=0;38;2;103;211;142:*.mdown=0;38;2;8;8;8:*.patch=0;38;2;103;211;142:*.rmeta=0;38;2;198;198;198:*.scala=0;38;2;103;211;142:*.shtml=0;38;2;8;8;8:*.swift=0;38;2;103;211;142:*.toast=0;38;2;8;8;8:*.woff2=0;38;2;8;8;8:*.xhtml=0;38;2;8;8;8:*Icon\r=0;38;2;198;198;198:*LEGACY=0;38;2;8;8;8:*NOTICE=0;38;2;8;8;8:*README=0;38;2;8;8;8:*go.mod=0;38;2;103;211;142:*go.sum=0;38;2;198;198;198:*passwd=0;38;2;103;211;142:*shadow=0;38;2;103;211;142:*stderr=0;38;2;198;198;198:*stdout=0;38;2;198;198;198:*.bashrc=0;38;2;103;211;142:*.config=0;38;2;103;211;142:*.dyn_hi=0;38;2;198;198;198:*.flake8=0;38;2;103;211;142:*.gradle=0;38;2;103;211;142:*.groovy=0;38;2;103;211;142:*.ignore=0;38;2;103;211;142:*.matlab=0;38;2;103;211;142:*.nimble=0;38;2;103;211;142:*COPYING=0;38;2;8;8;8:*INSTALL=0;38;2;8;8;8:*LICENCE=0;38;2;8;8;8:*LICENSE=0;38;2;8;8;8:*TODO.md=0;38;2;8;8;8:*VERSION=0;38;2;8;8;8:*.alembic=0;38;2;8;8;8:*.desktop=0;38;2;103;211;142:*.gemspec=0;38;2;103;211;142:*.mailmap=0;38;2;103;211;142:*Doxyfile=0;38;2;103;211;142:*Makefile=0;38;2;103;211;142:*TODO.txt=0;38;2;8;8;8:*setup.py=0;38;2;103;211;142:*.DS_Store=0;38;2;198;198;198:*.cmake.in=0;38;2;103;211;142:*.fdignore=0;38;2;103;211;142:*.kdevelop=0;38;2;103;211;142:*.markdown=0;38;2;8;8;8:*.rgignore=0;38;2;103;211;142:*.tfignore=0;38;2;103;211;142:*CHANGELOG=0;38;2;8;8;8:*COPYRIGHT=0;38;2;8;8;8:*README.md=0;38;2;8;8;8:*bun.lockb=0;38;2;198;198;198:*configure=0;38;2;103;211;142:*.gitconfig=0;38;2;103;211;142:*.gitignore=0;38;2;103;211;142:*.localized=0;38;2;198;198;198:*.scons_opt=0;38;2;198;198;198:*.timestamp=0;38;2;198;198;198:*CODEOWNERS=0;38;2;103;211;142:*Dockerfile=0;38;2;103;211;142:*INSTALL.md=0;38;2;8;8;8:*README.txt=0;38;2;8;8;8:*SConscript=0;38;2;103;211;142:*SConstruct=0;38;2;103;211;142:*.cirrus.yml=0;38;2;103;211;142:*.gitmodules=0;38;2;103;211;142:*.synctex.gz=0;38;2;198;198;198:*.travis.yml=0;38;2;103;211;142:*INSTALL.txt=0;38;2;8;8;8:*LICENSE-MIT=0;38;2;8;8;8:*MANIFEST.in=0;38;2;103;211;142:*Makefile.am=0;38;2;103;211;142:*Makefile.in=0;38;2;198;198;198:*.applescript=0;38;2;103;211;142:*.fdb_latexmk=0;38;2;198;198;198:*.webmanifest=0;38;2;103;211;142:*CHANGELOG.md=0;38;2;8;8;8:*CONTRIBUTING=0;38;2;8;8;8:*CONTRIBUTORS=0;38;2;8;8;8:*appveyor.yml=0;38;2;103;211;142:*configure.ac=0;38;2;103;211;142:*.bash_profile=0;38;2;103;211;142:*.clang-format=0;38;2;103;211;142:*.editorconfig=0;38;2;103;211;142:*CHANGELOG.txt=0;38;2;8;8;8:*.gitattributes=0;38;2;103;211;142:*.gitlab-ci.yml=0;38;2;103;211;142:*CMakeCache.txt=0;38;2;198;198;198:*CMakeLists.txt=0;38;2;103;211;142:*LICENSE-APACHE=0;38;2;8;8;8:*pyproject.toml=0;38;2;103;211;142:*CODE_OF_CONDUCT=0;38;2;8;8;8:*CONTRIBUTING.md=0;38;2;8;8;8:*CONTRIBUTORS.md=0;38;2;8;8;8:*.sconsign.dblite=0;38;2;198;198;198:*CONTRIBUTING.txt=0;38;2;8;8;8:*CONTRIBUTORS.txt=0;38;2;8;8;8:*requirements.txt=0;38;2;103;211;142:*package-lock.json=0;38;2;198;198;198:*CODE_OF_CONDUCT.md=0;38;2;8;8;8:*.CFUserTextEncoding=0;38;2;198;198;198:*CODE_OF_CONDUCT.txt=0;38;2;8;8;8:*azure-pipelines.yml=0;38;2;103;211;142'
```

**Step 3**: Open a fresh instance of VS Code (make sure it's a fresh restart each time when you change the `.zshrc`)

**Step 4**: Open the folder with that extension in vscode, navigate to `src/extension.ts`, press F5 to open a new instance with the extension loaded

**Step 5**: Run `Hello World` from the command palette 

**Step 6**: In version 1.100, you'll get an info log with info on AAA_TEST. In version 1.101, you'll get an info log that **does not** include it.



### Results in 1.100, with provided .zshrc
![Image](https://github.com/user-attachments/assets/c5790396-5220-4053-abd1-186176922e0c)

### Results in 1.101, with provided .zshrc
![Image](https://github.com/user-attachments/assets/08b7e819-90fd-4613-bd64-abf881cd8f55)

### Results in 1.101, with the LS_COLORS line commented out
![Image](https://github.com/user-attachments/assets/852780b1-b094-4eb4-bb60-864708262936)

### Results in Insiders, with provided .zshrc
![Image](https://github.com/user-attachments/assets/b54040d0-eac4-4092-b91b-21a5cf8524f3)

### Results in Insiders, with the LS_COLORS line commented out
![Image](https://github.com/user-attachments/assets/2008226a-c92b-4d19-9fe3-55c42a9594a5)

## Additional notes

If you set LS_COLORS to this string and then later overwrite LS_COLORS to something else within the same `.zshrc`, everything works fine - the issue seems to be with whatever the final values are, not intermediate ones 

I have not dug into which specific part of the string is causing issues; happy to try to dig in there; not sure if the issue is the length or some sort of internal characters.






VS Code version: Code 1.101.2 (Universal) (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Darwin arm64 24.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M1 Max (10 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|8, 6, 6|
|Memory (System)|64.00GB (3.67GB free)|
|Process Argv|--crash-reporter-id 20e65169-1759-4f39-aea0-85249ffb3618|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (21)</summary>

Extension|Author (truncated)|Version
---|---|---
Bookmarks|ale|13.5.0
vscode-eslint|dba|3.0.10
docker|doc|0.11.0
gitlens|eam|17.2.1
prettier-vscode|esb|11.0.0
copilot|Git|1.338.0
endwise|kai|1.5.1
vscode-rdbg|Koi|0.2.2
ruby-rubocop|mis|0.8.6
vscode-scss|mrm|0.10.0
vscode-containers|ms-|2.0.3
vscode-docker|ms-|2.0.0
debugpy|ms-|2025.8.0
python|ms-|2025.8.0
vscode-pylance|ms-|2025.6.2
live-server|ms-|0.4.15
vscode-coverage-gutters|rya|2.13.0
ruby-extensions-pack|Sho|0.1.13
sorbet-vscode-extension|sor|0.3.43
rewrap|stk|1.16.3
vscode-stylelint|sty|1.5.3


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805:30301674
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dwnewjupyter:31046869
pythonrstrctxt:31112756
nativeloc1:31192215
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7cf:31303019
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
nes-only-41muv2:31340494
9d2cg352:31339597
d2249276:31341129
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335814
82j33506:31327384
nes-diff-11:31337487
nes-set-on:31340697
6abeh943:31336334
yijiwantestdri0626-t:31336930

```

</details>

<!-- generated by issue reporter -->",2
Claude Sonnet 4,"
Type: <b>Bug</b>

Sorry, no response was returned. Many time!

Extension version: 0.28.5
VS Code version: Code 1.101.1 (18e3a1ec544e6907be1e944a94c496e302073435, 2025-06-18T13:35:12.605Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|12th Gen Intel(R) Core(TM) i7-12700H (20 x 2688)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|63.67GB (46.72GB free)|
|Process Argv|--crash-reporter-id 63bb5034-f92e-49ba-8db7-6e676350520a|
|Screen Reader|no|
|VM|40%|
</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
pythonvspyt551cf:31249601
vscod805cf:30301675
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
a9j8j154:30646983
962ge761:30959799
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dwnewjupytercf:31046870
pythonrstrctxt:31112756
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiagcf:31335740
996jf627:31283433
pythonrdcb7cf:31303019
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
jhi8h917:31341130
i851h500:31338111
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
6abeh943:31336334
yijiwantestdri0626-t:31336930

```

</details>

<!-- generated by issue reporter -->",2
Performance Issues,"
Type: <b>Performance Issue</b>

copilot has been performance issues cause lagging and delayed output

Extension version: 0.28.5
VS Code version: Code 1.101.2 (2901c5ac6db8a986a5666c3af51ff804d05af0d4, 2025-06-24T20:27:15.391Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|13th Gen Intel(R) Core(TM) i5-13420H (12 x 2611)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|63.71GB (41.59GB free)|
|Process Argv|--crash-reporter-id 4670e657-3daf-4270-aba0-7cb574b257c1|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>Process Info</summary>

```
CPU %	Mem MB	   PID	Process
    1	   313	 40024	code
    0	   318	  7800	   gpu-process
    0	   125	 31128	file-watcher [1]
    0	   213	 31728	shared-process
   10	  2391	 35548	window [1] (Running Extensions - madaamliit - Visual Studio Code)
    0	    36	 36772	   crashpad-handler
    0	    56	 37444	   utility-network-service
    0	   129	 42648	pty-host
    0	     9	  4240	     conpty-agent
    0	     9	 29588	     conpty-agent
    0	     9	 31172	     conpty-agent
    0	   130	 31444	     C:\Users\Franc\AppData\Local\Microsoft\WindowsApps\Microsoft.PowerShell_8wekyb3d8bbwe\pwsh.exe -noexit -command ""try { . \""c:\Program Files\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1\"" } catch {}""
    0	     9	 33952	     conpty-agent
    0	     9	 35792	     conpty-agent
    0	  2930	 43308	extension-host [1]
    0	    37	  2796	     c:\Users\Franc\.vscode\extensions\ms-vscode.cpptools-1.26.3-win32-x64\bin\cpptools.exe
    0	     8	 36116	       C:\Windows\system32\conhost.exe 0x4
    0	   159	 12188	     electron-nodejs (serverMain.js )
    0	     9	 14240	     C:\Windows\system32\cmd.exe /d /s /c """"c:\Users\Franc\.vscode\extensions\vscjava.vscode-gradle-3.16.4\lib\gradle-server.bat"" ""--port=58503"" ""--startBuildServer=true"" ""--languageServerPipePath=\\.\pipe\9b8e1598fa691e42e1e4a747b442a08a-sock"" ""--pipeName=\\.\pipe\b125f1c484485bc78f19298a15c6885e-sock"" ""--bundleDir=c:\Users\Franc\.vscode\extensions\vscjava.vscode-gradle-3.16.4\server""""
    0	   119	 35720	       ""c:\Users\Franc\.vscode\extensions\redhat.java-1.43.1-win32-x64\jre\21.0.7-win32-x86_64/bin/java.exe""  ""-Dfile.encoding=UTF-8""  --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.prefs/java.util.prefs=ALL-UNNAMED --add-opens=java.base/java.nio.charset=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED  -classpath ""c:\Users\Franc\.vscode\extensions\vscjava.vscode-gradle-3.16.4\lib\gradle-server.jar"" com.github.badsyntax.gradle.GradleServer ""--port=58503"" ""--startBuildServer=true"" ""--languageServerPipePath=\\.\pipe\9b8e1598fa691e42e1e4a747b442a08a-sock"" ""--pipeName=\\.\pipe\b125f1c484485bc78f19298a15c6885e-sock"" ""--bundleDir=c:\Users\Franc\.vscode\extensions\vscjava.vscode-gradle-3.16.4\server""
    0	     8	 41800	       C:\Windows\system32\conhost.exe 0x4
    0	    25	 19412	     C:\flutter\bin\cache\dart-sdk\bin\dart.exe tooling-daemon --machine
    0	     8	 21584	       C:\Windows\system32\conhost.exe 0x4
    0	    17	 22952	       C:\flutter\bin\cache\dart-sdk\bin\dartaotruntime.exe --new_gen_semi_max_size=32 --new_gen_growth_factor=4 C:\flutter\bin\cache\dart-sdk\bin\snapshots\dart_tooling_daemon_aot.dart.snapshot --machine
    0	   182	 19868	     electron-nodejs (tailwindModeServer.js )
    0	   123	 27972	     electron-nodejs (server.js )
    0	   111	 28132	     ""C:\Program Files\Microsoft VS Code\Code.exe"" ""c:\Program Files\Microsoft VS Code\resources\app\extensions\css-language-features\server\dist\node\cssServerMain"" --node-ipc --clientProcessId=43308
    0	   101	 31428	     electron-nodejs (eslintServer.js )
    0	   150	 31680	     electron-nodejs (tsserver.js )
    0	   131	 31684	     electron-nodejs (server.js )
    0	   106	 32036	     electron-nodejs (server.js )
    0	   106	 32324	     electron-nodejs (server.js )
    0	   217	 32900	     electron-nodejs (tsserver.js )
    0	   124	 25908	       electron-nodejs (typingsInstaller.js typesMap.js )
    0	   142	 34324	     electron-nodejs (index.js )
    0	   189	 35556	     c:\Users\Franc\.vscode\extensions\redhat.java-1.43.1-win32-x64\jre\21.0.7-win32-x86_64\bin\java --add-modules=ALL-SYSTEM --add-opens java.base/java.util=ALL-UNNAMED --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/sun.nio.fs=ALL-UNNAMED -Declipse.application=org.eclipse.jdt.ls.core.id1 -Dosgi.bundles.defaultStartLevel=4 -Declipse.product=org.eclipse.jdt.ls.core.product -Djava.import.generatesMetadataFilesAtProjectRoot=false -DDetectVMInstallationsJob.disabled=true -Dfile.encoding=utf8 -XX:+UseParallelGC -XX:GCTimeRatio=4 -XX:AdaptiveSizePolicyWeight=90 -Dsun.zip.disableMemoryMapping=true -Xmx2G -Xms100m -Xlog:disable -javaagent:c:\Users\Franc\.vscode\extensions\redhat.java-1.43.1-win32-x64\lombok\lombok-1.18.39-4050.jar -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=c:\Users\Franc\AppData\Roaming\Code\User\workspaceStorage\490bafe1ed33a01192a1a6a1ebb787f8\redhat.java -Daether.dependencyCollector.impl=bf -jar c:\Users\Franc\.vscode\extensions\redhat.java-1.43.1-win32-x64\server\plugins\org.eclipse.equinox.launcher_1.7.0.v20250519-0528.jar -configuration c:\Users\Franc\AppData\Roaming\Code\User\globalStorage\redhat.java\1.43.1\config_win -data c:\Users\Franc\AppData\Roaming\Code\User\workspaceStorage\490bafe1ed33a01192a1a6a1ebb787f8\redhat.java\jdt_ws --pipe=\\.\pipe\lsp-861818ea43ff4175bcaf6dc57d6c6ede-sock
    0	   222	 35624	     C:\flutter\bin\cache\dart-sdk\bin\dart.exe language-server --protocol=lsp --client-id=VS-Code --client-version=3.114.2
    0	     8	 40760	       C:\Windows\system32\conhost.exe 0x4
    0	   194	 37772	     ""C:\Program Files\Microsoft VS Code\Code.exe"" c:\Users\Franc\.vscode\extensions\streetsidesoftware.code-spell-checker-4.0.47\packages\_server\dist\main.cjs --node-ipc --clientProcessId=43308
    0	   176	 37820	     c:\Users\Franc\.vscode\extensions\ms-dotnettools.csharp-2.84.19-win32-x64\.roslyn\Microsoft.CodeAnalysis.LanguageServer.exe --logLevel Information --razorSourceGenerator c:\Users\Franc\.vscode\extensions\ms-dotnettools.csharp-2.84.19-win32-x64\.razor\Microsoft.CodeAnalysis.Razor.Compiler.dll --razorDesignTimePath c:\Users\Franc\.vscode\extensions\ms-dotnettools.csharp-2.84.19-win32-x64\.razor\Targets\Microsoft.NET.Sdk.Razor.DesignTime.targets --starredCompletionComponentPath c:\Users\Franc\.vscode\extensions\ms-dotnettools.vscodeintellicode-csharp-2.2.3-win32-x64\components\starred-suggestions\platforms\win32-x64\node_modules\@vsintellicode\starred-suggestions-csharp.win32-x64 --devKitDependencyPath c:\Users\Franc\.vscode\extensions\ms-dotnettools.csharp-2.84.19-win32-x64\.roslynDevKit\Microsoft.VisualStudio.LanguageServices.DevKit.dll --sessionId 9c6e6f77-b1c2-4a06-8ca7-79bd322930981751537349554 --extension c:\Users\Franc\.vscode\extensions\ms-dotnettools.csharp-2.84.19-win32-x64\.razorExtension\Microsoft.VisualStudioCode.RazorExtension.dll --extension c:\Users\Franc\.vscode\extensions\ms-dotnettools.csharp-2.84.19-win32-x64\.xamlTools\Microsoft.VisualStudio.DesignTools.CodeAnalysis.dll --extension c:\Users\Franc\.vscode\extensions\ms-dotnettools.csharp-2.84.19-win32-x64\.xamlTools\Microsoft.VisualStudio.DesignTools.CodeAnalysis.Diagnostics.dll --extension c:\Users\Franc\.vscode\extensions\ms-dotnettools.csdevkit-1.30.32-win32-x64\components\VisualStudio.Conversations\node_modules\@microsoft\visualstudio.copilot.roslyn.languageserver\Microsoft.VisualStudio.Copilot.Roslyn.LanguageServer.dll --telemetryLevel all --extensionLogDirectory c:\Users\Franc\AppData\Roaming\Code\logs\20250703T180909\window1\exthost\ms-dotnettools.csharp
    0	   178	 38196	     electron-nodejs (tailwindServer.js )
    0	   170	 40248	     ""C:\Program Files\Microsoft VS Code\Code.exe"" ""c:\Program Files\Microsoft VS Code\resources\app\extensions\html-language-features\server\dist\node\htmlServerMain"" --node-ipc --clientProcessId=43308
    0	     5	 41164	     electron-nodejs (config.js )
    0	     8	 39140	       C:\Windows\system32\conhost.exe 0x4
    0	   115	 40752	       electron-nodejs (config.js )
    0	    80	 24144	         ""c:\Users\Franc\.vscode\extensions\ms-dotnettools.csdevkit-1.30.32-win32-x64\components\vs-green-server\platforms\win32-x64\node_modules\@microsoft\visualstudio-controller.win32-x64/Microsoft.VisualStudio.Code.ServiceController"" 7919375c5234e736bacfd0dbe5be6b2bfd770af88d075af521bae0f9201a3050 /ControllerCooldownTimeout:30000 ""/TelemetrySession:{\""TelemetryLevel\"":\""all\"",\""IsOptedIn\"":false,\""HostName\"":\""Default\"",\""AppInsightsInstrumentationKey\"":null,\""AsimovInstrumentationKey\"":null,\""CollectorApiKey\"":\""0c6ae279ed8443289764825290e4f9e2-1a736e7c-1324-4338-be46-fc2a58ae4d14-7255\"",\""AppId\"":1010,\""UserId\"":\""07d975da-31c6-437e-96a9-d606d0352ac4\"",\""Id\"":\""9c6e6f77-b1c2-4a06-8ca7-79bd322930981751537349554\"",\""ProcessStartTime\"":133960115959400226,\""SkuName\"":null,\""VSExeVersion\"":null,\""BucketFiltersToEnableWatsonForFaults\"":[],\""BucketFiltersToAddDumpsToFaults\"":[]}""
    0	     8	 20744	           C:\Windows\system32\conhost.exe 0x4
    0	   106	 30880	           ""c:\Users\Franc\.vscode\extensions\ms-dotnettools.csdevkit-1.30.32-win32-x64\components\vs-green-server\platforms\win32-x64\node_modules\@microsoft\visualstudio-code-servicehost.win32-x64/Microsoft.VisualStudio.Code.ServiceHost.exe"" dotnet$C94B8CFE-E3FD-4BAF-A941-2866DBB566FE net.pipe://2414444F7DA3D81A288227B9325AD1C6513E4 ""/TelemetrySession:{\""TelemetryLevel\"":\""all\"",\""IsOptedIn\"":false,\""HostName\"":\""Default\"",\""AppInsightsInstrumentationKey\"":null,\""AsimovInstrumentationKey\"":null,\""CollectorApiKey\"":\""0c6ae279ed8443289764825290e4f9e2-1a736e7c-1324-4338-be46-fc2a58ae4d14-7255\"",\""AppId\"":1010,\""UserId\"":\""07d975da-31c6-437e-96a9-d606d0352ac4\"",\""Id\"":\""9c6e6f77-b1c2-4a06-8ca7-79bd322930981751537349554\"",\""ProcessStartTime\"":133960115959400226,\""SkuName\"":null,\""VSExeVersion\"":null,\""BucketFiltersToEnableWatsonForFaults\"":[],\""BucketFiltersToAddDumpsToFaults\"":[]}""
    0	     8	  4796	             C:\Windows\system32\conhost.exe 0x4
    0	    93	 34764	           ""c:\Users\Franc\.vscode\extensions\ms-dotnettools.csdevkit-1.30.32-win32-x64\components\vs-green-server\platforms\win32-x64\node_modules\@microsoft\visualstudio-code-servicehost.win32-x64\Microsoft.VisualStudio.Code.ServiceHost.exe"" dotnet.projectSystem$C94B8CFE-E3FD-4BAF-A941-2866DBB566FE net.pipe://2414444F7DA3D81A288227B9325AD1C6513E4 ""/TelemetrySession:{\""TelemetryLevel\"":\""all\"",\""IsOptedIn\"":false,\""HostName\"":\""Default\"",\""AppInsightsInstrumentationKey\"":null,\""AsimovInstrumentationKey\"":null,\""CollectorApiKey\"":\""0c6ae279ed8443289764825290e4f9e2-1a736e7c-1324-4338-be46-fc2a58ae4d14-7255\"",\""AppId\"":1010,\""UserId\"":\""07d975da-31c6-437e-96a9-d606d0352ac4\"",\""Id\"":\""9c6e6f77-b1c2-4a06-8ca7-79bd322930981751537349554\"",\""ProcessStartTime\"":133960115959400226,\""SkuName\"":null,\""VSExeVersion\"":null,\""BucketFiltersToEnableWatsonForFaults\"":[],\""BucketFiltersToAddDumpsToFaults\"":[]}""
    0	     8	 41564	             C:\Windows\system32\conhost.exe 0x4
    0	   103	 41416	     ""C:\Program Files\Microsoft VS Code\Code.exe"" ""c:\Program Files\Microsoft VS Code\resources\app\extensions\json-language-features\server\dist\node\jsonServerMain"" --node-ipc --clientProcessId=43308
    0	   343	 43196	     electron-nodejs (intelephense.js )
```

</details>
<details>
<summary>Workspace Info</summary>

```
|  Window (Running Extensions - madaamliit - Visual Studio Code)
|    Folder (madaamliit): 11330 files
|      File types: php(8340) md(283) json(171) stub(91) js(66) vue(54) css(39)
|                  rst(37) svg(34) jsx(28)
|      Conf files: dockerfile(7) tsconfig.json(2) tasks.json(1)
|                  package.json(1) jsconfig.json(1) makefile(1);
```

</details>
<details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805:30301674
binariesv615:30325510
c4g48928:30535728
azure-dev_surveyone:30548225
a9j8j154:30646983
962ge761:30959799
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dwnewjupyter:31046869
pythonrstrctxt:31112756
nativeloc2:31192216
5fd0e150:31155592
dwcopilot:31170013
6074i472:31201624
dwoutputs:31242946
customenabled:31248079
9064b325:31222308
copilot_t_ci:31333650
e5gg6876:31282496
pythoneinst12:31285622
bgtreat:31268568
4gafe986:31271826
c7cif404:31314491
pythonpulldiag:31335739
996jf627:31283433
pythonrdcb7:31303018
usemplatestapi:31297334
0aa6g176:31307128
7bj51361:31289155
747dc170:31275177
6518g693:31334701
aj953862:31281341
generatesymbolt:31295002
convertfstringf:31295003
9d2cg352:31339597
c4b42873:31341128
d7aab740:31338110
usemarketplace:31336439
nesew2to5:31336538
agentclaude:31335815
nes-diff-11:31337487
onetestforazureexp:31335613
6abeh943:31336334
yijiwantestdri0626-t:31336930

```

</details>

<!-- generated by issue reporter -->",2
lib new version not support 16kb pages in android,"### Issue:
We're using `libtensorflowlite_jni.so` (from `org.tensorflow:tensorflow-lite:2.17.0`) in an Android project.

We're targeting SDK 35+ on Android devices that **require 16 KB page size**. But when inspecting the `.so` using `readelf`, the memory segments are aligned to 4 KB (0x1000), not 16 KB (0x4000). As a result, we get the error: `Only 4 KB page compatible`.

### Question:
- Is there an officially supported build of `libtensorflowlite_jni.so` that supports 16 KB page size?
- If not, is there a CMake or Bazel flag to compile TensorFlow Lite with 16 KB alignment for Android?

### Info:
- TensorFlow Lite version: 2.17.0
- Target ABI: arm64-v8a
- Min SDK: 23
- NDK: r25+

### What we tried:
- Downloaded AAR from Maven Central
- Used `readelf -l libtensorflowlite_jni.so` to check LOAD alignment

",1
Remove reference to Google RBE api as is no longer open to customer,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

any

### Custom code

No

### OS platform and distribution

any

### Mobile device

any

### Python version

any

### Bazel version

any

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Remove from[ .bazelrc](https://github.com/tensorflow/tensorflow/blob/1db711bef04d6e04c624c8bee50a03faa7e68bf9/.bazelrc#L581) file the reference to google RBE api: `remotebuildexecution.googleapis.com` no longer open to the public.
Thanks

### Standalone code to reproduce the issue

```shell
remotebuildexecution.googleapis.com is no longer opened to the public
```

### Relevant log output

```shell

```",1
Fail to build libtensorflow_framework.so.2.20.0,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.19

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.1 LTS

### Mobile device

_No response_

### Python version

3.10

### Bazel version

7.4.1

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The following command is used to succeed to build bazel-bin/tensorflow/libtensorflow_framework.so.2.20.0
`bazel build //tensorflow:libtensorflow_framework.so.2.20.0`

but now got
`ERROR: Skipping '//tensorflow:libtensorflow_framework.so.2.20.0': no such target '//tensorflow:libtensorflow_framework.so.2.20.0': target 'libtensorflow_framework.so.2.20.0' not declared in package 'tensorflow' defined by /staging/tensorflow/tensorflow/BUILD
ERROR: no such target '//tensorflow:libtensorflow_framework.so.2.20.0': target 'libtensorflow_framework.so.2.20.0' not declared in package 'tensorflow' defined by /staging/tensorflow/tensorflow/BUILD`

The build command fails on
commit be1c6bfcf5ebbaf3efd06169256a4cc27ad01c57
    Integrate LLVM at llvm/llvm-project@696c0f92e0fe

The build command succeeds on (previous commit)
commit 76041619648de62e0d4ff89a1fdc8e6dfcd91708
    Integrate LLVM at llvm/llvm-project@842377882a3f


### Standalone code to reproduce the issue

```shell
bazel build //tensorflow:libtensorflow_framework.so.2.20.0
```

### Relevant log output

```shell

```",1
How do we silence noisy messages?,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

AlmaLinux9

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am seeing things like:

```
E0000 00:00:1751960796.835796  260759 check_numerics_op.cc:295] abnormal_detected_host @0x7fd6e3c05600 = {1, 0} Check if pdf output contains any NaNs of Infs
```

tens of lines. How do we silence this? It is getting in the way of debugging the code and it's pretty useless. At most I would print one line signalling that this problem was found n times.

### Standalone code to reproduce the issue

```shell
I am pretty sure a reproducer is not needed for this.
```

### Relevant log output

```shell

```",1
Support for 16KB page size on Android libs.tensorflow.lite.task.audio,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

0.4.4

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

There is a hard deadline from Google Play to support 16KB pages by [November 1st, 2025](https://developer.android.com/guide/practices/page-sizes#build). The latest version of this library 0.4.4 doesn't support 16KB page sizes. In future It's plan to upgrade with 16KB Page size? We need to resolve this one ASAP

The native library arm64-v8a/libtask_audio_jni.so (from org.tensorflow:tensorflow-lite-task-audio:0.4.4) is not 16 KB aligned

### Standalone code to reproduce the issue

```shell
There is a hard deadline from Google Play to support 16KB pages by November 1st, 2025. The latest version of this library 0.4.4 doesn't support 16KB page sizes. In future It's plan to upgrade with 16KB Page size? We need to resolve this one ASAP
```

### Relevant log output

```shell

```",1
Arch Linux x86 RTX 3050 source compilation Error in fail: tensorflow/core:stream_executor_headers_lib cannot depend on tensorflow/core:lib,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.19.0

### Custom code

No

### OS platform and distribution

Arch Linux - Linux linux 6.14.10-arch1-1 #1 SMP PREEMPT_DYNAMIC Wed, 04 Jun 2025 18:52:35 +0000 x86_64 GNU/Linux

### Mobile device

_No response_

### Python version

3.12

### Bazel version

6.5.0

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-smi: NVIDIA-SMI 575.57.08 - Driver Version: 575.57.08 - CUDA Version: 12.9 - nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2025 NVIDIA Corporation Built on Fri_Feb_21_20:23:50_PST_2025 Cuda compilation tools, release 12.8, V12.8.93 Build cuda_12.8.r12.8/compiler.35583870_0 - sudo pacman -Q cudnn:  cudnn 9.8.0.87-1

### GPU model and memory

nvidia-smi: NVIDIA GeForce RTX 3050 - 8192MiB

### Current behavior?

When compiling using these commands I get Error in fail: tensorflow/core:stream_executor_headers_lib cannot depend on tensorflow/core:lib
`
$ pip list | grep ""^nvidia-""
nvidia-cublas-cu12        12.8.3.14
nvidia-cuda-cupti-cu12    12.8.57
nvidia-cuda-nvcc-cu12     12.5.82
nvidia-cuda-nvrtc-cu12    12.8.61
nvidia-cuda-runtime-cu12  12.8.57
nvidia-cudnn-cu12         9.7.1.26
nvidia-cufft-cu12         11.3.3.41
nvidia-cufile-cu12        1.13.0.11
nvidia-curand-cu12        10.3.9.55
nvidia-cusolver-cu12      11.7.2.55
nvidia-cusparse-cu12      12.5.7.53
nvidia-cusparselt-cu12    0.6.3
nvidia-nccl-cu12          2.26.2
nvidia-nvjitlink-cu12     12.8.61
nvidia-nvtx-cu12          12.8.55

#compile from source https://www.tensorflow.org/install/source
$bazel --version
bazel 6.5.0

$clang --version
clang version 19.1.7
Target: x86_64-pc-linux-gnu
Thread model: posix
InstalledDir: /usr/bin


git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git tag -l ""*2.19.0*""
git checkout v2.19.0
pip install -U pip six numpy wheel setuptools mock future keras_applications keras_preprocessing
#CUDA hermetric version is 12.8.0
#CUDA compute capabilities 8.6
$nvidia-smi --query-gpu=compute_cap --format=csv
8.8

python ./configure.py

bazel build //tensorflow/tools/pip_package:wheel --repo_env=USE_PYWRAP_RULES=1 --repo_env=WHEEL_NAME=tensorflow --config=cuda --config=cuda_wheel

`

### Standalone code to reproduce the issue

```shell
when following mentioned instruction i get this error

`
Error:
DEBUG: /linux/.cache/bazel/_bazel_mnm/f45726776b68a67caa4bc0ae4cb18737/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:283:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_nvprune/linux-x86_64/cuda_nvprune-linux-x86_64-12.5.82-archive.tar.xz
ERROR: /linux/ai/ollama/tensorflow/tensorflow/compiler/tf2xla/ops/BUILD:49:21: in check_deps rule //tensorflow/compiler/tf2xla/ops:_xla_ops.so_check_deps: 
Traceback (most recent call last):
	File ""/linux/ai/ollama/tensorflow/tensorflow/tensorflow.bzl"", line 2275, column 21, in _check_deps_impl
		fail(
Error in fail: tensorflow/core:stream_executor_headers_lib cannot depend on tensorflow/core:lib. See: bazel query 'somepath(//tensorflow/core:stream_executor_headers_lib, //tensorflow/core:lib)'
ERROR: /linux/ai/ollama/tensorflow/tensorflow/compiler/tf2xla/ops/BUILD:49:21: Analysis of target '//tensorflow/compiler/tf2xla/ops:_xla_ops.so_check_deps' failed
ERROR: Analysis of target '//tensorflow/tools/pip_package:wheel' failed; build aborted: 
INFO: Elapsed time: 990.180s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (791 packages loaded, 55039 targets configured)
    Fetching repository @pypi_markupsafe; Running whl_library.ResolveRequirement(pypi_markupsafe, markupsafe==2.1.5)
    Fetching repository @pypi_pygments; Running whl_library.ResolveRequirement(pypi_pygments, pygments==2.18.0)
    Fetching repository @pypi_markdown_it_py; Running whl_library.ResolveRequirement(pypi_markdown_it_py, markdown-it-py==3.0.0)
`
```

### Relevant log output

```shell
$ bazel build //tensorflow/tools/pip_package:wheel --repo_env=USE_PYWRAP_RULES=1 --repo_env=WHEEL_NAME=tensorflow --config=cuda --config=cuda_wheel
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Reading 'startup' options from /linux/ai/ollama/tensorflow/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=237
INFO: Reading rc options for 'build' from /linux/ai/ollama/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /linux/ai/ollama/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /linux/ai/ollama/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/linux/ai/conda/envs/unsloth_env/bin/python --action_env PYTHON_LIB_PATH=/linux/ai/conda/envs/unsloth_env/lib/python3.12/site-packages --python_path=/linux/ai/conda/envs/unsloth_env/bin/python --action_env LD_LIBRARY_PATH=/opt/cuda/lib64::/opt/cuda/TensorRT-8.6.1.6/lib --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/bin/clang-19 --config=cuda_clang --action_env ANDROID_NDK_HOME=/linux/android/sdk/ndk/27.0.12077973 --action_env ANDROID_NDK_VERSION=27 --action_env ANDROID_NDK_API_LEVEL=25 --action_env ANDROID_BUILD_TOOLS_VERSION=34.0.0 --action_env ANDROID_SDK_API_LEVEL=34 --action_env ANDROID_SDK_HOME=/linux/android/sdk
INFO: Found applicable config definition build:short_logs in file /linux/ai/ollama/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /linux/ai/ollama/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda_clang in file /linux/ai/ollama/tensorflow/.bazelrc: --config=cuda --@local_config_cuda//:cuda_compiler=clang --copt=-Qunused-arguments --repo_env=HERMETIC_CUDA_COMPUTE_CAPABILITIES=sm_60,sm_70,sm_80,sm_89,compute_90 --copt=-Wno-unknown-cuda-version --host_linkopt=-fuse-ld=lld --host_linkopt=-lm --linkopt=-fuse-ld=lld --linkopt=-lm
INFO: Found applicable config definition build:cuda in file /linux/ai/ollama/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true
INFO: Found applicable config definition build:cuda in file /linux/ai/ollama/tensorflow/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=8.6
INFO: Found applicable config definition build:cuda_clang in file /linux/ai/ollama/tensorflow/.bazelrc: --config=cuda --@local_config_cuda//:cuda_compiler=clang --copt=-Qunused-arguments --repo_env=HERMETIC_CUDA_COMPUTE_CAPABILITIES=sm_60,sm_70,sm_80,sm_89,compute_90 --copt=-Wno-unknown-cuda-version --host_linkopt=-fuse-ld=lld --host_linkopt=-lm --linkopt=-fuse-ld=lld --linkopt=-lm
INFO: Found applicable config definition build:cuda in file /linux/ai/ollama/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true
INFO: Found applicable config definition build:cuda in file /linux/ai/ollama/tensorflow/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=8.6
INFO: Found applicable config definition build:cuda in file /linux/ai/ollama/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true
INFO: Found applicable config definition build:cuda in file /linux/ai/ollama/tensorflow/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=8.6
INFO: Found applicable config definition build:cuda_wheel in file /linux/ai/ollama/tensorflow/.bazelrc: --@local_config_cuda//cuda:include_cuda_libs=false
INFO: Found applicable config definition build:linux in file /linux/ai/ollama/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /linux/ai/ollama/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: /linux/.cache/bazel/_bazel_mnm/f45726776b68a67caa4bc0ae4cb18737/external/local_tsl/third_party/py/python_repo.bzl:154:14: 
HERMETIC_PYTHON_VERSION variable was not set correctly, using default version.
Python 3.12 will be used.
To select Python version, either set HERMETIC_PYTHON_VERSION env variable in
your shell:
  export HERMETIC_PYTHON_VERSION=3.12
OR pass it as an argument to bazel command directly or inside your .bazelrc
file:
  --repo_env=HERMETIC_PYTHON_VERSION=3.12
DEBUG: /linux/.cache/bazel/_bazel_mnm/f45726776b68a67caa4bc0ae4cb18737/external/local_tsl/third_party/py/python_repo.bzl:82:14: !!!Using pywrap rules instead of directly creating .so objects!!!
DEBUG: /linux/.cache/bazel/_bazel_mnm/f45726776b68a67caa4bc0ae4cb18737/external/local_tsl/third_party/py/python_repo.bzl:87:10: 
=============================
Hermetic Python configuration:
Version: ""3.12""
Kind: """"
Interpreter: ""default"" (provided by rules_python)
Requirements_lock label: ""@python_version_repo//:requirements_lock_3_12.txt""
=====================================
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
DEBUG: /linux/.cache/bazel/_bazel_mnm/f45726776b68a67caa4bc0ae4cb18737/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:283:10: Downloading and extracting https://developer.download.nvidia.com/compute/cudnn/redist/cudnn/linux-x86_64/cudnn-linux-x86_64-9.3.0.75_cuda12-archive.tar.xz
DEBUG: /linux/.cache/bazel/_bazel_mnm/f45726776b68a67caa4bc0ae4cb18737/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:283:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_cudart/linux-x86_64/cuda_cudart-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /linux/.cache/bazel/_bazel_mnm/f45726776b68a67caa4bc0ae4cb18737/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:283:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcusolver/linux-x86_64/libcusolver-linux-x86_64-11.6.3.83-archive.tar.xz
DEBUG: /linux/.cache/bazel/_bazel_mnm/f45726776b68a67caa4bc0ae4cb18737/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:283:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcusparse/linux-x86_64/libcusparse-linux-x86_64-12.5.1.3-archive.tar.xz
DEBUG: /linux/.cache/bazel/_bazel_mnm/f45726776b68a67caa4bc0ae4cb18737/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:283:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_cccl/linux-x86_64/cuda_cccl-linux-x86_64-12.5.39-archive.tar.xz
DEBUG: /linux/.cache/bazel/_bazel_mnm/f45726776b68a67caa4bc0ae4cb18737/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:283:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcurand/linux-x86_64/libcurand-linux-x86_64-10.3.6.82-archive.tar.xz
DEBUG: /linux/.cache/bazel/_bazel_mnm/f45726776b68a67caa4bc0ae4cb18737/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:283:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcufft/linux-x86_64/libcufft-linux-x86_64-11.2.3.61-archive.tar.xz
DEBUG: /linux/.cache/bazel/_bazel_mnm/f45726776b68a67caa4bc0ae4cb18737/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:283:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_nvcc/linux-x86_64/cuda_nvcc-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /linux/.cache/bazel/_bazel_mnm/f45726776b68a67caa4bc0ae4cb18737/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:283:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libnvjitlink/linux-x86_64/libnvjitlink-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /linux/.cache/bazel/_bazel_mnm/f45726776b68a67caa4bc0ae4cb18737/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:283:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_nvml_dev/linux-x86_64/cuda_nvml_dev-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /linux/.cache/bazel/_bazel_mnm/f45726776b68a67caa4bc0ae4cb18737/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:283:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_nvtx/linux-x86_64/cuda_nvtx-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /linux/.cache/bazel/_bazel_mnm/f45726776b68a67caa4bc0ae4cb18737/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:283:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcublas/linux-x86_64/libcublas-linux-x86_64-12.5.3.2-archive.tar.xz
DEBUG: /linux/.cache/bazel/_bazel_mnm/f45726776b68a67caa4bc0ae4cb18737/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:283:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_cupti/linux-x86_64/cuda_cupti-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /linux/.cache/bazel/_bazel_mnm/f45726776b68a67caa4bc0ae4cb18737/external/local_tsl/third_party/nccl/hermetic/nccl_redist_init_repository.bzl:85:10: Downloading and extracting https://files.pythonhosted.org/packages/ed/1f/6482380ec8dcec4894e7503490fc536d846b0d59694acad9cf99f27d0e7d/nvidia_nccl_cu12-2.23.4-py3-none-manylinux2014_x86_64.whl
DEBUG: /linux/.cache/bazel/_bazel_mnm/f45726776b68a67caa4bc0ae4cb18737/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:283:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_nvprune/linux-x86_64/cuda_nvprune-linux-x86_64-12.5.82-archive.tar.xz
ERROR: /linux/ai/ollama/tensorflow/tensorflow/compiler/tf2xla/ops/BUILD:49:21: in check_deps rule //tensorflow/compiler/tf2xla/ops:_xla_ops.so_check_deps: 
Traceback (most recent call last):
	File ""/linux/ai/ollama/tensorflow/tensorflow/tensorflow.bzl"", line 2275, column 21, in _check_deps_impl
		fail(
Error in fail: tensorflow/core:stream_executor_headers_lib cannot depend on tensorflow/core:lib. See: bazel query 'somepath(//tensorflow/core:stream_executor_headers_lib, //tensorflow/core:lib)'
ERROR: /linux/ai/ollama/tensorflow/tensorflow/compiler/tf2xla/ops/BUILD:49:21: Analysis of target '//tensorflow/compiler/tf2xla/ops:_xla_ops.so_check_deps' failed
ERROR: Analysis of target '//tensorflow/tools/pip_package:wheel' failed; build aborted: 
INFO: Elapsed time: 990.180s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (791 packages loaded, 55039 targets configured)
    Fetching repository @pypi_markupsafe; Running whl_library.ResolveRequirement(pypi_markupsafe, markupsafe==2.1.5)
    Fetching repository @pypi_pygments; Running whl_library.ResolveRequirement(pypi_pygments, pygments==2.18.0)
    Fetching repository @pypi_markdown_it_py; Running whl_library.ResolveRequirement(pypi_markdown_it_py, markdown-it-py==3.0.0)
```",1
CUDA-enabled build of v2.19.0 fails on ARM: analysis of target failed: tf2xla/ops:_xla_ops.so_check_deps: tensorflow/core:stream_executor_headers_lib cannot depend on tensorflow/core:lib,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.19

### Custom code

No

### OS platform and distribution

Linux Ubuntu 24.04 on ARM

### Mobile device

_No response_

### Python version

3.11.13

### Bazel version

6.5.0

### GCC/compiler version

Clang 18.1.8

### CUDA/cuDNN version

12.8.0, 9.7.0

### GPU model and memory

sm_75, sm_80, compute_90

### Current behavior?

CUDA-enabled build fails for TensorFlow v2.19.0 on ARM

### Standalone code to reproduce the issue

```shell
$> git clone https://github.com/tensorflow/tensorflow
$> git -C tensorflow checkout v2.19.0
$> git -c user.name=user -c user.email=email -C tensorflow am < /0001-h5py-upgrade-version-in-requirements_lock-for-python.patch
$> cd tensorflow
$tensorflow> export BAZEL_COMPILER=/usr/bin/clang
$tensorflow> export CC_OPT_FLAGS=""-march=armv8-a -mtune=generic -Wno-sign-compare""
$tensorflow> export CC=/usr/bin/clang
$tensorflow> export CLANG_CUDA_COMPILER_PATH=""/usr/lib/llvm-18/bin/clang""
$tensorflow> export HERMETIC_CUDA_COMPUTE_CAPABILITIES=""sm_75,sm_80,compute_90""
$tensorflow> export HERMETIC_CUDA_VERSION=""12.8.0""
$tensorflow> export HERMETIC_CUDNN_VERSION=""9.7.0""
$tensorflow> export PYTHON_BIN_PATH=""/py3.11/bin/python3""
$tensorflow> export PYTHON_LIB_PATH=""/py3.11/lib/python3.11/site-packages""
$tensorflow> export TF_NEED_CUDA=1
$tensorflow> export TF_NEED_ROCM=0
$tensorflow> yes """" | ./configure

$tensorflow> bazel build //tensorflow/tools/pip_package:wheel \
 --repo_env=USE_PYWRAP_RULES=1 \
 --repo_env=WHEEL_NAME=tensorflow \
 --config=opt \
 --config=cuda \
 --config=cuda_wheel
...
error shown below


The h5py patch `0001-h5py-upgrade-version-in-requirements_lock-for-python.patch` is here:
* https://github.com/tensorflow/tensorflow/pull/94289

From 4927a1d42647e68ac8005760e0d924d2161cf1ce Mon Sep 17 00:00:00 2001
From: eugeneswalker <eugenesunsetwalker@gmail.com>
Date: Wed, 2 Jul 2025 07:11:15 -0700
Subject: [PATCH] h5py: upgrade version in requirements_lock for python 3.11

---
 requirements_lock_3_11.txt | 49 +++++++++++++++++++++-----------------
 1 file changed, 27 insertions(+), 22 deletions(-)

diff --git a/requirements_lock_3_11.txt b/requirements_lock_3_11.txt
index bf067084003..b305e7e58f8 100644
--- a/requirements_lock_3_11.txt
+++ b/requirements_lock_3_11.txt
@@ -185,28 +185,33 @@ grpcio==1.64.1 \
     # via
     #   -r ci/official/requirements_updater/requirements.in
     #   tensorboard
-h5py==3.11.0 \
-    --hash=sha256:083e0329ae534a264940d6513f47f5ada617da536d8dccbafc3026aefc33c90e \
-    --hash=sha256:1625fd24ad6cfc9c1ccd44a66dac2396e7ee74940776792772819fc69f3a3731 \
-    --hash=sha256:21dbdc5343f53b2e25404673c4f00a3335aef25521bd5fa8c707ec3833934892 \
-    --hash=sha256:52c416f8eb0daae39dabe71415cb531f95dce2d81e1f61a74537a50c63b28ab3 \
-    --hash=sha256:55106b04e2c83dfb73dc8732e9abad69d83a436b5b82b773481d95d17b9685e1 \
-    --hash=sha256:67462d0669f8f5459529de179f7771bd697389fcb3faab54d63bf788599a48ea \
-    --hash=sha256:6c4b760082626120031d7902cd983d8c1f424cdba2809f1067511ef283629d4b \
-    --hash=sha256:731839240c59ba219d4cb3bc5880d438248533366f102402cfa0621b71796b62 \
-    --hash=sha256:754c0c2e373d13d6309f408325343b642eb0f40f1a6ad21779cfa9502209e150 \
-    --hash=sha256:75bd7b3d93fbeee40860fd70cdc88df4464e06b70a5ad9ce1446f5f32eb84007 \
-    --hash=sha256:77b19a40788e3e362b54af4dcf9e6fde59ca016db2c61360aa30b47c7b7cef00 \
-    --hash=sha256:7b7e8f78072a2edec87c9836f25f34203fd492a4475709a18b417a33cfb21fa9 \
-    --hash=sha256:8ec9df3dd2018904c4cc06331951e274f3f3fd091e6d6cc350aaa90fa9b42a76 \
-    --hash=sha256:a76cae64080210389a571c7d13c94a1a6cf8cb75153044fd1f822a962c97aeab \
-    --hash=sha256:aa6ae84a14103e8dc19266ef4c3e5d7c00b68f21d07f2966f0ca7bdb6c2761fb \
-    --hash=sha256:bbd732a08187a9e2a6ecf9e8af713f1d68256ee0f7c8b652a32795670fb481ba \
-    --hash=sha256:c072655ad1d5fe9ef462445d3e77a8166cbfa5e599045f8aa3c19b75315f10e5 \
-    --hash=sha256:d9c944d364688f827dc889cf83f1fca311caf4fa50b19f009d1f2b525edd33a3 \
-    --hash=sha256:ef4e2f338fc763f50a8113890f455e1a70acd42a4d083370ceb80c463d803972 \
-    --hash=sha256:f3736fe21da2b7d8a13fe8fe415f1272d2a1ccdeff4849c1421d2fb30fd533bc \
-    --hash=sha256:f4e025e852754ca833401777c25888acb96889ee2c27e7e629a19aee288833f0
+h5py==3.13.0 \
+    --hash=sha256:10894c55d46df502d82a7a4ed38f9c3fdbcb93efb42e25d275193e093071fade \
+    --hash=sha256:1870e46518720023da85d0895a1960ff2ce398c5671eac3b1a41ec696b7105c3 \
+    --hash=sha256:21daf38171753899b5905f3d82c99b0b1ec2cbbe282a037cad431feb620e62ec \
+    --hash=sha256:22ffe2a25770a2d67213a1b94f58006c14dce06933a42d2aaa0318c5868d1508 \
+    --hash=sha256:337af114616f3656da0c83b68fcf53ecd9ce9989a700b0883a6e7c483c3235d4 \
+    --hash=sha256:357e6dc20b101a805ccfd0024731fbaf6e8718c18c09baf3b5e4e9d198d13fca \
+    --hash=sha256:477c58307b6b9a2509c59c57811afb9f598aedede24a67da808262dfa0ee37b4 \
+    --hash=sha256:4f97ecde7ac6513b21cd95efdfc38dc6d19f96f6ca6f2a30550e94e551458e0a \
+    --hash=sha256:5540daee2b236d9569c950b417f13fd112d51d78b4c43012de05774908dff3f5 \
+    --hash=sha256:560e71220dc92dfa254b10a4dcb12d56b574d2d87e095db20466b32a93fec3f9 \
+    --hash=sha256:56dd172d862e850823c4af02dc4ddbc308f042b85472ffdaca67f1598dff4a57 \
+    --hash=sha256:57c4c74f627c616f02b7aec608a8c706fe08cb5b0ba7c08555a4eb1dde20805a \
+    --hash=sha256:782ff0ac39f455f21fd1c8ebc007328f65f43d56718a89327eec76677ebf238a \
+    --hash=sha256:82690e89c72b85addf4fc4d5058fb1e387b6c14eb063b0b879bf3f42c3b93c35 \
+    --hash=sha256:851ae3a8563d87a5a0dc49c2e2529c75b8842582ccaefbf84297d2cfceeacd61 \
+    --hash=sha256:8a8e38ef4ceb969f832cc230c0cf808c613cc47e31e768fd7b1106c55afa1cb8 \
+    --hash=sha256:9c82ece71ed1c2b807b6628e3933bc6eae57ea21dac207dca3470e3ceaaf437c \
+    --hash=sha256:be949b46b7388074c5acae017fbbe3e5ba303fd9daaa52157fdfef30bbdacadd \
+    --hash=sha256:c10f061764d8dce0a9592ce08bfd5f243a00703325c388f1086037e5d619c5f1 \
+    --hash=sha256:d2cf6a231a07c14acd504a945a6e9ec115e0007f675bde5e0de30a4dc8d86a31 \
+    --hash=sha256:d571644958c5e19a61c793d8d23cd02479572da828e333498c9acc463f4a3997 \
+    --hash=sha256:d6f13f9b5ce549448c01e4dfe08ea8d1772e6078799af2c1c8d09e941230a90d \
+    --hash=sha256:e520ec76de00943dd017c8ea3f354fa1d2f542eac994811943a8faedf2a7d5cb \
+    --hash=sha256:e79d8368cd9295045956bfb436656bea3f915beaa11d342e9f79f129f5178763 \
+    --hash=sha256:f35640e81b03c02a88b8bf99fb6a9d3023cc52f7c627694db2f379e0028f2868 \
+    --hash=sha256:fb267ce4b83f9c42560e9ff4d30f60f7ae492eacf9c7ede849edf8c1b860e16b
     # via
     #   -r ci/official/requirements_updater/requirements.in
     #   keras
--
2.43.0
```

### Relevant log output

```shell
...
=============================
Hermetic Python configuration:
Version: ""3.11""
Kind: """"
Interpreter: ""default"" (provided by rules_python)
Requirements_lock label: ""@python_version_repo//:requirements_lock_3_11.txt""
=====================================
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
ERROR: /tensorflow/tensorflow/compiler/tf2xla/ops/BUILD:49:21: in check_deps rule //tensorflow/compiler/tf2xla/ops:_xla_ops.so_check_deps:
Traceback (most recent call last):
	File ""/tensorflow/tensorflow/tensorflow.bzl"", line 2275, column 21, in _check_deps_impl
		fail(
Error in fail: tensorflow/core:stream_executor_headers_lib cannot depend on tensorflow/core:lib. See: bazel query 'somepath(//tensorflow/core:stream_executor_headers_lib, //tensorflow/core:lib)'
ERROR: /tensorflow/tensorflow/compiler/tf2xla/ops/BUILD:49:21: Analysis of target '//tensorflow/compiler/tf2xla/ops:_xla_ops.so_check_deps' failed
ERROR: Analysis of target '//tensorflow/tools/pip_package:wheel' failed; build aborted:
INFO: Elapsed time: 4.463s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (496 packages loaded, 29712 targets configured)
```",1
I am getting an exception on windows (Python version 3.11.4) that seems like a compatibility issue.,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tensorflow-2.19.0-cp311-cp311-win_amd64.whl.metadata 

### Custom code

Yes

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.11.4

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I get an exception that seems unrelated to the code that is pasted below.

### Standalone code to reproduce the issue

```shell
import pandas as pd
import tensorflow as tf
import numpy as np

# Load data from CSV
data = pd.read_csv('sentiment.csv')

# Text data and labels
texts = data['text'].tolist()
labels = data['sentiment'].values

# Manual tokenization
word_index = {}
sequences = []
for text in texts:
    words = text.lower().split()
    sequence = []
    for word in words:
        if word not in word_index:
            word_index[word] = len(word_index) + 1
        sequence.append(word_index[word])
    sequences.append(sequence)

# Padding sequences
max_length = max(len(sequence) for sequence in sequences)
padded_sequences = []
for sequence in sequences:
    padded_sequence = sequence[:max_length] + [0] * (max_length - len(sequence))
    padded_sequences.append(padded_sequence)

# Convert to numpy array
padded_sequences = np.array(padded_sequences)

# Model architecture
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(len(word_index) + 1, 16, input_length=max_length),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(3, activation='softmax')
])

# Compile the model
model.compile(loss='sparse_categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# Train the model
model.fit(padded_sequences, labels, epochs=15, verbose=1)


# Test the model
test_texts = [""The price was too high for the quality"", ""The interface is user-friendly"", ""I'm satisfied""]
test_sequences = []
for text in test_texts:
    words = text.lower().split()
    sequence = []
    for word in words:
        if word in word_index:
            sequence.append(word_index[word])
    test_sequences.append(sequence)

# Padding test sequences
padded_test_sequences = []
for sequence in test_sequences:
    padded_sequence = sequence[:max_length] + [0] * (max_length - len(sequence))
    padded_test_sequences.append(padded_sequence)

# Convert to numpy array
padded_test_sequences = np.array(padded_test_sequences)

# Make predictions
predictions = model.predict(padded_test_sequences)

# Print predicted sentiments
for i, text in enumerate(test_texts):
    print(f""Text: {text}, Predicted Sentiment: {np.argmax(predictions[i])}"")


# Evaluate the model
evaluation = model.evaluate(padded_sequences, labels, verbose=0)

# Extract loss and accuracy
loss = evaluation[0]
accuracy = evaluation[1]

# Print loss and accuracy
print(""Loss:"", loss)
print(""Accuracy:"", accuracy)
```

### Relevant log output

```shell
Traceback (most recent call last):
  File ""C:\Users\karth\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 73, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\karth\OneDrive - AMETEK Inc\LearnPython\DeepLearning\SentimentAnalaysisModel\sentiment analysis.py"", line 2, in <module>
    import tensorflow as tf
  File ""C:\Users\karth\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow\__init__.py"", line 40, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\karth\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 88, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Users\karth\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 73, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```",1
The native library arm64-v8a/libtask_audio_jni.so (from org.tensorflow:tensorflow-lite-task-audio:0.4.4) is not 16 KB aligned,"Currently, I am using **org.tensorflow:tensorflow-lite-task-audio:0.4.4** in my Android project. The lint check show that _**The native library arm64-v8a/libtask_audio_jni.so (from org.tensorflow:tensorflow-lite-task-audio:0.4.4) is not 16 KB aligned**_.

Any future update to clear this lint check.
Thank you very much.",0
Inefficient 2D convolution compared to PyTorch,"### Issue type

Performance

### TensorFlow version

2.19.0

### Current behavior?

When benchmarking 2D depthwise convolutions on an NVIDIA H200, I observed that TensorFlow’s implementation is noticeably slower and consumes more power compared to PyTorch.

Using a kernel-level profiler, I found that TensorFlow utilizes DepthwiseConv2dGPUKernelNHWC, which takes approximately 6.8ms per iteration in the following test case, while PyTorch uses [`conv_depthwise2d_forward_kernel`](https://github.com/pytorch/pytorch/blob/64f2ec77f869a7d495694519fb482e9ecaaa6da1/aten/src/ATen/native/cuda/DepthwiseConv2d.cu#L140-L213). The former takes 6.8ms per iteration in the following code, which only takes 4.8ms under the same conditions.

Given this discrepancy, I believe it may be helpful to review PyTorch’s implementation as a potential reference for improving TensorFlow’s kernel efficiency.

### Standalone code to reproduce the issue

```shell
import time
import tensorflow as tf
import numpy as np
import torch


def test_tf():
    print(""=== TensorFlow Depthwise Convolution Test ==="")
    print(f""TensorFlow version: {tf.__version__}"")
    print(f""GPU available: {tf.config.list_physical_devices('GPU')}"")
    print()

    dim = 256
    batch_size = 128
    height = width = 124
    num_iterations = 50

    dwconv_tf = tf.keras.layers.DepthwiseConv2D(
        kernel_size=(3, 3),
        strides=(1, 1),
        padding='same',
        depth_multiplier=1,
        use_bias=True
    )

    np_input = np.random.rand(batch_size, dim, height, width).astype('float32')
    tf_input = tf.convert_to_tensor(np.transpose(np_input, (0, 2, 3, 1)), dtype=tf.float32)
    
    print(f""Input shape: {tf_input.shape}"")
    print(f""Input device: {tf_input.device}"")

    print(""Building layer..."")
    _ = dwconv_tf(tf_input)

    print(f""\nWarming up (10 iterations)..."")
    warmup_start = time.time()
    for _ in range(10):
        _ = dwconv_tf(tf_input)
    warmup_time = time.time() - warmup_start
    print(f""Warmup time: {warmup_time:.3f}s"")

    print(f""\nRunning {num_iterations} iterations..."")
    start_time = time.time()
    
    for _ in range(num_iterations):
        with torch.profiler.record_function(""tf_op""):
            output = dwconv_tf(tf_input)

    total_time = time.time() - start_time

    print(f""\nOutput shape: {output.shape}"")
    print(f""\nResults:"")
    print(f""Total time: {total_time:.3f}s"")
    print(f""Time per iteration: {total_time/num_iterations*1000:.2f}ms"")
    print(f""Throughput: {num_iterations/total_time:.2f} iterations/s"")


def test_torch():
    print(""=== PyTorch Depthwise Convolution Test ==="")
    print(f""PyTorch version: {torch.__version__}"")
    print(f""CUDA available: {torch.cuda.is_available()}"")
    if torch.cuda.is_available():
        print(f""CUDA device: {torch.cuda.get_device_name(0)}"")
        print(f""CUDA version: {torch.version.cuda}"")
    print()

    dim = 256
    batch_size = 128
    height = width = 124
    num_iterations = 50

    model = torch.nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim).to('cuda')
    torch_input = torch.randn(batch_size, dim, height, width).to('cuda')
        
    print(f""Input shape: {torch_input.shape}"")
    print(f""Input device: {torch_input.device}"")
        
    model.eval()
        
    print(f""\nWarming up (10 iterations)..."")
    warmup_start = time.time()
    with torch.no_grad():
        for _ in range(10):
            output = model(torch_input)
    warmup_time = time.time() - warmup_start
    print(f""Warmup time: {warmup_time:.3f}s"")
        
    print(f""\nRunning {num_iterations} iterations..."")
    start_time = time.time()
        
    with torch.no_grad():
        for _ in range(num_iterations):
            output = model(torch_input)
        
    total_time = time.time() - start_time
        
    print(f""\nOutput shape: {output.shape}"")
    print(f""Output device: {output.device}"")
        
    print(f""\nResults:"")
    print(f""Total time: {total_time:.3f}s"")
    print(f""Time per iteration: {total_time/num_iterations*1000:.2f}ms"")
    print(f""Throughput: {num_iterations/total_time:.2f} iterations/s"")


if __name__ == ""__main__"":
    with torch.profiler.profile(
        activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],
    ) as prof, tf.device('/GPU:0'):
        test_tf()
        test_torch()
    prof.export_chrome_trace(""profile_tf.json"")
```
",1
crash when inference use libtensorflowlite_c.so and config threadnum >1  backend cpu,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

latest

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

dlopen libtensorflowlite_c.so，inference backend cpu and set threadnum=4 .
the crash as blow is low probability and happen when run more than 100 loops.  
and i set the threads to 0 and it is working and have not crash. or this way have not crash that don't use dlopen and dynamic link libtensorflowlite_c.so in .mk.


### Standalone code to reproduce the issue

```shell
dlopen libtensorflowlite_c.so，inference backend cpu and set threadnum=4 .
the crash as blow is low probability and happen when run more than 100 loops.  
and i set the threads to 0 and it is working and have not crash. or this way have not crash that don't use dlopen and dynamic link libtensorflowlite_c.so in .mk.
```

### Relevant log output

```shell
C35033A 01-04 05:29:57.710 498 12173 F libc : /buildbot/src/android/ndk-release-r18/external/libcxx/…/…/external/libcxxabi/src/abort_message.cpp:73: abort_message: assertion “cannot create thread specific key for __cxa_get_globals()” failed
C350342 01-04 05:29:57.712 498 12173 F libc : Fatal signal 6 (SIGABRT), code -1 (SI_QUEUE) in tid 12173 (Blur), pid 498 (provider@2.4-se)
C350630 01-04 05:29:57.860 12179 12179 F DEBUG : *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
C350631 01-04 05:29:57.860 12179 12179 F DEBUG : Native Crash TIME: 16201049
C350632 01-04 05:29:57.860 12179 12179 F DEBUG : *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
C350633 01-04 05:29:57.860 12179 12179 F DEBUG : Build fingerprint: ‘realme/RMP2105/RE87CCL1:11/RP1A.201005.001/1640606145:userdebug/test-keys’
C350634 01-04 05:29:57.861 12179 12179 F DEBUG : Revision: ‘0’
C350635 01-04 05:29:57.861 12179 12179 F DEBUG : ABI: ‘arm’
C350636 01-04 05:29:57.861 12179 12179 F DEBUG : Timestamp: 2022-01-04 05:29:57+0800
C350637 01-04 05:29:57.861 12179 12179 F DEBUG : pid: 498, tid: 12173, name: Blur >>> /vendor/bin/hw/android.hardware.camera.provider@2.4-service <<<
C350638 01-04 05:29:57.861 12179 12179 F DEBUG : uid: 1047
C350639 01-04 05:29:57.861 12179 12179 F DEBUG : signal 6 (SIGABRT), code -1 (SI_QUEUE), fault addr --------
C35063A 01-04 05:29:57.861 12179 12179 F DEBUG : Abort message: ‘/buildbot/src/android/ndk-release-r18/external/libcxx/…/…/external/libcxxabi/src/abort_message.cpp:73: abort_message: assertion “cannot create thread specific key for __cxa_get_globals()” failed’
C35063B 01-04 05:29:57.861 12179 12179 F DEBUG : r0 00000000 r1 00002f8d r2 00000006 r3 cc06d010
C35063C 01-04 05:29:57.861 12179 12179 F DEBUG : r4 cc06d024 r5 cc06d008 r6 000001f2 r7 0000016b
C35063D 01-04 05:29:57.861 12179 12179 F DEBUG : r8 cc06d010 r9 cc06d020 r10 cc06d040 r11 cc06d030
C35063E 01-04 05:29:57.861 12179 12179 F DEBUG : ip 00002f8d sp cc06cfe0 lr ee9087cd pc ee9087e0
C35064A 01-04 05:29:57.867 12179 12179 F DEBUG : backtrace:
C35064B 01-04 05:29:57.867 12179 12179 F DEBUG : #00 pc 000387e0 /apex/com.android.runtime/lib/bionic/libc.so (abort+172) (BuildId: 724f04e3eb055a58d7517ffbc7210561)
C35064C 01-04 05:29:57.867 12179 12179 F DEBUG : #01 pc 00038a87 /apex/com.android.runtime/lib/bionic/libc.so (__assert2+22) (BuildId: 724f04e3eb055a58d7517ffbc7210561)
C35064D 01-04 05:29:57.867 12179 12179 F DEBUG : #02 pc 001fde95 /vendor/lib/libtensorflowlite_c.so
C35064E 01-04 05:29:57.867 12179 12179 F DEBUG : #03 pc 001fc471 /vendor/lib/libtensorflowlite_c.so
C35064F 01-04 05:29:57.867 12179 12179 F DEBUG : #04 pc 00081cb5 /apex/com.android.runtime/lib/bionic/libc.so (pthread_once+76) (BuildId: 724f04e3eb055a58d7517ffbc7210561)
C350650 01-04 05:29:57.867 12179 12179 F DEBUG : #05 pc 001fc433 /vendor/lib/libtensorflowlite_c.so
C350651 01-04 05:29:57.867 12179 12179 F DEBUG : #06 pc 001fc3e1 /vendor/lib/libtensorflowlite_c.so
C350652 01-04 05:29:57.867 12179 12179 F DEBUG : #07 pc 001fc15b /vendor/lib/libtensorflowlite_c.so
C350653 01-04 05:29:57.867 12179 12179 F DEBUG : #08 pc 001fbf05 /vendor/lib/libtensorflowlite_c.so
C350654 01-04 05:29:57.867 12179 12179 F DEBUG : #09 pc 001fbfe5 /vendor/lib/libtensorflowlite_c.so
C350655 01-04 05:29:57.867 12179 12179 F DEBUG : #10 pc 001fbfa3 /vendor/lib/libtensorflowlite_c.so
C350656 01-04 05:29:57.867 12179 12179 F DEBUG : #11 pc 000fd855 /vendor/lib/libtensorflowlite_c.so
C350657 01-04 05:29:57.867 12179 12179 F DEBUG : #12 pc 00080973 /apex/com.android.runtime/lib/bionic/libc.so (__pthread_start(void*)+40) (BuildId: 724f04e3eb055a58d7517ffbc7210561)
C350658 01-04 05:29:57.867 12179 12179 F DEBUG : #13 pc 00039ce3 /apex/com.android.runtime/lib/bionic/libc.so (__start_thread+30) (BuildId: 724f04e3eb055a58d7517ffbc7210561)
```",2
"how to get profile of per operation that delegate gpu opencl like  cpu enable_op_profiling  result, rather than only ModifyGraphWithDelegate?","### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

latest or 2.13

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

how to get profile of per operation that delegate gpu opencl like  cpu enable_op_profiling  result, rather than only ModifyGraphWithDelegate?

### Standalone code to reproduce the issue

```shell
how to get profile of per operation that delegate gpu opencl like  cpu enable_op_profiling  result, rather than only ModifyGraphWithDelegate?
```

### Relevant log output

```shell

```",1
Multiple segmentation faults and aborted in some modules,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v1.12.1-127710-gd3d06fc4b99 2.20.0-dev20250623

### Custom code

Yes

### OS platform and distribution

Linux Fedora 42

### Mobile device

Linux Fedora 42

### Python version

3.10.16

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CPU

### GPU model and memory

_No response_

### Current behavior?

Hi,

For a research paper, we carried out a large-scale benchmark of [Pynguin](https://www.pynguin.eu/), an Automatic Unit Test Generation Tool for Python, to test its new feature that can find Python interpreter crashes. In this benchmark, we found more than 20 potential bugs in tensorflow, and we are making this issue to report them.

If possible, we would also be interested if you could answer these few questions:

- In our approach, we have considered that tensorflow's public API is the functions and classes that are public (meaning that they don't start with a _) and that are exposed by public modules (meaning that they don't start with a _ either). Does this correspond to the real tensorflow public API? If not, would you be interested in having potential bugs reported if they are not in the real public API but still in accessible functions (as we have done now)?
- In your opinion, is it desirable to have public functions that cause the interpreter to crash, or should these be rare exceptions?
- In your issue template, you ask for minimal examples, but would the tests directly generated by Pynguin (those named as raw in the repo) be enough?

Thank you in advance!

(Sorry for making only one issue, there were so many different causes, and I didn't want to spam by splitting it into 20 smaller issues.)

### Standalone code to reproduce the issue

The different tests that crash are available here: https://github.com/BergLucas/tensorflow-replication/tree/pynguin-subprocess-paper

### Relevant log output

```shell

```",2
Unable to use Tensorflow and Tensorflow-TPU on Google Colab,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18 and 2.19

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.11.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Google Clob V2-8 tpu runtime does not come with tensorflow installed. Installing tensorflow 2.18 and tensorflow-tpu 2.18, does not detect tpu.



### Standalone code to reproduce the issue

```shell
!pip install -qq tensorflow==2.18.0
!pip install -qq tensorflow==2.18.0rc0 #--find-links=https://storage.googleapis.com/libtpu-tf-releases/index.html

import tensorflow as tf
from tensorflow import keras

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local')#tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.TPUStrategy(resolver)
```

### Relevant log output

```shell
InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [tpu_embedding_config="""", compilation_failure_closes_chips=false, embedding_config="""", tpu_cancellation_closes_chips=2, enable_whole_mesh_compilations=false, is_global_init=false]
Registered devices: [CPU]
Registered kernels:
  <no registered kernels>

	 [[ConfigureDistributedTPU]] [Op:__inference__tpu_init_fn_8]

During handling of the above exception, another exception occurred:

NotFoundError                             Traceback (most recent call last)
[/usr/local/lib/python3.11/dist-packages/tensorflow/python/tpu/tpu_strategy_util.py](https://localhost:8080/#) in initialize_tpu_system_impl(cluster_resolver, tpu_cluster_resolver_cls)
    140       context.async_wait()
    141     except errors.InvalidArgumentError as e:
--> 142       raise errors.NotFoundError(
    143           None, None,
    144           ""TPUs not found in the cluster. Failed in initialization: ""

NotFoundError: TPUs not found in the cluster. Failed in initialization: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [tpu_embedding_config="""", compilation_failure_closes_chips=false, embedding_config="""", tpu_cancellation_closes_chips=2, enable_whole_mesh_compilations=false, is_global_init=false]
Registered devices: [CPU]
Registered kernels:
  <no registered kernels>

	 [[ConfigureDistributedTPU]] [Op:__inference__tpu_init_fn_8]
```",1
Inconsistent `tf.math.reciprocal` Behavior for complex128 `inf` between CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The tf.math.reciprocal operation exhibits inconsistent and partially incorrect behavior on the CPU when applied to complex tensors containing `inf`. The GPU behaves correctly and as mathematically expected.


### Standalone code to reproduce the issue

```python
import tensorflow as tf
import numpy as np

data = np.array([
    [0.0 + np.inf],
    [np.inf + 0.0],
    [np.inf + np.inf],
], dtype='complex128')
with tf.device(""CPU""):
    x = tf.constant(data)
    y = tf.math.reciprocal(x)
    print(y)
with tf.device(""GPU""):
    x_gpu = tf.constant(data)
    y_gpu = tf.math.reciprocal(x_gpu)
    print(y_gpu)
```

### Relevant log output

```shell
output:

tf.Tensor(
[[nan+nanj]
 [nan+nanj]
 [ 0. +0.j]], shape=(3, 1), dtype=complex128)
tf.Tensor(
[[0.+0.j]
 [0.+0.j]
 [0.+0.j]], shape=(3, 1), dtype=complex128)
```
Strangely, when the data vector contains NaN values, all results on the CPU become incorrect.

```python
data = np.array([
    [0.0 + np.inf],
    [np.inf + 0.0],
    [np.inf + np.inf],
    [np.nan + 0.0]
], dtype='complex128')
```


output:
```
tf.Tensor(
[[nan+nanj]
 [nan+nanj]
 [nan+nanj]
 [nan+nanj]], shape=(4, 1), dtype=complex128)
tf.Tensor(
[[ 0. +0.j]
 [ 0. +0.j]
 [ 0. +0.j]
 [nan+nanj]], shape=(4, 1), dtype=complex128)
```",1
Current Status Regarding TFLite vs LiteRT,"Hello, I would like to know what is the current status of development of TF Lite with respect to the LiteRT project. Especially what is the timeframe of TF Lite being deleted from TF.

Let me tell you what I think based on the info in public sources. It seems to me there is an attempt to hijack TFLite by G$$gle and make it obsolete, pushing LiteRT instead. But there are some forces that are against this and continue the development of TF Lite. The result is two separate codebases that diverge and are going to even more, making development much harder.
",0
how to use libtensorflowlite_c.so C API and delegate gpu opencl correctly?,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

latest

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

how to use libtensorflowlite_c.so C API and delegate gpu opencl correctly? follow have error？

//init
TfLiteModel* model = TfLiteModelCreateFromFile(""./model.tflite"");
TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();
TfLiteGpuDelegateOptionsV2 opts = TfLiteGpuDelegateOptionsV2Default();
TfLiteDelegate* gpuDelegate = TfLiteGpuDelegateV2Create(&opts);
TfLiteInterpreterOptionsAddDelegate(options, gpuDelegate);
TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);
TfLiteInterpreterAllocateTensors(interpreter);

//run
TfLiteInterpreterInvoke(interpreter);

//deinit
TfLiteGpuDelegateV2Delete(gpuDelegate);
TfLiteInterpreterDelete(interpreter);
TfLiteInterpreterOptionsDelete(options);
TfLiteModelDelete(model);

### Standalone code to reproduce the issue

```shell
//init
TfLiteModel* model = TfLiteModelCreateFromFile(""./model.tflite"");
TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();
TfLiteGpuDelegateOptionsV2 opts = TfLiteGpuDelegateOptionsV2Default();
TfLiteDelegate* gpuDelegate = TfLiteGpuDelegateV2Create(&opts);
TfLiteInterpreterOptionsAddDelegate(options, gpuDelegate);
TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);
TfLiteInterpreterAllocateTensors(interpreter);

//run
TfLiteInterpreterInvoke(interpreter);

//deinit
TfLiteGpuDelegateV2Delete(gpuDelegate);
TfLiteInterpreterDelete(interpreter);
TfLiteInterpreterOptionsDelete(options);
TfLiteModelDelete(model);
```

### Relevant log output

```shell

```",1
Aborted (core dumped) in `tf.distribute.Server`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.19

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

GTX 4090 24G

### Current behavior?

 `tf.distribute.Server` crashes when inter_op_parallelism_threads is set to -1 in `tf.compat.v1.ConfigProto`

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

server = tf.distribute.Server(
    tf.train.ClusterSpec({""worker"": [""localhost:5000""]}).as_dict(),
    job_name='worker',
    task_index=0,
    protocol='grpc',
    config=tf.compat.v1.ConfigProto(inter_op_parallelism_threads=-1)
)
```

### Relevant log output

```shell
2025-06-22 22:48:20.724080: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-06-22 22:48:20.742238: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1750603700.760855 2732395 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1750603700.766280 2732395 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1750603700.781352 2732395 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1750603700.781378 2732395 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1750603700.781381 2732395 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1750603700.781383 2732395 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-06-22 22:48:20.785849: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
I0000 00:00:1750603702.852113 2732395 gpu_device.cc:2019] Created device /job:worker/replica:0/task:0/device:GPU:0 with 22290 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
I0000 00:00:1750603702.855815 2732395 gpu_device.cc:2019] Created device /job:worker/replica:0/task:0/device:GPU:1 with 22290 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2025-06-22 22:48:22.857257: F external/local_xla/xla/tsl/platform/threadpool.cc:126] Check failed: num_threads >= 1 (1 vs. -1)
Aborted (core dumped)
```",2
Aborted (core dumped) in `tf.image.non_max_suppression`\`tf.raw_ops.NonMaxSuppressionV2`\`tf.raw_ops.NonMaxSuppressionV3`\`tf.raw_ops.NonMaxSuppressionV4`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf2.19

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

GTX 4090 24G

### Current behavior?

When max_output_size = -1, `tf.image.non_max_suppression`\ `tf.raw_ops.NonMaxSuppressionV2`\ `tf.raw_ops.NonMaxSuppressionV3` \ `tf.raw_ops.NonMaxSuppressionV4` crashes. 

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

boxes= tf.random.uniform([10, 4], maxval=100, dtype=tf.float32)
scores = tf.random.uniform([10], maxval=100, dtype=tf.float32)
max_output_size = -1

# crash
tf.image.non_max_suppression(boxes, scores, max_output_size, iou_threshold=0.5, score_threshold=float('-inf'))

# crash
# tf.raw_ops.NonMaxSuppressionV2(
#     boxes=boxes, scores=scores, max_output_size=max_output_size, iou_threshold=0.5, name=None
# )

# crash
# tf.raw_ops.NonMaxSuppressionV3(
#     boxes=boxes, scores=scores, max_output_size=max_output_size, iou_threshold=0.5, score_threshold=float('-inf'), name=None
# )

# crash
# tf.raw_ops.NonMaxSuppressionV4(boxes=boxes, scores=scores, max_output_size=max_output_size, iou_threshold=0.5, score_threshold=float('-inf'),pad_to_max_output_size=False,name=None)
```

### Relevant log output

```shell
2025-06-22 22:38:58.375799: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-06-22 22:38:58.441525: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1750603138.523790 2730428 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1750603138.547987 2730428 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1750603138.626220 2730428 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1750603138.626380 2730428 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1750603138.626421 2730428 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1750603138.626461 2730428 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-06-22 22:38:58.648911: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
I0000 00:00:1750603141.358320 2730428 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22290 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
I0000 00:00:1750603141.358751 2730428 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22290 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2025-06-22 22:39:02.556560: F tensorflow/core/framework/tensor_shape.cc:202] Non-OK-status: InitDims(dim_sizes)
Status: INVALID_ARGUMENT: Expected shape dimensions to be non-negative, got -1
Aborted (core dumped)
```",2
YoloX different Model Output for Python and Android,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf=2.18

### Custom code

Yes

### OS platform and distribution

Linux Mint 22

### Mobile device

Samsung Galaxy A51

### Python version

3.11.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hello,

I am currently trying to get YoloX working for object detection in a mobile app, while the model inference in Python produces the desired and expected output, Android on the other hand produces nonsense for the same model and test image. I asked for help on [Stackoverflow ](https://stackoverflow.com/questions/79650499/model-ouput-of-androidkotlin-tflite-model-not-matching-python-output-for-same) to no avail, and came to the conclusion that this discrepancy might be caused by a bug.  The input float array representation on Android is the same as the raveled Numpy array input of the Python version, while the outputs produced are different in every way. I have excluded the usual endianness problems, due to TensorImage being used which handles this automatically.  Also similar code does work for both RT-detr and Yolov11/v8, which indicates there might be something in the model behaving wrong for Android specifically.

Thanks in Advance


The entire code for both Android and Python can be found in this [repository](https://github.com/RNoahG/YoloXPythonAndroid).
For Android the relevant code is in the MainActivity.kt and YoloDetector.kt, there is some test code commented out.

### Standalone code to reproduce the issue

```shell

Here is an abridged version of the Android input pipeline:


val imgStream = assets.open(""TestImages/$imagePath"")
val decode = BitmapFactory.decodeStream(imgStream)
val imgmat = Mat()
Utils.bitmapToMat(decode,imgmat)
val imgmat3 = Mat()
Imgproc.cvtColor(imgmat,imgmat3,Imgproc.COLOR_RGBA2BGR)
val resizedmat =  Mat()
val paddedmat = Mat()

val size = Size((1920F*ratio).toDouble(),(1080F*ratio).toDouble())
val scalar = Scalar(114.0,114.0,114.0)
Imgproc.resize(imgmat3,resizedmat,size, 0.0, 0.0,INTER_LINEAR)
Core.copyMakeBorder(resizedmat,paddedmat,0,(imsize- (1080*ratio)).toInt(),0,0,Core.BORDER_CONSTANT,scalar)                           
val bitmap = createBitmap(paddedmat.cols(),paddedmat.width(),Bitmap.Config.ARGB_8888)
Imgproc.cvtColor(paddedmat,argbmat,Imgproc.COLOR_RGB2RGBA)
Utils.matToBitmap(argbmat,bitmap)

val image = TensorImage(DataType.UINT8)
image.load(bitmap)
val tensorproc = ImageProcessor.Builder().add(CastOp(INPUT_IMAGE_TYPE)).build()
val proctensor = tensorproc.process(image)
val imageBuffer = proctensor.buffer
val output = TensorBuffer.createFixedSize(intArrayOf(numChannel, numElements), OUTPUT_IMAGE_TYPE)
interpreter.run(imageBuffer, output.buffer)
 

Abriged Version of Python code:


if len(img.shape) == 3:
        padded_img = np.ones((input_size[0], input_size[1], 3), dtype=np.uint8) * 114
    else:
        padded_img = np.ones(input_size, dtype=np.uint8) * 114

r = min(input_size[0] / img.shape[0], input_size[1] / img.shape[1])
resized_img = cv2.resize(img,(int(img.shape[1] * r), int(img.shape[0] * r)),
                              interpolation=cv2.INTER_LINEAR,).astype(np.uint8)

padded_img[: int(img.shape[0] * r), : int(img.shape[1] * r)] = resized_img
padded_img = np.ascontiguousarray(padded_img, dtype=np.float32)

interpreter = tf.lite.Interpreter(model_path=MODEL_PATH)
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
interpreter.set_tensor(input_details[0]['index'], img[None, :, :, :])
interpreter.invoke()
output = interpreter.get_tensor(output_details[0]['index'])
output = np.squeeze(output)
```

### Relevant log output

```shell

```",1
building //tensorflow/lite/ios:TensorFlowLiteC_xcframework results in broken header files,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

hash 84dd28bbc29d75e6a6d917eb2998e4e8ea90ec56

### Custom code

No

### OS platform and distribution

macos 15.5

### Mobile device

iOS 18

### Python version

3.13.2

### Bazel version

7.4.1

### GCC/compiler version

?

### CUDA/cuDNN version

n/a

### GPU model and memory

Apple M2 Pro

### Current behavior?

I build TensorFlowLiteC for iOS:

`% bazel build -c opt --config=ios //tensorflow/lite/ios:TensorFlowLiteC_xcframework --define tflite_with_xnnpack=true --define tflite_with_xnnpack_qs8=true --define tflite_with_xnnpack_qu8=true`

It builds fine.

I unzip the build artifact:

```
% cd bazel-bin/tensorflow/lite/ios
% unzip TensorFlowLiteC_xcframework.zip
```

I navigate to the headers:

```
% cd TensorFlowLiteC.xcframework/ios-arm64_x86_64-simulator/TensorFlowLiteC.framework/Headers
```

I inspect the `c_api.h` header:

```
% cat -n c_api.h 
...
    28  #include ""builtin_ops.h""
    29  #include ""types.h""
    30  #include ""c_api_types.h""  // IWYU pragma: export
    31  #include ""operator.h""  // IWYU pragma: export
...
```

See line 29. There is no `types.h` header:

```
% pwd
<snip>/tensorflow/bazel-bin/tensorflow/lite/ios/TensorFlowLiteC.xcframework/ios-arm64_x86_64-simulator/TensorFlowLiteC.framework/Headers
% ls *h
builtin_ops.h*        c_api_experimental.h* c_api_types.h*        c_api.h*              common.h*             profiler.h*           telemetry_setting.h*  TensorFlowLiteC.h*    xnnpack_delegate.h*
```



### Standalone code to reproduce the issue

```shell
This is easy to reproduce:
1. Build the framework as described above.
2. Using XCode, create a Swift iOS app project.
3. Add `TensorFlowLiteC.framework` as a dependency to your main target.
4. Add a `foo.cpp` file your target to create a create a mixed Swift/C++ target. When XCode prompts you to add a bridging header, do so.
5. In `foo.cpp`, add an `#include <TensorFlowLiteC/c_api.h>`.
6. Build.
7. You will see the error `/Users/<snip>/Library/Developer/Xcode/DerivedData/test-erdzaqurjgfrrbarneayzpeywmra/Build/Products/Debug-iphonesimulator/TensorFlowLiteC.framework/Headers/c_api.h:29:10: error: 'types.h' file not found`
```

### Relevant log output

```shell
<module-includes>:1:9: note: in file included from <module-includes>:1:
#import ""Headers/TensorFlowLiteC.h""
        ^
/Users/<snip>/Library/Developer/Xcode/DerivedData/test-erdzaqurjgfrrbarneayzpeywmra/Build/Products/Debug-iphonesimulator/TensorFlowLiteC.framework/Headers/TensorFlowLiteC.h:2:9: note: in file included from /Users/<snip>/Library/Developer/Xcode/DerivedData/test-erdzaqurjgfrrbarneayzpeywmra/Build/Products/Debug-iphonesimulator/TensorFlowLiteC.framework/Headers/TensorFlowLiteC.h:2:
#import <TensorFlowLiteC/c_api.h>
        ^
/Users/<snip>/Library/Developer/Xcode/DerivedData/test-erdzaqurjgfrrbarneayzpeywmra/Build/Products/Debug-iphonesimulator/TensorFlowLiteC.framework/Headers/c_api.h:29:10: error: 'types.h' file not found
#include ""types.h""
         ^
/Users/<snip>/Library/Developer/Xcode/DerivedData/test-erdzaqurjgfrrbarneayzpeywmra/Build/Products/Debug-iphonesimulator/TensorFlowLite.framework/Modules/TensorFlowLite.swiftmodule/arm64.swiftinterface:8:8: error: could not build Objective-C module 'TensorFlowLiteC'
import TensorFlowLiteC
       ^
/Users/<snip>/Desktop/Development/testTarget/test/DependencyInjection.swift:10:8: error: failed to build module 'TensorFlowLite' for importation due to the errors above; the textual interface may be broken by project issues or a compiler bug
import TensorFlowLite
```",1
Some sorting related ops produce results inconsistent with NumPy when tensor contains NaN,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In 2.17.0 several sorting and indexing ops yield outputs that diverge from NumPy’s behavior on a tensor containing NaN. 
When run in nightly version, CPU and GPU implementations often produce different results.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

print(""TensorFlow version:"", tf.__version__)

# Prepare test tensor with NaN
x = tf.constant([1.0, float('nan'), 3.0], dtype=tf.float32)
x_n = np.array([1.0, np.nan, 3.0], dtype=np.float32)
lookup = np.array([0.0, np.nan, 2.0], dtype=np.float32)

# 1. tf.sort
with tf.device('/CPU:0'):
    sorted_cpu = tf.sort(x)
with tf.device('/GPU:0'):
    sorted_gpu = tf.sort(x)
numpy_sorted = np.sort(x_n)
print(""NumPy sort:          "", numpy_sorted)
print(""CPU sorted result:   "", sorted_cpu.numpy())
print(""GPU sorted result:   "", sorted_gpu.numpy(), ""\n"")

# 2. tf.argsort
with tf.device('/CPU:0'):
    argsort_cpu = tf.argsort(x)
with tf.device('/GPU:0'):
    argsort_gpu = tf.argsort(x)
numpy_argsort = np.argsort(x_n)
print(""NumPy argsort:       "", numpy_argsort)
print(""CPU argsort indices: "", argsort_cpu.numpy())
print(""GPU argsort indices: "", argsort_gpu.numpy(), ""\n"")

# 3. tf.math.top_k
k = 3
with tf.device('/CPU:0'):
    topk_cpu = tf.math.top_k(x, k=k, sorted=True)
with tf.device('/GPU:0'):
    topk_gpu = tf.math.top_k(x, k=k, sorted=True)
# NumPy equivalent for top-k: sort descending and take first k
numpy_topk_values = np.sort(x_n)[::-1][:k]
numpy_topk_indices = np.argsort(x_n)[::-1][:k]
print(""NumPy top_k values:  "", numpy_topk_values)
print(""NumPy top_k indices: "", numpy_topk_indices)
print(""CPU top_k values:    "", topk_cpu.values.numpy())
print(""CPU top_k indices:   "", topk_cpu.indices.numpy())
print(""GPU top_k values:    "", topk_gpu.values.numpy())
print(""GPU top_k indices:   "", topk_gpu.indices.numpy(), ""\n"")

# 4. tf.searchsorted
with tf.device('/CPU:0'):
    ss_cpu = tf.searchsorted(sorted_cpu, lookup)
with tf.device('/GPU:0'):
    ss_gpu = tf.searchsorted(sorted_gpu, lookup)
numpy_searchsorted = np.searchsorted(numpy_sorted, lookup)
print(""NumPy searchsorted:  "", numpy_searchsorted)
print(""CPU searchsorted:    "", ss_cpu.numpy())
print(""GPU searchsorted:    "", ss_gpu.numpy())
```

### Relevant log output

```shell
2.17:
TensorFlow version: 2.17.0
NumPy sort:           [ 1.  3. nan]
CPU sorted result:    [ 1. nan  3.]
GPU sorted result:    [ 1. nan  3.] 

NumPy argsort:        [0 2 1]
CPU argsort indices:  [0 1 2]
GPU argsort indices:  [0 1 2] 

NumPy top_k values:   [nan  3.  1.]
NumPy top_k indices:  [1 2 0]
CPU top_k values:     [ 3.  1. nan]
CPU top_k indices:    [2 0 1]
GPU top_k values:     [ 3.  1. nan]
GPU top_k indices:    [2 0 1] 

NumPy searchsorted:   [0 2 1]
CPU searchsorted:     [0 0 1]
GPU searchsorted:     [0 0 1]


nighly:
TensorFlow version: 2.20.0-dev20250604
NumPy sort:           [ 1.  3. nan]
CPU sorted result:    [ 1. nan  3.]
GPU sorted result:    [nan  1.  3.] 

NumPy argsort:        [0 2 1]
CPU argsort indices:  [0 1 2]
GPU argsort indices:  [1 0 2] 

NumPy top_k values:   [nan  3.  1.]
NumPy top_k indices:  [1 2 0]
CPU top_k values:     [ 3.  1. nan]
CPU top_k indices:    [2 0 1]
GPU top_k values:     [nan  3.  1.]
GPU top_k indices:    [1 2 0] 

NumPy searchsorted:   [0 2 1]
CPU searchsorted:     [0 0 1]
GPU searchsorted:     [0 0 2]
```",1
TensorFlow disables SwiftUI Previews,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

'TensorFlowLiteSwift', '~> 2.17.0'

### Custom code

Yes

### OS platform and distribution

iOS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

After adding tensor flow library to iOS App, SwiftUI previews stop working. Though it seems fine when running on a device or a simulator but run time SwiftUI Previews stop working which they shouldn't.

### Standalone code to reproduce the issue

```shell
Just create a simple SwiftUI app and then add TensorFlowLite Framework in the app. 
You will see that swiftUI Previews will stop working
```

### Relevant log output

```shell

```",1
How to run Android demo which uses NPU to inference?,"Hello!
I'm wondering where can I find a demo to inference with NPU. It's better to detect objection.
Thank you!",0
tf.data.experimental.prefetch_to_device has no effect inside tf.distribute.Strategy.distribute_datasets_from_function.,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.19.0

### Custom code

No

### OS platform and distribution

RHEL 9.4


### Python version

3.11


### CUDA/cuDNN version

12.5


### Current behavior?

MemcpyH2D does not overlap with model computation when using tf.data.experimental.prefetch_to_device inside tf.distribute.MirroredStrategy.distribute_datasets_from_function. I would expect this operations to overlap.

![Image](https://github.com/user-attachments/assets/ae8e9e94-eb9b-49e8-b335-c22d30a0aec0)

### Standalone code to reproduce the issue

```python
import tensorflow as tf

class Model(tf.keras.Model):
    def call(self, x):
        y = x / 1000
        for i in range(3):
            y = tf.matmul(y, x / 1000)
        return tf.reduce_sum(y, axis=[1, 2])

def get_dataset(ictx):
    ds = tf.data.Dataset.range(1, 1001, output_type=tf.float32)
    ds = ds.map(lambda i: (tf.ones((1024 * 5, 1024 * 5)) / i, 0.0))
    ds = ds.batch(8)
    ds = ds.apply(tf.data.experimental.prefetch_to_device('gpu'))
    return ds

strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
    ds = strategy.distribute_datasets_from_function(get_dataset)
    model = Model()
    model.compile(loss='mse')
    model.fit(
        ds,
        epochs=1,
        steps_per_epoch=30,
        callbacks=tf.keras.callbacks.TensorBoard(profile_batch=(15, 25)))
```

[gist](https://colab.research.google.com/drive/1LmGKFEtveVC5-3KgIcsjB7ZTE0yYdBEb?usp=sharing)",0
`tf.linalg.solve` behaves inconsistently on GPU for singular matrices depending on shape,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In a Linux + GPU environment, calling tf.linalg.solve on singular matrices yields different results on GPU depending on the matrix shape:
For a 3×3 singular matrix, the CPU backend raises InvalidArgumentError, but the GPU backend returns an identity matrix without error.
For a 2×2 singular matrix, both CPU and GPU backends correctly raise InvalidArgumentError.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

for name, matrix in [
    (""3×3 float64 singular matrix"", tf.constant([
        [1.0, 2.0, 3.0],
        [2.0, 5.0, 6.0],
        [3.0, 6.0, 9.0]
    ], dtype=tf.float64)),
    (""2×2 float64 singular matrix"", tf.constant([
        [1.0, 2.0],
        [2.0, 4.0]
    ], dtype=tf.float64))
]:
    print(f""\n=== {name} ==="")

    with tf.device('/CPU:0'):
        try:
            x_cpu = tf.linalg.solve(matrix, matrix)
            print(""CPU solve ->\n"", x_cpu.numpy())
        except Exception as e:
            print(""CPU solve raised:"", type(e).__name__, e)

    with tf.device('/GPU:0'):
        try:
            x_gpu = tf.linalg.solve(matrix, matrix)
            print(""GPU solve ->\n"", x_gpu.numpy())
        except Exception as e:
            print(""GPU solve raised:"", type(e).__name__, e)
```

### Relevant log output

```shell
=== 3×3 float64 singular matrix ===
CPU solve raised: InvalidArgumentError {{function_node __wrapped__MatrixSolve_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input matrix is not invertible. [Op:MatrixSolve] name: 
GPU solve ->
 [[1. 0. 0.]
 [0. 1. 0.]
 [0. 0. 1.]]

=== 2×2 float64 singular matrix ===
CPU solve raised: InvalidArgumentError {{function_node __wrapped__MatrixSolve_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input matrix is not invertible. [Op:MatrixSolve] name: 
GPU solve raised: InvalidArgumentError {{function_node __wrapped__MatrixSolve_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input matrix is not invertible. [Op:MatrixSolve] name:
```",1
tf.linalg.slogdet returns incorrect values on GPU for singular matrix,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When calling `tf.linalg.slogdet` on a singular matrix, the CPU backend returns the mathematically correct result (sign=0, logabsdet=-∞). However, on GPU the same input produces, which contradicts the expected behavior for a singular matrix. According to documentation, for any singular matrix, logabsdet should be negative infinity and sign should be zero.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

singular_np = np.array([
    [1.0, 2.0, 3.0],
    [2.0, 5.0, 6.0],
    [3.0, 6.0, 9.0]
], dtype=np.float32)
matrix = tf.constant(singular_np)

# 1. On CPU:
with tf.device('/CPU:0'):
    sign_cpu, logabs_cpu = tf.linalg.slogdet(matrix)
    print(""CPU slogdet -> sign ="", sign_cpu.numpy(), "", logabsdet ="", logabs_cpu.numpy())

# 2. On GPU:
with tf.device('/GPU:0'):
    sign_gpu, logabs_gpu = tf.linalg.slogdet(matrix)
    print(""GPU slogdet -> sign ="", sign_gpu.numpy(), "", logabsdet ="", logabs_gpu.numpy())
```

### Relevant log output

```shell
CPU slogdet -> sign = tf.Tensor(-0.0, shape=(), dtype=float32) , logabsdet = tf.Tensor(-inf, shape=(), dtype=float32)
GPU slogdet -> sign = tf.Tensor(1.0, shape=(), dtype=float32) , logabsdet = tf.Tensor(-15.131454, shape=(), dtype=float32)
```",1
Enhance Memory Optimizer with Dynamic Cost Model for Operation Recomputation,"## Background

TensorFlow's memory optimizer currently uses a static list to determine which operations are ""cheap"" to recompute rather than keep in memory. This approach, while functional, leaves significant room for optimization that could improve both memory usage and performance across different models and hardware configurations.

Current implementation in `tensorflow/core/grappler/optimizers/memory_optimizer.cc`:
```cpp
std::unordered_set<string> GetCheapToRecomputeOps() {
  std::unordered_set<string> cheap_ops = {""Add"", ""AddN"", ""BiasAdd"", ...};
  return cheap_ops;
}
```

## Proposed Enhancement

Replace the static list with a dynamic cost model that considers:

1. **Operation Characteristics**:
   - Actual computational complexity
   - Memory usage patterns
   - Input/output tensor sizes
   - Operation type-specific metrics

2. **Runtime Factors**:
   - Available system memory
   - Hardware capabilities (CPU/GPU/TPU)
   - Current memory pressure
   - Historical execution timing data

3. **Adaptive Decision Making**:
   - Cost-benefit ratio calculation for recomputation vs. storage
   - Dynamic thresholds based on system state
   - Learning from actual execution patterns

## Expected Benefits

1. **Improved Memory Efficiency**: Better decisions about when to recompute vs. store results
2. **Enhanced Performance**: More optimal use of available computational resources
3. **Hardware Adaptability**: Better adaptation to different hardware configurations
4. **Dynamic Optimization**: Responsive to changing runtime conditions
5. **Reduced Memory Pressure**: More intelligent memory management for large models

## Testing Strategy

1. Benchmark suite with various model architectures
2. Memory usage comparisons
3. Performance impact measurements
4. Hardware-specific test cases
5. Edge case validation

## Success Metrics

- Reduced peak memory usage in large models
- Improved training speed for memory-constrained scenarios
- Better resource utilization across different hardware configurations
",1
graph execution error bug with tfm.nlp.layers.MultiHeadRelativeAttention,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.19

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

P100

### Current behavior?

I'm creating a transformer encoder layer, and I was trying to add positional encoding, but I always encounter a graph execution error when using tfm.nlp.layers.MultiHeadRelativeAttention. Every time I use it, I get the posted error, and it may be an issue with how batches are being processed in the layer. The layer is really experimental, but I have tried many ways of getting around the error, but the error seems to persist.

### Standalone code to reproduce the issue

```shell
class TransformerBlock(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads, dff, rate=0.1):
        super().__init__()
        self.num_heads = num_heads
        self.d_model = d_model
        self.key_dim = d_model // num_heads

        # Attention layer
        self.att = tfm.nlp.layers.MultiHeadRelativeAttention(
            num_heads=num_heads,
            key_dim=self.key_dim
        )

        # Trainable bias parameters with correct shape
        self.content_bias = self.add_weight(
            name='content_bias',
            shape=[1, 1, num_heads, self.key_dim],  # [1, 1, H, Dk]
            initializer='zeros',
            trainable=True
        )
        self.position_bias = self.add_weight(
            name='position_bias',
            shape=[1, 1, num_heads, self.key_dim],  # [1, 1, H, Dk]
            initializer='zeros',
            trainable=True
        )

        # Rest of the network
        self.ffn = tf.keras.Sequential([
            tf.keras.layers.Dense(dff, activation='relu'),
            tf.keras.layers.Dense(d_model)
        ])
        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)
        self.rel_pos_encode = tfm.nlp.layers.RelativePositionEmbedding(d_model)

    def call(self, x, training=False, mask=None):
        batch_size = tf.shape(x)[0]
        seq_len = tf.shape(x)[1]
        H = self.num_heads
        Dk = self.key_dim

        # 1) Prepare biases with correct shape [B, L, H, Dk]
        content_attention_bias = tf.tile(self.content_bias, [batch_size, seq_len, 1, 1])
        positional_attention_bias = tf.tile(self.position_bias, [batch_size, seq_len, 1, 1])

        # 2) Generate relative position encoding [B*H, 2*L-1, Dk]
        rel_len = 2 * seq_len - 1
        rel_embedding = self.rel_pos_encode(inputs=None, length=rel_len)
        rel_embedding = tf.reshape(rel_embedding, [rel_len, H, Dk])
        rel_embedding = tf.transpose(rel_embedding, [1, 0, 2])  # [H, 2*L-1, Dk]
        rel_embedding = tf.tile(rel_embedding, [batch_size, 1, 1])  # [B*H, 2*L-1, Dk]

        # 3) Call attention with properly shaped biases
        attn_output = self.att(
            query=x,
            value=x,
            content_attention_bias=content_attention_bias,
            positional_attention_bias=positional_attention_bias,
            relative_position_encoding=rel_embedding,
            attention_mask=mask
        )

        # 4) Standard transformer operations
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(x + attn_output)
        
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)
```

### Relevant log output

```shell
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/tmp/ipykernel_35/1955170888.py in <cell line: 0>()
----> 1 history = model.fit(
      2     train,
      3     validation_data=val,
      4     epochs=50,
      5 )

/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py in error_handler(*args, **kwargs)
    120             # To get the full stack trace, call:
    121             # `keras.config.disable_traceback_filtering()`
--> 122             raise e.with_traceback(filtered_tb) from None
    123         finally:
    124             del filtered_tb

/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     57       e.message += "" name: "" + name
     58     raise core._status_to_exception(e) from None
---> 59   except TypeError as e:
     60     keras_symbolic_tensors = [x for x in inputs if _is_keras_symbolic_tensor(x)]
     61     if keras_symbolic_tensors:

InvalidArgumentError: Graph execution error:

Detected at node gradient_tape/improved_transformer_23_1/transformer_block_45_1/multi_head_relative_attention_44/add_2/BroadcastGradientArgs defined at (most recent call last):
<stack traces unavailable>
Incompatible shapes: [8,4,512,512] vs. [32,4,512,512]

Stack trace for op definition: 
File ""<frozen runpy>"", line 198, in _run_module_as_main
File ""<frozen runpy>"", line 88, in _run_code
File ""/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py"", line 37, in <module>
File ""/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py"", line 992, in launch_instance
File ""/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py"", line 712, in start
File ""/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py"", line 205, in start
File ""/usr/lib/python3.11/asyncio/base_events.py"", line 608, in run_forever
File ""/usr/lib/python3.11/asyncio/base_events.py"", line 1936, in _run_once
File ""/usr/lib/python3.11/asyncio/events.py"", line 84, in _run
File ""/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py"", line 510, in dispatch_queue
File ""/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py"", line 499, in process_one
File ""/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py"", line 406, in dispatch_shell
File ""/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py"", line 730, in execute_request
File ""/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py"", line 383, in do_execute
File ""/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py"", line 528, in run_cell
File ""/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py"", line 2975, in run_cell
File ""/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py"", line 3030, in _run_cell
File ""/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner
File ""/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async
File ""/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes
File ""/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code
File ""/tmp/ipykernel_35/1955170888.py"", line 1, in <cell line: 0>
File ""/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 371, in fit
File ""/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 219, in function
File ""/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 132, in multi_step_on_iterator
File ""/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 113, in one_step_on_data
File ""/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 77, in train_step

	 [[{{node gradient_tape/improved_transformer_23_1/transformer_block_45_1/multi_head_relative_attention_44/add_2/BroadcastGradientArgs}}]]
	tf2xla conversion failed while converting __inference_one_step_on_data_322878[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.
	 [[StatefulPartitionedCall]] [Op:__inference_multi_step_on_iterator_323180]
```",1
TensorFlow Docker `tensorflow/tensorflow:latest-gpu` fails to detect GPU due to CUDA/cuDNN mismatch,"**Title:** TensorFlow Docker `tensorflow/tensorflow:latest-gpu` fails to detect GPU due to CUDA/cuDNN mismatch

**Description:**

The latest TensorFlow GPU Docker image (`tensorflow/tensorflow:latest-gpu`, pulled as of May 31, 2025) is broken out-of-the-box for GPU usage.

Running:
```bash
docker run --gpus all -it tensorflow/tensorflow:latest-gpu bash
python3 -c 'import tensorflow as tf; print(tf.config.list_physical_devices(""GPU""))'
```

**Expected Behavior:**  
TensorFlow should detect and list available GPUs.

**Actual Behavior:**  
TensorFlow returns an empty list: `[]`.

**Error Logs:**

```
E tensorflow/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
E tensorflow/stream_executor/cuda/cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E tensorflow/stream_executor/cuda/cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W tensorflow/core/common_runtime/gpu/gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU.
Skipping registering GPU devices...
```

tf.sysconfig.get_build_info() reports:
```
OrderedDict([('cpu_compiler', '/usr/lib/llvm-18/bin/clang'), ('cuda_compute_capabilities', ['sm_60', 'sm_70', 'sm_80', 'sm_89', 'compute_90']), ('cuda_version', '12.5.1'), ('cudnn_version', '9'), ('is_cuda_build', True), ('is_rocm_build', False), ('is_tensorrt_build', False)])
```
However, the container only includes CUDA 12.3 and cuDNN 8.9.6, causing a runtime mismatch and failure to initialize GPU.
find / -name 'libcu*' 2>/dev/null
```
/var/lib/dpkg/info/libcurl4:amd64.symbols
/var/lib/dpkg/info/libcufft-12-3.md5sums
/var/lib/dpkg/info/libcufile-12-3.list
/var/lib/dpkg/info/libcurl4:amd64.list
/var/lib/dpkg/info/libcufile-12-3.postinst
/var/lib/dpkg/info/libcusolver-12-3.md5sums
/var/lib/dpkg/info/libcurand-12-3.list
/var/lib/dpkg/info/libcusparse-12-3.md5sums
/var/lib/dpkg/info/libcusolver-12-3.list
/var/lib/dpkg/info/libcublas-12-3.list
/var/lib/dpkg/info/libcufile-12-3.md5sums
/var/lib/dpkg/info/libcufft-12-3.list
/var/lib/dpkg/info/libcufile-12-3.prerm
/var/lib/dpkg/info/libcurl4:amd64.triggers
/var/lib/dpkg/info/libcufile-12-3.conffiles
/var/lib/dpkg/info/libcudnn8.md5sums
/var/lib/dpkg/info/libcudnn8.list
/var/lib/dpkg/info/libcurl4:amd64.shlibs
/var/lib/dpkg/info/libcurand-12-3.md5sums
/var/lib/dpkg/info/libcublas-12-3.md5sums
/var/lib/dpkg/info/libcurl4:amd64.md5sums
/var/lib/dpkg/info/libcusparse-12-3.list
/var/lib/dpkg/info/libcurl3-gnutls:amd64.triggers
/var/lib/dpkg/info/libcurl3-gnutls:amd64.list
/var/lib/dpkg/info/libcurl3-gnutls:amd64.symbols
/var/lib/dpkg/info/libcurl3-gnutls:amd64.shlibs
/var/lib/dpkg/info/libcurl3-gnutls:amd64.md5sums
/usr/local/cuda-12.3/compat/libcuda.so.545.23.06
/usr/local/cuda-12.3/compat/libcudadebugger.so.545.23.06
/usr/local/cuda-12.3/compat/libcudadebugger.so.1
/usr/local/cuda-12.3/compat/libcuda.so
/usr/local/cuda-12.3/compat/libcuda.so.1
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudart.so.12
/usr/local/cuda-12.3/targets/x86_64-linux/lib/stubs/libcuda.so
/usr/local/cuda-12.3/targets/x86_64-linux/lib/stubs/libcuda.so.1
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcuinj64.so
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcublas.so.12
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcusolverMg.so.11
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcuinj64.so.12.3.101
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcupti.so.2023.3.1
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcufft.so.11.0.12.1
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcusparse.so.12.2.0.103
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcuinj64.so.12.3
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcufile.so.1.8.1
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcusolver.so.11.5.4.101
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcufft.so.11
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcufile_rdma.so.1.8.1
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcufile.so.0
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libculibos.a
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcusolverMg.so.11.5.4.101
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcurand.so.10.3.4.107
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcupti.so
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcufile_rdma.so.1
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcufftw.so.11
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcurand.so.10
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcupti.so.12
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcublasLt.so.12.3.4.1
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-12.3/targets/x86_64-linux/lib/cmake/libcudacxx
/usr/local/cuda-12.3/targets/x86_64-linux/lib/cmake/libcudacxx/libcudacxx-config.cmake
/usr/local/cuda-12.3/targets/x86_64-linux/lib/cmake/libcudacxx/libcudacxx-header-search.cmake
/usr/local/cuda-12.3/targets/x86_64-linux/lib/cmake/libcudacxx/libcudacxx-config-version.cmake
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcufftw.so.11.0.12.1
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudart.so.12.3.101
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcusolver.so.11
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudart.so
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcublas.so.12.3.4.1
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcublasLt.so.12
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudadevrt.a
/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcusparse.so.12
/usr/local/cuda-12.3/extras/Debugger/lib64/libcudacore.a
/usr/local/cuda-12.3/extras/Debugger/include/libcudacore.h
/usr/lib/x86_64-linux-gnu/libcudadebugger.so.1
/usr/lib/x86_64-linux-gnu/libcuda.so
/usr/lib/x86_64-linux-gnu/libcuda.so.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8
/usr/lib/x86_64-linux-gnu/libcurl.so.4.7.0
/usr/lib/x86_64-linux-gnu/libcudnn.so.8
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcurl.so.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcurl-gnutls.so.4
/usr/lib/x86_64-linux-gnu/libcurl-gnutls.so.4.7.0
/usr/lib/x86_64-linux-gnu/libcurl-gnutls.so.3
/usr/lib/wsl/drivers/nv_dispig.inf_amd64_0afec3f2050014a0/libcuda_loader.so
/usr/lib/wsl/drivers/nv_dispig.inf_amd64_0afec3f2050014a0/libcuda.so.1.1
/usr/share/lintian/overrides/libcudnn8
/usr/share/lintian/overrides/libcurl3-gnutls
/usr/share/doc/libcudnn8
/usr/share/doc/libcublas-12-3
/usr/share/doc/libcurand-12-3
/usr/share/doc/libcufft-12-3
/usr/share/doc/libcusolver-12-3
/usr/share/doc/libcufile-12-3
/usr/share/doc/libcusparse-12-3
/usr/share/doc/libcurl4
/usr/share/doc/libcurl3-gnutls
```
Expected behavior:
The latest-gpu image should include libraries matching the TensorFlow build (CUDA 12.5.1, cuDNN 9) or fall back to a compatible combination.
",1
`tf.math.cumprod` on `complex128` with `Inf` produces incorrect result on both CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When computing a cumulative product of a `complex128` tensor containing `Inf`, TensorFlow’s `tf.math.cumprod` produces the wrong first element—returning `NaN+infj` instead of `1+infj`—on both CPU and GPU. NumPy’s `np.cumprod` yields the mathematically correct result.


### Standalone code to reproduce the issue

```python
import tensorflow as tf
import numpy as np

vals = [complex(1, np.inf), complex(2, 3)]
out_np = np.cumprod(vals)
print(""NumPy cumprod result:"", out_np)
with tf.device('/CPU:0'):
  x_cpu = tf.constant(vals, dtype=tf.complex128)
  out_cpu = tf.math.cumprod(x_cpu, axis=0)
  print(out_cpu)

with tf.device('/GPU:0'):
  vals = [complex(1, np.inf), complex(2, 3)]
  x_gpu = tf.constant(vals, dtype=tf.complex128)
  out_gpu = tf.math.cumprod(x_gpu, axis=0)
  print(out_gpu)
```

### Relevant log output

```shell
NumPy cumprod result: [  1.+infj -inf+infj]
tf.Tensor([ nan+infj -inf+infj], shape=(2,), dtype=complex128)
tf.Tensor([ nan+infj -inf+infj], shape=(2,), dtype=complex128)
```",1
Crash in `tf.raw_ops.MaxPoolGradGradWithArgmax` when executing on GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.20.0-dev20250526

### Custom code

Yes

### OS platform and distribution

Linux CPU & GPU

### Mobile device

_No response_

### Python version

3.13.2

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA 12.5.1

### GPU model and memory

_No response_

### Current behavior?

TensorFlow crashes when executing `tf.raw_ops.MaxPoolGradGradWithArgmax` on GPU, but runs fine on CPU.

The issue can be reproduced in this Colab notebook: https://colab.research.google.com/drive/1oXq25c_qxoT2QD1zNt_IBPM6bdslAtB6?usp=sharing

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

with tf.device('/cpu:0'):
    try:
        input_tensor = tf.constant([0, 0, 0, 0, 0], dtype=tf.float32, shape=[5,1,1,1])
        grad = tf.constant([0, 0, 0, 0, 0], dtype=tf.float32, shape=[5,1,1,1])
        argmax = tf.constant([0, 25288767438848, -1099511627776, -1, 4294967295], dtype=tf.int64, shape=[5,1,1,1])
        ksize = [1, 1, 1, 1]
        strides = [1, 1, 1, 1]
        padding = 'VALID'
        include_batch_in_index = True
        
        result = tf.raw_ops.MaxPoolGradGradWithArgmax(
            input=input_tensor,
            grad=grad,
            argmax=argmax,
            ksize=ksize,
            strides=strides,
            padding=padding,
            include_batch_in_index=include_batch_in_index,
            name=None
        )
        print(""MaxPoolGradGradWithArgmax executed successfully on CPU."")
    except Exception as e:
        print(f""Exception on CPU: {e}"")

with tf.device('/gpu:0'):
    try:
        input_tensor = tf.constant([0, 0, 0, 0, 0], dtype=tf.float32, shape=[5,1,1,1])
        grad = tf.constant([0, 0, 0, 0, 0], dtype=tf.float32, shape=[5,1,1,1])
        argmax = tf.constant([0, 25288767438848, -1099511627776, -1, 4294967295], dtype=tf.int64, shape=[5,1,1,1])
        ksize = [1, 1, 1, 1]
        strides = [1, 1, 1, 1]
        padding = 'VALID'
        include_batch_in_index = True
        
        result = tf.raw_ops.MaxPoolGradGradWithArgmax(
            input=input_tensor,
            grad=grad,
            argmax=argmax,
            ksize=ksize,
            strides=strides,
            padding=padding,
            include_batch_in_index=include_batch_in_index,
            name=None
        )
        print(""MaxPoolGradGradWithArgmax executed successfully on GPU."")
    except Exception as e:
        print(f""Exception on GPU: {e}"")
```

### Relevant log output

```shell
Output:


I0000 00:00:1748442135.660934 1229154 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2622 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9
MaxPoolGradGradWithArgmax executed successfully on CPU.
F0000 00:00:1748442136.223198 1229230 device_event_mgr.cc:226] Unexpected Event status: 1
*** Check failure stack trace: ***
    @     0x7f8949a43424  absl::lts_20230802::log_internal::LogMessage::SendToLog()
    @     0x7f8949a42dc4  absl::lts_20230802::log_internal::LogMessage::Flush()
    @     0x7f8949a438b9  absl::lts_20230802::log_internal::LogMessageFatal::~LogMessageFatal()
    @     0x7f8947cf97f3  tensorflow::EventMgr::PollEvents()::$_0::operator()<>()
    @     0x7f8947cf8edb  tensorflow::EventMgr::PollEvents()
    @     0x7f8947cf8b89  tensorflow::EventMgr::PollLoop()
    @     0x7f8949766314  Eigen::ThreadPoolTempl<>::WorkerLoop()
    @     0x7f8949766191  std::__invoke_impl<>()
    @     0x7f894975228f  tsl::(anonymous namespace)::PThread::ThreadFn()
    @     0x7f894b2417eb  (unknown)
    @     0x7f894b2c518c  (unknown)
Aborted (core dumped)


Error Logs on Colab:


I0000 00:00:1748442857.184649     177 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5
F0000 00:00:1748442857.444401    1608 device_event_mgr.cc:226] Unexpected Event status: 1
*** Check failure stack trace: ***
    @     0x7eee0fb53424  absl::lts_20230802::log_internal::LogMessage::SendToLog()
    @     0x7eee0fb52dc4  absl::lts_20230802::log_internal::LogMessage::Flush()
    @     0x7eee0fb538b9  absl::lts_20230802::log_internal::LogMessageFatal::~LogMessageFatal()
    @     0x7eee0de097f3  tensorflow::EventMgr::PollEvents()::$_0::operator()<>()
    @     0x7eee0de08edb  tensorflow::EventMgr::PollEvents()
    @     0x7eee0de08b89  tensorflow::EventMgr::PollLoop()
    @     0x7eee0f876314  Eigen::ThreadPoolTempl<>::WorkerLoop()
    @     0x7eee0f876191  std::__invoke_impl<>()
    @     0x7eee0f86228f  tsl::(anonymous namespace)::PThread::ThreadFn()
    @     0x7eee5d1a4ac3  (unknown)
```",2
Crash in `tf.raw_ops.BiasAdd` when executing on GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.20.0-dev20250526

### Custom code

Yes

### OS platform and distribution

Linux GPU

### Mobile device

_No response_

### Python version

3.13.2

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA 12.5.1

### GPU model and memory

_No response_

### Current behavior?

TensorFlow crashes with a fatal error `Check failed: d < dims() (2 vs. 2)` when executing broadcasting addition by `BiasAdd` on GPU, but  runs fine on CPU.

The crash occurs in tensorflow/core/framework/tensor_shape.cc:359 and results in an aborted process with core dump.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

with tf.device('/gpu:0'):
    try:
        value = tf.constant([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=tf.int32, shape=[2,6])
        bias = tf.constant([1, 2, 3, 4, 5, 6], dtype=tf.int32, shape=[6])
        data_format = ""NCHW""
        result = tf.raw_ops.BiasAdd(
            value=value,
            bias=bias,
            data_format=data_format,
            name=None
        )
        print(result)
    except Exception as e:
        print(e)
```

### Relevant log output

```shell
I0000 00:00:1748440976.276949 1213984 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2367 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9
2025-05-28 22:02:56.315991: F tensorflow/core/framework/tensor_shape.cc:359] Check failed: d < dims() (2 vs. 2)
Aborted (core dumped)
```",2
Inconsistent Error Handling in `tf.raw_ops.SparseToDense` Between CPU and GPU Implementations,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.20.0-dev20250526

### Custom code

Yes

### OS platform and distribution

Linux CPU & GPU

### Mobile device

_No response_

### Python version

3.13.2

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA 12.5.1

### GPU model and memory

_No response_

### Current behavior?

The `tf.raw_ops.SparseToDense` function exhibits inconsistent behavior between CPU and GPU implementations.

When provided with out-of-range index `0`, the CPU version raise an exception normally while the GPU version acquiesces to negative indexing or causes an abort.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

with tf.device('/cpu:0'):
    try:
        sparse_indices = tf.constant([1], dtype=tf.int32, shape=[1])
        output_shape = tf.constant([0], dtype=tf.int32, shape=[1])
        sparse_values = tf.constant([0], dtype=tf.uint16, shape=[1])
        default_value = tf.constant([0], dtype=tf.uint16, shape=[])
        validate_indices = False
        result = tf.raw_ops.SparseToDense(
            sparse_indices=sparse_indices,
            output_shape=output_shape,
            sparse_values=sparse_values,
            default_value=default_value,
            validate_indices=validate_indices,
            name=None
        )
        print(""SparseToDense executed successfully on CPU"")
    except Exception as e:
        print(""Exception on CPU:"", e)

with tf.device('/gpu:0'):
    try:
        sparse_indices = tf.constant([1], dtype=tf.int32, shape=[1])
        output_shape = tf.constant([0], dtype=tf.int32, shape=[1])
        sparse_values = tf.constant([0], dtype=tf.uint16, shape=[1])
        default_value = tf.constant([0], dtype=tf.uint16, shape=[])
        validate_indices = False
        result = tf.raw_ops.SparseToDense(
            sparse_indices=sparse_indices,
            output_shape=output_shape,
            sparse_values=sparse_values,
            default_value=default_value,
            validate_indices=validate_indices,
            name=None
        )
        print(""SparseToDense executed successfully on GPU"")
    except Exception as e:
        print(""Exception on GPU:"", e)
```

### Relevant log output

```shell
Output:


I0000 00:00:1748439889.789792 1199715 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1796 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9
2025-05-28 21:44:49.808211: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Indices are not valid (out of bounds).  Shape: [0]
Exception on CPU: {{function_node __wrapped__SparseToDense_device_/job:localhost/replica:0/task:0/device:CPU:0}} Indices are not valid (out of bounds).  Shape: [0] [Op:SparseToDense] name: 
SparseToDense executed successfully on GPU
```",1
Inconsistent Error Handling in `tf.raw_ops.SparseSegmentSqrtNGradV2` Between CPU and GPU Implementations,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.20.0-dev20250526

### Custom code

Yes

### OS platform and distribution

Linux CPU & GPU

### Mobile device

_No response_

### Python version

3.13.2

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA 12.5.1

### GPU model and memory

_No response_

### Current behavior?

Bug similar to tensorflow/tensorflow#94151

The `tf.raw_ops.SparseSegmentSqrtNGradV2` function exhibits inconsistent behavior between CPU and GPU implementations.

When provided with out-of-range negative index, the CPU version raise an exception normally while the GPU version acquiesces to negative indexing or causes an abort.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

with tf.device('/cpu:0'):
    try:
        grad = tf.constant([0, 0, 0, 0], dtype=tf.float64, shape=[4])
        indices = tf.constant([-1], dtype=tf.int64, shape=[1])
        segment_ids = tf.constant([-1], dtype=tf.int64, shape=[1])
        dense_output_dim0 = tf.constant([1], dtype=tf.int32, shape=[])

        tf.raw_ops.SparseSegmentSqrtNGradV2(
            grad=grad,
            indices=indices,
            segment_ids=segment_ids,
            dense_output_dim0=dense_output_dim0,
            name=None
        )
        print(""SparseSegmentSqrtNGradV2 executed successfully on CPU"")
    except Exception as e:
        print(f""Exception on CPU: {e}"")
        
with tf.device('/gpu:0'):
    try:
        grad = tf.constant([0, 0, 0, 0], dtype=tf.float64, shape=[4])
        indices = tf.constant([-1], dtype=tf.int64, shape=[1])
        segment_ids = tf.constant([-1], dtype=tf.int64, shape=[1])
        dense_output_dim0 = tf.constant([1], dtype=tf.int32, shape=[])

        tf.raw_ops.SparseSegmentSqrtNGradV2(
            grad=grad,
            indices=indices,
            segment_ids=segment_ids,
            dense_output_dim0=dense_output_dim0,
            name=None
        )
        print(""SparseSegmenSqrtNGradV2 executed successfully on GPU"")
    except Exception as e:
        print(f""Exception on GPU: {e}"")
```

### Relevant log output

```shell
Output:


I0000 00:00:1748438686.325530  285106 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31141 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:c1:00.0, compute capability: 7.0
I0000 00:00:1748438686.326316  285106 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 31029 MB memory:  -> device: 1, name: Tesla V100-PCIE-32GB, pci bus id: 0000:e1:00.0, compute capability: 7.0
2025-05-28 13:24:46.358801: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Index -1 out of range [0, 1).
Exception on CPU: {{function_node __wrapped__SparseSegmentSqrtNGradV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Index -1 out of range [0, 1). [Op:SparseSegmentSqrtNGradV2] name: 
SparseSegmenSqrtNGradV2 executed successfully on GPU
```",1
Scale/zero-point restrictions on the input and output of quantized operators,"Some operators (like transpose and resize_bilinear) have the restriction ""Input and outputs must all have same scale/zero_point"" in  [int8 quantized operator specifications](https://ai.google.dev/edge/litert/models/quantization_spec#int8_quantized_operator_specifications). However, this is not checked in [TFLite Kernel codes](https://source.chromium.org/chromium/chromium/src/+/main:third_party/tflite/src/tensorflow/lite/kernels/transpose.cc;l=71;drc=70343319512a4883095fdb100796ea70fba34b9e).

I suppose this is the requirement for a valid quantized operator to get right results and TFLite should check it. Can anyone clarify this? Thanks.",0
Inconsistent Error Handling in `tf.raw_ops.SparseSegmentSumGradV2` Between CPU and GPU Implementations,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.20.0-dev20250526

### Custom code

Yes

### OS platform and distribution

Linux CPU & GPU

### Mobile device

_No response_

### Python version

3.13.2

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA 12.5.1

### GPU model and memory

_No response_

### Current behavior?

The `tf.raw_ops.SparseSegmentSumGradV2` function exhibits inconsistent behavior between CPU and GPU implementations.

When provided with  out-of-range negative index, the CPU version raise an exception normally while the GPU version acquiesces to negative indexing or causes an abort.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

with tf.device('/cpu:0'):
    try:
        grad = tf.constant([0, 0, 0, 0], dtype=tf.float64, shape=[4])
        indices = tf.constant([-1], dtype=tf.int64, shape=[1])
        segment_ids = tf.constant([-1], dtype=tf.int64, shape=[1])
        dense_output_dim0 = tf.constant([1], dtype=tf.int32, shape=[])

        tf.raw_ops.SparseSegmentSumGradV2(
            grad=grad,
            indices=indices,
            segment_ids=segment_ids,
            dense_output_dim0=dense_output_dim0,
            name=None
        )
        print(""SparseSegmentSumGradV2 executed successfully on CPU"")
    except Exception as e:
        print(f""Exception on CPU: {e}"")
        
with tf.device('/gpu:0'):
    try:
        grad = tf.constant([0, 0, 0, 0], dtype=tf.float64, shape=[4])
        indices = tf.constant([-1], dtype=tf.int64, shape=[1])
        segment_ids = tf.constant([-1], dtype=tf.int64, shape=[1])
        dense_output_dim0 = tf.constant([1], dtype=tf.int32, shape=[])

        tf.raw_ops.SparseSegmentSumGradV2(
            grad=grad,
            indices=indices,
            segment_ids=segment_ids,
            dense_output_dim0=dense_output_dim0,
            name=None
        )
        print(""SparseSegmentSumGradV2 executed successfully on GPU"")
    except Exception as e:
        print(f""Exception on GPU: {e}"")
```

### Relevant log output

```shell
Output:


I0000 00:00:1748278378.253343  624834 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2033 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9
2025-05-27 00:52:58.280430: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Index -1 out of range [0, 1).
Exception on CPU: {{function_node __wrapped__SparseSegmentSumGradV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Index -1 out of range [0, 1). [Op:SparseSegmentSumGradV2] name: 
SparseSegmentSumGradV2 executed successfully on GPU
```",1
Fatal crash in `tf.raw_ops.TensorScatterMax` with malformed indices,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.20.0-dev20250516

### Custom code

Yes

### OS platform and distribution

Linux CPU

### Mobile device

_No response_

### Python version

3.12.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.raw_ops.TensorScatterMax` causes a fatal crash with Aborted (core dumped) when provided with malformed indices tensor. 

The issue can be reproduced in this Google Colab notebook: https://colab.research.google.com/drive/19n9h_phK6X1usawpwCdp1S9hFnQR5Q9N?usp=sharing

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

print(""TensorFlow version:"", tf.__version__)

# Create tensors with problematic shapes
input_tensor = tf.constant(np.random.random((1, 10, 4)), dtype=tf.float64)
indices_tensor = tf.constant([[[0]]], dtype=tf.int64)  # Malformed: shape (1,1,1) instead of (1,3)
updates_tensor = tf.constant([0.0], dtype=tf.float64)

print(""Input Tensor shape:"", input_tensor.shape)
print(""Indices Tensor shape:"", indices_tensor.shape) 
print(""Updates Tensor shape:"", updates_tensor.shape)

print(""--- Testing TensorScatterMax ---"")
# This causes a fatal crash
result = tf.raw_ops.TensorScatterMax(
    tensor=input_tensor,
    indices=indices_tensor,
    updates=updates_tensor
)
```

### Relevant log output

```shell
2025-05-25 19:56:43.286749: F tensorflow/core/framework/tensor_shape.cc:359] Check failed: d < dims() (1 vs. 1)
Aborted (core dumped)
```",2
Fatal crash in `tf.raw_ops.TensorScatterMin` with malformed indices,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.20.0-dev20250516

### Custom code

Yes

### OS platform and distribution

Linux

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.raw_ops.TensorScatterMin` causes a fatal crash with Aborted (core dumped) when provided with malformed indices tensor. 

The issue can be reproduced in this Google Colab notebook: https://colab.research.google.com/drive/1XZ33kWSTCkKA_INqY4QOvgbvyVyeND2l?usp=sharing

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

print(""TensorFlow version:"", tf.__version__)

# Create tensors with problematic shapes
input_tensor = tf.constant(np.random.random((1, 10, 4)), dtype=tf.float64)
indices_tensor = tf.constant([[[0]]], dtype=tf.int64)  # Malformed: shape (1,1,1) instead of (1,3)
updates_tensor = tf.constant([0.0], dtype=tf.float64)

print(""Input Tensor shape:"", input_tensor.shape)
print(""Indices Tensor shape:"", indices_tensor.shape) 
print(""Updates Tensor shape:"", updates_tensor.shape)

# This causes a fatal crash
result = tf.raw_ops.TensorScatterMin(
    tensor=input_tensor,
    indices=indices_tensor,
    updates=updates_tensor
)
```

### Relevant log output

```shell
2025-05-25 19:52:28.822273: F tensorflow/core/framework/tensor_shape.cc:359] Check failed: d < dims() (1 vs. 1)
Aborted (core dumped)
```",2
Segmentation Fault in `tf.raw_ops.ResourceSparseApplyMomentum`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.20.0-dev20250516

### Custom code

Yes

### OS platform and distribution

Linux

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.raw_ops.ResourceSparseApplyMomentum` causes a segmentation fault when called with mismatched tensor shapes, specifically when the gradient tensor shape doesn't align with the variable dimensions and indices.

Fatal Error: `Check failed: d < dims() (1 vs. 1) in tensorflow/core/framework/tensor_shape.cc:359`

Stack Trace Location:

Primary failure point: `tensorflow::SparseApplyMomentumOp<Eigen::bfloat16, int>::Compute`
Shape validation failure: `tensorflow::TensorShapeBase<tensorflow::TensorShape>::dim_size(int) const`

colab: https://colab.research.google.com/drive/1OZrZ-tWh6m3ojrLX_2vsGIVsuzj1DQEg?usp=sharing

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

print(""TensorFlow version:"", tf.__version__)

# Create resource variables
var = tf.Variable(np.random.random((6, 6)).astype(np.float32), dtype=tf.bfloat16)
accum = tf.Variable(np.zeros((6, 6)).astype(np.float32), dtype=tf.bfloat16)

# Create tensors with problematic configurations
lr_tensor = tf.constant(0.0, dtype=tf.bfloat16)
grad_tensor = tf.constant(np.zeros(10), dtype=tf.bfloat16)  # Shape [10] - mismatch
indices_tensor = tf.constant([0, 0, 0, 1, 1, 2, 3, 4], dtype=tf.int32)  # 8 indices
momentum_tensor = tf.constant(0.9, dtype=tf.bfloat16)

# This crashes the process
result = tf.raw_ops.ResourceSparseApplyMomentum(
    var=var.handle,
    accum=accum.handle,
    lr=lr_tensor,
    grad=grad_tensor,
    indices=indices_tensor,
    momentum=momentum_tensor,
    use_locking=True,
    use_nesterov=True
)
```

### Relevant log output

```shell
2025-05-25 19:36:24.885970: F tensorflow/core/framework/tensor_shape.cc:359] Check failed: d < dims() (1 vs. 1)
Aborted (core dumped)
```",2
Crash in `tf.raw_ops.ResourceSparseApplyAdagradDA`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.20.0-dev20250516

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.12.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The `tf.raw_ops.ResourceSparseApplyAdagradDA` operation causes a fatal crash with the error `Check failed: d < dims() (1 vs. 0)` when called with a scalar gradient tensor and multi-dimensional indices tensor.



Colab Reproduction: https://colab.research.google.com/drive/1X_plMhFhjig9v4zwmIHkEiyQocY5vXfA?usp=sharing

Stack Trace Location: The crash occurs in `tensorflow::SparseApplyAdagradDAOp<float, int>::Compute()` at line 2467 in `/tensorflow/core/kernels/training_ops.cc`, specifically when calling `Tensor::dim_size(int)`` on a tensor with incompatible dimensions.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# Create variable and accumulators
var = tf.Variable([[0.0, 0.0]] * 10, dtype=tf.float32)
gradient_accumulator = tf.Variable([[0.0, 0.0]] * 10, dtype=tf.float32)
gradient_squared_accumulator = tf.Variable([[0.0, 0.0]] * 10, dtype=tf.float32)

# Problematic parameters - scalar grad with 2D indices
grad = tf.constant(0.0, dtype=tf.float32)  # Scalar gradient
indices = tf.constant([0, 0], dtype=tf.int32)  # 2D indices

# Other parameters
lr = tf.constant(0.0, dtype=tf.float32)
l1 = tf.constant(0.0, dtype=tf.float32)
l2 = tf.constant(0.0, dtype=tf.float32)
global_step = tf.constant(1, dtype=tf.int64)

# This will crash
tf.raw_ops.ResourceSparseApplyAdagradDA(
    var=var.handle,
    gradient_accumulator=gradient_accumulator.handle,
    gradient_squared_accumulator=gradient_squared_accumulator.handle,
    grad=grad,
    indices=indices,
    lr=lr,
    l1=l1,
    l2=l2,
    global_step=global_step,
    use_locking=True
)
```

### Relevant log output

```shell
2025-05-25 19:23:06.603818: F tensorflow/core/framework/tensor_shape.cc:359] Check failed: d < dims() (1 vs. 0)
Aborted (core dumped)
```",2
Segmentation Fault in `tf.raw_ops.HistogramFixedWidth`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.20.0-dev20250516

### Custom code

Yes

### OS platform and distribution

Linux CPU 

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?



`tf.raw_ops.HistogramFixedWidth` causes a segmentation fault when processing large int64 values with int32 output dtype during graph optimization phase.

Colab Reproduction: https://colab.research.google.com/drive/1IsOQ0LXGHjezeselGGNHUoxj_Wl3OvWf?usp=sharing






### Standalone code to reproduce the issue

```shell
import tensorflow as tf

def reproduce_crash():
    # Large int64 values that trigger the crash
    values = tf.constant([
        [-9114860691027166365, 9114861777597660793],
        [9114861777597660798, -9114860691027166365],
        [9114861777597660793, 9114861777597660798],
        [9114860691027166365, -9114861777597660793]
    ], dtype=tf.int64)
    
    # Value range
    value_range = tf.constant([9114861777597660672, 9114861777597660798], dtype=tf.int64)
    
    # Number of bins
    nbins = tf.constant(35, dtype=tf.int32)
    
    # This causes segmentation fault
    result = tf.raw_ops.HistogramFixedWidth(
        values=values,
        value_range=value_range,
        nbins=nbins,
        dtype=tf.int32
    )
    return result

# Run this to reproduce the crash
reproduce_crash()
```

### Relevant log output

```shell
Segmentation fault (core dumped)


Stack:
The crash occurs during constant folding optimization:

tensorflow::functor::HistogramFixedWidthFunctor<Eigen::ThreadPoolDevice, long, int>::Compute
└── tensorflow::HistogramFixedWidthOp<Eigen::ThreadPoolDevice, long, int>::Compute  
    └── tensorflow::grappler::ConstantFolding::EvaluateNode
        └── [Graph optimization pipeline]
```",2
`Segmentation Fault` in CropAndResizeGradBoxes,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.20.0-dev20250516

### Custom code

Yes

### OS platform and distribution

Linux (Docker container) CPU with AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

TensorFlow crashes with a segmentation fault (SEGV) when using tf.raw_ops.CropAndResizeGradBoxes with NaN values in the boxes tensor. The crash occurs during memory access in the crop and resize backpropagation computation at /tensorflow/core/kernels/image/crop_and_resize_op.cc:743:36. The NaN values cause invalid memory addresses to be computed, leading to out-of-bounds memory access and process termination.

The issue can be reproduced in this Colab notebook:
https://colab.research.google.com/drive/1nZmPF_gqw6wLMIlhqT_1-g7NieOml_U0?usp=sharing

The crash occurs in the CropAndResizeBackpropBoxes functor:
```
#0 tensorflow::functor::CropAndResizeBackpropBoxes<Eigen::ThreadPoolDevice, unsigned short>::operator()
   /proc/self/cwd/tensorflow/core/kernels/image/crop_and_resize_op.cc:743:36
```

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
import os

# oneDNN may be involved but crash occurs regardless
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'

# Grads tensor - shape [2,3,3,2] with extreme float values
grads_data = np.array([
    [[[-1.14306452e+18, -4.13623095e-14], [2.83210481e+20, 1.0], [1.0, 1.0]],
     [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]],
     [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]]],
    [[[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]],
     [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]], 
     [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]]]
], dtype=np.float32)
grads = tf.constant(grads_data, dtype=tf.float32)

# Image tensor - shape [1,5,5,2] with uint16 values
image_data = np.random.randint(0, 65535, size=(1, 5, 5, 2), dtype=np.uint16)
image = tf.constant(image_data, dtype=tf.uint16)

# CRITICAL: Boxes tensor with NaN values - this causes the crash
boxes_data = np.array([
    [np.nan, 1.0, 1.0, 1.0],  # NaN in first coordinate
    [0.0, 0.0, 1.0, 1.0]      # Valid box
], dtype=np.float32)
boxes = tf.constant(boxes_data, dtype=tf.float32)

# Box indices tensor
box_ind = tf.constant([0, 0], dtype=tf.int32)

# This call causes segmentation fault due to NaN in boxes
result = tf.raw_ops.CropAndResizeGradBoxes(
    grads=grads,
    image=image,
    boxes=boxes,
    box_ind=box_ind
)
```

### Relevant log output

```shell
Actual Behavior:

Segmentation fault (core dumped)


Complete Error Log:

2025-05-24 18:26:29.840318: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-24 18:26:29.889297: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-05-24 18:26:30.924979: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
TensorFlow version: 2.20.0-dev20250516
2025-05-24 18:26:31.200686: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
Input tensor details:
Grads shape: (2, 3, 3, 2), dtype: <dtype: 'float32'>
Image shape: (1, 5, 5, 2), dtype: <dtype: 'uint16'>
Boxes shape: (2, 4), dtype: <dtype: 'float32'>
Box indices shape: (2,), dtype: <dtype: 'int32'>

Tensor values:
Grads (first few): [-1.1430645e+18 -4.1362309e-14  2.8321048e+20  1.0000000e+00
  1.0000000e+00  1.0000000e+00]
Image (first few): [17572 16952 32988 18161 57827  6528]
Boxes: [[nan  1.  1.  1.]
 [ 0.  0.  1.  1.]]
Box indices: [0 0]
Segmentation fault (core dumped)


Full Stack Trace:

#0 tensorflow::functor::CropAndResizeBackpropBoxes<Eigen::ThreadPoolDevice, unsigned short>::operator() 
   /tensorflow/core/kernels/image/crop_and_resize_op.cc:743:36
#1 tensorflow::CropAndResizeGradBoxesOp<Eigen::ThreadPoolDevice, unsigned short>::ComputeAsync()::lambda()::operator()() 
   /tensorflow/core/kernels/image/crop_and_resize_op.cc:658:27
#2-#7 [TensorFlow executor and threading infrastructure]
```",2
FPE in oneDNN `MaxPool3D` with Invalid Dimensions,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.20.0-dev20250516

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.12.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

TensorFlow crashes with Floating Point Exception (FPE) when using `tf.raw_ops.MaxPool3D` with oneDNN optimizations enabled (`TF_ENABLE_ONEDNN_OPTS=1`). The issue is specific to oneDNN-optimized code paths and does not occur with oneDNN disabled.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
import os

# CRITICAL: Crash only occurs with oneDNN enabled
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'

# Create input tensor with problematic dimensions and extreme bfloat16 values
input_data = np.full((9, 6, 7, 4, 4), 5.00741786e-32, dtype=np.float32)
input_tensor = tf.constant(input_data, dtype=tf.bfloat16)

# Pooling parameters that create invalid output dimensions
ksize = [1, 4, 5, 5, 1]      # [batch, depth, height, width, channels]
strides = [1, 3, 2, 1, 1]    # [batch, depth, height, width, channels]
padding = ""VALID""
data_format = ""NDHWC""

# This call triggers division by zero in oneDNN
result = tf.raw_ops.MaxPool3D(
    input=input_tensor,
    ksize=ksize,
    strides=strides,
    padding=padding,
    data_format=data_format
)
```

### Relevant log output

```shell
2025-05-24 18:14:23.759566: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-24 18:14:23.807014: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-05-24 18:14:24.775612: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
TensorFlow version: 2.20.0-dev20250516
2025-05-24 18:14:25.002516: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
Input parameters:
Input shape: (9, 6, 7, 4, 4)
Input dtype: <dtype: 'bfloat16'>
Ksize: [1, 4, 5, 5, 1]
Strides: [1, 3, 2, 1, 1]
Padding: VALID
Data format: NDHWC

Input values (first few): [5.00742e-32 5.00742e-32 5.00742e-32 5.00742e-32 5.00742e-32 5.00742e-32
 5.00742e-32 5.00742e-32 5.00742e-32 5.00742e-32]
Floating point exception (core dumped)


Full Stack Trace:

#0 dnnl::impl::utils::div_up<int, int>(int, int) /external/onednn/src/common/utils.hpp:313:72
#1 dnnl::impl::cpu::x64::jit_uni_pool_kernel<...>::generate()::lambda(int, bool) /external/onednn/src/cpu/x64/jit_uni_pool_kernel.cpp:1543:37
#2 dnnl::impl::cpu::x64::jit_uni_pool_kernel<...>::generate() /external/onednn/src/cpu/x64/jit_uni_pool_kernel.cpp:1605:5
#3 dnnl::impl::cpu::x64::jit_generator::create_kernel() /external/onednn/src/cpu/x64/jit_generator.hpp:2723:9
#4 dnnl::impl::cpu::x64::jit_uni_pooling_fwd_t<...>::init(dnnl_engine*) /external/onednn/src/cpu/x64/jit_uni_pooling.cpp:501:21
...
#22 tensorflow::MklMaxPoolingOp<Eigen::ThreadPoolDevice, Eigen::bfloat16, true>::Compute(tensorflow::OpKernelContext*) /tensorflow/core/kernels/mkl/mkl_maxpooling_op.cc:152:21
```",2
Floating Point Exception in `AvgPool3DGrad`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.20.0-dev20250516

### Custom code

Yes

### OS platform and distribution

Linux (Docker container)

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

TensorFlow crashes with a **Floating Point Exception (FPE)** and **core dump** when using `tf.raw_ops.AvgPool3DGrad` with small bfloat16 values. The crash occurs during gradient computation for 3D average pooling, likely due to division by zero or invalid floating-point arithmetic in the pooling area calculation

It need to be reproduced with `TF_ENABLE_ONEDNN_OPTS=1`

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

# Create input tensors matching the crash scenario
orig_input_shape = tf.constant([8, 10, 7, 1, 9], dtype=tf.int32)

# Grad tensor with extremely small bfloat16 values that trigger the crash
grad_data = np.full((8, 10, 7, 1, 9), 9.18355e-41, dtype=np.float32)  
grad = tf.constant(grad_data, dtype=tf.bfloat16)

# Pool parameters
ksize = [1, 2, 2, 2, 1]      # [batch, depth, height, width, channels]
strides = [1, 1, 1, 1, 1]    # [batch, depth, height, width, channels]
padding = ""VALID""
data_format = ""NDHWC""

# This call causes the floating point exception and core dump
result = tf.raw_ops.AvgPool3DGrad(
    orig_input_shape=orig_input_shape,
    grad=grad,
    ksize=ksize,
    strides=strides,
    padding=padding,
    data_format=data_format
)
```

### Relevant log output

```shell
Floating point exception (core dumped)



2025-05-24 18:05:18.592275: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-24 18:05:18.641089: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-05-24 18:05:19.612237: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
TensorFlow version: 2.20.0-dev20250516
2025-05-24 18:05:19.828300: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
Input parameters:
Original input shape: [ 8 10  7  1  9]
Grad shape: (8, 10, 7, 1, 9)
Grad dtype: <dtype: 'bfloat16'>
Ksize: [1, 2, 2, 2, 1]
Strides: [1, 1, 1, 1, 1]
Padding: VALID
Data format: NDHWC

Grad values (first few): [9.18355e-41 9.18355e-41 9.18355e-41 9.18355e-41 9.18355e-41 9.18355e-41
 9.18355e-41 9.18355e-41 9.18355e-41 9.18355e-41]
Floating point exception (core dumped)


Some stack trace:

    #0 0x57f1d5a9bfd2 in dnnl::impl::utils::remove_reference<int>::type dnnl::impl::utils::div_up<int, int>(int, int) /proc/self/cwd/external/onednn/src/common/utils.hpp:313:72
    #1 0x57f1d7db5806 in dnnl::impl::cpu::x64::jit_uni_pool_kernel<(dnnl::impl::cpu::x64::cpu_isa_t)880>::generate()::'lambda'(int, bool)::operator()(int, bool) const /proc/self/cwd/external/onednn/src/cpu/x64/jit_uni_pool_kernel.cpp:1543:37
    #2 0x57f1d7db4f59 in dnnl::impl::cpu::x64::jit_uni_pool_kernel<(dnnl::impl::cpu::x64::cpu_isa_t)880>::generate() /proc/self/cwd/external/onednn/src/cpu/x64/jit_uni_pool_kernel.cpp:1605:5
    #3 0x57f1d58b29b2 in dnnl::impl::cpu::x64::jit_generator::create_kernel() /proc/self/cwd/external/onednn/src/cpu/x64/jit_generator.hpp:2723:9
    #4 0x57f1d7de5ac8 in dnnl::impl::cpu::x64::jit_uni_pooling_bwd_t<(dnnl::impl::cpu::x64::cpu_isa_t)880, (dnnl_data_type_t)2>::init(dnnl_engine*) /proc/self/cwd/external/onednn/src/cpu/x64/jit_uni_pooling.cpp:886:21
    #5 0x57f1d5760915 in dnnl::impl::primitive_t::init(dnnl_engine*, bool, dnnl::impl::cache_blob_t const&) /proc/self/cwd/external/onednn/src/common/primitive.hpp:53:9
    #6 0x57f1d5e5754b in dnnl_status_t dnnl::impl::primitive_t::create_primitive_common<dnnl::impl::cpu::x64::jit_uni_pooling_bwd_t<(dnnl::impl::cpu::x64::cpu_isa_t)880, (dnnl_data_type_t)2>, dnnl::impl::cpu::x64::jit_uni_pooling_bwd_t<(dnnl::impl::cpu::x64::cpu_isa_t)880, (dnnl_data_type_t)2>::pd_t>(std::pair<std::shared_ptr<dnnl::impl::primitive_t>, bool>&, dnnl::impl::cpu::x64::jit_uni_pooling_bwd_t<(dnnl::impl::cpu::x64::cpu_isa_t)880, (dnnl_data_type_t)2>::pd_t const*, dnnl_engine*, bool, dnnl::impl::cache_blob_t const&)::'lambda'(void*)::operator()(void*) const /proc/self/cwd/external/onednn/src/common/primitive.hpp:107:26
    #7 0x57f1d5e57464 in dnnl_status_t dnnl::impl::primitive_t::create_primitive_common<dnnl::impl::cpu::x64::jit_uni_pooling_bwd_t<(dnnl::impl::cpu::x64::cpu_isa_t)880, (dnnl_data_type_t)2>, dnnl::impl::cpu::x64::jit_uni_pooling_bwd_t<(dnnl::impl::cpu::x64::cpu_isa_t)880, (dnnl_data_type_t)2>::pd_t>(std::pair<std::shared_ptr<dnnl::impl::primitive_t>, bool>&, dnnl::impl::cpu::x64::jit_uni_pooling_bwd_t<(dnnl::impl::cpu::x64::cpu_isa_t)880, (dnnl_data_type_t)2>::pd_t const*, dnnl_engine*, bool, dnnl::impl::cache_blob_t const&)::'lambda'(void*)::__invoke(void*) /proc/self/cwd/external/onednn/src/common/primitive.hpp:103:61
    #8 0x57f1d5512e32 in dnnl::impl::utils::cache_t<dnnl::impl::primitive_hashing::key_t, dnnl::impl::primitive_t, dnnl::impl::primitive_cache_iface_t::result_t, &dnnl::impl::primitive_cache_t::update_key(dnnl::impl::primitive_hashing::key_t const&, dnnl::impl::primitive_t const&)>::get_or_create(dnnl::impl::primitive_hashing::key_t const&, dnnl::impl::primitive_cache_iface_t::result_t (&)(void*), void*) /proc/self/cwd/external/onednn/src/common/cache_utils.hpp:90:33
    #9 0x57f1d55074b0 in dnnl::impl::primitive_cache_t::get_or_create(dnnl::impl::primitive_hashing::key_t const&, dnnl::impl::primitive_cache_iface_t::result_t (&)(void*), void*) /proc/self/cwd/external/onednn/src/common/primitive_cache.cpp:52:23
    #10 0x57f1d5507383 in dnnl::impl::primitive_cache_iface_t::get_or_create(dnnl::impl::primitive_hashing::key_t const&, dnnl::impl::primitive_cache_iface_t::result_t (&)(void*), void*) /proc/self/cwd/external/onednn/src/common/primitive_cache.cpp:132:21
    #11 0x57f1d5e572cf in dnnl_status_t dnnl::impl::primitive_t::create_primitive_common<dnnl::impl::cpu::x64::jit_uni_pooling_bwd_t<(dnnl::impl::cpu::x64::cpu_isa_t)880, (dnnl_data_type_t)2>, dnnl::impl::cpu::x64::jit_uni_pooling_bwd_t<(dnnl::impl::cpu::x64::cpu_isa_t)880, (dnnl_data_type_t)2>::pd_t>(std::pair<std::shared_ptr<dnnl::impl::primitive_t>, bool>&, dnnl::impl::cpu::x64::jit_uni_pooling_bwd_t<(dnnl::impl::cpu::x64::cpu_isa_t)880, (dnnl_data_type_t)2>::pd_t const*, dnnl_engine*, bool, dnnl::impl::cache_blob_t const&) /proc/self/cwd/external/onednn/src/common/primitive.hpp:112:42
    #12 0x57f1d5e56692 in dnnl::impl::cpu::x64::jit_uni_pooling_bwd_t<(dnnl::impl::cpu::x64::cpu_isa_t)880, (dnnl_data_type_t)2>::pd_t::create_primitive(std::pair<std::shared_ptr<dnnl::impl::primitive_t>, bool>&, dnnl_engine*, dnnl::impl::cache_blob_t const&) const /proc/self/cwd/external/onednn/src/cpu/x64/jit_uni_pooling.hpp:124:9
    #13 0x57f1d551aa91 in dnnl_primitive_desc::create_primitive_iface(std::pair<dnnl_primitive*, bool>&, dnnl::impl::cache_blob_t const&) const /proc/self/cwd/external/onednn/src/common/primitive_desc_iface.cpp:98:27
    #14 0x57f1d552d784 in dnnl::impl::primitive_create(dnnl_primitive**, dnnl_primitive_desc const*, dnnl::impl::cache_blob_t const&) /proc/self/cwd/external/onednn/src/common/primitive_iface.cpp:80:9
    #15 0x57f1d552e405 in dnnl_primitive_create /proc/self/cwd/external/onednn/src/common/primitive_iface.cpp:162:12
    #16 0x57f1cf1cbfcd in dnnl::primitive::primitive(dnnl_primitive_desc const*) /proc/self/cwd/external/onednn/include/oneapi/dnnl/dnnl.hpp:13752:23
    #17 0x57f1cf26feb2 in dnnl::primitive::primitive(dnnl::primitive_desc const&) /proc/self/cwd/external/onednn/include/oneapi/dnnl/dnnl.hpp:13768:57
    #18 0x57f1cfb43dd0 in dnnl::pooling_backward::pooling_backward(dnnl::pooling_backward::primitive_desc const&) /proc/self/cwd/external/onednn/include/oneapi/dnnl/dnnl.hpp:13205:50
    #19 0x57f1cfb44efe in tensorflow::MklPoolingBwdPrimitive<Eigen::bfloat16>::Setup(tensorflow::MklPoolingParams const&) /proc/self/cwd/tensorflow/core/kernels/mkl/mkl_pooling_ops_common.cc:211:28
    #20 0x57f1cfaf98d2 in tensorflow::MklPoolingBwdPrimitive<Eigen::bfloat16>::MklPoolingBwdPrimitive(tensorflow::MklPoolingParams const&) /proc/self/cwd/./tensorflow/core/kernels/mkl/mkl_pooling_ops_common.h:251:34
    #21 0x57f1cfaf94b8 in tensorflow::MklPoolingBwdPrimitiveFactory<Eigen::bfloat16>::Get(tensorflow::MklPoolingParams const&) /proc/self/cwd/./tensorflow/core/kernels/mkl/mkl_pooling_ops_common.h:353:30
    #22 0x57f1cfafe210 in tensorflow::MklAvgPoolingGradOp<Eigen::ThreadPoolDevice, Eigen::bfloat16, true>::Compute(tensorflow::OpKernelContext*) /proc/self/cwd/tensorflow/core/kernels/mkl/mkl_avgpooling_op.cc:352:11
    #23 0x57f1ddeb9dad in tensorflow::Device::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) /proc/self/cwd/./tensorflow/core/framework/device.h:93:16
    #24 0x57f1df7190ff in tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ProcessSync(tensorflow::NodeItem const&, tensorflow::OpKernelContext::Params*, absl::lts_20230802::InlinedVector<tensorflow::Entry, 4ul, std::allocator<tensorflow::Entry>>*, tensorflow::NodeExecStatsInterface*) /proc/self/cwd/tensorflow/core/common_runtime/executor.cc:610:13
    #25 0x57f1df71628f in tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ProcessInline(tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*, long) /proc/self/cwd/tensorflow/core/common_runtime/executor.cc:930:13
    #26 0x57f1df710291 in tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::Process(tensorflow::SimplePropagatorState::TaggedNode const&, long) /proc/self/cwd/tensorflow/core/common_runtime/executor.cc:737:10
    #27 0x57f1df7132de in tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()::operator()() const /proc/self/cwd/tensorflow/core/common_runtime/executor.cc:1308:25
    #28 0x57f1df713271 in void tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::RunTask<tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()>(tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()&&, int)::'lambda'()::operator()() /proc/self/cwd/tensorflow/core/common_runtime/executor.cc:489:5
    #29 0x57f1df7130f0 in tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'() std::__invoke_impl<void, void tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::RunTask<tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()>(tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()&&, int)::'lambda'()&>(std::__invoke_other, void tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::RunTask<tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()>(tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()&&, int)::'lambda'()&) /usr/lib/gcc/x86_64-linux-gnu/13/../../../../include/c++/13/bits/invoke.h:61:14
    #30 0x57f1df713030 in std::enable_if<is_invocable_r_v<tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'(), void tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::RunTask<tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()>(tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()&&, int)::'lambda'()&>, tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()>::type std::__invoke_r<void, void tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::RunTask<tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()>(tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()&&, int)::'lambda'()&>(void tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::RunTask<tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()>(tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()&&, int)::'lambda'()&) /usr/lib/gcc/x86_64-linux-gnu/13/../../../../include/c++/13/bits/invoke.h:111:2
    #31 0x57f1df712dc8 in std::_Function_handler<void (), void tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::RunTask<tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()>(tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()&&, int)::'lambda'()>::_M_invoke(std::_Any_data const&) /usr/lib/gcc/x86_64-linux-gnu/13/../../../../include/c++/13/bits/std_function.h:290:9
    #32 0x57f1c7d00617 in std::function<void ()>::operator()() const /usr/lib/gcc/x86_64-linux-gnu/13/../../../../include/c++/13/bits/std_function.h:591:9
    #33 0x57f1df685578 in tensorflow::GraphRunner::Run(tensorflow::Graph*, tensorflow::FunctionLibraryRuntime*, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>>> const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>> const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*)::$_2::operator()(std::function<void ()>) const /proc/self/cwd/tensorflow/core/common_runtime/graph_runner.cc:156:49
    #34 0x57f1df685505 in void std::__invoke_impl<void, tensorflow::GraphRunner::Run(tensorflow::Graph*, tensorflow::FunctionLibraryRuntime*, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>>> const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>> const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*)::$_2&, std::function<void ()>>(std::__invoke_other, tensorflow::GraphRunner::Run(tensorflow::Graph*, tensorflow::FunctionLibraryRuntime*, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>>> const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>> const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*)::$_2&, std::function<void ()>&&) /usr/lib/gcc/x86_64-linux-gnu/13/../../../../include/c++/13/bits/invoke.h:61:14
    #35 0x57f1df685420 in std::enable_if<is_invocable_r_v<void, tensorflow::GraphRunner::Run(tensorflow::Graph*, tensorflow::FunctionLibraryRuntime*, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>>> const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>> const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*)::$_2&, std::function<void ()>>, void>::type std::__invoke_r<void, tensorflow::GraphRunner::Run(tensorflow::Graph*, tensorflow::FunctionLibraryRuntime*, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>>> const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>> const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*)::$_2&, std::function<void ()>>(tensorflow::GraphRunner::Run(tensorflow::Graph*, tensorflow::FunctionLibraryRuntime*, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>>> const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>> const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*)::$_2&, std::function<void ()>&&) /usr/lib/gcc/x86_64-linux-gnu/13/../../../../include/c++/13/bits/invoke.h:111:2
    #36 0x57f1df685238 in std::_Function_handler<void (std::function<void ()>), tensorflow::GraphRunner::Run(tensorflow::Graph*, tensorflow::FunctionLibraryRuntime*, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>>> const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>> const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*)::$_2>::_M_invoke(std::_Any_data const&, std::function<void ()>&&) /usr/lib/gcc/x86_64-linux-gnu/13/../../../../include/c++/13/bits/std_function.h:290:9
    #37 0x57f1d508c097 in std::function<void (std::function<void ()>)>::operator()(std::function<void ()>) const /usr/lib/gcc/x86_64-linux-gnu/13/../../../../include/c++/13/bits/std_function.h:591:9
    #38 0x57f1df70fc5b in void tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::RunTask<tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()>(tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()&&, int) /proc/self/cwd/tensorflow/core/common_runtime/executor.cc:487:3
    #39 0x57f1df70d1d3 in tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*) /proc/self/cwd/tensorflow/core/common_runtime/executor.cc:1308:9
    #40 0x57f1df6cb284 in tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::RunAsync(std::function<void (absl::lts_20230802::Status const&)>) /proc/self/cwd/tensorflow/core/common_runtime/executor.cc:517:5
    #41 0x57f1df6c8939 in tensorflow::(anonymous namespace)::ExecutorImpl::RunAsyncInternal(tensorflow::Executor::Args const&, std::function<void (absl::lts_20230802::Status const&)>) /proc/self/cwd/tensorflow/core/common_runtime/executor.cc:1527:11
    #42 0x57f1c7ca90ed in tensorflow::Executor::RunAsync(tensorflow::Executor::Args const&, std::function<void (absl::lts_20230802::Status const&)>) /proc/self/cwd/./tensorflow/core/common_runtime/executor.h:132:5
    #43 0x57f1df5393d8 in tensorflow::Executor::Run(tensorflow::Executor::Args const&) /proc/self/cwd/./tensorflow/core/common_runtime/executor.h:142:5
    #44 0x57f1df68224d in tensorflow::GraphRunner::Run(tensorflow::Graph*, tensorflow::FunctionLibraryRuntime*, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>>> const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>> const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) /proc/self/cwd/tensorflow/core/common_runtime/graph_runner.cc:193:3
    #45 0x57f1df4ebe70 in tensorflow::ConstantFold(tensorflow::ConstantFoldingOptions const&, tensorflow::FunctionLibraryRuntime*, tsl::Env*, tensorflow::Device const*, tensorflow::Graph*, bool*) /proc/self/cwd/tensorflow/core/common_runtime/constant_folding.cc:693:21
    #46 0x57f1df4ea68b in tensorflow::GraphOptimizer::Optimize(tensorflow::FunctionLibraryRuntime*, tsl::Env*, tensorflow::Device const*, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph>>*, tensorflow::GraphOptimizer::Options const&) /proc/self/cwd/tensorflow/core/common_runtime/graph_optimizer.cc:83:7
    #47 0x57f1c7cb3933 in tensorflow::DirectSession::CreateExecutors(tensorflow::CallableOptions const&, std::unique_ptr<tensorflow::DirectSession::ExecutorsAndKeys, std::default_delete<tensorflow::DirectSession::ExecutorsAndKeys>>*, std::unique_ptr<tensorflow::DirectSession::FunctionInfo, std::default_delete<tensorflow::DirectSession::FunctionInfo>>*, tensorflow::DirectSession::RunStateArgs*) /proc/self/cwd/tensorflow/core/common_runtime/direct_session.cc:1427:15
    #48 0x57f1c7cac65a in tensorflow::DirectSession::GetOrCreateExecutors(absl::lts_20230802::Span<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>> const>, absl::lts_20230802::Span<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>> const>, absl::lts_20230802::Span<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>> const>, tensorflow::DirectSession::ExecutorsAndKeys**, tensorflow::DirectSession::RunStateArgs*) /proc/self/cwd/tensorflow/core/common_runtime/direct_session.cc:1583:3
    #49 0x57f1c7caa3ec in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>>> const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>> const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>> const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*, tensorflow::RunMetadata*, tsl::thread::ThreadPoolOptions const&) /proc/self/cwd/tensorflow/core/common_runtime/direct_session.cc:879:3
    #50 0x57f1c7ca9eeb in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>>> const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>> const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>> const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*, tensorflow::RunMetadata*) /proc/self/cwd/tensorflow/core/common_runtime/direct_session.cc:849:10
    #51 0x57f1c7c5cd1c in tensorflow::ClientSession::Run(tensorflow::RunOptions const&, std::unordered_map<tensorflow::Output, tensorflow::Input::Initializer, tensorflow::OutputHash, std::equal_to<tensorflow::Output>, std::allocator<std::pair<tensorflow::Output const, tensorflow::Input::Initializer>>> const&, std::vector<tensorflow::Output, std::allocator<tensorflow::Output>> const&, std::vector<tensorflow::Operation, std::allocator<tensorflow::Operation>> const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*, tensorflow::RunMetadata*) const /proc/self/cwd/tensorflow/cc/client/client_session.cc:131:28
    #52 0x57f1c7c5c540 in tensorflow::ClientSession::Run(std::unordered_map<tensorflow::Output, tensorflow::Input::Initializer, tensorflow::OutputHash, std::equal_to<tensorflow::Output>, std::allocator<std::pair<tensorflow::Output const, tensorflow::Input::Initializer>>> const&, std::vector<tensorflow::Output, std::allocator<tensorflow::Output>> const&, std::vector<tensorflow::Operation, std::allocator<tensorflow::Operation>> const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) const /proc/self/cwd/tensorflow/cc/client/client_session.cc:89:10
    #53 0x57f1c7c5c428 in tensorflow::ClientSession::Run(std::vector<tensorflow::Output, std::allocator<tensorflow::Output>> const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) const /proc/self/cwd/tensorflow/cc/client/client_session.cc:76:10
    #54 0x57f1c7c5221c in LLVMFuzzerTestOneInput /proc/self/cwd/fuzz/tf.raw_ops.AvgPool3DGrad/fuzz.cpp:195:45
    #55 0x57f1c7c1da0a in fuzzer::Fuzzer::ExecuteCallback(unsigned char const*, unsigned long) crtstuff.c
    #56 0x57f1c7c057d3 in fuzzer::RunOneTest(fuzzer::Fuzzer*, char const*, unsigned long) crtstuff.c
    #57 0x57f1c7c0b991 in fuzzer::FuzzerDriver(int*, char***, int (*)(unsigned char const*, unsigned long)) crtstuff.c
    #58 0x57f1c7c370a6 in main (/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/bazel-out/k8-opt/bin/fuzz/tf.raw_ops.AvgPool3DGrad/fuzz+0xcf380a6) (BuildId: 1e0e6a184195e9b05743c88318128017bcc38166)
    #59 0x72029e9381c9  (/lib/x86_64-linux-gnu/libc.so.6+0x2a1c9) (BuildId: 42c84c92e6f98126b3e2230ebfdead22c235b667)
    #60 0x72029e93828a in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x2a28a) (BuildId: 42c84c92e6f98126b3e2230ebfdead22c235b667)
    #61 0x57f1c7c003a4 in _start (/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/bazel-out/k8-opt/bin/fuzz/tf.raw_ops.AvgPool3DGrad/fuzz+0xcf013a4) (BuildId: 1e0e6a184195e9b05743c88318128017bcc38166)

==868004==Register values:
rax = 0x00000000ffffffff  rbx = 0x00007ffeec726a60  rcx = 0x0000000000000000  rdx = 0x00000000ffffffff  
rdi = 0x0000000000000000  rsi = 0x0000000000000000  rbp = 0x00007ffeec7249e0  rsp = 0x00007ffeec7249e0  
 r8 = 0x0000000000001b50   r9 = 0x000000000000000b  r10 = 0x000057f210b3b860  r11 = 0x00007ffeec7249b0  
r12 = 0x000057f1e66d86b0  r13 = 0x000057f1e6180a00  r14 = 0x00007ffeec726a78  r15 = 0x000057f210a4d860  
UndefinedBehaviorSanitizer can not provide additional info.
SUMMARY: UndefinedBehaviorSanitizer: FPE /proc/self/cwd/external/onednn/src/common/utils.hpp:313:72 in dnnl::impl::utils::remove_reference<int>::type dnnl::impl::utils::div_up<int, int>(int, int)
```",2
Crash in `SparseBincount` due to overflow,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.20.0-dev20250516

### Custom code

Yes

### OS platform and distribution

Linux (Docker container)

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

TensorFlow crashes with a fatal error `Non-OK-status: InitDims(dim_sizes)` when using `tf.raw_ops.SparseBincount` with large int64 values in the `dense_shape` parameter. The crash occurs in `tensorflow/core/framework/tensor_shape.cc:202` due to integer overflow when multiplying dimension sizes: `8970181431921507452 * 2088533116 = -1 (overflow result)`.

The issue can be reproduced in this Colab notebook:
https://colab.research.google.com/drive/1Tc3X-iRD3cE_QoyikVXacsEHGO_cqmbC?usp=sharing

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# Create tensors with problematic large integer values
indices = tf.constant([[3026414551496130560, 2738188573441261603],
                      [8970181431921507328, 8970181431921507328]], 
                     dtype=tf.int64)

values = tf.constant([2088533116, 2088533116], dtype=tf.int32)

# This dense_shape causes integer overflow when dimensions are multiplied
dense_shape = tf.constant([8970181431921507452, 8970181431921507452], dtype=tf.int64)

size = tf.constant(2088533116, dtype=tf.int32)
weights = tf.constant([], dtype=tf.int32)

# This call causes the fatal crash
result = tf.raw_ops.SparseBincount(
    indices=indices,
    values=values,
    dense_shape=dense_shape,
    size=size,
    weights=weights,
    binary_output=False
)
```

### Relevant log output

```shell
2025-05-24 17:48:55.931637: F tensorflow/core/framework/tensor_shape.cc:202] Non-OK-status: InitDims(dim_sizes)
Status: INVALID_ARGUMENT: Encountered overflow when multiplying 8970181431921507452 with 2088533116, result: -1
Aborted (core dumped)


Complete Log:

2025-05-24 17:48:54.678039: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-24 17:48:54.721352: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-05-24 17:48:55.665479: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
TensorFlow version: 2.20.0-dev20250516
2025-05-24 17:48:55.875841: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
Input tensor details:
Indices shape: (2, 2), dtype: <dtype: 'int64'>
Values shape: (2,), dtype: <dtype: 'int32'>
Dense shape: (2,), dtype: <dtype: 'int64'>
Size: 2088533116
Weights shape: (0,)
Binary output: False

Tensor values:
Indices: [[3026414551496130560 2738188573441261603]
 [8970181431921507328 8970181431921507328]]
Values: [2088533116 2088533116]
Dense shape: [8970181431921507452 8970181431921507452]
2025-05-24 17:48:55.931637: F tensorflow/core/framework/tensor_shape.cc:202] Non-OK-status: InitDims(dim_sizes)
Status: INVALID_ARGUMENT: Encountered overflow when multiplying 8970181431921507452 with 2088533116, result: -1
Aborted (core dumped)
```",2
Crash in `ResourceSparseApplyProximalAdagrad`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.20.0-dev20250516

### Custom code

Yes

### OS platform and distribution

Linux (Docker container)

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

TensorFlow crashes with a fatal error `Check failed: d < dims() (1 vs. 1)` when using tf.raw_ops.ResourceSparseApplyProximalAdagrad with large float32 values. 

The crash occurs in tensorflow/core/framework/tensor_shape.cc:359 and results in an aborted process with core dump.

Should raise a Python exception with a descriptive error message.

The issue can be reproduced in this Colab notebook: https://colab.research.google.com/drive/1HoC35YNBUC-Fs_6vOgrimGoLxqBisKHr?usp=sharing

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

# Create resource variables with extreme values
large_val = 5.24393461e+36

var = tf.Variable([[large_val, large_val, large_val],
                   [large_val, large_val, large_val]], 
                  dtype=tf.float32, name=""var"")

accum = tf.Variable([[large_val, large_val, large_val],
                     [large_val, large_val, large_val]], 
                    dtype=tf.float32, name=""accum"")

# Scalar tensors with large values
lr = tf.constant(large_val, dtype=tf.float32)
l1 = tf.constant(large_val, dtype=tf.float32) 
l2 = tf.constant(large_val, dtype=tf.float32)

# Gradient and indices
grad = tf.constant([7.90505e+31], dtype=tf.float32)
indices = tf.constant([0], dtype=tf.int32)

# This call causes the crash
result = tf.raw_ops.ResourceSparseApplyProximalAdagrad(
    var=var.handle,
    accum=accum.handle,
    lr=lr,
    l1=l1,
    l2=l2,
    grad=grad,
    indices=indices,
    use_locking=False
)
```

### Relevant log output

```shell
The program crashes with a fatal error:

2025-05-24 17:38:59.747282: F tensorflow/core/framework/tensor_shape.cc:359] Check failed: d < dims() (1 vs. 1)
Aborted (core dumped)


Complete Log:


2025-05-24 17:38:58.413411: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-24 17:38:58.458995: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-05-24 17:38:59.456168: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
TensorFlow version: 2.20.0-dev20250516
2025-05-24 17:38:59.672876: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
Tensor shapes:
var: (2, 3)
accum: (2, 3)
grad: (1,)
indices: (1,)
Tensor values:
var values: [[5.2439346e+36 5.2439346e+36 5.2439346e+36]
 [5.2439346e+36 5.2439346e+36 5.2439346e+36]]
lr: 5.2439346057351246e+36  
grad: [7.90505e+31]
2025-05-24 17:38:59.747282: F tensorflow/core/framework/tensor_shape.cc:359] Check failed: d < dims() (1 vs. 1)
Aborted (core dumped)
```",2
Segmentation Fault in `tf.raw_ops.QuantizeAndDequantizeV3`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.20.0-dev20250516 (git: v1.12.1-126285-g20148f52365)

### Custom code

Yes

### OS platform and distribution

Linux (CPU execution)

### Mobile device

_No response_

### Python version

3.12.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.raw_ops.QuantizeAndDequantizeV3` crashes with a segmentation fault due to a tensor shape validation failure. The crash occurs in tensor_shape.cc with a bounds checking assertion failure when processing negative axis values. The code may lack proper validation of axis parameters before processing.

Colab reproduction link: https://colab.research.google.com/drive/19QlV0TIQqkN_S66VATzOX_-vkTyg4I9K?usp=sharing

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

# Create input tensor with extreme values
input_tensor = tf.constant([
    [[4.3136053036349129e-244, 1.2524328545813582e-21, 7.71590127777328e+26, 1.0, 2.0, 3.0],
     [1e-100, 1e+100, -1e+50, 4.0, 5.0, 6.0]]
], dtype=tf.float64)  # Shape [1, 2, 6]

# Extreme range values
input_min = tf.constant(-5.4785109376353583e-282, dtype=tf.float64)
input_max = tf.constant(1.4455588399771524e+73, dtype=tf.float64)
num_bits = tf.constant(2, dtype=tf.int32)

# This crashes with segfault
result = tf.raw_ops.QuantizeAndDequantizeV3(
    input=input_tensor,
    input_min=input_min,
    input_max=input_max,
    num_bits=num_bits,
    signed_input=False,
    range_given=True,
    narrow_range=True,
    axis=-2  # Negative axis triggers the crash
)
```

### Relevant log output

```shell
2025-05-24 16:12:02.945026: F tensorflow/core/framework/tensor_shape.cc:358] Check failed: d >= 0 (0 vs. -2)
Aborted (core dumped)
```",2
Segmentation Fault in `tf.raw_ops.MaxPool` with oneDNN and `NCHW_VECT_C`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.20.0-dev20250516 (git: v1.12.1-126285-g20148f52365)

### Custom code

Yes

### OS platform and distribution

Ubuntu 24.04 noble

### Mobile device

_No response_

### Python version

3.12.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.raw_ops.MaxPool` with `data_format=""NCHW_VECT_C""` crashes with a segmentation fault when oneDNN optimizations are enabled. The crash occurs in the MKL layout pass with an INVALID_ARGUMENT error, but instead of gracefully handling the error, the process aborts.


Critical: This crash only occurs with oneDNN enabled

Crash occurs: When oneDNN is enabled (default in recent TF builds)
No crash: When oneDNN is disabled (TF_ENABLE_ONEDNN_OPTS=0)

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

print(f""TensorFlow version: {tf.version.VERSION}"")
print(f""oneDNN enabled: {'TF_ENABLE_ONEDNN_OPTS' not in os.environ or os.environ.get('TF_ENABLE_ONEDNN_OPTS', '1') == '1'}"")

# Create input tensor
input_tensor = tf.random.normal([10, 1, 1, 9], dtype=tf.float32)

try:
    # This crashes with oneDNN enabled, works with oneDNN disabled
    result = tf.raw_ops.MaxPool(
        input=input_tensor,
        ksize=[1, 1, 1, 1],
        strides=[1, 1, 1, 1], 
        padding=""SAME"",
        data_format=""NCHW_VECT_C""  # Unsupported by MklNativeMaxPool
    )
    print(""No crash occurred"")
except Exception as e:
    print(f""Exception (expected): {e}"")
```

### Relevant log output

```shell
2025-05-24 16:03:32.656644: F tensorflow/core/common_runtime/mkl_layout_pass.cc:3727] Non-OK-status: ret_status
Status: INVALID_ARGUMENT: Value for attr 'data_format' of ""NCHW_VECT_C"" is not in the list of allowed values: ""NHWC"", ""NCHW""
        ; NodeDef: {{node MaxPool}}; Op<name=_MklNativeMaxPool; signature=input:T -> output:T, workspace:uint8; attr=T:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_HALF, DT_BFLOAT16]; attr=ksize:list(int),min=4; attr=strides:list(int),min=4; attr=padding:string,allowed=[""SAME"", ""VALID""]; attr=data_format:string,default=""NHWC"",allowed=[""NHWC"", ""NCHW""]; attr=explicit_paddings:list(int),default=[]; attr=workspace_enabled:bool,default=false>
Aborted (core dumped)
```",2
Segmentation Fault in `tf.raw_ops.Conv3DBackpropInputV2`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf v1.12.1-126285-g20148f52365 2.20.0-dev20250516

### Custom code

Yes

### OS platform and distribution

Colab & Linux CPU

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.raw_ops.Conv3DBackpropInputV2` crashes with a segmentation fault when provided with malformed input_sizes parameter. The crash occurs in tensor_format.h with a bounds checking assertion failure. Should Handle malformed `input_sizes` gracefully with proper error reporting and validate inputs.



### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

print(tf.version.GIT_VERSION, tf.version.VERSION)

# Reproduce the crash
input_sizes = tf.constant([5], dtype=tf.int32)  # Invalid: should be 5D for Conv3D
filter_tensor = tf.random.normal([7, 6, 9, 2, 3], dtype=tf.float32)
out_backprop = tf.ones([1, 1, 1, 1, 1], dtype=tf.float32)

# This crashes with segfault
result = tf.raw_ops.Conv3DBackpropInputV2(
    input_sizes=input_sizes,
    filter=filter_tensor,
    out_backprop=out_backprop,
    strides=[1, 1, 1, 1, 1],
    padding=""VALID"",
    data_format=""NCDHW"",
    dilations=[1, 1, 1, 1, 1]
)
```

### Relevant log output

```shell
(venv) root@19e6d947fcb3:~# python3 a.py 
2025-05-24 15:44:18.025592: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-24 15:44:18.072503: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-05-24 15:44:19.127664: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-24 15:44:19.383268: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2025-05-24 15:44:19.469043: F ./tensorflow/core/util/tensor_format.h:428] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 1, 1, C
Aborted (core dumped)
```",2
`tf.raw_ops.Mfcc` causes segmentation fault,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.19

### Custom code

Yes

### OS platform and distribution

Colab CPU

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When calling `tf.raw_ops.Mfcc` with parameters that result in insufficient frequency resolution for the requested filterbank channels, TensorFlow crashes with a segmentation fault instead of raising a proper Python exception. The error is detected and logged, but then the process terminates with a core dump.

## Reproducible Example

Google Colab link: https://colab.research.google.com/drive/1nEpVxBQkh-uP1ACB5pVBIUNCgQsCBio0?usp=sharing

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

# Create minimal spectrogram with insufficient frequency resolution
# Shape [1, 1, 2] means only 2 frequency bins available
spectrogram = tf.constant([[[1.0, 2.0]]], dtype=tf.float32)

# Parameters that cause the segmentation fault
sample_rate = tf.constant(1744830464, dtype=tf.int32)  # Very large sample rate
lower_freq = float('nan')  # NaN lower frequency limit
upper_freq = 0.498889
filterbank_channels = 18  # Requesting 18 channels but only 2 freq bins available

# This crashes with segmentation fault after logging the error
result = tf.raw_ops.Mfcc(
    spectrogram=spectrogram,
    sample_rate=sample_rate,
    upper_frequency_limit=upper_freq,
    lower_frequency_limit=lower_freq,
    filterbank_channel_count=filterbank_channels,
    dct_coefficient_count=4
)
```

### Relevant log output

```shell
E tensorflow/core/kernels/mfcc_mel_filterbank.cc:165] Missing 17 bands starting at 1 in mel-frequency design. Perhaps too many channels or not enough frequency resolution in spectrum. (input_length: 2 input_sample_rate: 1.74483e+09 output_channel_count: 18 lower_frequency_limit: nan upper_frequency_limit: 0.498889
Segmentation fault (core dumped)
```",2
`tf.raw_ops.QuantizedConv2D` crashes with assertion failure instead of proper error handling,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.19

### Custom code

Yes

### OS platform and distribution

Linux (Google Colab)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

Only CPU

### Current behavior?

When calling `tf.raw_ops.QuantizedConv2D` with incompatible tensor dimensions, TensorFlow crashes with a fatal assertion failure instead of raising a proper Python exception. The process terminates with a core dump, making it impossible to handle the error gracefully in user code.

## Reproducible Example

Google Colab link: https://colab.research.google.com/drive/19Qf7Wy-vlkPJuGSA_Z-w86zB8GC5fX_I?usp=sharing

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

# Create input tensor with shape [10,10,2,1] 
input_data = np.random.randint(0, 255, size=(10, 10, 2, 1), dtype=np.uint8)
input_tensor = tf.constant(input_data, dtype=tf.quint8)

# Create filter tensor with shape [3,3,1,3]
filter_data = np.random.randint(0, 255, size=(3, 3, 1, 3), dtype=np.uint8)
filter_tensor = tf.constant(filter_data, dtype=tf.quint8)

# Min/Max tensors for quantization
min_input = tf.constant(0.0, dtype=tf.float32)
max_input = tf.constant(1.0, dtype=tf.float32)
min_filter = tf.constant(0.0, dtype=tf.float32)
max_filter = tf.constant(1.0, dtype=tf.float32)

# This crashes with assertion failure instead of raising proper exception
result = tf.raw_ops.QuantizedConv2D(
    input=input_tensor,
    filter=filter_tensor,
    min_input=min_input,
    max_input=max_input,
    min_filter=min_filter,
    max_filter=max_filter,
    strides=[1, 1, 1, 1],
    padding=""VALID"",
    dilations=[1, 1, 1, 1]
)
```

### Relevant log output

```shell
Error Message:


F0000 00:00:1748030662.839173 1610502 quantized_conv_ops.cc:581] Check failed: out_cols > 0 (0 vs. 0) 
*** Check failure stack trace: ***
    @     0x7f02274cc3f4  absl::lts_20230802::log_internal::LogMessage::SendToLog()
    @     0x7f02274cc264  absl::lts_20230802::log_internal::LogMessage::Flush()
    @     0x7f02274cc819  absl::lts_20230802::log_internal::LogMessageFatal::~LogMessageFatal()
    @     0x7f021a8b15d4  tensorflow::QuantizedConv2DOp<>::Compute()
    @     0x7f02300f4320  tensorflow::ThreadPoolDevice::Compute()
    [... full stack trace ...]
Aborted (core dumped)
```",2
xprof compilation fails with gcc 14.2,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

current master v1.12.1-126604-g3d72b9f063c 2.20.0-dev0+selfbuilt

### Custom code

No

### OS platform and distribution

Ubuntu 25.04 +venv

### Mobile device

_No response_

### Python version

3.13

### Bazel version

7.4.1

### GCC/compiler version

gcc 14.2

### CUDA/cuDNN version

none

### GPU model and memory

none

### Current behavior?

compilation fails with gcc 14.2, succeeds with clang 20.1

### Standalone code to reproduce the issue

```shell
export TF_NEED_CUDA=0
export CC_OPT_FLAGS=""-march=native -Wno-error -Wno-unused-result -Wno-sign-compare -w""

bazel build --config=opt --verbose_failures //tensorflow/tools/pip_package:wheel --repo_env=USE_PYWRAP_RULES=1 --repo_env=WHEEL_NAME=tensorflow_cpu
```

### Relevant log output

```shell
/usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++14' -MD -MF bazel-out/k8-opt/bin/external/org_xprof/xprof/utils/_objs/derived_timeline/derived_timeline.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/org_xprof/xprof/utils/_objs/derived_timeline/derived_timeline.pic.o' -fPIC '-DEIGEN_MAX_ALIGN_BYTES=64' -DEIGEN_ALLOW_UNALIGNED_SCALARS '-DEIGEN_USE_AVX512_GEMM_KERNELS=0' -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY -DTENSORFLOW_USE_NUMA -DTF_ENABLE_ACTIVITY_WATCHER '-DTF_MAJOR_VERSION=2' '-DTF_MINOR_VERSION=20' '-DTF_PATCH_VERSION=0' '-DTF_VERSION_SUFFIX=""-dev0+selfbuilt""' -iquote external/org_xprof -iquote bazel-out/k8-opt/bin/external/org_xprof -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/local_xla -iquote bazel-out/k8-opt/bin/external/local_xla -iquote external/local_tsl -iquote bazel-out/k8-opt/bin/external/local_tsl -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/ml_dtypes_py -iquote bazel-out/k8-opt/bin/external/ml_dtypes_py -iquote external/com_google_protobuf -iquote bazel-out/k8-opt/bin/external/com_google_protobuf -iquote external/zlib -iquote bazel-out/k8-opt/bin/external/zlib -iquote . -iquote bazel-out/k8-opt/bin -iquote external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/snappy -iquote bazel-out/k8-opt/bin/external/snappy -iquote external/hwloc -iquote bazel-out/k8-opt/bin/external/hwloc -iquote external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -iquote external/cuda_cudart -iquote bazel-out/k8-opt/bin/external/cuda_cudart -iquote external/cuda_cublas -iquote bazel-out/k8-opt/bin/external/cuda_cublas -iquote external/cuda_cccl -iquote bazel-out/k8-opt/bin/external/cuda_cccl -iquote external/cuda_nvtx -iquote bazel-out/k8-opt/bin/external/cuda_nvtx -iquote external/cuda_nvcc -iquote bazel-out/k8-opt/bin/external/cuda_nvcc -iquote external/cuda_cusolver -iquote bazel-out/k8-opt/bin/external/cuda_cusolver -iquote external/cuda_cufft -iquote bazel-out/k8-opt/bin/external/cuda_cufft -iquote external/cuda_cusparse -iquote bazel-out/k8-opt/bin/external/cuda_cusparse -iquote external/cuda_curand -iquote bazel-out/k8-opt/bin/external/cuda_curand -iquote external/cuda_cupti -iquote bazel-out/k8-opt/bin/external/cuda_cupti -iquote external/cuda_nvml -iquote bazel-out/k8-opt/bin/external/cuda_nvml -iquote external/cuda_nvjitlink -iquote bazel-out/k8-opt/bin/external/cuda_nvjitlink -iquote external/local_config_tensorrt -iquote bazel-out/k8-opt/bin/external/local_config_tensorrt -iquote external/nvshmem -iquote bazel-out/k8-opt/bin/external/nvshmem -iquote external/local_config_nccl -iquote bazel-out/k8-opt/bin/external/local_config_nccl -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers -Ibazel-out/k8-opt/bin/external/cuda_cudart/_virtual_includes/headers -Ibazel-out/k8-opt/bin/external/cuda_cublas/_virtual_includes/headers -Ibazel-out/k8-opt/bin/external/cuda_cccl/_virtual_includes/headers -Ibazel-out/k8-opt/bin/external/cuda_nvtx/_virtual_includes/headers -Ibazel-out/k8-opt/bin/external/cuda_nvcc/_virtual_includes/headers -Ibazel-out/k8-opt/bin/external/cuda_cusolver/_virtual_includes/headers -Ibazel-out/k8-opt/bin/external/cuda_cufft/_virtual_includes/headers -Ibazel-out/k8-opt/bin/external/cuda_cusparse/_virtual_includes/headers -Ibazel-out/k8-opt/bin/external/cuda_curand/_virtual_includes/headers -Ibazel-out/k8-opt/bin/external/cuda_cupti/_virtual_includes/headers -Ibazel-out/k8-opt/bin/external/cuda_nvml/_virtual_includes/headers -Ibazel-out/k8-opt/bin/external/cuda_nvjitlink/_virtual_includes/headers -Ibazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -Ibazel-out/k8-opt/bin/external/nvshmem/_virtual_includes/nvshmem_config -Ibazel-out/k8-opt/bin/external/local_config_nccl/_virtual_includes/nccl_config -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/eigen_archive/mkl_include -isystem bazel-out/k8-opt/bin/external/eigen_archive/mkl_include -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/hwloc/hwloc -isystem bazel-out/k8-opt/bin/external/hwloc/hwloc -isystem external/hwloc/include -isystem bazel-out/k8-opt/bin/external/hwloc/include -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/cuda_cudart/include -isystem bazel-out/k8-opt/bin/external/cuda_cudart/include -isystem external/cuda_cublas/include -isystem bazel-out/k8-opt/bin/external/cuda_cublas/include -isystem external/cuda_cccl/include -isystem bazel-out/k8-opt/bin/external/cuda_cccl/include -isystem external/cuda_nvtx/include -isystem bazel-out/k8-opt/bin/external/cuda_nvtx/include -isystem external/cuda_nvcc/include -isystem bazel-out/k8-opt/bin/external/cuda_nvcc/include -isystem external/cuda_cusolver/include -isystem bazel-out/k8-opt/bin/external/cuda_cusolver/include -isystem external/cuda_cufft/include -isystem bazel-out/k8-opt/bin/external/cuda_cufft/include -isystem external/cuda_cusparse/include -isystem bazel-out/k8-opt/bin/external/cuda_cusparse/include -isystem external/cuda_curand/include -isystem bazel-out/k8-opt/bin/external/cuda_curand/include -isystem external/cuda_cupti/include -isystem bazel-out/k8-opt/bin/external/cuda_cupti/include -isystem external/cuda_nvml/include -isystem bazel-out/k8-opt/bin/external/cuda_nvml/include -isystem external/cuda_nvjitlink/include -isystem bazel-out/k8-opt/bin/external/cuda_nvjitlink/include -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' -DAUTOLOAD_DYNAMIC_KERNELS '-march=native' -Wno-error -Wno-unused-result -Wno-sign-compare -w '-std=c++17' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/org_xprof/xprof/utils/derived_timeline.cc -o bazel-out/k8-opt/bin/external/org_xprof/xprof/utils/_objs/derived_timeline/derived_timeline.pic.o)
# Configuration: cb965c66e4f49eed21c1b8690bb14256f5148b5976fa6de7af0f9c3ebf43475f
# Execution platform: @@local_execution_config_platform//:platform
external/org_xprof/xprof/utils/derived_timeline.cc: In function 'std::vector<long int> tensorflow::profiler::{anonymous}::DeriveEventsFromAnnotationsForLines(const tensorflow::profiler::SymbolResolver&, tensorflow::profiler::XPlane*, absl::lts_20230802::Span<const long int>, int64_t, const tensorflow::profiler::ScopeRangeIdTree*)':
external/org_xprof/xprof/utils/derived_timeline.cc:194:8: error: 'GetSortedEvents' was not declared in this scope; did you mean 'tsl::profiler::GetSortedEvents'?
  194 |        GetSortedEvents<XEventVisitor>(plane_visitor, false, line_ids)) {
      |        ^~~~~~~~~~~~~~~
      |        tsl::profiler::GetSortedEvents
In file included from external/org_xprof/xprof/utils/derived_timeline.cc:45:
external/local_xla/xla/tsl/profiler/utils/xplane_utils.h:133:20: note: 'tsl::profiler::GetSortedEvents' declared here
  133 | std::vector<Event> GetSortedEvents(Plane& plane,
      |                    ^~~~~~~~~~~~~~~
external/org_xprof/xprof/utils/derived_timeline.cc:194:37: error: expected primary-expression before '>' token
  194 |        GetSortedEvents<XEventVisitor>(plane_visitor, false, line_ids)) {
      |                                     ^
external/org_xprof/xprof/utils/derived_timeline.cc: In function 'void tensorflow::profiler::DeriveStepEventsFromGroups(const tsl::profiler::GroupMetadataMap&, XPlane*)':
external/org_xprof/xprof/utils/derived_timeline.cc:482:8: error: 'GetSortedEvents' was not declared in this scope; did you mean 'tsl::profiler::GetSortedEvents'?
  482 |        GetSortedEvents<XEventVisitor>(plane_visitor)) {
      |        ^~~~~~~~~~~~~~~
      |        tsl::profiler::GetSortedEvents
external/local_xla/xla/tsl/profiler/utils/xplane_utils.h:133:20: note: 'tsl::profiler::GetSortedEvents' declared here
  133 | std::vector<Event> GetSortedEvents(Plane& plane,
      |                    ^~~~~~~~~~~~~~~
external/org_xprof/xprof/utils/derived_timeline.cc:482:37: error: expected primary-expression before '>' token
  482 |        GetSortedEvents<XEventVisitor>(plane_visitor)) {
      |                                     ^
external/org_xprof/xprof/utils/derived_timeline.cc: In function 'void tensorflow::profiler::DeriveLinesFromStats(XPlane*)':
external/org_xprof/xprof/utils/derived_timeline.cc:696:8: error: 'GetSortedEvents' was not declared in this scope; did you mean 'tsl::profiler::GetSortedEvents'?
  696 |        GetSortedEvents<XEventVisitor>(plane_visitor, true)) {
      |        ^~~~~~~~~~~~~~~
      |        tsl::profiler::GetSortedEvents
external/local_xla/xla/tsl/profiler/utils/xplane_utils.h:133:20: note: 'tsl::profiler::GetSortedEvents' declared here
  133 | std::vector<Event> GetSortedEvents(Plane& plane,
      |                    ^~~~~~~~~~~~~~~
external/org_xprof/xprof/utils/derived_timeline.cc:696:37: error: expected primary-expression before '>' token
  696 |        GetSortedEvents<XEventVisitor>(plane_visitor, true)) {
      |                                     ^
Target //tensorflow/tools/pip_package:wheel failed to build
```",1
AttributeError with Protobuf >= 6.30,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

current master   v1.12.1-126604-g3d72b9f063c   2.20.0-dev0+selfbuilt

### Custom code

No

### OS platform and distribution

Ubuntu 25.04 +venv

### Mobile device

_No response_

### Python version

3.13

### Bazel version

7,4.1

### GCC/compiler version

clang 20.1

### CUDA/cuDNN version

-

### GPU model and memory

-

### Current behavior?

Have a version >= 6.30.0 of python-protobuf, such as 6.31.0
When importing tensorflow the error message is displayed.


### Standalone code to reproduce the issue

```shell
python -c ""import tensorflow""
```

### Relevant log output

```shell
AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'
```",1
Psycopg crashes using OpenSSL if tensorflow is imported beforehand,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.19.0

### Custom code

No

### OS platform and distribution

Linux

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Issue reported to the Psycopg project in https://github.com/psycopg/psycopg/issues/1097 and https://github.com/psycopg/psycopg2/issues/1784 by @OlivierVerhaegen. Below the OP description and repro.

After a log search and trial and error I found the following bug regarding the usage of psycopg in combination with Tensorflow.
Every time the application would have a segmentation error when starting up the SSL connection.

![Image](https://github.com/user-attachments/assets/f01f6169-a881-4ede-9f57-6f771127b480)

Workaround was to import psycopg before tensorflow.
**FYI: I tried all packages of psycopg (binary, c)**

![Image](https://github.com/user-attachments/assets/faf26415-5ddc-45a6-b6d2-c5892f078942)

Result afterwards:

![Image](https://github.com/user-attachments/assets/d4349c1b-d15e-495a-bd20-13ac4bbc1425)

Hopefully this post helps other people to avoid this 

### Standalone code to reproduce the issue

**Dockerfile**
```dockerfile
FROM python:3.12-slim

RUN apt update && \
    apt install -y libpq-dev gcc python3-dev

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY main.py .

ENTRYPOINT [ ""python"", ""-u"", ""main.py"" ]
```

**requirements.txt**
```txt
pandas==2.2.3
numpy==2.0.2
scikit-learn==1.5.2
tensorflow-cpu==2.19.0
psycopg[binary]
confluent-kafka==2.10.0
```

**main.py**
```python
#Importing libraries
import psycopg
import tensorflow as tf
import logging
import json
import joblib
import os
import sys
import time
logging.basicConfig(level=logging.DEBUG, format=""%(asctime)s %(levelname)s %(message)s"")
logging.getLogger(""psycopg"").setLevel(logging.DEBUG)
print(""importing libraries done"")

#Getting environment variables for database connection
dbname=os.getenv(""DB_NAME"")
user=os.getenv(""DB_USER"")
password=os.getenv(""DB_PWD"")
host=os.getenv(""DB_HOST"")
port=os.getenv(""DB_PORT"")
env_vars = {
    ""dbname"": dbname,
    ""user"": user,
    ""password"": password,
    ""host"": host,
    ""port"": port,
    ""sslmode"": ""require"",
}
#Checking if all environment variables were given
print(""Checking environment variables..."")
empty = 0
for name, value in env_vars.items():
    if value == None:
        print(f'No {name} was given for database connection')
        empty = empty +1

if not empty == 0:
    time.sleep(5)
    sys.exit(""Exiting program..."")

try:
    print(""Connecting to database..."")
    # Establishing a connection to the database
    connection = psycopg.connect(**env_vars)
    print(""Connected to database"")
    # Creating a cursor object to interact with the database
    cursor = connection.cursor()
    # Performing database operations here...
except (Exception, psycopg.Error) as error:
    error_str = f""Error connecting to the database: {error}""
    print(error_str)
    time.sleep(5)
    sys.exit(error_str)
```",2
`tf.linalg.svd` crashes on CUDA raising SIGABRT after facing OOM,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

Ubuntu 24.04.2 LTS

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.6

### GPU model and memory

_No response_

### Current behavior?

When running a 6D float32 tensor on `tf.linalg.svd` on CUDA crashes with `Aborted (core dumped)` after facing OOM. I can calculate the svd with `Numpy` without facing any memory issues/signal raised. I also tried to reproduce with [colab](https://colab.research.google.com/drive/1seHhYGTrXwJa5-oPKayx4-nSHHYnrT3a?usp=sharing) and the session did in fact crash.

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf
rng = np.random.default_rng(225)
np_input = rng.uniform(-36.999996, -32.00001, (21, 21, 12, 11, 5, 3)).astype(np.float32)

# try numpy
svd_np = np.linalg.svd(np_input, compute_uv=False)
print(""Numpy Successful\n\n"")

with tf.device(""/GPU:0""):
    input_tensor = tf.constant(np_input)
    svd_tf = tf.linalg.svd(input_tensor, compute_uv=False)
```

### Relevant log output

```shell
...
2025-05-22 05:27:02.208208: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114]      Summary of in-use Chunks by size: 
2025-05-22 05:27:02.208214: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 1280 totalling 1.2KiB
2025-05-22 05:27:02.208217: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 232960 totalling 227.5KiB
2025-05-22 05:27:02.208220: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 698624 totalling 682.2KiB
2025-05-22 05:27:02.208223: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 11550 Chunks of size 919296 totalling 9.89GiB
2025-05-22 05:27:02.208225: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 1558272 totalling 1.49MiB
2025-05-22 05:27:02.208227: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 2095872 totalling 2.00MiB
2025-05-22 05:27:02.208229: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 4 Chunks of size 3492864 totalling 13.32MiB
2025-05-22 05:27:02.208232: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1121] Sum Total of in-use chunks: 9.91GiB
2025-05-22 05:27:02.208235: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1123] Total bytes in pool: 10636427264 memory_limit_: 10636427264 available bytes: 0 curr_region_allocation_bytes_: 21272854528
2025-05-22 05:27:02.208239: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1128] Stats: 
Limit:                     10636427264
InUse:                     10636427264
MaxInUse:                  10636427264
NumAllocs:                       11559
MaxAllocSize:                  3492864
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2025-05-22 05:27:02.208326: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:508] ****************************************************************************************************
F0000 00:00:1747906022.208340 1974098 gpu_solvers.h:618] Non-OK-status: context->allocate_temp(DataTypeToEnum<Scalar>::value, shape, &scratch_tensor_, alloc_attr)
Status: RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[229824] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
*** Check failure stack trace: ***
    @     0x7ce77da67bc4  absl::lts_20230802::log_internal::LogMessage::SendToLog()
    @     0x7ce77da679c4  absl::lts_20230802::log_internal::LogMessage::Flush()
    @     0x7ce77da67fe9  absl::lts_20230802::log_internal::LogMessageFatal::~LogMessageFatal()
    @     0x7ce7741bbe10  tensorflow::ScratchSpace<>::ScratchSpace()
    @     0x7ce7741bbb54  tensorflow::GpuSolver::GetScratchSpace<>()
    @     0x7ce7741b4b66  tensorflow::GpuSolver::Gesvd<>()
    @     0x7ce771421389  tensorflow::SvdOpGpu<>::RunSVD()
    @     0x7ce7714204fc  tensorflow::SvdOpGpu<>::PerformSVD_MgeqN()
    @     0x7ce77141ffe7  tensorflow::SvdOpGpu<>::ComputeAsync()
    @     0x7ce78657e96d  tensorflow::AsyncOpKernel::Compute()
    @     0x7ce7863474d5  tensorflow::BaseGPUDevice::Compute()
    @     0x7ce7863f8f48  tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run()
    @     0x7ce7863b8724  tensorflow::FunctionLibraryRuntimeImpl::RunSync()
    @     0x7ce7863c5730  tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync()
    @     0x7ce7863cbb7d  tensorflow::ProcessFunctionLibraryRuntime::RunSync()
    @     0x7ce76df3ddf0  tensorflow::KernelAndDeviceFunc::Run()
    @     0x7ce76dee98d6  tensorflow::EagerKernelExecute()
    @     0x7ce76def33b0  tensorflow::ExecuteNode::Run()
    @     0x7ce76df39244  tensorflow::EagerExecutor::SyncExecute()
    @     0x7ce76dee925b  tensorflow::(anonymous namespace)::EagerLocalExecute()
    @     0x7ce76dee6929  tensorflow::DoEagerExecute()
    @     0x7ce76deea660  tensorflow::EagerExecute()
    @     0x7ce76d5209f7  tensorflow::EagerOperation::Execute()
    @     0x7ce76df37943  tensorflow::CustomDeviceOpHandler::Execute()
    @     0x7ce76ad38cf5  TFE_Execute
    @     0x7ce77f431efa  TFE_Py_FastPathExecute_C()
    @     0x7ce7992d5893  pybind11::detail::argument_loader<>::call<>()
    @     0x7ce7992d57cf  pybind11::cpp_function::initialize<>()::{lambda()#1}::__invoke()
    @     0x7ce7992af8df  pybind11::cpp_function::dispatcher()
    @           0x58208f  (unknown)
    @           0x549185  _PyObject_MakeTpCall
    @           0x5d73c9  _PyEval_EvalFrameDefault
    @           0x5d58eb  PyEval_EvalCode
    @           0x608b42  (unknown)
    @           0x6b4e93  (unknown)
    @           0x6b4bfa  _PyRun_SimpleFileObject
    @           0x6b4a2f  _PyRun_AnyFileObject
    @           0x6bca95  Py_RunMain
    @           0x6bc57d  Py_BytesMain
    @     0x7ce7c6a2a1ca  (unknown)
    @     0x7ce7c6a2a28b  __libc_start_main
    @           0x657ce5  _start
Aborted (core dumped)
```",2
"`tf.conv2d_backprop_filter_v2` Fails with ""no registered kernels"" Error","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.18.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The `tf.conv2d_backprop_filter_v2` operation fails with a `NotFoundError` (or similar) indicating ""Could not find device for node"" and ""no registered kernels"" when attempting to execute a simple convolution backward pass for filter gradients. This occurs even when the input tensors and parameters correctly defined. 

Colab link: https://colab.research.google.com/drive/1AJj0v0bgt1pATdJwH7xEku1N6gL4YB1j?usp=sharing



### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

print(tf.__version__)

def run_conv2d_backprop_filter_v2_bug():
    """"""
    Attempts to demonstrate tf.conv2d_backprop_filter_v2 and triggers the error.
    """"""

    # --- Define the input tensors based on provided information ---
    # Input Tensor: Tensor<type: float shape: [1,3,28,3]>
    input_values = np.zeros((1, 3, 28, 3), dtype=np.float32)
    input_values[0, 0, 0, 0] = -2.16744329e+12
    input_values[0, 0, 0, 1] = 1.23339599e+30
    input_values[0, 0, 0, 2] = 8.00276139e+12
    input_tensor = tf.constant(input_values, dtype=tf.float32)

    # Filter shape: Tensor<type: int shape: [4]> representing [height, width, in_channels, out_channels]
    filter_shape = tf.constant([3, 1, 3, 1], dtype=tf.int32)

    # Out Backprop Tensor: Tensor<type: float shape: [1,2,10,1]>
    out_backprop_values = np.zeros((1, 2, 10, 1), dtype=np.float32)
    out_backprop_values[0, 0, 0, 0] = 2.54932688e-24
    out_backprop_values[0, 0, 1, 0] = 3.49802447e+36
    out_backprop_values[0, 0, 2, 0] = 7.77881911e+12
    out_backprop = tf.constant(out_backprop_values, dtype=tf.float32)

    # --- Define the convolution parameters ---
    data_format = ""NCHW""
    strides = [1, 1, 2, 3] # [N, C, H, W]
    padding = ""SAME""
    dilations = [1, 1, 2, 3] # [N, C, H, W]

    print(f""Input Tensor Shape: {input_tensor.shape}"")
    print(f""Filter Shape: {filter_shape.numpy()}"")
    print(f""Out Backprop Tensor Shape: {out_backprop.shape}"")
    print(f""Strides: {strides}"")
    print(f""Padding: {padding}"")
    print(f""Data Format: {data_format}"")
    print(f""Dilations: {dilations}"")
    print(""-"" * 30)

    try:
        grad_filter = tf.conv2d_backprop_filter_v2(
            input=input_tensor,
            filter=filter_shape,
            out_backprop=out_backprop,
            strides=strides,
            padding=padding,
            data_format=data_format,
            dilations=dilations
        )
        print(""tf.conv2d_backprop_filter_v2 operation completed successfully!"")
        print(f""Shape of the gradient of the filter: {grad_filter.shape}"")
    except Exception as e:
        print(f""An error occurred during tf.conv2d_backprop_filter_v2: {e}"")

if __name__ == ""__main__"":
    run_conv2d_backprop_filter_v2_bug()
```

### Relevant log output

```shell
Output:

2.18.0
Input Tensor Shape: (1, 3, 28, 3)
Filter Shape: [3 1 3 1]
Out Backprop Tensor Shape: (1, 2, 10, 1)
Strides: [1, 1, 2, 3]
Padding: SAME
Data Format: NCHW
Dilations: [1, 1, 2, 3]
------------------------------
An error occurred during tf.conv2d_backprop_filter_v2: Could not find device for node: {{node Conv2DBackpropFilterV2}} = Conv2DBackpropFilterV2[T=DT_FLOAT, data_format=""NCHW"", dilations=[1, 1, 2, 3], explicit_paddings=[], padding=""SAME"", strides=[1, 1, 2, 3], use_cudnn_on_gpu=true]
All kernels registered for op Conv2DBackpropFilterV2:
  <no registered kernels>
 [Op:Conv2DBackpropFilterV2] name:
```",1
Bazel build failure : Flex delegate,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.5 LTS
- TensorFlow installation (pip package or built from source): Source
- TensorFlow library (version, if pip package or github SHA, if built from source): v2.15.0

### 2. Code

Provide code to help us reproduce your issues using one of the following options:


#### Option B: Paste your code here or provide a link to a custom end-to-end colab

My Model
```
 def magnitudeLSTMDropout_phaseLSTM_parallel_test_2(self):
        
        input_layer = keras.layers.Input(shape=(410, 1))
        
        magnitude_model = keras.models.Sequential([
                                                    # Convolutional block to extract local features
                                                    keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu',  padding='same'),
                                                    keras.layers.MaxPooling1D(pool_size=2),
                                                    keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),
                                                    keras.layers.MaxPooling1D(pool_size=2),
                                                    keras.layers.TimeDistributed(keras.layers.Flatten()),
                                                    
                                                    # # LSTM layers for sequential processing
                                                    keras.layers.LSTM(16, return_sequences=True, 
                                                                      dropout= 0.2, recurrent_dropout=0.2),  # Keeps sequence for the next LSTM layer
                                                    keras.layers.LSTM(16, return_sequences=True),
                                                    keras.layers.LSTM(16, return_sequences=True, 
                                                                      dropout= 0.2, recurrent_dropout=0.2),
                                                    keras.layers.LSTM(16, return_sequences=True),
                                                    keras.layers.LSTM(16, return_sequences=True, 
                                                                      dropout= 0.2, recurrent_dropout=0.2),
                                                    keras.layers.LSTM(16, return_sequences=True),
                                                    keras.layers.LSTM(8, return_sequences=False, 
                                                                      dropout= 0.2, recurrent_dropout=0.2),# Last LSTM layer with return_sequences=False
                                                    
                                                    # Fully connected layers
                                                    
                                                    keras.layers.BatchNormalization(),  # Helps stabilize training
                                                    keras.layers.Dropout(0.2),
                                                    keras.layers.Dense(51,activation=""sigmoid"", name=""magnitude"")  # Final layer to output 102 FFT values
                                                ])
        
        phase_model = keras.models.Sequential([
                                                    # Convolutional block to extract local features
                                                    keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu',  padding='same'),
                                                    keras.layers.MaxPooling1D(pool_size=2),
                                                    keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),
                                                    keras.layers.MaxPooling1D(pool_size=2),
                                                    keras.layers.TimeDistributed(keras.layers.Flatten()),
                                                    
                                                    # LSTM layers for sequential processing
                                                    keras.layers.LSTM(64, return_sequences=True),  # Keeps sequence for the next LSTM layer
                                                    keras.layers.LSTM(64, return_sequences=True),
                                                    keras.layers.LSTM(32, return_sequences=True),
                                                    keras.layers.LSTM(32, return_sequences=True),
                                                    keras.layers.LSTM(16, return_sequences=True),
                                                    keras.layers.LSTM(16, return_sequences=True),
                                                    keras.layers.LSTM(8, return_sequences=False),  # Last LSTM layer with return_sequences=False
                                                    
                                                    # Fully connected layers
                                                    keras.layers.Dense(256, activation=""relu""),
                                                    keras.layers.BatchNormalization(),  # Helps stabilize training
                                                    keras.layers.Dense(128, activation='relu'),
                                                    keras.layers.Dropout(0.2),
                                                    keras.layers.BatchNormalization(),  # Helps stabilize training
                                                    keras.layers.Dense(256, activation=""relu""),
                                                    keras.layers.BatchNormalization(),  # Helps stabilize training
                                                    keras.layers.Dense(128, activation='relu'),
                                                    keras.layers.Dense(51,activation=""sigmoid"", name=""phase"")  # Final layer to output 102 FFT values
                                                ])
        
        magnitude_output = magnitude_model(input_layer)
        magnitude_output = keras.layers.Lambda(lambda x: x, name=""magnitude"")(magnitude_output)  # Explicitly name it

        phase_output = phase_model(input_layer)
        phase_output = keras.layers.Lambda(lambda x: x, name=""phase"")(phase_output)  # Explicitly name it

        
        self.model = keras.models.Model(inputs=input_layer, 
                                        outputs={""magnitude"": magnitude_output, ""phase"": phase_output})
             
                                           
        self.model.compile(optimizer='adam', 
                           loss={""magnitude"": ""mse"", ""phase"": ""mse""}, 
                           metrics={""magnitude"":['mae', 'mse'], ""phase"":['mae', 'mse']})
        self.model.summary()

        return self.model
```

Model conversion to tflite

```
import sys
import os
import tensorflow as tf

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from Inference.inference import Inference


inference_module = Inference()
model,_=inference_module.load_model()

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,  # Default TFLite ops
    tf.lite.OpsSet.SELECT_TF_OPS     # Enable full TF ops when needed
]
converter._experimental_lower_tensor_list_ops = False 
tf_lite_model = converter.convert()

script_dir = os.path.dirname(__file__)
lite_dir = os.path.join(script_dir, 'LITE_SAVED_MODELS')
if os.path.exists(lite_dir) == False:
    os.makedirs(lite_dir)
    
lite_model_path = os.path.join(lite_dir, 'lite_model.tflite')
with open(lite_model_path, 'wb') as f:
    f.write(tf_lite_model)

```

The TensorFlow model gets converted into TFLite successfully, with the use of tf.lite.OpsSet.SELECT_TF_OPS

### 3. Issue Definition
The issue I'm facing is with the inferencing. at the time of inferencing on the edge device using tflite im getting runtime error.
```
RuntimeError: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_selectNode number 1 (FlexTensorListReserve) failed to prepare.Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_selectNode number 1 (FlexTensorListReserve) failed to prepare.
```
### 4. Steps taken to Resolve the Issue
Tried to build flex delegate shared library for the tensorflow [guide](https://ai.google.dev/edge/litert/models/ops_select#cc).
Im running the build in TensorFlow devel docker image [tensorflow/tensorflow:devel](https://hub.docker.com/r/tensorflow/tensorflow/tags/).

Command used to build:
```
bazel build -c opt --config=monolithic --config=elinux_armhf tensorflow/lite/delegates/flex:tensorflowlite_flex

```
Error message while building

```
ERROR: /home/harshith/Documents/docker_method/tensorflow_src/bazel-ci_build-cache/.cache/bazel/_bazel_harshith/68a62076e91007a7908bc42a32e4cff9/external/com_github_grpc_grpc/BUILD:1853:16: Compiling src/core/ext/transport/chttp2/client/secure/secure_channel_create.cc failed: (Exit 1): arm-none-linux-gnueabihf-gcc failed: error executing command (from target @com_github_grpc_grpc//:grpc_transport_chttp2_client_secure) /home/harshith/Documents/docker_method/tensorflow_src/bazel-ci_build-cache/.cache/bazel/_bazel_harshith/68a62076e91007a7908bc42a32e4cff9/external/armhf_linux_toolchain/bin/arm-none-linux-gnueabihf-gcc ... (remaining 85 arguments skipped)
In file included from external/com_github_grpc_grpc/src/core/lib/gprpp/memory.h:27,
                 from external/com_github_grpc_grpc/src/core/lib/gprpp/global_config_generic.h:24,
                 from external/com_github_grpc_grpc/src/core/lib/gprpp/global_config_env.h:24,
                 from external/com_github_grpc_grpc/src/core/lib/gprpp/global_config.h:92,
                 from external/com_github_grpc_grpc/src/core/lib/debug/trace.h:27,
                 from external/com_github_grpc_grpc/src/core/lib/channel/channel_stack.h:44,
                 from external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel_channelz.h:27,
                 from external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.h:24,
                 from external/com_github_grpc_grpc/src/core/ext/transport/chttp2/client/secure/secure_channel_create.cc:28:
/home/harshith/Documents/docker_method/tensorflow_src/bazel-ci_build-cache/.cache/bazel/_bazel_harshith/68a62076e91007a7908bc42a32e4cff9/external/armhf_linux_toolchain/arm-none-linux-gnueabihf/include/c++/11.3.1/limits:1673:7: internal compiler error: Illegal instruction
 1673 |       min() _GLIBCXX_USE_NOEXCEPT { return __FLT_MIN__; }
      |       ^~~
0x169d200 diagnostic_impl(rich_location*, diagnostic_metadata const*, int, char const*, __va_list_tag (*) [1], diagnostic_t)
	???:0
0x169de86 internal_error(char const*, ...)
	???:0
0xcefe7f crash_signal(int)
	???:0
0x18a6649 __gmpn_mul_basecase
	???:0
Please submit a full bug report,
with preprocessed source if appropriate.
Please include the complete backtrace with any bug report.
See <https://bugs.linaro.org/> for instructions.
Target //tensorflow/lite/delegates/flex:tensorflowlite_flex failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 360.326s, Critical Path: 19.27s
INFO: 849 processes: 309 internal, 540 local.
FAILED: Build did NOT complete successfully

```


### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
I am building flex delegate for armv7 architecture (ARM cortex a9)
```
",2
Broken Link: Microsoft Visual C++ Redistributable (pip install),"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.10

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The link for [Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019](https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads) on https://www.tensorflow.org/install/pip is broken.



### Standalone code to reproduce the issue

```shell
When navigating to the TensorFlow pip installation guide at https://www.tensorflow.org/install/pip, I observed a broken link for the ""Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019"".

Steps to reproduce:

Go to the TensorFlow installation guide: https://www.tensorflow.org/install/pip
Scroll down to the ""Windows install"" section (or the section mentioning Visual C++ Redistributable).
Click on the link titled: Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019
The link points to: https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads
Expected result: The link should navigate to a valid Microsoft support page where users can download the Visual C++ Redistributable.
Actual result: The link leads to a ""Page not found"" or ""404 Error"" page on the Microsoft support website, indicating the resource is no longer available at that URL.
```

### Relevant log output

```shell

```",1
`tf.linalg.pinv` crashes with after raising SIGABRT with a 6D tensor due to OOM on CUDA,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

Ubuntu 24.04.2 LTS

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.6

### GPU model and memory

_No response_

### Current behavior?

Running `tf.linalg.pinv` with a 6D tensor is causing the code to crash with `Aborted (core dumped)` due to being out of memory, instead of closing gracefully on CUDA. It does not face any problems on CPU. I tried to run it on [colab](https://colab.research.google.com/drive/122og20vCZPZhYEkMwcJSJybUZHP1bSii?usp=sharing) as well, it crashed there too.

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

rng = np.random.default_rng(24)

with tf.device('/GPU:0'):
    input_tensor = tf.constant(rng.uniform(-10.9999152216621, 6.999802457840147,(8, 10, 13, 12, 2, 4)), dtype=tf.float64)
    pinverse_tensor = tf.linalg.pinv(input_tensor)
```

### Relevant log output

```shell
...
2025-05-21 07:11:44.456637: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114]      Summary of in-use Chunks by size: 
2025-05-21 07:11:44.456641: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 256 totalling 256B
2025-05-21 07:11:44.456646: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 1280 totalling 1.2KiB
2025-05-21 07:11:44.456649: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 49920 totalling 48.8KiB
2025-05-21 07:11:44.456651: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 199680 totalling 195.0KiB
2025-05-21 07:11:44.456654: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 399360 totalling 390.0KiB
2025-05-21 07:11:44.456656: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 4 Chunks of size 798720 totalling 3.05MiB
2025-05-21 07:11:44.456659: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 5782 Chunks of size 1838592 totalling 9.90GiB
2025-05-21 07:11:44.456661: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 1842944 totalling 1.76MiB
2025-05-21 07:11:44.456663: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1121] Sum Total of in-use chunks: 9.91GiB
2025-05-21 07:11:44.456666: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1123] Total bytes in pool: 10636427264 memory_limit_: 10636427264 available bytes: 0 curr_region_allocation_bytes_: 21272854528
2025-05-21 07:11:44.456670: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1128] Stats: 
Limit:                     10636427264
InUse:                     10636427264
MaxInUse:                  10636427264
NumAllocs:                        5792
MaxAllocSize:                  1842944
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2025-05-21 07:11:44.456725: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:508] ****************************************************************************************************
F0000 00:00:1747825904.456738  861754 gpu_solvers.h:618] Non-OK-status: context->allocate_temp(DataTypeToEnum<Scalar>::value, shape, &scratch_tensor_, alloc_attr)
Status: RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[229824] and type double on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
*** Check failure stack trace: ***
    @     0x7ad5d1067bc4  absl::lts_20230802::log_internal::LogMessage::SendToLog()
    @     0x7ad5d10679c4  absl::lts_20230802::log_internal::LogMessage::Flush()
    @     0x7ad5d1067fe9  absl::lts_20230802::log_internal::LogMessageFatal::~LogMessageFatal()
    @     0x7ad5c77bc180  tensorflow::ScratchSpace<>::ScratchSpace()
    @     0x7ad5c77bbec4  tensorflow::GpuSolver::GetScratchSpace<>()
    @     0x7ad5c77b4da6  tensorflow::GpuSolver::Gesvd<>()
    @     0x7ad5c4a23e29  tensorflow::SvdOpGpu<>::RunSVD()
    @     0x7ad5c4a232fb  tensorflow::SvdOpGpu<>::PerformSVD_MlessN()
    @     0x7ad5c4a22999  tensorflow::SvdOpGpu<>::ComputeAsync()
    @     0x7ad5d9b7e96d  tensorflow::AsyncOpKernel::Compute()
    @     0x7ad5d99474d5  tensorflow::BaseGPUDevice::Compute()
    @     0x7ad5d99f8f48  tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run()
    @     0x7ad5d99b8724  tensorflow::FunctionLibraryRuntimeImpl::RunSync()
    @     0x7ad5d99c5730  tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync()
    @     0x7ad5d99cbb7d  tensorflow::ProcessFunctionLibraryRuntime::RunSync()
    @     0x7ad5c153ddf0  tensorflow::KernelAndDeviceFunc::Run()
    @     0x7ad5c14e98d6  tensorflow::EagerKernelExecute()
    @     0x7ad5c14f33b0  tensorflow::ExecuteNode::Run()
    @     0x7ad5c1539244  tensorflow::EagerExecutor::SyncExecute()
    @     0x7ad5c14e925b  tensorflow::(anonymous namespace)::EagerLocalExecute()
    @     0x7ad5c14e6929  tensorflow::DoEagerExecute()
    @     0x7ad5c14ea660  tensorflow::EagerExecute()
    @     0x7ad5c0b209f7  tensorflow::EagerOperation::Execute()
    @     0x7ad5c1537943  tensorflow::CustomDeviceOpHandler::Execute()
    @     0x7ad5be338cf5  TFE_Execute
    @     0x7ad5d2a31efa  TFE_Py_FastPathExecute_C()
    @     0x7ad5ef2d5893  pybind11::detail::argument_loader<>::call<>()
    @     0x7ad5ef2d57cf  pybind11::cpp_function::initialize<>()::{lambda()#1}::__invoke()
    @     0x7ad5ef2af8df  pybind11::cpp_function::dispatcher()
    @           0x58208f  (unknown)
    @           0x549185  _PyObject_MakeTpCall
    @           0x5d73c9  _PyEval_EvalFrameDefault
    @           0x5d58eb  PyEval_EvalCode
    @           0x608b42  (unknown)
    @           0x6b4e93  (unknown)
    @           0x6b4bfa  _PyRun_SimpleFileObject
    @           0x6b4a2f  _PyRun_AnyFileObject
    @           0x6bca95  Py_RunMain
    @           0x6bc57d  Py_BytesMain
    @     0x7ad61a22a1ca  (unknown)
    @     0x7ad61a22a28b  __libc_start_main
    @           0x657ce5  _start
Aborted (core dumped)
```",2
"`tf.nn.conv2d_transpose` crashes with ""Illegal instruction (core dumped)""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.19

### Custom code

Yes

### OS platform and distribution

Ubuntu 24.04.2 LTS

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.6

### GPU model and memory

_No response_

### Current behavior?

When I run the following code setting the device as CPU, it crashes with this error message: ""could not create a primitive descriptor for a convolution forward propagation primitive"", then it raises the signal ""Illegal instruction (core dumped)"". I tried it with two different machines, both gave me the same error.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
rng = np.random.default_rng(40)

tf.config.experimental.enable_op_determinism()

with tf.device(""/CPU:0""):
    input_tensor = tf.constant(rng.uniform(-3.9808033, -3.0141976, (1, 4, 4, 5)), dtype=tf.float32)
    weight_tensor = tf.constant(rng.uniform(2.0, 2.0, (4, 4, 4, 4)), dtype=tf.float32)
    stride = 2
    padding = 3
    output_padding = 0
    groups = 1
    dilation = 1
    
    input_tensor = tf.transpose(input_tensor, [0, 2, 3, 1])
    weight_tensor = tf.transpose(weight_tensor, [2, 3, 1, 0])

    batch_size = tf.shape(input_tensor)[0]
    input_height = input_tensor.shape[1]
    input_width = input_tensor.shape[2]
    filter_height = weight_tensor.shape[0]
    filter_width = weight_tensor.shape[1]
    
    out_channels = weight_tensor.shape[3]
    
    h_out = (input_height - 1) * stride - 2 * padding + dilation * (filter_height - 1) + 1 + output_padding
    w_out = (input_width - 1) * stride - 2 * padding + dilation * (filter_width - 1) + 1 + output_padding
    output_shape = [batch_size, h_out, w_out, out_channels]

    output = tf.nn.conv2d_transpose(
        input_tensor, weight_tensor, output_shape=output_shape, strides=[1, stride, stride, 1],
        padding='VALID' if padding == 0 else 'SAME', dilations=[1, dilation, dilation, 1]
    )
```

### Relevant log output

```shell
could not create a primitive descriptor for a convolution forward propagation primitiveIllegal instruction (core dumped)
```",2
TensorFlow `tf.scatter_nd` leads to crash via shape mismatch in indices input,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.19

### Custom code

Yes

### OS platform and distribution

linux ubuntu 18.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When passing a crafted indices input with an incorrect shape to tf.scatter_nd, TensorFlow crashes with a CHECK-failure during shape validation. Specifically, it terminates with the following error:
F tensorflow/core/framework/tensor_shape.cc:359] Check failed: d < dims() (1 vs. 1)
Aborted (core dumped)

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
indices = tf.constant([[4], [3], [1], [7]])
updates = tf.constant([9, 10, 11, 12])
shape = tf.constant([8])
scatter_nd = tf.scatter_nd(tf.expand_dims(indices, axis=(- 1)), updates, shape)
```

### Relevant log output

```shell

```",2
Casting NaN to int32 yields different results on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When using tf.cast to convert a float32 tensor containing NaN values into int32, TensorFlow silently produces inconsistent outputs on different devices:
CPU: -2147483648
GPU: 0

Maybe that should add a warning or error when casting tensors containing NaN to integer types, or define a consistent value across all devices.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

with tf.device('/CPU:0'):
    x_cpu = tf.constant([float('nan')], dtype=tf.float32)
    x_int_cpu = tf.cast(x_cpu, tf.int32)
    print(""On CPU cast to int32:"", x_int_cpu.numpy())


with tf.device('/GPU:0'):
    y_gpu = tf.constant([float('nan')], dtype=tf.float32)
    y_int_gpu = tf.cast(y_gpu, tf.int32)
    print(""On GPU cast to int32:"", y_int_gpu.numpy())
```

### Relevant log output

```shell
On CPU cast to int32: [-2147483648]
On GPU cast to int32: [0]
```",1
TF 2.19 GCC aarch64 Linux CUDA build failure,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.19.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 24.04

### Mobile device

_No response_

### Python version

3.12.9

### Bazel version

6.5.0

### GCC/compiler version

13.2.0

### CUDA/cuDNN version

12.9.0/8.9.7.29

### GPU model and memory

cuda_arch=80

### Current behavior?

I'm trying to build TF 2.19 but I encounter the following build issue:
```
ERROR: /tmp/root/spack-stage/spack-stage-py-tensorflow-2.19.0-soea5tbork4puqcza5gxjkajpgbjv63s/spack-src/tensorflow/BUILD:1322:21: Linking tensorflow/libtensorflow_cc.so.2.19.0 failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command (from target //tensorflow:libtensorflow_cc.so.2.19.0)
...
bazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o): in function `mlir::RegisteredOperationName::Model<mlir::tfg::GraphOp>::setPropertiesFromAttr(mlir::OperationName, mlir::OpaqueProperties, mlir::Attribute, llvm::function_ref<mlir::InFlightDiagnostic ()>)':
ops.cc:(.text._ZN4mlir23RegisteredOperationName5ModelINS_3tfg7GraphOpEE21setPropertiesFromAttrENS_13OperationNameENS_16OpaquePropertiesENS_9AttributeEN4llvm12function_refIFNS_18InFlightDiagnosticEvEEE[_ZN4mlir23RegisteredOperationName5ModelINS_3tfg7GraphOpEE21setPropertiesFromAttrENS_13OperationNameENS_16OpaquePropertiesENS_9AttributeEN4llvm12function_refIFNS_18InFlightDiagnosticEvEEE]+0x10): relocation truncated to fit: R_AARCH64_JUMP26 against symbol `mlir::tfg::GraphOp::setPropertiesFromAttr(mlir::tfg::detail::GraphOpGenericAdaptorBase::Properties&, mlir::Attribute, llvm::function_ref<mlir::InFlightDiagnostic ()>)' defined in .text._ZN4mlir3tfg7GraphOp21setPropertiesFromAttrERNS0_6detail25GraphOpGenericAdaptorBase10PropertiesENS_9AttributeEN4llvm12function_refIFNS_18InFlightDiagnosticEvEEE section in bazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o)
bazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o): in function `mlir::tfg::StatefulWhileRegionOp::readProperties(mlir::DialectBytecodeReader&, mlir::OperationState&)':
ops.cc:(.text._ZN4mlir3tfg21StatefulWhileRegionOp14readPropertiesERNS_21DialectBytecodeReaderERNS_14OperationStateE+0x2c): relocation truncated to fit: R_AARCH64_CALL26 against symbol `mlir::tfg::detail::StatefulWhileRegionOpGenericAdaptorBase::Properties& mlir::OperationState::getOrAddProperties<mlir::tfg::detail::StatefulWhileRegionOpGenericAdaptorBase::Properties>()' defined in .text._ZN4mlir14OperationState18getOrAddPropertiesINS_3tfg6detail39StatefulWhileRegionOpGenericAdaptorBase10PropertiesEEERT_v[_ZN4mlir14OperationState18getOrAddPropertiesINS_3tfg6detail39StatefulWhileRegionOpGenericAdaptorBase10PropertiesEEERT_v] section in bazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o)
bazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o): in function `mlir::tfg::WhileRegionOp::readProperties(mlir::DialectBytecodeReader&, mlir::OperationState&)':
ops.cc:(.text._ZN4mlir3tfg13WhileRegionOp14readPropertiesERNS_21DialectBytecodeReaderERNS_14OperationStateE+0x2c): relocation truncated to fit: R_AARCH64_CALL26 against symbol `mlir::tfg::detail::WhileRegionOpGenericAdaptorBase::Properties& mlir::OperationState::getOrAddProperties<mlir::tfg::detail::WhileRegionOpGenericAdaptorBase::Properties>()' defined in .text._ZN4mlir14OperationState18getOrAddPropertiesINS_3tfg6detail31WhileRegionOpGenericAdaptorBase10PropertiesEEERT_v[_ZN4mlir14OperationState18getOrAddPropertiesINS_3tfg6detail31WhileRegionOpGenericAdaptorBase10PropertiesEEERT_v] section in bazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o)
bazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o): in function `mlir::tfg::StatefulWhileRegionOp::parse(mlir::OpAsmParser&, mlir::OperationState&)':
ops.cc:(.text._ZN4mlir3tfg21StatefulWhileRegionOp5parseERNS_11OpAsmParserERNS_14OperationStateE+0x358): relocation truncated to fit: R_AARCH64_CALL26 against symbol `mlir::tfg::detail::StatefulWhileRegionOpGenericAdaptorBase::Properties& mlir::OperationState::getOrAddProperties<mlir::tfg::detail::StatefulWhileRegionOpGenericAdaptorBase::Properties>()' defined in .text._ZN4mlir14OperationState18getOrAddPropertiesINS_3tfg6detail39StatefulWhileRegionOpGenericAdaptorBase10PropertiesEEERT_v[_ZN4mlir14OperationState18getOrAddPropertiesINS_3tfg6detail39StatefulWhileRegionOpGenericAdaptorBase10PropertiesEEERT_v] section in bazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o)
bazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o): in function `mlir::tfg::StatelessWhileRegionOp::parse(mlir::OpAsmParser&, mlir::OperationState&)':
ops.cc:(.text._ZN4mlir3tfg22StatelessWhileRegionOp5parseERNS_11OpAsmParserERNS_14OperationStateE+0x2dc): relocation truncated to fit: R_AARCH64_CALL26 against symbol `mlir::tfg::StatelessWhileRegionOp::verifyInherentAttrs(mlir::OperationName, mlir::NamedAttrList&, llvm::function_ref<mlir::InFlightDiagnostic ()>)' defined in .text._ZN4mlir3tfg22StatelessWhileRegionOp19verifyInherentAttrsENS_13OperationNameERNS_13NamedAttrListEN4llvm12function_refIFNS_18InFlightDiagnosticEvEEE section in bazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o)
bazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o): in function `mlir::tfg::WhileRegionOp::parse(mlir::OpAsmParser&, mlir::OperationState&)':
ops.cc:(.text._ZN4mlir3tfg13WhileRegionOp5parseERNS_11OpAsmParserERNS_14OperationStateE+0x358): relocation truncated to fit: R_AARCH64_CALL26 against symbol `mlir::tfg::detail::WhileRegionOpGenericAdaptorBase::Properties& mlir::OperationState::getOrAddProperties<mlir::tfg::detail::WhileRegionOpGenericAdaptorBase::Properties>()' defined in .text._ZN4mlir14OperationState18getOrAddPropertiesINS_3tfg6detail31WhileRegionOpGenericAdaptorBase10PropertiesEEERT_v[_ZN4mlir14OperationState18getOrAddPropertiesINS_3tfg6detail31WhileRegionOpGenericAdaptorBase10PropertiesEEERT_v] section in bazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o)
bazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o): in function `mlir::tfg::ForRegionOp::parse(mlir::OpAsmParser&, mlir::OperationState&)':
ops.cc:(.text._ZN4mlir3tfg11ForRegionOp5parseERNS_11OpAsmParserERNS_14OperationStateE+0x3f0): relocation truncated to fit: R_AARCH64_CALL26 against symbol `mlir::tfg::ForRegionOp::verifyInherentAttrs(mlir::OperationName, mlir::NamedAttrList&, llvm::function_ref<mlir::InFlightDiagnostic ()>)' defined in .text._ZN4mlir3tfg11ForRegionOp19verifyInherentAttrsENS_13OperationNameERNS_13NamedAttrListEN4llvm12function_refIFNS_18InFlightDiagnosticEvEEE section in bazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o)
bazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o): in function `mlir::tfg::GetStaticallyKnownBranch(mlir::Attribute)':
ops.cc:(.text._ZN4mlir3tfgL24GetStaticallyKnownBranchENS_9AttributeE+0xf4): relocation truncated to fit: R_AARCH64_CALL26 against symbol `std::unique_ptr<mlir::detail::ElementsAttrIndexer::NonContiguousState::OpaqueIteratorBase, std::default_delete<mlir::detail::ElementsAttrIndexer::NonContiguousState::OpaqueIteratorBase> >::~unique_ptr()' defined in .text._ZNSt10unique_ptrIN4mlir6detail19ElementsAttrIndexer18NonContiguousState18OpaqueIteratorBaseESt14default_deleteIS4_EED2Ev[_ZNSt10unique_ptrIN4mlir6detail19ElementsAttrIndexer18NonContiguousState18OpaqueIteratorBaseESt14default_deleteIS4_EED5Ev] section in bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/quantization/stablehlo/libpasses.pic.a(convert_func_to_bfloat16.pic.o)
bazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o): in function `std::enable_if<std::is_same<mlir::Attribute, unsigned int>::value||(!std::is_base_of<mlir::Attribute, unsigned int>::value), std::optional<mlir::detail::ElementsAttrIterator<unsigned int> > >::type mlir::ElementsAttr::try_value_begin<unsigned int>() const':
ops.cc:(.text._ZNK4mlir12ElementsAttr15try_value_beginIjEENSt9enable_ifIXoosrSt7is_sameINS_9AttributeET_E5valuentsrSt10is_base_ofIS4_S5_E5valueESt8optionalINS_6detail20ElementsAttrIteratorIS5_EEEE4typeEv[_ZNK4mlir12ElementsAttr15try_value_beginIjEENSt9enable_ifIXoosrSt7is_sameINS_9AttributeET_E5valuentsrSt10is_base_ofIS4_S5_E5valueESt8optionalINS_6detail20ElementsAttrIteratorIS5_EEEE4typeEv]+0x100): relocation truncated to fit: R_AARCH64_CALL26 against symbol `__cxa_guard_release@@CXXABI_1.3' defined in .text section in /usr/lib/gcc/aarch64-linux-gnu/13/libstdc++.so
ops.cc:(.text._ZNK4mlir12ElementsAttr15try_value_beginIjEENSt9enable_ifIXoosrSt7is_sameINS_9AttributeET_E5valuentsrSt10is_base_ofIS4_S5_E5valueESt8optionalINS_6detail20ElementsAttrIteratorIS5_EEEE4typeEv[_ZNK4mlir12ElementsAttr15try_value_beginIjEENSt9enable_ifIXoosrSt7is_sameINS_9AttributeET_E5valuentsrSt10is_base_ofIS4_S5_E5valueESt8optionalINS_6detail20ElementsAttrIteratorIS5_EEEE4typeEv]+0x178): relocation truncated to fit: R_AARCH64_CALL26 against symbol `__cxa_guard_release@@CXXABI_1.3' defined in .text section in /usr/lib/gcc/aarch64-linux-gnu/13/libstdc++.so
bazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o): in function `llvm::StringRef llvm::getTypeName<unsigned int>()':
ops.cc:(.text._ZN4llvm11getTypeNameIjEENS_9StringRefEv[_ZN4llvm11getTypeNameIjEENS_9StringRefEv]+0xa8): additional relocation overflows omitted from the output
collect2: error: ld returned 1 exit status
```
This issue only occurs when building for aarch64 CUDA, it does not occur for the x86_64 CUDA build or the aarch64 CPU-only build.

### Standalone code to reproduce the issue

```console
> git clone git@github.com:adamjstewart/spack.git
> cd spack
> git checkout packages/py-tensorflow
> . share/spack/setup-env.sh
> spack install py-tensorflow+cuda cuda_arch=80
```

### Relevant log output

* [build log](https://github.com/user-attachments/files/20281014/spack-build-out.txt.gz)
* [build env](https://github.com/user-attachments/files/20281015/spack-build-env.txt.gz)

Possibly related to https://github.com/tensorflow/tensorflow/issues/69951",1
[TPU] Cannot initialize TPU system on TPUv4-Pod with TensorFlow 2.19,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.19

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Note that `tpu-vm-tf-2.19.0-pod-se` and `tpu-vm-tf-2.19.0-pod-pijt` perform differently.

### `tpu-vm-tf-2.19.0-pod-pijt`  
`tpu-vm-tf-2.19.0-pod-pijt`  stuck at `tf.config.experimental_connect_to_cluster(cluster_resolver)`

latest output before stuck:
```
I0518 15:49:45.456652 140338081622016 transport.py:157] Attempting refresh to obtain initial access_token
W0518 15:49:45.564130 140338081622016 context.py:916] Configuring coordination service type may not be effective because the context is already initialized.
I0518 15:49:45.564608 140338081622016 remote.py:225] default session config: device_count {
  key: ""GPU""
  value: 0
}
device_count {
  key: ""CPU""
  value: 1
}
gpu_options {
  experimental {
  }
}
allow_soft_placement: true
experimental {
  collective_group_leader: ""/job:worker/replica:0/task:0""
  coordination_config {
    service_type: ""standalone""
    service_leader: ""/job:worker/task:0""
  }
}
pluggable_device_options {
  experimental {
  }
}

```

### `tpu-vm-tf-2.19.0-pod-se`  

`tpu-vm-tf-2.19.0-pod-se`   can pass the `tf.config.experimental_connect_to_cluster(cluster_resolver)`, but raised error in next step `tf.tpu.experimental.initialize_tpu_system(cluster_resolver)`

error output:
```
    tf.tpu.experimental.initialize_tpu_system(cluster_resolver)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py"", line 72, in initialize_tpu_system
    return tpu_strategy_util.initialize_tpu_system_impl(
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/tpu/tpu_strategy_util.py"", line 140, in initialize_tpu_system_impl
    context.async_wait()
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py"", line 3115, in async_wait
    context().sync_executors()
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py"", line 883, in sync_executors
    pywrap_tfe.TFE_ContextSyncExecutors(self._context_handle)
tensorflow.python.framework.errors_impl.InternalError: {{function_node __inference__tpu_init_fn_16}} RET_CHECK failure (learning/45eac/google/xla/tpu_node_interfaces.cc:432) *platform
         [[{{node disconnect_tpu_host/_1}}]]

```




`

### Standalone code to reproduce the issue

```shell
cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(name)
tf.config.experimental_connect_to_cluster(cluster_resolver) # tpu-vm-tf-2.19.0-pod-pijt stuck
tf.tpu.experimental.initialize_tpu_system(cluster_resolver) # tpu-vm-tf-2.19.0-pod-se error


strategy = tf.distribute.TPUStrategy(cluster_resolver)
```",1
Create a docker image from research/object_detection/dockerfiles/tf2/Dockerfile,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.6.2

### Custom code

No

### OS platform and distribution

Ubuntu 22

### Mobile device

_No response_

### Python version

python3.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA 12.6 - Driver Version: 560.35.03

### GPU model and memory

NVIDIA GeForce RTX 2080 - 12GB

### Current behavior?

I follow th command:

docker build -f research/object_detection/dockerfiles/tf2/Dockerfile -t od .


docker run --name geo-train-tensorflow2-gpu --gpus all -it -v /product/git/tensorflow-training:/product/git/tensorflow-training -v /product/dataset:/product/dataset -p 6006:6006 -p 8888:8888 --ipc=host --ulimit memlock=-1 --ulimit stack=67108864  od

I use colab notebook https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_tf2_colab.ipynb



$env

LS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:
LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
LESSCLOSE=/usr/bin/lesspipe %s %s
LANG=C.UTF-8
HOSTNAME=4fade06f19b5
NVIDIA_VISIBLE_DEVICES=all
PWD=/home/tensorflow/models/research
HOME=/home/tensorflow
TF_CPP_MIN_LOG_LEVEL=3
TERM=xterm-256color
CUDA_PKG_VERSION=10-1=10.1.243-1
CUDA_VERSION=10.1.243
NVIDIA_DRIVER_CAPABILITIES=compute,utility
SHLVL=1
NVIDIA_REQUIRE_CUDA=cuda>=10.1 brand=tesla,driver>=384,driver<385 brand=tesla,driver>=396,driver<397 brand=tesla,driver>=410,driver<411
PATH=/home/tensorflow/.local/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
PS1=\[\e]0;\u@\h: \w\a\]${debian_chroot:+($debian_chroot)}\[\033[01;32m\]\u@\h\[\033[00m\]:\[\033[01;34m\]\w\[\033[00m\]\$ 
LESSOPEN=| /usr/bin/lesspipe %s
_=/usr/bin/env


pip list:

Package                       Version
----------------------------- ---------------
absl-py                       0.12.0
apache-beam                   2.38.0
argon2-cffi                   21.3.0
argon2-cffi-bindings          21.2.0
asn1crypto                    0.24.0
astunparse                    1.6.3
async-generator               1.10
attrs                         22.2.0
avro-python3                  1.10.2
backcall                      0.2.0
beautifulsoup4                4.6.0
bleach                        4.1.0
cached-property               1.5.2
cachetools                    4.1.0
cairocffi                     0.8.0
certifi                       2025.4.26
cffi                          1.11.5
chardet                       3.0.4
charset-normalizer            2.0.12
clang                         5.0
cloudpickle                   2.2.1
cmake                         3.28.4
colorama                      0.4.5
contextlib2                   21.6.0
crcmod                        1.7
cryptography                  2.1.4
cycler                        0.11.0
Cython                        3.0.12
dataclasses                   0.8
decorator                     5.1.1
defusedxml                    0.7.1
dill                          0.3.1.1
dm-tree                       0.1.8
docopt                        0.6.2
entrypoints                   0.4
fastavro                      1.4.7
flatbuffers                   1.12
gast                          0.4.0
gin-config                    0.5.0
google-api-core               2.8.2
google-api-python-client      2.52.0
google-auth                   1.35.0
google-auth-httplib2          0.2.0
google-auth-oauthlib          0.4.1
google-pasta                  0.2.0
googleapis-common-protos      1.56.3
grpcio                        1.48.2
h5py                          3.1.0
hdfs                          2.7.3
html5lib                      0.999999999
httplib2                      0.19.1
idna                          2.6
importlib-metadata            4.8.3
importlib-resources           5.4.0
ipykernel                     5.5.6
ipython                       7.16.3
ipython-genutils              0.2.0
jedi                          0.17.2
Jinja2                        3.0.3
joblib                        1.1.1
jsonschema                    3.2.0
jupyter-client                7.1.2
jupyter-core                  4.9.2
jupyterlab-pygments           0.1.2
kaggle                        1.6.17
keras                         2.6.0
Keras-Preprocessing           1.1.2
keyring                       10.6.0
keyrings.alt                  3.0
kiwisolver                    1.3.1
lvis                          0.5.3
lxml                          4.2.1
Markdown                      3.2.1
MarkupSafe                    2.0.1
matplotlib                    3.3.4
mistune                       0.8.4
nbclient                      0.5.9
nbconvert                     6.0.7
nbformat                      5.1.3
nest-asyncio                  1.6.0
notebook                      6.4.10
numpy                         1.19.5
oauth2client                  4.1.3
oauthlib                      3.1.0
object-detection              0.1
olefile                       0.45.1
opencv-python                 4.11.0.86
opencv-python-headless        4.11.0.86
opt-einsum                    3.3.0
orjson                        3.6.1
packaging                     21.3
pandas                        1.1.5
pandocfilters                 1.5.1
parso                         0.7.1
pexpect                       4.9.0
pickleshare                   0.7.5
Pillow                        8.4.0
pip                           21.3.1
ply                           3.11
portalocker                   2.7.0
prometheus-client             0.17.1
promise                       2.3
prompt-toolkit                3.0.36
proto-plus                    1.23.0
protobuf                      3.19.6
psutil                        7.0.0
ptyprocess                    0.7.0
py-cpuinfo                    9.0.0
pyarrow                       6.0.1
pyasn1                        0.4.8
pyasn1-modules                0.2.8
pycocotools                   2.0.7
pycparser                     2.18
pycrypto                      2.6.1
pydot                         1.4.2
Pygments                      2.14.0
PyGObject                     3.26.1
pymongo                       3.13.0
pyparsing                     2.4.7
pyrsistent                    0.18.0
python-apt                    1.6.5+ubuntu0.2
python-dateutil               2.9.0.post0
python-slugify                6.1.2
pytz                          2025.2
pyxdg                         0.25
PyYAML                        6.0.1
pyzmq                         25.1.2
regex                         2023.8.8
requests                      2.27.1
requests-oauthlib             1.3.0
rsa                           4.0
sacrebleu                     2.2.0
scikit-learn                  0.24.2
scipy                         1.4.1
SecretStorage                 2.3.1
Send2Trash                    1.8.3
sentencepiece                 0.2.0
seqeval                       1.2.2
setuptools                    46.1.3
six                           1.15.0
tabulate                      0.8.10
tensorboard                   2.6.0
tensorboard-data-server       0.6.1
tensorboard-plugin-wit        1.6.0.post3
tensorflow                    2.6.2
tensorflow-addons             0.14.0
tensorflow-datasets           4.5.2
tensorflow-estimator          2.6.0
tensorflow-gpu                2.2.0
tensorflow-hub                0.16.0
tensorflow-io                 0.21.0
tensorflow-io-gcs-filesystem  0.21.0
tensorflow-metadata           1.2.0
tensorflow-model-optimization 0.7.3
tensorflow-text               2.6.0
termcolor                     1.1.0
terminado                     0.12.1
testpath                      0.6.0
text-unidecode                1.3
tf-models-official            2.7.2
tf-slim                       1.1.0
threadpoolctl                 3.1.0
tornado                       6.1
tqdm                          4.64.1
traitlets                     4.3.3
typeguard                     2.13.3
typing-extensions             3.7.4.3
uritemplate                   4.1.1
urllib3                       1.25.9
wcwidth                       0.2.13
webencodings                  0.5
Werkzeug                      1.0.1
wheel                         0.37.1
wrapt                         1.12.1
xcffib                        0.5.1
zipp                          3.6.0

When import "" import tensorflow as tf"" I have this error:

NotFoundError                             Traceback (most recent call last)
<ipython-input-1-49156a41fe80> in <module>
      8 from PIL import Image, ImageDraw, ImageFont
      9 
---> 10 import tensorflow as tf
     11 
     12 from object_detection.utils import label_map_util

~/.local/lib/python3.6/site-packages/tensorflow/__init__.py in <module>
    436     _main_dir = _os.path.join(_s, 'tensorflow/core/kernels')
    437     if _os.path.exists(_main_dir):
--> 438       _ll.load_library(_main_dir)
    439 
    440     # Load third party dynamic kernels.

~/.local/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py in load_library(library_location)
    152 
    153     for lib in kernel_libraries:
--> 154       py_tf.TF_LoadLibrary(lib)
    155 
    156   else:

NotFoundError: /usr/local/lib/python3.6/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow8OpKernel11TraceStringEPNS_15OpKernelContextEb

### Standalone code to reproduce the issue

```shell
git clone https://github.com/tensorflow/models.git
 
docker build -f research/object_detection/dockerfiles/tf2/Dockerfile -t od .


docker run --name geo-train-tensorflow2-gpu --gpus all -it -v /product/git/newtrain:/product/git/newtrain -v /product/git/tensorflow-training:/product/git/tensorflow-training -v /product/dataset:/product/dataset -p 6006:6006 -p 8888:8888 --ipc=host --ulimit memlock=-1 --ulimit stack=67108864  od

docker exec -it geo-train-tensorflow2-gpu  /bin/bash

pip install notebook

open the notebook :  https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_tf2_colab.ipynb

and execute it
```

### Relevant log output

```shell
NotFoundError                             Traceback (most recent call last)
<ipython-input-1-49156a41fe80> in <module>
      8 from PIL import Image, ImageDraw, ImageFont
      9 
---> 10 import tensorflow as tf
     11 
     12 from object_detection.utils import label_map_util

~/.local/lib/python3.6/site-packages/tensorflow/__init__.py in <module>
    436     _main_dir = _os.path.join(_s, 'tensorflow/core/kernels')
    437     if _os.path.exists(_main_dir):
--> 438       _ll.load_library(_main_dir)
    439 
    440     # Load third party dynamic kernels.

~/.local/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py in load_library(library_location)
    152 
    153     for lib in kernel_libraries:
--> 154       py_tf.TF_LoadLibrary(lib)
    155 
    156   else:

NotFoundError: /usr/local/lib/python3.6/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow8OpKernel11TraceStringEPNS_15OpKernelContextEb
```",1
Loss suddenly becoming NaN on TPUs,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.10.16

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

Kaggle TPU VM v3-8

### Current behavior?

I came across a very strange issue on Kaggle's TPUs.  I'm not sure whether it's hardware or configuration issue on their side or a TF issue (I don't have the ability to test elsewhere), but since I got no feedback from them after almost one month I figure it may be worth checking on TF side.

The observation is that after a fixed number of batches, at all epochs, the loss suddenly becomes NaN.  This happens even with dummy data on a dummy model, whatever the tensor dimensions.

The original report can be found here : https://www.kaggle.com/discussions/product-feedback/574221



### Standalone code to reproduce the issue

```shell
MWE Kaggle notebook : https://www.kaggle.com/code/gguillard/a-very-strange-tpu-easter-egg
```

### Relevant log output

```shell

```",1
"use ''build_aar.sh"" to generate the tensorflow-lite.aar failure","### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.19.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04 LTS

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

6.5.0

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

build success and generate the tensorflow-lite.aar file and the tensorflow-lite-select-tf-ops.aar file

### Standalone code to reproduce the issue

```shell
root@parallels-Parallels-Virtual-Platform:/tensorflow-2.19.0# bash tensorflow/lite/tools/build_aar.sh  --input_models=model/walletBillModel.tflite --target_archs=arm64-v8a,armeabi-v7a
+++ dirname tensorflow/lite/tools/build_aar.sh
```

### Relevant log output

```shell
root@parallels-Parallels-Virtual-Platform:/tensorflow-2.19.0# bash tensorflow/lite/tools/build_aar.sh  --input_models=model/walletBillModel.tflite --target_archs=arm64-v8a,armeabi-v7a
+++ dirname tensorflow/lite/tools/build_aar.sh
++ cd tensorflow/lite/tools
++ pwd
+ SCRIPT_DIR=/tensorflow-2.19.0/tensorflow/lite/tools
++ cd /tensorflow-2.19.0/tensorflow/lite/tools/../../../
++ pwd
+ ROOT_DIR=/tensorflow-2.19.0
+ TARGET_ARCHS=x86,x86_64,arm64-v8a,armeabi-v7a
+ '[' '!' -z ']'
+ '[' 2 -gt 4 ']'
+ for i in ""$@""
+ case $i in
+ FLAG_MODELS=model/walletBillModel.tflite
+ shift
+ for i in ""$@""
+ case $i in
+ TARGET_ARCHS=arm64-v8a,armeabi-v7a
+ shift
+ cd /tensorflow-2.19.0
+ '[' '!' -f /tensorflow-2.19.0/.tf_configure.bazelrc ']'
+ grep -q ANDROID_SDK_HOME /tensorflow-2.19.0/.tf_configure.bazelrc
+ '[' -z model/walletBillModel.tflite ']'
+ TMP_DIR=/tensorflow-2.19.0/tmp/
+ rm -rf /tensorflow-2.19.0/tmp/
+ mkdir -p /tensorflow-2.19.0/tmp/
+ MODEL_NAMES=
++ echo model/walletBillModel.tflite
++ sed 's/,/ /g'
+ for model in $(echo ${FLAG_MODELS} | sed ""s/,/ /g"")
+ cp model/walletBillModel.tflite /tensorflow-2.19.0/tmp/
++ basename model/walletBillModel.tflite
+ MODEL_NAMES=,walletBillModel.tflite
+ TFLITE_OPS_SRCS=
++ echo
++ sed 's/,/ /g'
+ generate_tflite_aar
+ pushd /tensorflow-2.19.0/tmp/
+ message=('load(""//tensorflow/lite:build_def.bzl"", ""tflite_custom_android_library"")' 'load(""//tensorflow/lite/java:aar_with_jni.bzl"", ""aar_with_jni"")' '' 'tflite_custom_android_library(' '    name = ""custom_tensorflowlite"",')
+ message+=('    '$(generate_list_field ""models"" $MODEL_NAMES))
++ generate_list_field models ,walletBillModel.tflite
++ local name=models
++ local list_string=,walletBillModel.tflite
++ list=('walletBillModel.tflite')
++ local list
++ message=('models=[')
++ local message
++ for item in ""${list[@]}""
++ message+=(""\""$item\"","")
++ message+=('],')
++ printf %s 'models=[' '""walletBillModel.tflite"",' '],'
+ message+=('    '$(generate_list_field ""srcs"" $TFLITE_OPS_SRCS))
++ generate_list_field srcs
++ local name=srcs
++ local list_string=
++ list=()
++ local list
++ message=('srcs=[')
++ local message
++ message+=('],')
++ printf %s 'srcs=[' '],'
+ message+=('    '$(generate_list_field ""deps"" $FLAG_TFLITE_OPS_DEPS))
++ generate_list_field deps
++ local name=deps
++ local list_string=
++ list=()
++ local list
++ message=('deps=[')
++ local message
++ message+=('],')
++ printf %s 'deps=[' '],'
+ message+=(')' '' 'aar_with_jni(' '    name = ""tensorflow-lite"",' '    android_library = "":custom_tensorflowlite"",' ')' '')
+ printf '%s\n' 'load(""//tensorflow/lite:build_def.bzl"", ""tflite_custom_android_library"")' 'load(""//tensorflow/lite/java:aar_with_jni.bzl"", ""aar_with_jni"")' '' 'tflite_custom_android_library(' '    name = ""custom_tensorflowlite"",' '    models=[""walletBillModel.tflite"",],' '    srcs=[],' '    deps=[],' ')' '' 'aar_with_jni(' '    name = ""tensorflow-lite"",' '    android_library = "":custom_tensorflowlite"",' ')' ''
+ popd
+ bazel build -c opt --config=opt --cxxopt=--std=c++17 --fat_apk_cpu=arm64-v8a,armeabi-v7a --define=android_dexmerger_tool=d8_dexmerger --define=android_incremental_dexing_tool=d8_dexbuilder --host_crosstool_top=@bazel_tools//tools/cpp:toolchain //tmp:tensorflow-lite
INFO: Reading 'startup' options from /tensorflow-2.19.0/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=157
INFO: Reading rc options for 'build' from /tensorflow-2.19.0/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /tensorflow-2.19.0/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /tensorflow-2.19.0/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env ANDROID_NDK_HOME=/android/ndk --action_env ANDROID_NDK_VERSION=25 --action_env ANDROID_NDK_API_LEVEL=30 --action_env ANDROID_BUILD_TOOLS_VERSION=31.0.0 --action_env ANDROID_SDK_API_LEVEL=30 --action_env ANDROID_SDK_HOME=/android/sdk
INFO: Found applicable config definition build:short_logs in file /tensorflow-2.19.0/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /tensorflow-2.19.0/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:opt in file /tensorflow-2.19.0/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare
INFO: Found applicable config definition build:linux in file /tensorflow-2.19.0/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow-2.19.0/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Build option --fat_apk_cpu has changed, discarding analysis cache.
INFO: Analyzed target //tmp:tensorflow-lite (1 packages loaded, 15708 targets configured).
INFO: Found 1 target...
Target //tmp:tensorflow-lite up-to-date:
  bazel-bin/tmp/tensorflow-lite.aar
INFO: Elapsed time: 1112.653s, Critical Path: 80.66s
INFO: 1378 processes: 1 internal, 1377 local.
INFO: Build completed successfully, 1378 total actions
+ OUT_FILES=' bazel-bin/tmp/tensorflow-lite.aar'
+ bazel build -c opt --config=monolithic //tensorflow/lite/tools:list_flex_ops_no_kernel_main
INFO: Reading 'startup' options from /tensorflow-2.19.0/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=157
INFO: Reading rc options for 'build' from /tensorflow-2.19.0/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /tensorflow-2.19.0/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /tensorflow-2.19.0/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env ANDROID_NDK_HOME=/android/ndk --action_env ANDROID_NDK_VERSION=25 --action_env ANDROID_NDK_API_LEVEL=30 --action_env ANDROID_BUILD_TOOLS_VERSION=31.0.0 --action_env ANDROID_SDK_API_LEVEL=30 --action_env ANDROID_SDK_HOME=/android/sdk
INFO: Found applicable config definition build:short_logs in file /tensorflow-2.19.0/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /tensorflow-2.19.0/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:monolithic in file /tensorflow-2.19.0/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false
INFO: Found applicable config definition build:linux in file /tensorflow-2.19.0/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow-2.19.0/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Build options --copt, --cxxopt, --define, and 2 more have changed, discarding analysis cache.
INFO: Analyzed target //tensorflow/lite/tools:list_flex_ops_no_kernel_main (5 packages loaded, 1287 targets configured).
INFO: Found 1 target...
Target //tensorflow/lite/tools:list_flex_ops_no_kernel_main up-to-date:
  bazel-bin/tensorflow/lite/tools/list_flex_ops_no_kernel_main
INFO: Elapsed time: 122.614s, Critical Path: 45.26s
INFO: 46 processes: 1 internal, 45 local.
INFO: Build completed successfully, 46 total actions
+ bazel-bin/tensorflow/lite/tools/list_flex_ops_no_kernel_main --graphs=model/walletBillModel.tflite
++ cat /tensorflow-2.19.0/tmp//ops_list.txt
+ [[ [""TensorListReserve"",""TensorListSetItem"",""TensorListStack""] != \[\] ]]
+ generate_flex_aar
+ pushd /tensorflow-2.19.0/tmp/
/tensorflow-2.19.0/tmp /tensorflow-2.19.0
+ message=('load(""//tensorflow/lite/delegates/flex:build_def.bzl"", ""tflite_flex_android_library"")' 'load(""//tensorflow/lite/java:aar_with_jni.bzl"", ""aar_with_jni"")' '' 'tflite_flex_android_library(' '    name = ""custom_tensorflowlite_flex"",')
+ message+=('    '$(generate_list_field ""models"" $MODEL_NAMES))
++ generate_list_field models ,walletBillModel.tflite
++ local name=models
++ local list_string=,walletBillModel.tflite
++ list=('walletBillModel.tflite')
++ local list
++ message=('models=[')
++ local message
++ for item in ""${list[@]}""
++ message+=(""\""$item\"","")
++ message+=('],')
++ printf %s 'models=[' '""walletBillModel.tflite"",' '],'
+ message+=(')' '' 'aar_with_jni(' '    name = ""tensorflow-lite-select-tf-ops"",' '    android_library = "":custom_tensorflowlite_flex"",' ')')
+ printf '%s\n' 'load(""//tensorflow/lite/delegates/flex:build_def.bzl"", ""tflite_flex_android_library"")' 'load(""//tensorflow/lite/java:aar_with_jni.bzl"", ""aar_with_jni"")' '' 'tflite_flex_android_library(' '    name = ""custom_tensorflowlite_flex"",' '    models=[""walletBillModel.tflite"",],' ')' '' 'aar_with_jni(' '    name = ""tensorflow-lite-select-tf-ops"",' '    android_library = "":custom_tensorflowlite_flex"",' ')'
+ cp /tensorflow-2.19.0/tensorflow/lite/java/AndroidManifest.xml .
+ cp /tensorflow-2.19.0/tensorflow/lite/java/proguard.flags .
+ popd
/tensorflow-2.19.0
+ bazel build -c opt --config=opt --cxxopt=--std=c++17 --fat_apk_cpu=arm64-v8a,armeabi-v7a --define=android_dexmerger_tool=d8_dexmerger --define=android_incremental_dexing_tool=d8_dexbuilder --host_crosstool_top=@bazel_tools//tools/cpp:toolchain //tmp:tensorflow-lite-select-tf-ops
INFO: Reading 'startup' options from /tensorflow-2.19.0/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=157
INFO: Reading rc options for 'build' from /tensorflow-2.19.0/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /tensorflow-2.19.0/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /tensorflow-2.19.0/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env ANDROID_NDK_HOME=/android/ndk --action_env ANDROID_NDK_VERSION=25 --action_env ANDROID_NDK_API_LEVEL=30 --action_env ANDROID_BUILD_TOOLS_VERSION=31.0.0 --action_env ANDROID_SDK_API_LEVEL=30 --action_env ANDROID_SDK_HOME=/android/sdk
INFO: Found applicable config definition build:short_logs in file /tensorflow-2.19.0/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /tensorflow-2.19.0/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:opt in file /tensorflow-2.19.0/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare
INFO: Found applicable config definition build:linux in file /tensorflow-2.19.0/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow-2.19.0/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Build options --copt, --cxxopt, --define, and 2 more have changed, discarding analysis cache.
INFO: Analyzed target //tmp:tensorflow-lite-select-tf-ops (550 packages loaded, 55015 targets configured).
INFO: Found 1 target...
ERROR: /tensorflow-2.19.0/tensorflow/compiler/mlir/lite/BUILD:1306:11: Compiling tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc [for tool] failed: (Exit 1): gcc failed: error executing command (from target //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize) /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 206 arguments skipped)
In file included from /usr/include/c++/11/memory:76,
                 from tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:16:
/usr/include/c++/11/bits/unique_ptr.h: In instantiation of 'typename std::_MakeUniq<_Tp>::__single_object std::make_unique(_Args&& ...) [with _Tp = mlir::TFL::{anonymous}::DefaultQuantParamsPass; _Args = {const mlir::TFL::DefaultQuantParamsPassOptions&}; typename std::_MakeUniq<_Tp>::__single_object = std::unique_ptr<mlir::TFL::{anonymous}::DefaultQuantParamsPass, std::default_delete<mlir::TFL::{anonymous}::DefaultQuantParamsPass> >]':
tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:249:50:   required from here
/usr/include/c++/11/bits/unique_ptr.h:962:30: error: call of overloaded 'DefaultQuantParamsPass(const mlir::TFL::DefaultQuantParamsPassOptions&)' is ambiguous
  962 |     { return unique_ptr<_Tp>(new _Tp(std::forward<_Args>(__args)...)); }
      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:52:
bazel-out/k8-opt-exec-50AE0418/bin/tensorflow/compiler/mlir/lite/transforms/passes.h.inc:162:3: note: candidate: 'mlir::TFL::{anonymous}::impl::DefaultQuantParamsPassBase<DerivedT>::DefaultQuantParamsPassBase(mlir::TFL::DefaultQuantParamsPassOptions) [with DerivedT = mlir::TFL::{anonymous}::DefaultQuantParamsPass]'
  162 |   DefaultQuantParamsPassBase(DefaultQuantParamsPassOptions options) : DefaultQuantParamsPassBase() {
      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~
tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:57:37: note:   inherited here
   57 |   using DefaultQuantParamsPassBase::DefaultQuantParamsPassBase;
      |                                     ^~~~~~~~~~~~~~~~~~~~~~~~~~
tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:66:12: note: candidate: 'mlir::TFL::{anonymous}::DefaultQuantParamsPass::DefaultQuantParamsPass(const mlir::TFL::DefaultQuantParamsPassOptions&)'
   66 |   explicit DefaultQuantParamsPass(
      |            ^~~~~~~~~~~~~~~~~~~~~~
tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:54:7: note: candidate: 'mlir::TFL::{anonymous}::DefaultQuantParamsPass::DefaultQuantParamsPass(const mlir::TFL::{anonymous}::DefaultQuantParamsPass&)'
   54 | class DefaultQuantParamsPass
      |       ^~~~~~~~~~~~~~~~~~~~~~
tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:54:7: note: candidate: 'mlir::TFL::{anonymous}::DefaultQuantParamsPass::DefaultQuantParamsPass(mlir::TFL::{anonymous}::DefaultQuantParamsPass&&)' (deleted)
Target //tmp:tensorflow-lite-select-tf-ops failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 2270.665s, Critical Path: 132.12s
INFO: 1303 processes: 36 internal, 1267 local.
FAILED: Build did NOT complete successfully
```",1
Depth anything V2 Tflite outputs constants on qualcomm gpus,"### 1. System information

- Ubuntu 20.04
- bulit grom source tensorflow and tflite 2.17.1
- v2.17.1 tensorflow 3c92ac03cab816044f7b18a86eb86aa01a294d95

### 2. Code

Provide code to help us reproduce your issues using one of the following options:
tflite: https://huggingface.co/qualcomm/Depth-Anything-V2/tree/main

pip3 install qai-hub
qai-hub configure --api_token INSERT_API_TOKEN

import qai_hub as hub
import numpy as np


# TFlite model path
tflite_model_id = ""tflite_path""

# Setup input data
input_tensor = np.arange(np.prod((1, 518, 518, 3)), dtype=np.float32).reshape((1, 518, 518, 3))

# Submit inference job
job = hub.submit_inference_job(
    model=tflite_model_id,
    device=hub.Device(""QCS8550 (Proxy)""),
    name=""depth_anything_v2"",
    inputs=dict(image=[input_tensor]),
    options=""--compute_unit gpu""
)

# Load the output data into a dictionary of numpy arrays
output_tensors = job.download_output_data()


### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

Model produces wrong results on qualcomm gpus. In qai-hub I replaced the GELU layers in the huggingface config to gelu_new and the model seems to export just fine using qai-hub. When I run it on the QRB5165 it seems to run fine on the gpu but outputs a constant image with value 0.088684. This model works fine if I run it on the cpu though. Also other models are working on the gpu if I use them. Also it seems like this model works completely fine on gpu delegate using opencl on cuda hardware on my laptop. It seems like if the model works on cpu the tflite is correct and hence how the graph is executed depends on gpu delegates.
",0
Segmentation fault in tf.data.experimental.service.DispatchServer,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.19.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.11.11

### Bazel version

6.5.0

### GCC/compiler version

clang 18.1.8

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Segfault when using tf.data.experimental.service.DispatchServer

### Standalone code to reproduce the issue

```shell
import pickle
import tensorflow as tf
import time
import pprint
import numpy as np

print(tf.__version__)

mylist = [None,False]
pprint.pprint(mylist)
mydict = {}

try:
    tf.data.experimental.service.DispatchServer(*mylist,**mydict)
except:
    pass

print(""done"")
```

### Relevant log output

```shell
2.19.0
[None, False]
Segmentation fault (core dumped)
```",1
[Bug] `libtensorflow` does not include `tensor.h` (has just 2 files under `tensorflow/core/`),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.11

### Custom code

No

### OS platform and distribution

[_GitHub Workflow_](https://docs.github.com/en/actions/writing-workflows/about-workflows) on _Ubuntu_

### Mobile device

_GitHub Workflow_

### Python version

Not applicable (_C++ TensorFlow_)

### Bazel version

Not applicable

### GCC/compiler version

Not applicable

### CUDA/cuDNN version

Not applicable

### GPU model and memory

Not applicable

### Current behavior?

https://github.com/SwuduSusuwu/SusuLib/actions/runs/14992373759/job/42118538367#step:5:130
>   [2025-05-13 08:53:50] [build-stdout] ./cxx/ClassTensorFlowCns.hxx:16:10: fatal error: 'tensorflow/core/framework/tensor.h' file not found

### Standalone code to reproduce the issue

Issue is on version `libtensorflow-*-2.11.0` (`libtensorflow-*-2.16.0` is not available to download)
```shell
~ $ wget -q https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-2.11.0.tar.gz
~ $ tar --list -f libtensorflow-cpu-linux-x86_64-2.11.0.tar.gz  | grep ""tensorflow/core""
include/tensorflow/core/
include/tensorflow/core/platform/
include/tensorflow/core/platform/ctstring.h
include/tensorflow/core/platform/ctstring_internal.h
~ $ tar --list -f libtensorflow-cpu-linux-x86_64-2.11.0.tar.gz  | grep ""tensor.h""
include/tensorflow/c/tf_tensor.h
~ $ wget https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-2.16.0.tar.gz
--2025-05-13 01:56:01--  https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-2.16.0.tar.gz
Resolving storage.googleapis.com (storage.googleapis.com)... 2607:f8b0:4007:809::201b, 2607:f8b0:4007:817::201b, 2607:f8b0:4007:818::201b, ...
Connecting to storage.googleapis.com (storage.googleapis.com)|2607:f8b0:4007:809::201b|:443... connected.
HTTP request sent, awaiting response... 404 Not Found
2025-05-13 01:56:02 ERROR 404: Not Found.
```
`libtensorflow-*-2.16.1` was also `404 Not Found`.

### Relevant log output
All relevant [_GitHub_ Autobuild](https://github.com/SwuduSusuwu/SusuLib/actions/runs/15001257493/job/42148531253?pr=63#step:5:64) output from <https://github.com/SwuduSusuwu/SusuLib/blob/57c936847e8c7f739b92cfe972562afef5381bc6/build.sh#L32-L95>:
```shell
  [2025-05-13 15:59:09] [build-stdout] [Notice: `./build.sh` was executed through one of GitHub's Workflows, will auto-install `libeigen3-dev` and `libtensorflow`.]
  [2025-05-13 15:59:09] [build-stdout] WARNING: apt does not have a stable CLI interface. Use with caution in scripts.
  [2025-05-13 15:59:09] [build-stdout] Reading package lists...
  [2025-05-13 15:59:09] [build-stdout] Building dependency tree...
  [2025-05-13 15:59:09] [build-stdout] Reading state information...
  [2025-05-13 15:59:09] [build-stdout] Suggested packages:
  [2025-05-13 15:59:09] [build-stdout]   libeigen3-doc libmpfrc++-dev
  [2025-05-13 15:59:09] [build-stdout] The following NEW packages will be installed:
  [2025-05-13 15:59:09] [build-stdout]   libeigen3-dev
  [2025-05-13 15:59:09] [build-stdout] 0 upgraded, 1 newly installed, 0 to remove and 24 not upgraded.
  [2025-05-13 15:59:09] [build-stdout] Need to get 1056 kB of archives.
  [2025-05-13 15:59:09] [build-stdout] After this operation, 9081 kB of additional disk space will be used.
  [2025-05-13 15:59:09] [build-stdout] Get:1 file:/etc/apt/apt-mirrors.txt Mirrorlist [144 B]
  [2025-05-13 15:59:09] [build-stdout] Get:2 http://azure.archive.ubuntu.com/ubuntu jammy/universe amd64 libeigen3-dev all 3.4.0-2ubuntu2 [1056 kB]
  [2025-05-13 15:59:10] [build-stdout] Fetched 1056 kB in 0s (14.5 MB/s)
  [2025-05-13 15:59:10] [build-stdout] Selecting previously unselected package libeigen3-dev.
  [2025-05-13 15:59:16] [build-stdout] (Reading database ... 272241 files and directories currently installed.)
  [2025-05-13 15:59:16] [build-stdout] Preparing to unpack .../libeigen3-dev_3.4.0-2ubuntu2_all.deb ...
  [2025-05-13 15:59:16] [build-stdout] Unpacking libeigen3-dev (3.4.0-2ubuntu2) ...
  [2025-05-13 15:59:16] [build-stdout] Setting up libeigen3-dev (3.4.0-2ubuntu2) ...
  [2025-05-13 15:59:17] [build-stdout] NEEDRESTART-VER: 3.5
  [2025-05-13 15:59:18] [build-stdout] NEEDRESTART-KCUR: 6.8.0-1027-azure
  [2025-05-13 15:59:18] [build-stdout] NEEDRESTART-KEXP: 6.8.0-1027-azure
  [2025-05-13 15:59:18] [build-stdout] NEEDRESTART-KSTA: 1
  [2025-05-13 15:59:19] [build-stdout] WARNING: apt does not have a stable CLI interface. Use with caution in scripts.
  [2025-05-13 15:59:19] [build-stdout] Reading package lists...
  [2025-05-13 15:59:20] [build-stdout] Building dependency tree...
  [2025-05-13 15:59:20] [build-stdout] Reading state information...
  [2025-05-13 15:59:20] [build-stdout] E: Unable to locate package libtensorflow
  [2025-05-13 15:59:20] [build-stdout] 2025-05-13 15:59:20 URL:https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-2.11.0.tar.gz [121659491/121659491] -> ""libtensorflow-cpu-linux-x86_64-2.11.0.tar.gz"" [1]
  [2025-05-13 15:59:23] [build-stdout] .
  [2025-05-13 15:59:23] [build-stdout] ..
  [2025-05-13 15:59:23] [build-stdout] tensorflow
  [2025-05-13 15:59:23] [build-stdout] .
  [2025-05-13 15:59:23] [build-stdout] ..
  [2025-05-13 15:59:23] [build-stdout] c
  [2025-05-13 15:59:23] [build-stdout] core
  [2025-05-13 15:59:23] [build-stdout] tsl
  [2025-05-13 15:59:23] [build-stdout] .
  [2025-05-13 15:59:23] [build-stdout] ..
  [2025-05-13 15:59:23] [build-stdout] platform
  [2025-05-13 15:59:23] [build-stdout] ls: cannot access 'include/tensorflow/third-party': No such file or directory
  [2025-05-13 15:59:23] [build-stdout] Cloning into 'xla'...
  [2025-05-13 15:59:24] [build-stdout] [Notice: SUSUWU_DEPENDENCY_INCLUDE(): Found package `libtensorflow`, FLAGS_USER will use `-F./include/`.]
  [2025-05-13 15:59:24] [build-stdout] find: ‘./tensorflow/’: No such file or directory
  [2025-05-13 15:59:24] [build-stdout] [Notice: SUSUWU_DEPENDENCY_INCLUDE(): Found package `xla`, FLAGS_USER will use `-F./xla/`.]
  [2025-05-13 15:59:24] [build-stdout] [Notice: SUSUWU_DEPENDENCY_INCLUDE(): Found package `tsl`, FLAGS_USER will use `-F./xla/third_party/tsl/`.]
  [2025-05-13 15:59:24] [build-stdout] [Notice: SUSUWU_DEPENDENCY_INCLUDE(): Found package `ml_dtypes`, FLAGS_USER will use `-F./xla/third_party/py/`.]
  [2025-05-13 15:59:24] [build-stdout] [Notice: SUSUWU_DEPENDENCY_INCLUDE(): Found package `eigen`, FLAGS_USER will use `-F/usr/include/eigen3`.]
  [2025-05-13 15:59:24] [build-stdout] [Warning: SUSUWU_PRODUCTION_USE(): `git branch` is ""HEAD""; for production use, use `git switch trunk`.]
  [2025-05-13 15:59:24] [build-stdout] [Notice: SUSUWU_PROCESS_RELEASE_DEBUG(): `./build.sh ` defaults to `./build.sh  --debug`.]
  [2025-05-13 15:59:24] [build-stdout] [Notice: SUSUWU_PROCESS_RELEASE_DEBUG(): `./build.sh  --debug` is slow (use `./build.sh  --release` to improve how fast `./${BINDIR}/${OUTPUT}` executes).]
  [2025-05-13 15:59:24] [build-stdout] [Notice: SUSUWU_SETUP_OBJDIR(): To redirect `clang++ -c ... -o ""${OBJDIR}${OBJ}.o""` (which has `OBJDIR=""./obj/""`), use `OBJDIR=""<new-path>""` (where ""<new-path>"" is a directory which you choose).]
  [2025-05-13 15:59:24] [build-stdout] [Notice: SUSUWU_SETUP_BINDIR(): To redirect `clang++ ... -o ""${BINDIR}${OUTPUT}""` (which has `BINDIR=""./bin/""`), use `BINDIR=""<new-path>""` (where ""<new-path>"" is a directory which you choose).]
  [2025-05-13 15:59:53] [build-stdout] ./cxx/ClassTensorFlowCns.hxx:16:10: fatal error: 'tensorflow/core/framework/tensor.h' file not found
  [2025-05-13 15:59:53] [build-stdout] #include <tensorflow/core/framework/tensor.h> /* tensorflow::Tensor tensorflow::TensorShape */
  [2025-05-13 15:59:53] [build-stdout]          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  [2025-05-13 15:59:53] [build-stdout] 1 error generated.
  [2025-05-13 15:59:57] [build-stdout] [Notice: `clang++  -g -Og -fno-omit-frame-pointer -std=c++11 -fsanitize=address -fno-sanitize-recover=all -fsanitize=float-divide-by-zero -fsanitize=float-cast-overflow -fno-sanitize=null -fno-sanitize=alignment -DSUSUWU_EXPERIMENTAL -DSUSUWU_DEFAULT_BRANCH=""trunk"" -I./include/ -I./xla/ -I./xla/third_party/tsl/ -I./xla/third_party/py/ -I/usr/include/eigen3 -Wall -Wno-unused-function -Wno-unused-function -Wextra -Wno-unused-parameter -Wno-ignored-qualifiers -Wpedantic ./cxx/ClassTensorFlowCns.hxx` failed, will not enable `CXXFLAGS=""${CXXFLAGS} -std=c++17 -DUSE_TENSORFLOW_CNS""` (skipped). If `libtensorflow` is installed, insert `-std=c++17 -DUSE_TENSORFLOW_CNS` into `./build.sh:FLAGS_USER`. To troubleshoot, use `cd ./include/ && ./configure`]
```",2
`tf.sparse.cross` performs differently on cpu and gpu,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v2.19.0-rc0-6-ge36baa30292 2.19.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

NVIDIA H100 - 80GB

### Current behavior?

tf.sparse.cross performs differently on cpu and gpu

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
input1 = tf.constant([[float('inf')],[0]],dtype=tf.float32)
input2 = tf.constant([[0],[0]],dtype=tf.float32)
with tf.device('/cpu:0'):
    result1 = tf.sparse.cross([input1,input2])
    print(""cpu:"",result1)
with tf.device('/gpu:0'):
    result2 = tf.sparse.cross([input1,input2])
    print(""gpu:"",result2)
```

### Relevant log output

```shell
cpu: SparseTensor(indices=tf.Tensor(
[[0 0]
 [1 0]], shape=(2, 2), dtype=int64), values=tf.Tensor([b'-9223372036854775808_X_0' b'0_X_0'], shape=(2,), dtype=string), dense_shape=tf.Tensor([2 1], shape=(2,), dtype=int64))
gpu: SparseTensor(indices=tf.Tensor(
[[0 0]
 [1 0]], shape=(2, 2), dtype=int64), values=tf.Tensor([b'9223372036854775807_X_0' b'0_X_0'], shape=(2,), dtype=string), dense_shape=tf.Tensor([2 1], shape=(2,), dtype=int64))
```",1
`tf.sparse.segment_mean` performs differently on cpu and gpu (variant of #93136),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v2.19.0-rc0-6-ge36baa30292 2.19.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

NVIDIA H100 - 80GB

### Current behavior?

tf.sparse.segment_mean performs differently on cpu and gpu, this is different from #93136, when using argument `num_segments`, in cpu case, is shows that segment ids must be < num_segments. However, in gpu case, the error log shows that segment ids must be >= 0, while in the code, the argument `segment_ids` satisfies this condition, the error message is clearly wrong.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

data = tf.constant([float('inf')],dtype=tf.float32)
indices = tf.constant([0],dtype=tf.int32)
segment_ids = tf.constant([0],dtype=tf.int32)
num_segments = tf.constant(0,dtype=tf.int32)
with tf.device('/cpu:0'):
    result1 = tf.sparse.segment_mean(data,indices,segment_ids,num_segments)
    print(result1)
with tf.device('/gpu:0'):
    result2 = tf.sparse.segment_mean(data,indices,segment_ids,num_segments)
    print(result2)
```

### Relevant log output

```shell
cpu case 

---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
Cell In[4], line 8
      6 num_segments = tf.constant(0,dtype=tf.int32)
      7 with tf.device('/cpu:0'):
----> 8     result1 = tf.sparse.segment_mean(data,indices,segment_ids,num_segments)
      9     print(result1)
     10 # with tf.device('/gpu:0'):
     11 #     result2 = tf.sparse.segment_mean(data,indices,segment_ids,num_segments)
     12 #     print(result2)

File ~/anaconda3/envs/cotfuzz/lib/python3.11/site-packages/tensorflow/python/ops/math_ops.py:5106, in sparse_segment_mean_v2(data, indices, segment_ids, num_segments, name, sparse_gradient)
   5065 @tf_export(""sparse.segment_mean"", v1=[])
   5066 def sparse_segment_mean_v2(
   5067     data,
   (...)   5072     sparse_gradient=False,
   5073 ):
   5074   r""""""Computes the mean along sparse segments of a tensor.
   5075 
   5076   Read [the section on
   (...)   5104     inferred for the last element in `segments_ids`.
   5105   """"""
-> 5106   return sparse_segment_mean(
   5107       data,
   5108       indices,
...
   6004 def raise_from_not_ok_status(e, name) -> NoReturn:
   6005   e.message += ("" name: "" + str(name if name is not None else """"))
-> 6006   raise core._status_to_exception(e) from None

InvalidArgumentError: {{function_node __wrapped__SparseSegmentMeanWithNumSegments_device_/job:localhost/replica:0/task:0/device:CPU:0}} segment ids must be < num_segments [Op:SparseSegmentMeanWithNumSegments] name:


gpu case

---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
Cell In[5], line 11
      7 # with tf.device('/cpu:0'):
      8 #     result1 = tf.sparse.segment_mean(data,indices,segment_ids,num_segments)
      9 #     print(result1)
     10 with tf.device('/gpu:0'):
---> 11     result2 = tf.sparse.segment_mean(data,indices,segment_ids,num_segments)
     12     print(result2)

File ~/anaconda3/envs/cotfuzz/lib/python3.11/site-packages/tensorflow/python/ops/math_ops.py:5106, in sparse_segment_mean_v2(data, indices, segment_ids, num_segments, name, sparse_gradient)
   5065 @tf_export(""sparse.segment_mean"", v1=[])
   5066 def sparse_segment_mean_v2(
   5067     data,
   (...)   5072     sparse_gradient=False,
   5073 ):
   5074   r""""""Computes the mean along sparse segments of a tensor.
   5075 
   5076   Read [the section on
   (...)   5104     inferred for the last element in `segments_ids`.
   5105   """"""
-> 5106   return sparse_segment_mean(
   5107       data,
   5108       indices,
   5109       segment_ids,
...
   6004 def raise_from_not_ok_status(e, name) -> NoReturn:
   6005   e.message += ("" name: "" + str(name if name is not None else """"))
-> 6006   raise core._status_to_exception(e) from None

InvalidArgumentError: {{function_node __wrapped__SparseSegmentMeanWithNumSegments_device_/job:localhost/replica:0/task:0/device:GPU:0}} segment ids must be >= 0 [Op:SparseSegmentMeanWithNumSegments] name:
```",1
`../tensorflow/third_party/xla/third_party/tsl/tsl/platform/ml_dtypes.h:19:10: error: 'ml_dtypes/include/float8.h' file not found [clang-diagnostic-error]`,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

- https://github.com/SwuduSusuwu/SusuLib/blob/304e6ecb8a339e4a74cd5845cd54649552f9b6ba/build.sh#L32-L93
- https://github.com/SwuduSusuwu/SusuLib/blob/304e6ecb8a339e4a74cd5845cd54649552f9b6ba/cxx/ClassTensorFlowCns.hxx

### TensorFlow version

commit d92ed6014f19e396a16b980c4f8969a74e8ada3a (grafted, HEAD -> master, origin/master, origin/HEAD)

### Custom code

Yes

### OS platform and distribution

Local [_Termux_](https://github.com/termux-play-store) on stock [_AOSP_](https://github.com/aosp-mirror) _Arm64_.
- Reproduced on [GitHub Workflow (Ubuntu _x86-64_))](https://github.com/tensorflow/tensorflow/issues/93130#issuecomment-2876987576):

### Mobile device

_Google Pixel 6_

### Python version

Not applicable (_C++_)

### Bazel version

Not applicable (not linker test, just compiler test)

### GCC/compiler version

`clang version 20.1.3`

### CUDA/cuDNN version

Not applicable (_Arm64_)

### GPU model and memory

Not applicable (_Arm64_)

### Current behavior?

```
In file included from ./cxx/ClassTensorFlowCns.hxx:16:
In file included from ../tensorflow/tensorflow/core/framework/tensor.h:27:
In file included from ../tensorflow/tensorflow/core/framework/allocator.h:26:
In file included from ../tensorflow/third_party/xla/xla/tsl/framework/allocator.h:26:
In file included from ../tensorflow/third_party/xla/xla/tsl/framework/numeric_types.h:22:
In file included from ../tensorflow/third_party/xla/xla/tsl/platform/types.h:22:
../tensorflow/third_party/xla/third_party/tsl/tsl/platform/ml_dtypes.h:19:10: fatal error: 'ml_dtypes/include/float8.h' file not found
   19 | #include ""ml_dtypes/include/float8.h""  // from @ml_dtypes_py
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
1 error generated.
```

### Standalone code to reproduce the issue

```shell
git clone https://github.com/tensorflow/tensorflow.git; cd tensorflow && `clang++ -I../tensorflow/ -I../tensorflow/third_party/xla/ -I../tensorflow/third_party/xla/third_party/tsl/ -I../tensorflow/third_party/xla/third_party/py/ -I/data/data/com.termux/files/home/../usr/include/eigen3 ../tensorflow/tensorflow/core/framework/tensor.h
fatal: destination path 'tensorflow' already exists and is not an empty directory.
clang++: warning: treating 'c-header' input as 'c++-header' when in C++ mode, this behavior is deprecated [-Wdeprecated]
In file included from ../tensorflow/tensorflow/core/framework/tensor.h:27:
In file included from ../tensorflow/tensorflow/core/framework/allocator.h:26:
In file included from ../tensorflow/third_party/xla/xla/tsl/framework/allocator.h:26:
In file included from ../tensorflow/third_party/xla/xla/tsl/framework/numeric_types.h:22:
In file included from ../tensorflow/third_party/xla/xla/tsl/platform/types.h:22:
../tensorflow/third_party/xla/third_party/tsl/tsl/platform/ml_dtypes.h:19:10: fatal error: 'ml_dtypes/include/float8.h' file not found
   19 | #include ""ml_dtypes/include/float8.h""  // from @ml_dtypes_py
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
1 error generated.
```

### Relevant log output
**Local (_Termux_)**:
```shell
~ $ git clone https://github.com/SwuduSusuwu/SusuLib.git
fatal: destination path 'SusuLib' already exists and is not an empty directory.
~ $ cd SusuLib
~/SusuLib $ git switch preview
D       cxx/ClassTensorFlowCns.hxx
Already on 'preview'
Your branch is up to date with 'origin/preview'.
~/SusuLib $ ./build.sh
~/SusuLib $ ./build.sh
[Notice: (C) 2024 Swudu Susuwu, dual licenses: choose [GPLv2](./LICENSE_GPLv2) or [Apache 2](./LICENSE), allows all uses.]
[Warning: `git branch` is ""experimental"" (which is unstable & sets `-DSUSUWU_EXPERIMENTAL`); for production use, execute `git switch trunk`.]
[Warning: SUSUWU_DEPENDENCY_INCLUDE(): Package `libtensorflow` not found. To install, use `sudo apt install libtensorflow`]
[Notice: SUSUWU_DEPENDENCY_INCLUDE(): Found package `tensorflow`, FLAGS_USER will use `-F../tensorflow/`.]
[Notice: SUSUWU_DEPENDENCY_INCLUDE(): Found package `xla`, FLAGS_USER will use `-F../tensorflow/third_party/xla/`.]
[Notice: SUSUWU_DEPENDENCY_INCLUDE(): Found package `tsl`, FLAGS_USER will use `-F../tensorflow/third_party/xla/third_party/tsl/`.]
[Notice: SUSUWU_DEPENDENCY_INCLUDE(): Found package `ml_dtypes`, FLAGS_USER will use `-F../tensorflow/third_party/xla/third_party/py/`.]
[Notice: SUSUWU_DEPENDENCY_INCLUDE(): Found package `eigen`, FLAGS_USER will use `-F/data/data/com.termux/files/home/../usr/include/eigen3`.]
[Warning: SUSUWU_PRODUCTION_USE(): `git branch` is ""preview""; for production use, use `git switch trunk`.]
[Notice: SUSUWU_PROCESS_RELEASE_DEBUG(): `./build.sh ` defaults to `./build.sh  --debug`.]
[Notice: SUSUWU_PROCESS_RELEASE_DEBUG(): `./build.sh  --debug` is slow (use `./build.sh  --release` to improve how fast `./${BINDIR}/${OUTPUT}` executes).]
[Notice: SUSUWU_SETUP_OBJDIR(): To redirect `clang++ -c ... -o ""${OBJDIR}${OBJ}.o""` (which has `OBJDIR=""./obj/""`), use `OBJDIR=""<new-path>""` (where ""<new-path>"" is a directory which you choose).]
[Notice: SUSUWU_SETUP_BINDIR(): To redirect `clang++ ... -o ""${BINDIR}${OUTPUT}""` (which has `BINDIR=""./bin/""`), use `BINDIR=""<new-path>""` (where ""<new-path>"" is a directory which you choose).]
In file included from ./cxx/ClassTensorFlowCns.hxx:16:
In file included from ../tensorflow/tensorflow/core/framework/tensor.h:27:
In file included from ../tensorflow/tensorflow/core/framework/allocator.h:26:
In file included from ../tensorflow/third_party/xla/xla/tsl/framework/allocator.h:26:
In file included from ../tensorflow/third_party/xla/xla/tsl/framework/numeric_types.h:22:
In file included from ../tensorflow/third_party/xla/xla/tsl/platform/types.h:22:
../tensorflow/third_party/xla/third_party/tsl/tsl/platform/ml_dtypes.h:19:10: fatal error: 'ml_dtypes/include/float8.h' file not found
   19 | #include ""ml_dtypes/include/float8.h""  // from @ml_dtypes_py
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
1 error generated.
[Notice: `clang++  -g -Og -fno-omit-frame-pointer -std=c++11 -fsanitize=address -fno-sanitize-recover=all -fsanitize=float-divide-by-zero -fsanitize=float-cast-overflow -fno-sanitize=null -fno-sanitize=alignment -DSUSUWU_EXPERIMENTAL -DSUSUWU_DEFAULT_BRANCH=""trunk"" -I../tensorflow/ -I../tensorflow/third_party/xla/ -I../tensorflow/third_party/xla/third_party/tsl/ -I../tensorflow/third_party/xla/third_party/py/ -I/data/data/com.termux/files/home/../usr/include/eigen3 -Wall -Wno-unused-function -Wno-unused-function -Wextra -Wno-unused-parameter -Wno-ignored-qualifiers -Wpedantic ./cxx/ClassTensorFlowCns.hxx` failed, will not enable `CXXFLAGS=""${CXXFLAGS} -std=c++17 -DUSE_TENSORFLOW_CNS""` (skipped). If `libtensorflow` is installed, insert `-std=c++17 -DUSE_TENSORFLOW_CNS` into `./build.sh:FLAGS_USER`. To troubleshoot, use `cd ../tensorflow/ && ./configure`]
[Notice: SUSUWU_BUILD_CTAGS(): Was called with less than 2 params; will default to `SUSUWU_BUILD_CTAGS -R --exclude=.git/ --exclude=*.html --exclude=compile_commands.json .`.]
[Success: SUSUWU_BUILD_EXECUTABLE(): Reused ""./bin/Susuwu.out"" (5964408 bytes).]
```

",2
clang: error: unknown argument: '-fno-canonical-system-headers',"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.14 2.16 2.17

### Custom code

Yes

### OS platform and distribution

rhel9 x86_64 / haswell

### Mobile device

_No response_

### Python version

3.11

### Bazel version

6.5.0

### GCC/compiler version

10.5 11.5 and clang@16.06 clang@17.06

### CUDA/cuDNN version

12.3 12.4

### GPU model and memory

_No response_

### Current behavior?

trying build TF using Spack. Also, tried building with clang but getting the same error.

### Standalone code to reproduce the issue

```shell
spack install -j32 py-tensorflow@2.16.2+cuda cuda_arch=61 ^cuda@12.3.2
```

### Relevant log output

```shell
==> py-tensorflow: Executing phase: 'configure'
==> [2025-05-08-16:24:05.555774] './configure'
You have bazel 6.5.0- (@non-git) installed.
Found CUDA 12.3 in:
    /mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/cuda-12.3.2-olr6qk5ain5oieyb4jksgpzb63b3yqdy/targets/x86_64-linux/lib
    /mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/cuda-12.3.2-olr6qk5ain5oieyb4jksgpzb63b3yqdy/targets/x86_64-linux/include
Found cuDNN 8 in:
    /mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/cudnn-8.9.7.29-12-odtmtnstxtmuaznc5ldwg2dxue3nnh3i/lib
    /mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/cudnn-8.9.7.29-12-odtmtnstxtmuaznc5ldwg2dxue3nnh3i/include


Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v1          	# Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=nogcp       	# Disable GCP support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished
==> [2025-05-08-16:24:06.181298] FILTER FILE: configure.py [replacing ""if workspace_has_any_android_rule\(\)""]
==> [2025-05-08-16:24:06.184889] FILTER FILE: .tf_configure.bazelrc [replacing ""--define with_gcp_support=true""]
==> [2025-05-08-16:24:06.186852] Find (max depth = None): ['/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/cuda-12.3.2-olr6qk5ain5oieyb4jksgpzb63b3yqdy/lib64'] ['libcudart.so']
==> [2025-05-08-16:24:06.187139] Find complete: ['/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/cuda-12.3.2-olr6qk5ain5oieyb4jksgpzb63b3yqdy/lib64'] ['libcudart.so']
==> [2025-05-08-16:24:06.187760] Find (max depth = None): ['/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/cudnn-8.9.7.29-12-odtmtnstxtmuaznc5ldwg2dxue3nnh3i/lib'] ['libcudnn.so']
==> [2025-05-08-16:24:06.187903] Find complete: ['/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/cudnn-8.9.7.29-12-odtmtnstxtmuaznc5ldwg2dxue3nnh3i/lib'] ['libcudnn.so']
==> [2025-05-08-16:24:06.188420] FILTER FILE: .tf_configure.bazelrc [replacing ""build:opt --copt=-march=native""]
==> [2025-05-08-16:24:06.189215] FILTER FILE: .tf_configure.bazelrc [replacing ""build:opt --host_copt=-march=native""]
==> py-tensorflow: Executing phase: 'build'
==> [2025-05-08-16:24:06.224403] '/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/bazel-6.5.0-ies2icgygngut5kqhleockdraoduue4r/bin/bazel' '--nohome_rc' '--nosystem_rc' '--output_user_root=/cache/spackou1sj9n7' 'build' '--color=no' '--jobs=32' '--config=opt' '--verbose_failures' '--config=dynamic_kernels' '--config=cuda' '--config=noaws' '--config=nogcp' '--config=nohdfs' '--config=nonccl' '--define=with_numa_support=False' '--config=v2' '//tensorflow/tools/pip_package:build_pip_package'
$TEST_TMPDIR defined: output root default is '/cache/spackou1sj9n7' and max_idle_secs default is '15'.
Extracting Bazel installation...
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [dynamic_kernels, cuda, v2]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Reading 'startup' options from /mnt/local/cache/sbulut/spack-stage/spack-stage-py-tensorflow-2.16.2-ozxh37iccgmfd356bav4j4rjxwfrce5i/spack-src/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=0 --terminal_columns=80
INFO: Reading rc options for 'build' from /mnt/local/cache/sbulut/spack-stage/spack-stage-py-tensorflow-2.16.2-ozxh37iccgmfd356bav4j4rjxwfrce5i/spack-src/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /mnt/local/cache/sbulut/spack-stage/spack-stage-py-tensorflow-2.16.2-ozxh37iccgmfd356bav4j4rjxwfrce5i/spack-src/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /mnt/local/cache/sbulut/spack-stage/spack-stage-py-tensorflow-2.16.2-ozxh37iccgmfd356bav4j4rjxwfrce5i/spack-src/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/python-venv-1.0-72xw7sn2bvqtdjc5bxnth65yobe5mlgv/bin/python3 --action_env PYTHON_LIB_PATH=/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-tensorflow-2.16.2-ozxh37iccgmfd356bav4j4rjxwfrce5i/lib/python3.11/site-packages --python_path=/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/python-venv-1.0-72xw7sn2bvqtdjc5bxnth65yobe5mlgv/bin/python3 --define=with_xla_support=true --action_env TF_CUDA_VERSION=12.3 --action_env TF_CUDNN_VERSION=8 --action_env TF_CUDA_PATHS=/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/cuda-12.3.2-olr6qk5ain5oieyb4jksgpzb63b3yqdy,/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/cudnn-8.9.7.29-12-odtmtnstxtmuaznc5ldwg2dxue3nnh3i --action_env CUDA_TOOLKIT_PATH=/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/cuda-12.3.2-olr6qk5ain5oieyb4jksgpzb63b3yqdy --action_env CUDNN_INSTALL_PATH=/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/cudnn-8.9.7.29-12-odtmtnstxtmuaznc5ldwg2dxue3nnh3i --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1 --action_env LD_LIBRARY_PATH=/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/cudnn-8.9.7.29-12-odtmtnstxtmuaznc5ldwg2dxue3nnh3i/lib:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/hdf5-1.14.6-3kpgk2uobjnccpbvdgssfwbeto6r6ac7/lib --action_env GCC_HOST_COMPILER_PATH=/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/compiler-wrapper-1.0-ojy52stxi3tfb6th7vndxcn5fjukh7mj/libexec/spack/cc --config=cuda --distinct_host_configuration=false --action_env PYTHONPATH=/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-ml-dtypes-0.3.1-kcodfk4rjntycndsp7zcmuxjlo2wsfjh/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-gast-0.5.4-cdyjrjmhe4suz5adsomfmqql62ivce6j/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-flatbuffers-24.3.25-jmyf5qii6t4ft6ydzehxf267cl4x42da/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-pybind11-2.13.6-3utr5ymowzezrhn7j4d37izsudv6jmrl/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-wrapt-1.15.0-hfjpdeyexhk5hthisljckngtbq66netz/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-urllib3-2.1.0-l22gty64c23nrfm3mlletuns2jggxj54/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-requests-2.32.3-jti6sqxmx73lkfg3ab6crrerbsl5js5h/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-astunparse-1.6.3-gosrtirjawjygyjl5kpqmbvnijglzzyg/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-typing-extensions-4.13.2-rlur6mmmalr5v3aebqrv6s5fdr77nrdx/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-grpcio-1.62.2-xjayeqclhxd2uk2ab2fwfytvi4gkqo6s/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-markdown-3.4.1-ebyoct4zdnypwz3wvpyox6hhkgenz6wi/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-tensorboard-data-server-0.7.0-elaoarv3i2o4ev6hunws2vvy6aacgxjt/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-markupsafe-2.1.3-yz6eaqc53qvnlu7g7i5nvejbtdxpfadf/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-werkzeug-3.1.3-6lkbcww2tl2tmvkuop2lfx4xlfw5iape/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-tensorboard-2.16.2-7uwonqm7krmcinbtdk7hbn3in5y6uzvw/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-charset-normalizer-3.3.0-7gou7pggsfnb7t5cz4alugklomgjpbk2/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-idna-3.4-4qs2qo26iewa5o23p45ovpg25s4d6hwd/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-libclang-16.0.0-jtrmsoyj7zmxyw3c2p2xeq5ss2nywt34/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-termcolor-1.1.0-coym2bgeppbtdighuo7garpyoyfdnrjr/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-certifi-2023.7.22-agf4cmce7ioqyl237zmkcxdq2hpu6jn2/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-opt-einsum-3.3.0-3iu2j2m3kz5tlomtwoxwvkremokzy5xc/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-protobuf-4.25.3-twel24lekycw3d4xihnrpqssdzvohnhk/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-absl-py-1.4.0-sli6ljxtvoh7pp6kbzx44w6ix7v4x336/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-packaging-25.0-e2qlowj6bfy2mmiwd3b6yqwxlu7wa37c/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-six-1.17.0-nvvojh6cvt7fqz5mz44ah2bv7bsskj24/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-google-pasta-0.2.0-e3dbxprzz5ln666evd53qny54k5kivst/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-cython-0.29.36-fxtpriyk5hhxcjmg46fsv4mtkljjbpda/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-numpy-1.25.2-bnmtjwwqlglo4voeogjhkjzlqddgzn4h/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-h5py-3.12.1-xiqslww3clb3vcvq3j5q6bzbojekxqb3/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-pip-25.0.1-ts4ddba3qcoy4c5fp2bm6edi66hmtu4y/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-setuptools-57.4.0-26ktizaht36bcbldhcyonetgsa2g2op6/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-wheel-0.45.1-jqf4agtkdevrobld6gapd6fzkosvtebv/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-coverage-7.2.6-doz6e5fhkufjjt4yiqkyxs2wtbfc3v2m/lib/python3.11/site-packages:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/python-venv-1.0-72xw7sn2bvqtdjc5bxnth65yobe5mlgv/lib/python3.11/site-packages --action_env LD_LIBRARY_PATH=/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/cuda-12.3.2-olr6qk5ain5oieyb4jksgpzb63b3yqdy/lib64:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/cudnn-8.9.7.29-12-odtmtnstxtmuaznc5ldwg2dxue3nnh3i/lib
INFO: Found applicable config definition build:short_logs in file /mnt/local/cache/sbulut/spack-stage/spack-stage-py-tensorflow-2.16.2-ozxh37iccgmfd356bav4j4rjxwfrce5i/spack-src/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /mnt/local/cache/sbulut/spack-stage/spack-stage-py-tensorflow-2.16.2-ozxh37iccgmfd356bav4j4rjxwfrce5i/spack-src/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda in file /mnt/local/cache/sbulut/spack-stage/spack-stage-py-tensorflow-2.16.2-ozxh37iccgmfd356bav4j4rjxwfrce5i/spack-src/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:opt in file /mnt/local/cache/sbulut/spack-stage/spack-stage-py-tensorflow-2.16.2-ozxh37iccgmfd356bav4j4rjxwfrce5i/spack-src/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare
INFO: Found applicable config definition build:dynamic_kernels in file /mnt/local/cache/sbulut/spack-stage/spack-stage-py-tensorflow-2.16.2-ozxh37iccgmfd356bav4j4rjxwfrce5i/spack-src/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Found applicable config definition build:cuda in file /mnt/local/cache/sbulut/spack-stage/spack-stage-py-tensorflow-2.16.2-ozxh37iccgmfd356bav4j4rjxwfrce5i/spack-src/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:noaws in file /mnt/local/cache/sbulut/spack-stage/spack-stage-py-tensorflow-2.16.2-ozxh37iccgmfd356bav4j4rjxwfrce5i/spack-src/.bazelrc: --define=no_aws_support=true
INFO: Found applicable config definition build:nogcp in file /mnt/local/cache/sbulut/spack-stage/spack-stage-py-tensorflow-2.16.2-ozxh37iccgmfd356bav4j4rjxwfrce5i/spack-src/.bazelrc: --define=no_gcp_support=true
INFO: Found applicable config definition build:nohdfs in file /mnt/local/cache/sbulut/spack-stage/spack-stage-py-tensorflow-2.16.2-ozxh37iccgmfd356bav4j4rjxwfrce5i/spack-src/.bazelrc: --define=no_hdfs_support=true
INFO: Found applicable config definition build:nonccl in file /mnt/local/cache/sbulut/spack-stage/spack-stage-py-tensorflow-2.16.2-ozxh37iccgmfd356bav4j4rjxwfrce5i/spack-src/.bazelrc: --define=no_nccl_support=true
INFO: Found applicable config definition build:v2 in file /mnt/local/cache/sbulut/spack-stage/spack-stage-py-tensorflow-2.16.2-ozxh37iccgmfd356bav4j4rjxwfrce5i/spack-src/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:linux in file /mnt/local/cache/sbulut/spack-stage/spack-stage-py-tensorflow-2.16.2-ozxh37iccgmfd356bav4j4rjxwfrce5i/spack-src/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /mnt/local/cache/sbulut/spack-stage/spack-stage-py-tensorflow-2.16.2-ozxh37iccgmfd356bav4j4rjxwfrce5i/spack-src/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
Loading: 
Loading: 
Loading: 
Loading: 
Loading: 
Loading: 
Loading: 
Loading: 
Loading: 
Loading: 
WARNING: The following configs were expanded more than once: [dynamic_kernels, cuda, v2]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
Loading: 
Loading: 1 packages loaded
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (2 packages loaded, 0 targets configured)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (65 packages loaded, 17 targets configured)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (268 packages loaded, 4215 targets configured)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (282 packages loaded, 4240 targets configured)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (283 packages loaded, 4243 targets configured)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (380 packages loaded, 5057 targets configured)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (435 packages loaded, 10514 targets configured)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (615 packages loaded, 32369 targets configured)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (672 packages loaded, 41237 targets configured)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (679 packages loaded, 41746 targets configured)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (679 packages loaded, 41746 targets configured)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (686 packages loaded, 47850 targets configured)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (701 packages loaded, 49136 targets configured)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (705 packages loaded, 50973 targets configured)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (705 packages loaded, 50973 targets configured)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (708 packages loaded, 51474 targets configured)
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (709 packages loaded, 51570 targets configured).
INFO: Found 1 target...
[0 / 892] [Prepa] Expanding template tensorflow/lite/tools/visualize ... (27 actions, 10 running)
ERROR: /mnt/local/cache/spackou1sj9n7/c949fe40c57eb2a81e4d05add6b26733/external/com_google_absl/absl/base/BUILD.bazel:53:11: Compiling absl/base/log_severity.cc [for tool] failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command (from target @com_google_absl//absl/base:log_severity) 
  (cd /mnt/local/cache/spackou1sj9n7/c949fe40c57eb2a81e4d05add6b26733/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/cudnn-8.9.7.29-12-odtmtnstxtmuaznc5ldwg2dxue3nnh3i/lib:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/hdf5-1.14.6-3kpgk2uobjnccpbvdgssfwbeto6r6ac7/lib \
    PATH=/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/compiler-wrapper-1.0-ojy52stxi3tfb6th7vndxcn5fjukh7mj/libexec/spack/clang:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/compiler-wrapper-1.0-ojy52stxi3tfb6th7vndxcn5fjukh7mj/libexec/spack/case-insensitive:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/compiler-wrapper-1.0-ojy52stxi3tfb6th7vndxcn5fjukh7mj/libexec/spack:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/compiler-wrapper-1.0-ojy52stxi3tfb6th7vndxcn5fjukh7mj/libexec/spack/clang:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/compiler-wrapper-1.0-ojy52stxi3tfb6th7vndxcn5fjukh7mj/libexec/spack/case-insensitive:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/compiler-wrapper-1.0-ojy52stxi3tfb6th7vndxcn5fjukh7mj/libexec/spack:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/bazel-6.5.0-ies2icgygngut5kqhleockdraoduue4r/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/llvm-16.0.6-xxcwda3mdgd5bqdbhz2f5dfsbagk526e/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/patchelf-0.17.2-urobsm3fhvps5ig3ere4wznbnts4piz3/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-pip-25.0.1-ts4ddba3qcoy4c5fp2bm6edi66hmtu4y/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-tensorboard-2.16.2-7uwonqm7krmcinbtdk7hbn3in5y6uzvw/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/zip-3.0-6kvpuekl4eo6kmmqwcb34xpxqlsh2kvj/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/cuda-12.3.2-olr6qk5ain5oieyb4jksgpzb63b3yqdy/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/swig-4.1.1-yyzgyywh5lccsj2lr5ukckdatix4efqz/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/hdf5-1.14.6-3kpgk2uobjnccpbvdgssfwbeto6r6ac7/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-charset-normalizer-3.3.0-7gou7pggsfnb7t5cz4alugklomgjpbk2/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-markdown-3.4.1-ebyoct4zdnypwz3wvpyox6hhkgenz6wi/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-numpy-1.25.2-bnmtjwwqlglo4voeogjhkjzlqddgzn4h/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/curl-8.11.1-iixywleeuswzonjtz52mfl2tjehufdnl/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/unzip-6.0-xrwpgcc2wsk6rzozjbrgwpweiry2tstn/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/pkgconf-2.3.0-ewmupmdrurlf3dfs4wyfvmfrssuaw6rh/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-coverage-7.2.6-doz6e5fhkufjjt4yiqkyxs2wtbfc3v2m/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-cython-0.29.36-fxtpriyk5hhxcjmg46fsv4mtkljjbpda/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-wheel-0.45.1-jqf4agtkdevrobld6gapd6fzkosvtebv/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-pybind11-2.13.6-3utr5ymowzezrhn7j4d37izsudv6jmrl/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-markdown-3.4.1-ebyoct4zdnypwz3wvpyox6hhkgenz6wi/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-tensorboard-2.16.2-7uwonqm7krmcinbtdk7hbn3in5y6uzvw/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-charset-normalizer-3.3.0-7gou7pggsfnb7t5cz4alugklomgjpbk2/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-cython-0.29.36-fxtpriyk5hhxcjmg46fsv4mtkljjbpda/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-numpy-1.25.2-bnmtjwwqlglo4voeogjhkjzlqddgzn4h/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-pip-25.0.1-ts4ddba3qcoy4c5fp2bm6edi66hmtu4y/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-wheel-0.45.1-jqf4agtkdevrobld6gapd6fzkosvtebv/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-coverage-7.2.6-doz6e5fhkufjjt4yiqkyxs2wtbfc3v2m/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/python-venv-1.0-72xw7sn2bvqtdjc5bxnth65yobe5mlgv/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/python-3.11.11-vd4myutyooo63sszk722do7rhm7djqve/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/tar-1.35-adezgvdgvdi7ikzjqwql7fevftrrgh42/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/bzip2-1.0.8-k3qsr5illwshgm6dtvqysvv25tlll422/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/pigz-2.8-lfguiakvmqrlidunieegzdnxhq5z2xue/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/xz-5.6.3-d66spib5syfri7p7xrmqavm5fexl3vcq/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/zstd-1.5.7-t7q5uokmgurcqsie6u36zs5jtvdoapdn/bin:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/openjdk-11.0.23_9-xha5rqin3tazuvtkk4yiktcy2ln4tkrr/bin:/mnt/local/sbulut/spack_dev_2025_may_05/var/spack/environments/test_qgis_env/.spack-env/view/bin:/mnt/local/sbulut/spack_dev_2025_may_05/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/bin:/opt/sge/bin/lx-amd64:/var/lib/snapd/snap/bin:/disk/home/sbulut/bin:/disk/home/sbulut/.local/bin \
    PWD=/proc/self/cwd \
    SPACK_CC=/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/llvm-16.0.6-xxcwda3mdgd5bqdbhz2f5dfsbagk526e/bin/clang \
    SPACK_CC_LINKER_ARG=-Wl, \
    SPACK_CC_RPATH_ARG=-Wl,-rpath, \
    SPACK_COMPILER_FLAGS_KEEP='' \
    SPACK_COMPILER_FLAGS_REPLACE='-Werror-|-Wno-error= -Werror|-Wno-error' \
    SPACK_COMPILER_IMPLICIT_RPATHS=/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/llvm-16.0.6-xxcwda3mdgd5bqdbhz2f5dfsbagk526e/lib/x86_64-unknown-linux-gnu \
    SPACK_COMPILER_WRAPPER_PATH=/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/compiler-wrapper-1.0-ojy52stxi3tfb6th7vndxcn5fjukh7mj/libexec/spack/clang:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/compiler-wrapper-1.0-ojy52stxi3tfb6th7vndxcn5fjukh7mj/libexec/spack/case-insensitive:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/compiler-wrapper-1.0-ojy52stxi3tfb6th7vndxcn5fjukh7mj/libexec/spack:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/compiler-wrapper-1.0-ojy52stxi3tfb6th7vndxcn5fjukh7mj/libexec/spack/clang:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/compiler-wrapper-1.0-ojy52stxi3tfb6th7vndxcn5fjukh7mj/libexec/spack/case-insensitive:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/compiler-wrapper-1.0-ojy52stxi3tfb6th7vndxcn5fjukh7mj/libexec/spack \
    SPACK_CXX=/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/llvm-16.0.6-xxcwda3mdgd5bqdbhz2f5dfsbagk526e/bin/clang++ \
    SPACK_CXX_LINKER_ARG=-Wl, \
    SPACK_CXX_RPATH_ARG=-Wl,-rpath, \
    SPACK_DEBUG_LOG_DIR=/mnt/local/sbulut/spack_dev_2025_may_05/share \
    SPACK_DEBUG_LOG_ID=py-tensorflow-ozxh37i \
    SPACK_DISABLE_NEW_DTAGS=--disable-new-dtags \
    SPACK_DTAGS_TO_ADD=--disable-new-dtags \
    SPACK_DTAGS_TO_STRIP=--enable-new-dtags \
    SPACK_ENABLE_NEW_DTAGS=--enable-new-dtags \
    SPACK_INCLUDE_DIRS='' \
    SPACK_LINK_DIRS='' \
    SPACK_MANAGED_DIRS='""/cache/sbulut/spack-stage/""*|""/mnt/local/cache/sbulut/spack-stage/""*|""/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/""*' \
    SPACK_PYTHON=/usr/bin/python3 \
    SPACK_ROOT=/mnt/local/sbulut/spack_dev_2025_may_05 \
    SPACK_RPATH_DIRS='' \
    SPACK_SHORT_SPEC='py-tensorflow@2.16.2~android~aws~computecpp+cuda+dynamic_kernels~gcp~gdr~hdfs~ios~jemalloc~mkl~monolithic~mpi~nccl~ngraph~numa~opencl~rocm~tensorrt~verbs+xla build_system=generic cuda_arch:=61 patches:=2017b3e,e061875 arch=linux-rhel9-haswell/ozxh37i' \
    SPACK_STORE_INCLUDE_DIRS=/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/zlib-ng-2.2.4-kwkard3rsgpcafgqghpfw6ir4kj4dwih/include:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/xz-5.6.3-d66spib5syfri7p7xrmqavm5fexl3vcq/include:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/libiconv-1.18-mlpyi4xzq2rj3db62our4lufvhi2f54a/include:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/libxml2-2.13.5-vz4eurm6llrnwbpgzdpbesvxts6k4sfy/include/libxml2:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/libxml2-2.13.5-vz4eurm6llrnwbpgzdpbesvxts6k4sfy/include:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/cuda-12.3.2-olr6qk5ain5oieyb4jksgpzb63b3yqdy/include:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/cudnn-8.9.7.29-12-odtmtnstxtmuaznc5ldwg2dxue3nnh3i/include \
    SPACK_STORE_LINK_DIRS=/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/gcc-runtime-11.5.0-v3xbknhojgxceq6n2q4d5shptsw4gmjf/lib:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/zlib-ng-2.2.4-kwkard3rsgpcafgqghpfw6ir4kj4dwih/lib:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/xz-5.6.3-d66spib5syfri7p7xrmqavm5fexl3vcq/lib:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/libiconv-1.18-mlpyi4xzq2rj3db62our4lufvhi2f54a/lib:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/libxml2-2.13.5-vz4eurm6llrnwbpgzdpbesvxts6k4sfy/lib:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/cuda-12.3.2-olr6qk5ain5oieyb4jksgpzb63b3yqdy/lib64:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/cudnn-8.9.7.29-12-odtmtnstxtmuaznc5ldwg2dxue3nnh3i/lib \
    SPACK_STORE_RPATH_DIRS=/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-tensorflow-2.16.2-ozxh37iccgmfd356bav4j4rjxwfrce5i/lib:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/py-tensorflow-2.16.2-ozxh37iccgmfd356bav4j4rjxwfrce5i/lib64:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/gcc-runtime-11.5.0-v3xbknhojgxceq6n2q4d5shptsw4gmjf/lib:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/zlib-ng-2.2.4-kwkard3rsgpcafgqghpfw6ir4kj4dwih/lib:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/xz-5.6.3-d66spib5syfri7p7xrmqavm5fexl3vcq/lib:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/libiconv-1.18-mlpyi4xzq2rj3db62our4lufvhi2f54a/lib:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/libxml2-2.13.5-vz4eurm6llrnwbpgzdpbesvxts6k4sfy/lib:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/cuda-12.3.2-olr6qk5ain5oieyb4jksgpzb63b3yqdy/lib64:/mnt/local/sbulut/spack_dev_2025_may_05/opt/spack/linux-haswell/cudnn-8.9.7.29-12-odtmtnstxtmuaznc5ldwg2dxue3nnh3i/lib \
    SPACK_SYSTEM_DIRS='""/""|""//""|""/bin""|""/bin/""|""/bin64""|""/bin64/""|""/include""|""/include/""|""/lib""|""/lib/""|""/lib64""|""/lib64/""|""/usr""|""/usr/""|""/usr/bin""|""/usr/bin/""|""/usr/bin64""|""/usr/bin64/""|""/usr/include""|""/usr/include/""|""/usr/lib""|""/usr/lib/""|""/usr/lib64""|""/usr/lib64/""|""/usr/local""|""/usr/local/""|""/usr/local/bin""|""/usr/local/bin/""|""/usr/local/bin64""|""/usr/local/bin64/""|""/usr/local/include""|""/usr/local/include/""|""/usr/local/lib""|""/usr/local/lib/""|""/usr/local/lib64""|""/usr/local/lib64/""' \
    SPACK_TARGET_ARGS_CC='-march=haswell -mtune=haswell' \
    SPACK_TARGET_ARGS_CXX='-march=haswell -mtune=haswell' \
    SPACK_USER_CACHE_PATH=/mnt/local/sbulut/spack_dev_2025_may_05/dot.spack.user_cache \
    SPACK_USER_CONFIG_PATH=/mnt/local/sbulut/spack_dev_2025_may_05/dot.spack.user_config \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt-exec-50AE0418/bin/external/com_google_absl/absl/base/_objs/log_severity/log_severity.d '-frandom-seed=bazel-out/k8-opt-exec-50AE0418/bin/external/com_google_absl/absl/base/_objs/log_severity/log_severity.o' '-DBAZEL_CURRENT_REPOSITORY=""com_google_absl""' -iquote external/com_google_absl -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/com_google_absl -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 -w -Wno-sign-compare -g0 '-std=c++17' -Wall -Wextra -Wcast-qual -Wconversion-null -Wformat-security -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wundef -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwrite-strings -DNOMINMAX -c external/com_google_absl/absl/base/log_severity.cc -o bazel-out/k8-opt-exec-50AE0418/bin/external/com_google_absl/absl/base/_objs/log_severity/log_severity.o)
# Configuration: 5a89b2169535a29e2da36a546379f8638973b916884150eca49862cac7ff0f11
# Execution platform: @local_execution_config_platform//:platform
clang: error: unknown argument: '-fno-canonical-system-headers'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 56.767s, Critical Path: 0.50s
INFO: 180 processes: 137 internal, 43 local.
FAILED: Build did NOT complete successfully
```",1
`tf.truncatediv` doesn't work on complex64 and complex128,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v2.19.0-rc0-6-ge36baa30292 2.19.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

NVIDIA H100 - 80GB

### Current behavior?

Similar to #92873 . The doc of tf.math.reciprocal illustrates that the input must be one of the following types:bfloat16, half, float32, float64, uint8, int8, uint16, int16, int32, uint32, uint64, int64, complex64, complex128. While it doesn't work on complex64 and complex128. Other dtype works well. XLA_JIT the same.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

a = tf.constant(1+2j,dtype=tf.complex128)
b = tf.constant(1+2j,dtype=tf.complex128)
c = tf.truncatediv(a,b)
print(c)
```

### Relevant log output

```shell
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
Cell In[182], line 5
      3 a = tf.constant(1+2j,dtype=tf.complex128)
      4 b = tf.constant(1+2j,dtype=tf.complex128)
----> 5 c = tf.truncatediv(a,b)
      6 print(c)

File ~/anaconda3/envs/cotfuzz/lib/python3.11/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142, in weak_tensor_binary_op_wrapper.<locals>.wrapper(*args, **kwargs)
    140 def wrapper(*args, **kwargs):
    141   if not ops.is_auto_dtype_conversion_enabled():
--> 142     return op(*args, **kwargs)
    143   bound_arguments = signature.bind(*args, **kwargs)
    144   bound_arguments.apply_defaults()

File ~/anaconda3/envs/cotfuzz/lib/python3.11/site-packages/tensorflow/python/ops/gen_math_ops.py:12702, in truncate_div(x, y, name)
  12700   return _result
  12701 except _core._NotOkStatusException as e:
> 12702   _ops.raise_from_not_ok_status(e, name)
  12703 except _core._FallbackException:
  12704   pass

File ~/anaconda3/envs/cotfuzz/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:6006, in raise_from_not_ok_status(e, name)
   6004 def raise_from_not_ok_status(e, name) -> NoReturn:
   6005   e.message += ("" name: "" + str(name if name is not None else """"))
...
  device='CPU'; T in [DT_UINT64]
  device='CPU'; T in [DT_UINT32]
  device='CPU'; T in [DT_UINT16]
  device='CPU'; T in [DT_UINT8]
 [Op:TruncateDiv] name:
```",1
"Compatibility Challenges of Transformer Models (MobileBERT, Mobile ViT) with TensorFlow Lite and GPU Delegation","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 22.04
- TensorFlow installed from (source or binary): source 
- TensorFlow version (or github SHA if from source): 2.18.0

**Model link**
https://www.kaggle.com/models/tensorflow/mobilebert
https://huggingface.co/qualcomm/Mobile_Vit


**Any other info / logs**

Hello TensorFlow team,

I'm a student currently researching TensorFlow internals and working on inference for transformer-based models such as MobileBERT and Mobile Vision Transformer (MViT) using TFLite.

While inference works correctly with the CPU (using the XNNPACK delegate), I've encountered GPU delegate issues that consistently occur with both models. I believe the problems stem from the following two core issues:

-  Batch size != 1 not supported

ERROR: TfLiteGpuDelegate Init: FromTensorConverter: Batch size != 1 is not supported.

Although the attention layers appear as single-head attention in Netron or similar tools, they internally behave like multi-head attention, where each head is processed as a batch element. Structurally, this likely results from optimization during model conversion (e.g., from TensorFlow or ONNX to TFLite).

As a result, although the model input has batch size 1, some internal layers effectively expand the batch dimension (e.g., to represent multiple heads). Since the GPU delegate strictly requires batch size == 1, it fails during initialization.


- Batch size mismatch (expected 1 but got N)

ERROR: TfLiteGpuDelegate Init: Batch size mismatch, expected 1 but got 2304

This error seems to stem from shape misinterpretation by the GPU delegate. In the attention block, layers like Transpose and Reshape rearrange tensor dimensions for attention computation. However, the delegate appears to treat the first dimension of these reshaped tensors as the batch size, which leads to incorrect assumptions and failures during GPU delegate preparation.

**Questions**

1. Is my interpretation of these two issues correct? Do multi-head attention implementations and shape rearrangements indeed confuse the GPU delegate's batch handling?
2. I've not seen successful TFLite GPU inference with transformer-based models—even with MobileBERT, which is relatively TFLite-friendly. Aside from manually modifying the model structure or conversion logic, is there any recommended way to make these models GPU-compatible with TFLite?
3. More broadly, even non-generative transformer models like MobileBERT and Mobile ViT struggle with GPU inference in TFLite. This makes me wonder if decoder-style generative models (like GPT) face even greater challenges—not only due to GPU delegate limitations, but also due to the fundamental mismatch between their dynamic nature and TensorFlow’s static graph optimization paradigm. I’ve noticed some recent development under tensorflow/lite/experimental/genai for generative inference. Is there a roadmap or plan to address these issues and better support such models, especially for GPU acceleration?

Thank you very much for your time and for maintaining this great framework.

Best regards, @easyhardhoon

",1
"Compiling 2.19.0 through cmake on Windows, there are too many exported DLL symbols and an increase in disk size","### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.19.0

### Custom code

Yes

### OS platform and distribution

windows 10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

msvc2019

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Compiling 2.19.0 through cmake on Windows, there are 9306 exported symbols in the DLL, and the disk size of the constructed DLL is 3mb larger than 2.18.0. I think this is abnormal. Is there any solution

### Standalone code to reproduce the issue

```shell
cmake ../lite -DTFLITE_ENABLE_XNNPACK=OFF -DTFLITE_ENABLE_INSTALL=OFF  -DBUILD_SHARED_LIBS=OFF -DCMAKE_INSTALL_PREFIX=""install""
I have defined a TFLITE_SHARED_LIB for TensorFlow Lite to compile DLLs instead of BUILD_SHARED_LIBS, as using BUILD_SHARED_LIBS directly will cause other lib packages to fail to compile.
```

### Relevant log output

```shell
Here are some export functions for exporting tables. There are too many, so please extract a portion of them.
Microsoft (R) COFF/PE Dumper Version 14.29.30154.0
Copyright (C) Microsoft Corporation.  All rights reserved.


Dump of file E:\kweb\tensorflow\tensorflow\build-re3\Release\tensorflow-lite.dll

File Type: DLL

  Section contains the following exports for tensorflow-lite.dll

    00000000 characteristics
    FFFFFFFF time date stamp
        0.00 version
           1 ordinal base
        9306 number of functions
        9306 number of names

    ordinal hint RVA      name

          1    0 00627EE8 ?$TSS0@?1??manage_caching_sizes@internal@Eigen@@YAXW4Action@3@PEA_J11@Z@4HA
          2    1 00627DC0 ?$TSS0@?1??manage_caching_sizes@internal@EigenForTFLite@@YAXW4Action@3@PEA_J11@Z@4HA
          3    2 0004F540 ?<lambda_invoker_cdecl>@_Closure_wrapper_5e580802_10@@CAMM@Z
          4    3 0004F540 ?<lambda_invoker_cdecl>@_Closure_wrapper_5e580802_11@@CAMM@Z
          5    4 0004F540 ?<lambda_invoker_cdecl>@_Closure_wrapper_5e580802_12@@CAMM@Z
          6    5 0004F540 ?<lambda_invoker_cdecl>@_Closure_wrapper_5e580802_13@@CAMM@Z
          7    6 0004F570 ?<lambda_invoker_cdecl>@_Closure_wrapper_5e580802_14@@CANN@Z
          8    7 0004F580 ?<lambda_invoker_cdecl>@_Closure_wrapper_5e580802_15@@CANN@Z
          9    8 0004F570 ?<lambda_invoker_cdecl>@_Closure_wrapper_5e580802_16@@CANN@Z
         10    9 0004F580 ?<lambda_invoker_cdecl>@_Closure_wrapper_5e580802_17@@CANN@Z
         11    A 0004F5B0 ?<lambda_invoker_cdecl>@_Closure_wrapper_5e580802_2@@CAMM@Z
         12    B 0004F5B0 ?<lambda_invoker_cdecl>@_Closure_wrapper_5e580802_3@@CAMM@Z
         13    C 0004F5B0 ?<lambda_invoker_cdecl>@_Closure_wrapper_5e580802_4@@CAMM@Z
         14    D 0004F5B0 ?<lambda_invoker_cdecl>@_Closure_wrapper_5e580802_5@@CAMM@Z
         15    E 0004F5B0 ?<lambda_invoker_cdecl>@_Closure_wrapper_5e580802_6@@CAMM@Z
         16    F 0004F5B0 ?<lambda_invoker_cdecl>@_Closure_wrapper_5e580802_7@@CAMM@Z
         17   10 0004F540 ?<lambda_invoker_cdecl>@_Closure_wrapper_5e580802_8@@CAMM@Z
         18   11 0004F540 ?<lambda_invoker_cdecl>@_Closure_wrapper_5e580802_9@@CAMM@Z
         19   12 00192540 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_10@@CAHHH@Z
         20   13 00192550 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_11@@CAHHH@Z
         21   14 00192570 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_12@@CAHHH@Z
         22   15 00192590 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_13@@CA_J_J0@Z
         23   16 001925A0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_14@@CA_J_J0@Z
         24   17 001925B0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_15@@CA_J_J0@Z
         25   18 001925C0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_16@@CA_J_J0@Z
         26   19 001925D0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_17@@CA_J_J0@Z
         27   1A 001925F0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_18@@CA_J_J0@Z
         28   1B 0004F6F0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_19@@CAEEE@Z
         29   1C 00192610 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_1@@CAMMM@Z
         30   1D 00192620 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_20@@CAEEE@Z
         31   1E 00192630 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_21@@CAEEE@Z
         32   1F 00192640 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_22@@CAEEE@Z
         33   20 00192650 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_23@@CAEEE@Z
         34   21 00192660 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_24@@CAEEE@Z
         35   22 0004F6F0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_25@@CACCC@Z
         36   23 00192670 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_26@@CACCC@Z
         37   24 00192680 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_27@@CACCC@Z
         38   25 00192690 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_28@@CACCC@Z
         39   26 00192650 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_29@@CACCC@Z
         40   27 001926A0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_2@@CAMMM@Z
         41   28 00192660 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_30@@CACCC@Z
         42   29 0004F6F0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_31@@CAFFF@Z
         43   2A 001926B0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_32@@CAFFF@Z
         44   2B 001926C0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_33@@CAFFF@Z
         45   2C 001926D0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_34@@CAFFF@Z
         46   2D 001926E0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_35@@CAFFF@Z
         47   2E 00192700 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_36@@CAFFF@Z
         48   2F 00192720 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_37@@CA_N_N0@Z
         49   30 00192730 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_38@@CA_N_N0@Z
         50   31 00192630 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_39@@CA_N_N0@Z
         51   32 00192740 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_3@@CAMMM@Z
         52   33 00192640 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_40@@CA_N_N0@Z
         53   34 00192650 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_41@@CA_N_N0@Z
         54   35 00192660 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_42@@CA_N_N0@Z
         55   36 00192610 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_43@@CAMMM@Z
         56   37 001926A0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_44@@CAMMM@Z
         57   38 00192740 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_45@@CAMMM@Z
         58   39 00192750 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_46@@CAMMM@Z
         59   3A 00192760 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_47@@CAMMM@Z
         60   3B 00192790 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_48@@CAMMM@Z
         61   3C 0004F700 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_49@@CAHHH@Z
         62   3D 00192750 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_4@@CAMMM@Z
         63   3E 000A7D80 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_50@@CAHHH@Z
         64   3F 001927C0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_51@@CAHHH@Z
         65   40 00192540 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_52@@CAHHH@Z
         66   41 00192550 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_53@@CAHHH@Z
         67   42 00192570 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_54@@CAHHH@Z
         68   43 00192590 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_55@@CA_J_J0@Z
         69   44 001925A0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_56@@CA_J_J0@Z
         70   45 001925B0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_57@@CA_J_J0@Z
         71   46 001925C0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_58@@CA_J_J0@Z
         72   47 001925D0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_59@@CA_J_J0@Z
         73   48 00192760 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_5@@CAMMM@Z
         74   49 001925F0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_60@@CA_J_J0@Z
         75   4A 0004F6F0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_61@@CAEEE@Z
         76   4B 00192620 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_62@@CAEEE@Z
         77   4C 00192630 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_63@@CAEEE@Z
         78   4D 00192640 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_64@@CAEEE@Z
         79   4E 00192650 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_65@@CAEEE@Z
         80   4F 00192660 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_66@@CAEEE@Z
         81   50 0004F6F0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_67@@CACCC@Z
         82   51 00192670 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_68@@CACCC@Z
         83   52 00192680 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_69@@CACCC@Z
         84   53 00192790 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_6@@CAMMM@Z
         85   54 00192690 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_70@@CACCC@Z
         86   55 00192650 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_71@@CACCC@Z
         87   56 00192660 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_72@@CACCC@Z
         88   57 0004F6F0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_73@@CAFFF@Z
         89   58 001926B0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_74@@CAFFF@Z
         90   59 001926C0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_75@@CAFFF@Z
         91   5A 001926D0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_76@@CAFFF@Z
         92   5B 001926E0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_77@@CAFFF@Z
         93   5C 00192700 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_78@@CAFFF@Z
         94   5D 00192720 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_79@@CA_N_N0@Z
         95   5E 0004F700 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_7@@CAHHH@Z
         96   5F 00192730 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_80@@CA_N_N0@Z
         97   60 00192630 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_81@@CA_N_N0@Z
         98   61 00192640 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_82@@CA_N_N0@Z
         99   62 00192650 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_83@@CA_N_N0@Z
        100   63 00192660 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_84@@CA_N_N0@Z
        101   64 001927D0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_85@@CAHHE@Z
        102   65 001927E0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_86@@CAHHC@Z
        103   66 001927F0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_87@@CAHHF@Z
        104   67 00192610 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_88@@CAMMM@Z
        105   68 00192800 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_89@@CA_J_JH@Z
        106   69 000A7D80 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_8@@CAHHH@Z
        107   6A 00192590 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_90@@CA_J_J0@Z
        108   6B 001927C0 ?<lambda_invoker_cdecl>@_Closure_wrapper_6bca357c_9@@CAHHH@Z
        109   6C 000028D0 ?<lambda_invoker_cdecl>@_Closure_wrapper_e3bc3c41_1@@CAXPEAUTfLiteDelegate@@@Z
        110   6D 0002BB20 ?<lambda_invoker_cdecl>@_Closure_wrapper_f6402486_3@@CAXPEAX@Z
        111   6E 0002BB20 ?<lambda_invoker_cdecl>@_Closure_wrapper_f6402486_4@@CAXPEAX@Z
        112   6F 0002BB20 ?<lambda_invoker_cdecl>@_Closure_wrapper_f6402486_5@@CAXPEAX@Z
        113   70 0002AE20 ??$?0AEAPEBDVAsyncSignatureRunner@async@tflite@@$0A@@?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@VAsyncSignatureRunner@async@tflite@@@std@@QEAA@AEAPEBD$$QEAVAsyncSignatureRunner@async@tflite@@@Z
        114   71 00020E00 ??$?0AEAPEBDVSignatureRunner@impl@tflite@@$0A@@?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@VSignatureRunner@impl@tflite@@@std@@QEAA@AEAPEBD$$QEAVSignatureRunner@impl@tflite@@@Z
        115   72 001DF7D0 ??$?0PEAH$0A@@?$vector@HV?$allocator@H@std@@@std@@QEAA@PEAH0AEBV?$allocator@H@1@@Z
        116   73 0000FAC0 ??$?0PEBQEBD$0A@@?$vector@PEBDV?$allocator@PEBD@std@@@std@@QEAA@PEBQEBD0AEBV?$allocator@PEBD@1@@Z
        117   74 001FA750 ??$?0PEB_J$0A@@?$vector@_JV?$allocator@_J@std@@@std@@QEAA@PEB_J0AEBV?$allocator@_J@1@@Z
        118   75 00192810 ??$?4AEAP6ACCC@Z$0A@@?$function@$$A6ACCC@Z@std@@QEAAAEAV01@AEAP6ACCC@Z@Z
        119   76 001929B0 ??$?4AEAP6AEEE@Z$0A@@?$function@$$A6AEEE@Z@std@@QEAAAEAV01@AEAP6AEEE@Z@Z
        120   77 00192B50 ??$?4AEAP6AFFF@Z$0A@@?$function@$$A6AFFF@Z@std@@QEAAAEAV01@AEAP6AFFF@Z@Z
        121   78 00192CF0 ??$?4AEAP6AHHH@Z$0A@@?$function@$$A6AHHH@Z@std@@QEAAAEAV01@AEAP6AHHH@Z@Z
        122   79 00192E90 ??$?4AEAP6AMMM@Z$0A@@?$function@$$A6AMMM@Z@std@@QEAAAEAV01@AEAP6AMMM@Z@Z
        123   7A 00193030 ??$?4AEAP6A_J_J0@Z$0A@@?$function@$$A6A_J_J0@Z@std@@QEAAAEAV01@AEAP6A_J_J0@Z@Z
        124   7B 001931D0 ??$?4AEAP6A_N_N0@Z$0A@@?$function@$$A6A_N_N0@Z@std@@QEAAAEAV01@AEAP6A_N_N0@Z@Z
        125   7C 00254F80 ??$?6U?$char_traits@D@std@@@std@@YAAEAV?$basic_ostream@DU?$char_traits@D@std@@@0@AEAV10@PEBD@Z
        126   7D 000111B0 ??$?8$$A6APEBUTfLiteOperator@@PEAXPEBDH@Z@std@@YA_NAEBV?$function@$$A6APEBUTfLiteOperator@@PEAXPEBDH@Z@0@$$T@Z
        127   7E 000111B0 ??$?8$$A6APEBUTfLiteOperator@@PEAXW4TfLiteBuiltinOperator@@H@Z@std@@YA_NAEBV?$function@$$A6APEBUTfLiteOperator@@PEAXW4TfLiteBuiltinOperator@@H@Z@0@$$T@Z
        128   7F 000111B0 ??$?8$$A6APEBUTfLiteRegistration@@PEAXPEBDH@Z@std@@YA_NAEBV?$function@$$A6APEBUTfLiteRegistration@@PEAXPEBDH@Z@0@$$T@Z
        129   80 000111B0 ??$?8$$A6APEBUTfLiteRegistration@@PEAXW4TfLiteBuiltinOperator@@H@Z@std@@YA_NAEBV?$function@$$A6APEBUTfLiteRegistration@@PEAXW4TfLiteBuiltinOperator@@H@Z@0@$$T@Z
        130   81 000111B0 ??$?8$$A6APEBUTfLiteRegistration_V1@@PEAXPEBDH@Z@std@@YA_NAEBV?$function@$$A6APEBUTfLiteRegistration_V1@@PEAXPEBDH@Z@0@$$T@Z
        131   82 000111B0 ??$?8$$A6APEBUTfLiteRegistration_V1@@PEAXW4TfLiteBuiltinOperator@@H@Z@std@@YA_NAEBV?$function@$$A6APEBUTfLiteRegistration_V1@@PEAXW4TfLiteBuiltinOperator@@H@Z@0@$$T@Z
        132   83 000111B0 ??$?8$$A6APEBUTfLiteRegistration_V2@@PEAXPEBDH@Z@std@@YA_NAEBV?$function@$$A6APEBUTfLiteRegistration_V2@@PEAXPEBDH@Z@0@$$T@Z
        133   84 000111B0 ??$?8$$A6APEBUTfLiteRegistration_V2@@PEAXW4TfLiteBuiltinOperator@@H@Z@std@@YA_NAEBV?$function@$$A6APEBUTfLiteRegistration_V2@@PEAXW4TfLiteBuiltinOperator@@H@Z@0@$$T@Z
        134   85 000111B0 ??$?8$$A6APEBUTfLiteRegistration_V3@@PEAXPEBDH@Z@std@@YA_NAEBV?$function@$$A6APEBUTfLiteRegistration_V3@@PEAXPEBDH@Z@0@$$T@Z
        135   86 000111B0 ??$?8$$A6APEBUTfLiteRegistration_V3@@PEAXW4TfLiteBuiltinOperator@@H@Z@std@@YA_NAEBV?$function@$$A6APEBUTfLiteRegistration_V3@@PEAXW4TfLiteBuiltinOperator@@H@Z@0@$$T@Z
        136   87 000046C0 ??$?8$$BY0A@$$CBDU?$default_delete@$$BY0A@$$CBD@std@@@std@@YA_NAEBV?$unique_ptr@$$BY0A@$$CBDU?$default_delete@$$BY0A@$$CBD@std@@@0@$$T@Z
        137   88 0002BB30 ??$?8HVTensorTag@internal@model_builder@tflite@@@internal@model_builder@tflite@@YA_NAEBU?$StrongType@HVTensorTag@internal@model_builder@tflite@@@012@0@Z
        138   89 00020EC0 ??$?8UTfLiteDelegate@@P6AXPEAU0@@Z@std@@YA_NAEBV?$unique_ptr@UTfLiteDelegate@@P6AXPEAU1@@Z@0@$$T@Z
        139   8A 00020EC0 ??$?8UTfLiteIntArray@@P6AXPEAU0@@Z@std@@YA_NAEBV?$unique_ptr@UTfLiteIntArray@@P6AXPEAU1@@Z@0@$$T@Z
        140   8B 000046C0 ??$?8VExecutionTask@async@tflite@@U?$default_delete@VExecutionTask@async@tflite@@@std@@@std@@YA_NAEBV?$unique_ptr@VExecutionTask@async@tflite@@U?$default_delete@VExecutionTask@async@tflite@@@std@@@0@$$T@Z
        141   8C 000046C0 ??$?8VMemoryPlanner@tflite@@U?$default_delete@VMemoryPlanner@tflite@@@std@@@std@@YA_NAEBV?$unique_ptr@VMemoryPlanner@tflite@@U?$default_delete@VMemoryPlanner@tflite@@@std@@@0@$$T@Z
        142   8D 000046C0 ??$?8VProfiler@tflite@@U?$default_delete@VProfiler@tflite@@@std@@@std@@YA_NAEBV?$unique_ptr@VProfiler@tflite@@U?$default_delete@VProfiler@tflite@@@std@@@0@$$T@Z
        143   8E 000046C0 ??$?8VRootProfiler@profiling@tflite@@U?$default_delete@VRootProfiler@profiling@tflite@@@std@@@std@@YA_NAEBV?$unique_ptr@VRootProfiler@profiling@tflite@@U?$default_delete@VRootProfiler@profiling@tflite@@@std@@@0@$$T@Z
        144   8F 000046C0 ??$?8VTelemetryProfiler@telemetry@tflite@@U?$default_delete@VTelemetryProfiler@telemetry@tflite@@@std@@@std@@YA_NAEBV?$unique_ptr@VTelemetryProfiler@telemetry@tflite@@U?$default_delete@VTelemetryProfiler@telemetry@tflite@@@std@@@0@$$T@Z
        145   90 0004F5C0 ??$?DF$01$01@gemmlowp@@YA?AV?$FixedPoint@F$03@0@V?$FixedPoint@F$01@0@0@Z
        146   91 0004F5C0 ??$?DF$0A@$01@gemmlowp@@YA?AV?$FixedPoint@F$01@0@V?$FixedPoint@F$0A@@0@V10@@Z
        147   92 0004F5C0 ??$?DF$0A@$03@gemmlowp@@YA?AV?$FixedPoint@F$03@0@V?$FixedPoint@F$0A@@0@V10@@Z
        148   93 0004F5C0 ??$?DF$0A@$0A@@gemmlowp@@YA?AV?$FixedPoint@F$0A@@0@V10@0@Z
        149   94 0004F620 ??$?DH$01$01@gemmlowp@@YA?AV?$FixedPoint@H$03@0@V?$FixedPoint@H$01@0@0@Z
        150   95 0004F620 ??$?DH$02$02@gemmlowp@@YA?AV?$FixedPoint@H$05@0@V?$FixedPoint@H$02@0@0@Z
        151   96 0004F620 ??$?DH$02$0A@@gemmlowp@@YA?AV?$FixedPoint@H$02@0@V10@V?$FixedPoint@H$0A@@0@@Z
        152   97 0004F620 ??$?DH$05$02@gemmlowp@@YA?AV?$FixedPoint@H$08@0@V?$FixedPoint@H$05@0@V?$FixedPoint@H$02@0@@Z
        153   98 0004F620 ??$?DH$05$0A@@gemmlowp@@YA?AV?$FixedPoint@H$05@0@V10@V?$FixedPoint@H$0A@@0@@Z
        154   99 0004F620 ??$?DH$0A@$01@gemmlowp@@YA?AV?$FixedPoint@H$01@0@V?$FixedPoint@H$0A@@0@V10@@Z
        155   9A 0004F620 ??$?DH$0A@$0A@@gemmlowp@@YA?AV?$FixedPoint@H$0A@@0@V10@0@Z
        156   9B 000EE340 ??$?DV?$Block@V?$Map@$$CBV?$Matrix@M$0?0$0?0$0A@$0?0$0?0@Eigen@@$0A@V?$Stride@$0A@$0A@@2@@Eigen@@$0?0$00$00@Eigen@@@?$MatrixBase@V?$Map@$$CBV?$Matrix@M$0?0$0?0$00$0?0$0?0@Eigen@@$0A@V?$Stride@$0A@$0A@@2@@Eigen@@@Eigen@@QEBA?BV?$Product@V?$Map@$$CBV?$Matrix@M$0?0$0?0$00$0?0$0?0@Eigen@@$0A@V?$Stride@$0A@$0A@@2@@Eigen@@V?$Block@V?$Map@$$CBV?$Matrix@M$0?0$0?0$0A@$0?0$0?0@Eigen@@$0A@V?$Stride@$0A@$0A@@2@@Eigen@@$0?0$00$00@2@$0A@@1@AEBV?$MatrixBase@V?$Block@V?$Map@$$CBV?$Matrix@M$0?0$0?0$0A@$0?0$0?0@Eigen@@$0A@V?$Stride@$0A@$0A@@2@@Eigen@@$0?0$00$00@Eigen@@@1@@Z
        157   9C 000EE3A0 ??$?DV?$Map@$$CBV?$Matrix@M$0?0$0?0$0A@$0?0$0?0@Eigen@@$0A@V?$Stride@$0A@$0A@@2@@Eigen@@@?$MatrixBase@V?$Block@V?$Map@$$CBV?$Matrix@M$0?0$0?0$00$0?0$0?0@Eigen@@$0A@V?$Stride@$0A@$0A@@2@@Eigen@@$00$0?0$00@Eigen@@@Eigen@@QEBA?BV?$Product@V?$Block@V?$Map@$$CBV?$Matrix@M$0?0$0?0$00$0?0$0?0@Eigen@@$0A@V?$Stride@$0A@$0A@@2@@Eigen@@$00$0?0$00@Eigen@@V?$Map@$$CBV?$Matrix@M$0?0$0?0$0A@$0?0$0?0@Eigen@@$0A@V?$Stride@$0A@$0A@@2@@2@$0A@@1@AEBV?$MatrixBase@V?$Map@$$CBV?$Matrix@M$0?0$0?0$0A@$0?0$0?0@Eigen@@$0A@V?$Stride@$0A@$0A@@2@@Eigen@@@1@@Z
        158   9D 000EE400 ??$?DV?$Map@$$CBV?$Matrix@M$0?0$0?0$0A@$0?0$0?0@Eigen@@$0A@V?$Stride@$0A@$0A@@2@@Eigen@@@?$MatrixBase@V?$Map@$$CBV?$Matrix@M$0?0$0?0$00$0?0$0?0@Eigen@@$0A@V?$Stride@$0A@$0A@@2@@Eigen@@@Eigen@@QEBA?BV?$Product@V?$Map@$$CBV?$Matrix@M$0?0$0?0$00$0?0$0?0@Eigen@@$0A@V?$Stride@$0A@$0A@@2@@Eigen@@V?$Map@$$CBV?$Matrix@M$0?0$0?0$0A@$0?0$0?0@Eigen@@$0A@V?$Stride@$0A@$0A@@2@@2@$0A@@1@AEBV?$MatrixBase@V?$Map@$$CBV?$Matrix@M$0?0$0?0$0A@$0?0$0?0@Eigen@@$0A@V?$Stride@$0A@$0A@@2@@Eigen@@@1@@Z
        159   9E 0004F680 ??$?GF$00@gemmlowp@@YA?AV?$FixedPoint@F$00@0@V10@0@Z
        160   9F 0004F690 ??$?GF$00@gemmlowp@@YA?AV?$FixedPoint@F$00@0@V10@@Z
        161   A0 0004F680 ??$?GF$01@gemmlowp@@YA?AV?$FixedPoint@F$01@0@V10@0@Z
        162   A1 0004F690 ??$?GF$01@gemmlowp@@YA?AV?$FixedPoint@F$01@0@V10@@Z
        163   A2 0004F680 ??$?GF$02@gemmlowp@@YA?AV?$FixedPoint@F$02@0@V10@0@Z
        164   A3 0004F690 ??$?GF$02@gemmlowp@@YA?AV?$FixedPoint@F$02@0@V10@@Z
        165   A4 0004F680 ??$?GF$03@gemmlowp@@YA?AV?$FixedPoint@F$03@0@V10@0@Z
        166   A5 0004F690 ??$?GF$03@gemmlowp@@YA?AV?$FixedPoint@F$03@0@V10@@Z
        167   A6 0004F680 ??$?GF$04@gemmlowp@@YA?AV?$FixedPoint@F$04@0@V10@0@Z
        168   A7 0004F690 ??$?GF$04@gemmlowp@@YA?AV?$FixedPoint@F$04@0@V10@@Z
        169   A8 0004F680 ??$?GF$05@gemmlowp@@YA?AV?$FixedPoint@F$05@0@V10@0@Z
        170   A9 0004F690 ??$?GF$05@gemmlowp@@YA?AV?$FixedPoint@F$05@0@V10@@Z
        171   AA 0004F680 ??$?GF$06@gemmlowp@@YA?AV?$FixedPoint@F$06@0@V10@0@Z
        172   AB 0004F680 ??$?GF$0A@@gemmlowp@@YA?AV?$FixedPoint@F$0A@@0@V10@0@Z
        173   AC 0004F690 ??$?GF$0A@@gemmlowp@@YA?AV?$FixedPoint@F$0A@@0@V10@@Z
        174   AD 0004F6A0 ??$?GH$01@gemmlowp@@YA?AV?$FixedPoint@H$01@0@V10@0@Z
        175   AE 0004F6A0 ??$?GH$04@gemmlowp@@YA?AV?$FixedPoint@H$04@0@V10@0@Z
        176   AF 0004F6A0 ??$?GH$05@gemmlowp@@YA?AV?$FixedPoint@H$05@0@V10@0@Z
        177   B0 0004F6A0 ??$?GH$0A@@gemmlowp@@YA?AV?$FixedPoint@H$0A@@0@V10@0@Z
        178   B1 0004F6B0 ??$?HF$01@gemmlowp@@YA?AV?$FixedPoint@F$01@0@V10@0@Z
        179   B2 0004F6B0 ??$?HF$0A@@gemmlowp@@YA?AV?$FixedPoint@F$0A@@0@V10@0@Z
        180   B3 0004F6C0 ??$?HH$01@gemmlowp@@YA?AV?$FixedPoint@H$01@0@V10@0@Z
        181   B4 0004F6C0 ??$?HH$05@gemmlowp@@YA?AV?$FixedPoint@H$05@0@V10@0@Z
        182   B5 0004F6C0 ??$?HH$0A@@gemmlowp@@YA?AV?$FixedPoint@H$0A@@0@V10@0@Z
        183   B6 0004F6C0 ??$?HH$0M@@gemmlowp@@YA?AV?$FixedPoint@H$0M@@0@V10@0@Z
        184   B7 0004F6D0 ??$?IF$00@gemmlowp@@YA?AV?$FixedPoint@F$00@0@V10@0@Z
        185   B8 0004F6D0 ??$?IF$01@gemmlowp@@YA?AV?$FixedPoint@F$01@0@V10@0@Z
        186   B9 0004F6D0 ??$?IF$02@gemmlowp@@YA?AV?$FixedPoint@F$02@0@V10@0@Z
        187   BA 0004F6D0 ??$?IF$03@gemmlowp@@YA?AV?$FixedPoint@F$03@0@V10@0@Z
        188   BB 0004F6D0 ??$?IF$04@gemmlowp@@YA?AV?$FixedPoint@F$04@0@V10@0@Z
        189   BC 0004F6D0 ??$?IF$05@gemmlowp@@YA?AV?$FixedPoint@F$05@0@V10@0@Z
        190   BD 0004F6D0 ??$?IF$06@gemmlowp@@YA?AV?$FixedPoint@F$06@0@V10@0@Z
        191   BE 0004F6E0 ??$?IH$04@gemmlowp@@YA?AV?$FixedPoint@H$04@0@V10@0@Z
        192   BF 000390B0 ??$?RHAEBH@EmplaceDecomposable@?$raw_hash_set@U?$FlatHashSetPolicy@H@container_internal@lts_20230802@absl@@U?$Hash@H@hash_internal@34@U?$equal_to@H@std@@V?$allocator@H@8@@container_internal@lts_20230802@absl@@QEBA?AU?$pair@Viterator@?$raw_hash_set@U?$FlatHashSetPolicy@H@container_internal@lts_20230802@absl@@U?$Hash@H@hash_internal@34@U?$equal_to@H@std@@V?$allocator@H@8@@container_internal@lts_20230802@absl@@_N@std@@AEBH0@Z
        193   C0 00039220 ??$?RII@EmplaceDecomposable@?$raw_hash_set@U?$FlatHashSetPolicy@H@container_internal@lts_20230802@absl@@U?$Hash@H@hash_internal@34@U?$equal_to@H@std@@V?$allocator@H@8@@container_internal@lts_20230802@absl@@QEBA?AU?$pair@Viterator@?$raw_hash_set@U?$FlatHashSetPolicy@H@container_internal@lts_20230802@absl@@U?$Hash@H@hash_internal@34@U?$equal_to@H@std@@V?$allocator@H@8@@container_internal@lts_20230802@absl@@_N@std@@AEBI$$QEAI@Z
        194   C1 00253630 ??$?RU?$pair@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@H@std@@@?$_Uhash_compare@U?$pair@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@H@std@@U?$OperatorKeyHasher@U?$pair@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@H@std@@@op_resolver_hasher@tflite@@U?$equal_to@U?$pair@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@H@std@@@2@@std@@QEBA_KAEBU?$pair@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@H@1@@Z
        195   C2 002536F0 ??$?RU?$pair@W4BuiltinOperator@tflite@@H@std@@@?$_Uhash_compare@U?$pair@W4BuiltinOperator@tflite@@H@std@@U?$OperatorKeyHasher@U?$pair@W4BuiltinOperator@tflite@@H@std@@@op_resolver_hasher@tflite@@U?$equal_to@U?$pair@W4BuiltinOperator@tflite@@H@std@@@2@@std@@QEBA_KAEBU?$pair@W4BuiltinOperator@tflite@@H@1@@Z
        196   C3 001905A0 ??$?RV?$mersenne_twister_engine@I$0CA@$0CHA@$0BIN@$0BP@$0JJAILANP@$0L@$0PPPPPPPP@$06$0JNCMFGIA@$0P@$0OPMGAAAA@$0BC@$0GMAHIJGF@@std@@@?$uniform_int@H@std@@QEBAHAEAV?$mersenne_twister_engine@I$0CA@$0CHA@$0BIN@$0BP@$0JJAILANP@$0L@$0PPPPPPPP@$06$0JNCMFGIA@$0P@$0OPMGAAAA@$0BC@$0GMAHIJGF@@1@@Z
        197   C4 0002BB40 ??$AbslHashValue@VMixingHashState@hash_internal@lts_20230802@absl@@HVTensorTag@internal@model_builder@tflite@@@internal@model_builder@tflite@@YA?AVMixingHashState@hash_internal@lts_20230802@absl@@V3456@AEBU?$StrongType@HVTensorTag@internal@model_builder@tflite@@@012@@Z
        198   C5 0004F6F0 ??$Add@F@gemmlowp@@YAFFF@Z
        199   C6 0004F700 ??$Add@H@gemmlowp@@YAHHH@Z
        200   C7 00063E60 ??$Add@H@optimized_ops@tflite@@YAXAEBUArithmeticParams@1@AEBVRuntimeShape@1@PEBH121PEAH@Z
        201   C8 00064310 ??$Add@H@reference_ops@tflite@@YAXAEBUArithmeticParams@1@AEBVRuntimeShape@1@PEBH121PEAH@Z
        202   C9 000643D0 ??$Add@M@reference_ops@tflite@@YAXAEBUArithmeticParams@1@AEBVRuntimeShape@1@PEBM121PEAM@Z
        203   CA 000645E0 ??$Add@_J@optimized_ops@tflite@@YAXAEBUArithmeticParams@1@AEBVRuntimeShape@1@PEB_J121PEA_J@Z
        204   CB 00064B60 ??$Add@_J@reference_ops@tflite@@YAXAEBUArithmeticParams@1@AEBVRuntimeShape@1@PEB_J121PEA_J@Z
        205   CC 00064C20 ??$AddBroadcast@M@reference_ops@tflite@@YAXPEBM0PEAM_KMM@Z
        206   CD 000032C0 ??$AddConstant@$0A@U?$RegisterBlock@H$00$00@gemmlowp@@@gemmlowp@@YAXPEAU?$RegisterBlock@H$00$00@0@@Z
        207   CE 00064DB0 ??$AddElementwise@E@reference_ops@tflite@@YAXHAEBUArithmeticParams@1@PEBE1PEAE@Z
        208   CF 00065000 ??$AddElementwise@F@reference_ops@tflite@@YAXHAEBUArithmeticParams@1@PEBF1PEAF@Z
        209   D0 00065250 ??$AddElementwise@F@reference_ops@tflite@@YAXPEBF0PEAF_KFF@Z
        210   D1 000652A0 ??$AddElementwise@M@reference_ops@tflite@@YAXPEBM0PEAM_KMM@Z
        211   D2 00065450 ??$AddElementwise@_J@reference_ops@tflite@@YAXPEB_J0PEA_J_K_J3@Z
        212   D3 001E05D0 ??$AddIndices@H@builtin@ops@tflite@@YA?AV?$vector@HV?$allocator@H@std@@@std@@AEBV34@0@Z
        213   D4 001E06B0 ??$AddIndices@_J@builtin@ops@tflite@@YA?AV?$vector@_JV?$allocator@_J@std@@@std@@AEBV34@0@Z
        214   D5 0006B4F0 ??$AddN@H@optimized_ops@tflite@@YAXAEBVRuntimeShape@1@_KPEBQEBHPEAH3PEAVCpuBackendContext@1@@Z
        215   D6 0006B920 ??$AddN@M@optimized_ops@tflite@@YAXAEBVRuntimeShape@1@_KPEBQEBMPEAM3PEAVCpuBackendContext@1@@Z
        216   D7 0004F710 ??$AddSaturatingIf16Bit@F$0A@@gemmlowp@@YA?AV?$FixedPoint@F$0A@@0@V10@0@Z
        217   D8 0004F740 ??$AddSaturatingIf16Bit@F@gemmlowp@@YAFFF@Z
        218   D9 0004F6C0 ??$AddSaturatingIf16Bit@H$0A@@gemmlowp@@YA?AV?$FixedPoint@H$0A@@0@V10@0@Z
        219   DA 0004F700 ??$AddSaturatingIf16Bit@H@gemmlowp@@YAHHH@Z
        220   DB 0018B130 ??$AffineQuantize@MC@reference_ops@tflite@@YAXAEBUQuantizationParams@1@AEBVRuntimeShape@1@PEBM1PEAC@Z
        221   DC 0018B330 ??$AffineQuantize@ME@reference_ops@tflite@@YAXAEBUQuantizationParams@1@AEBVRuntimeShape@1@PEBM1PEAE@Z
        222   DD 0018B520 ??$AffineQuantize@MF@reference_ops@tflite@@YAXAEBUQuantizationParams@1@AEBVRuntimeShape@1@PEBM1PEAF@Z
        223   DE 0002BB60 ??$Allocate@$07V?$allocator@D@std@@@container_internal@lts_20230802@absl@@YAPEAXPEAV?$allocator@D@std@@_K@Z
        224   DF 001DD530 ??$ApplyComputation@M$04@builtin@ops@tflite@@YAMMM@Z
        225   E0 001DD530 ??$ApplyComputation@N$04@builtin@ops@tflite@@YANNN@Z
        226   E1 001DD530 ??$ApplyComputation@Uhalf@Eigen@@$04@builtin@ops@tflite@@YA?AUhalf@Eigen@@U34@0@Z
        227   E2 00074DE0 ??$ApplyMultiplier@HC@ruy@@YAXAEBV?$MulParams@HC@0@HPEAH@Z
        228   E3 00074DE0 ??$ApplyMultiplier@HE@ruy@@YAXAEBV?$MulParams@HE@0@HPEAH@Z
        229   E4 00097A40 ??$ApplyMultiplier@HF@ruy@@YAXAEBV?$MulParams@HF@0@HPEAH@Z
        230   E5 000032C0 ??$ApplyMultiplier@HH@ruy@@YAXAEBV?$MulParams@HH@0@HPEAH@Z
        231   E6 000032C0 ??$ApplyMultiplier@MM@ruy@@YAXAEBV?$MulParams@MM@0@HPEAM@Z
        232   E7 0004F760 ??$ApplyPrelu@M@activations@builtin@ops@tflite@@YAMMM@Z
        233   E8 0006D8A0 ??$ArgMinMax@CHH@optimized_ops@tflite@@YAXAEBVRuntimeShape@1@PEBCPEBH0PEAH_N@Z
        234   E9 0006DBF0 ??$ArgMinMax@CHH@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBCPEBH0PEAH_N@Z
        235   EA 0006DC80 ??$ArgMinMax@CHHV?$function@$$A6A_NCC@Z@std@@@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBCPEBH0PEAHAEBV?$function@$$A6A_NCC@Z@std@@@Z
        236   EB 0006D8A0 ??$ArgMinMax@CH_J@optimized_ops@tflite@@YAXAEBVRuntimeShape@1@PEBCPEB_J0PEAH_N@Z
        237   EC 0006DBF0 ??$ArgMinMax@CH_J@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBCPEB_J0PEAH_N@Z
        238   ED 0006DC80 ??$ArgMinMax@CH_JV?$function@$$A6A_NCC@Z@std@@@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBCPEB_J0PEAHAEBV?$function@$$A6A_NCC@Z@std@@@Z
        239   EE 0006DE80 ??$ArgMinMax@C_JH@optimized_ops@tflite@@YAXAEBVRuntimeShape@1@PEBCPEBH0PEA_J_N@Z
        240   EF 0006E1D0 ??$ArgMinMax@C_JH@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBCPEBH0PEA_J_N@Z
        241   F0 0006E260 ??$ArgMinMax@C_JHV?$function@$$A6A_NCC@Z@std@@@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBCPEBH0PEA_JAEBV?$function@$$A6A_NCC@Z@std@@@Z
        242   F1 0006DE80 ??$ArgMinMax@C_J_J@optimized_ops@tflite@@YAXAEBVRuntimeShape@1@PEBCPEB_J0PEA_J_N@Z
        243   F2 0006E1D0 ??$ArgMinMax@C_J_J@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBCPEB_J0PEA_J_N@Z
        244   F3 0006E260 ??$ArgMinMax@C_J_JV?$function@$$A6A_NCC@Z@std@@@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBCPEB_J0PEA_JAEBV?$function@$$A6A_NCC@Z@std@@@Z
        245   F4 0006E460 ??$ArgMinMax@EHH@optimized_ops@tflite@@YAXAEBVRuntimeShape@1@PEBEPEBH0PEAH_N@Z
        246   F5 0006E7B0 ??$ArgMinMax@EHH@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBEPEBH0PEAH_N@Z
        247   F6 0006DC80 ??$ArgMinMax@EHHV?$function@$$A6A_NEE@Z@std@@@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBEPEBH0PEAHAEBV?$function@$$A6A_NEE@Z@std@@@Z
        248   F7 0006E460 ??$ArgMinMax@EH_J@optimized_ops@tflite@@YAXAEBVRuntimeShape@1@PEBEPEB_J0PEAH_N@Z
        249   F8 0006E7B0 ??$ArgMinMax@EH_J@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBEPEB_J0PEAH_N@Z
        250   F9 0006DC80 ??$ArgMinMax@EH_JV?$function@$$A6A_NEE@Z@std@@@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBEPEB_J0PEAHAEBV?$function@$$A6A_NEE@Z@std@@@Z
        251   FA 0006E840 ??$ArgMinMax@E_JH@optimized_ops@tflite@@YAXAEBVRuntimeShape@1@PEBEPEBH0PEA_J_N@Z
        252   FB 0006EB90 ??$ArgMinMax@E_JH@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBEPEBH0PEA_J_N@Z
        253   FC 0006E260 ??$ArgMinMax@E_JHV?$function@$$A6A_NEE@Z@std@@@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBEPEBH0PEA_JAEBV?$function@$$A6A_NEE@Z@std@@@Z
        254   FD 0006E840 ??$ArgMinMax@E_J_J@optimized_ops@tflite@@YAXAEBVRuntimeShape@1@PEBEPEB_J0PEA_J_N@Z
        255   FE 0006EB90 ??$ArgMinMax@E_J_J@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBEPEB_J0PEA_J_N@Z
        256   FF 0006E260 ??$ArgMinMax@E_J_JV?$function@$$A6A_NEE@Z@std@@@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBEPEB_J0PEA_JAEBV?$function@$$A6A_NEE@Z@std@@@Z
        257  100 0006EC20 ??$ArgMinMax@HHH@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBH10PEAH_N@Z
        258  101 0006ECB0 ??$ArgMinMax@HHHV?$function@$$A6A_NHH@Z@std@@@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBH10PEAHAEBV?$function@$$A6A_NHH@Z@std@@@Z
        259  102 0006EC20 ??$ArgMinMax@HH_J@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBHPEB_J0PEAH_N@Z
        260  103 0006ECB0 ??$ArgMinMax@HH_JV?$function@$$A6A_NHH@Z@std@@@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBHPEB_J0PEAHAEBV?$function@$$A6A_NHH@Z@std@@@Z
        261  104 0006EEC0 ??$ArgMinMax@H_JH@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBH10PEA_J_N@Z
        262  105 0006EF50 ??$ArgMinMax@H_JHV?$function@$$A6A_NHH@Z@std@@@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBH10PEA_JAEBV?$function@$$A6A_NHH@Z@std@@@Z
        263  106 0006EEC0 ??$ArgMinMax@H_J_J@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBHPEB_J0PEA_J_N@Z
        264  107 0006EF50 ??$ArgMinMax@H_J_JV?$function@$$A6A_NHH@Z@std@@@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBHPEB_J0PEA_JAEBV?$function@$$A6A_NHH@Z@std@@@Z
        265  108 0006F160 ??$ArgMinMax@MHH@optimized_ops@tflite@@YAXAEBVRuntimeShape@1@PEBMPEBH0PEAH_N@Z
        266  109 0006F3A0 ??$ArgMinMax@MHH@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBMPEBH0PEAH_N@Z
        267  10A 0006F430 ??$ArgMinMax@MHHV?$function@$$A6A_NMM@Z@std@@@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBMPEBH0PEAHAEBV?$function@$$A6A_NMM@Z@std@@@Z
        268  10B 0006F160 ??$ArgMinMax@MH_J@optimized_ops@tflite@@YAXAEBVRuntimeShape@1@PEBMPEB_J0PEAH_N@Z
        269  10C 0006F3A0 ??$ArgMinMax@MH_J@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBMPEB_J0PEAH_N@Z
        270  10D 0006F430 ??$ArgMinMax@MH_JV?$function@$$A6A_NMM@Z@std@@@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBMPEB_J0PEAHAEBV?$function@$$A6A_NMM@Z@std@@@Z
        271  10E 0006F630 ??$ArgMinMax@M_JH@optimized_ops@tflite@@YAXAEBVRuntimeShape@1@PEBMPEBH0PEA_J_N@Z
        272  10F 0006F870 ??$ArgMinMax@M_JH@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBMPEBH0PEA_J_N@Z
        273  110 0006F900 ??$ArgMinMax@M_JHV?$function@$$A6A_NMM@Z@std@@@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBMPEBH0PEA_JAEBV?$function@$$A6A_NMM@Z@std@@@Z
        274  111 0006F630 ??$ArgMinMax@M_J_J@optimized_ops@tflite@@YAXAEBVRuntimeShape@1@PEBMPEB_J0PEA_J_N@Z
        275  112 0006F870 ??$ArgMinMax@M_J_J@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBMPEB_J0PEA_J_N@Z
        276  113 0006F900 ??$ArgMinMax@M_J_JV?$function@$$A6A_NMM@Z@std@@@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBMPEB_J0PEA_JAEBV?$function@$$A6A_NMM@Z@std@@@Z
        277  114 0006FB00 ??$ArgMinMax@_NHH@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEB_NPEBH0PEAH_N@Z
        278  115 0006DC80 ??$ArgMinMax@_NHHV?$function@$$A6A_N_N0@Z@std@@@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEB_NPEBH0PEAHAEBV?$function@$$A6A_N_N0@Z@std@@@Z
        279  116 0006FB00 ??$ArgMinMax@_NH_J@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEB_NPEB_J0PEAH_N@Z
        280  117 0006DC80 ??$ArgMinMax@_NH_JV?$function@$$A6A_N_N0@Z@std@@@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEB_NPEB_J0PEAHAEBV?$function@$$A6A_N_N0@Z@std@@@Z
        281  118 0006FB90 ??$ArgMinMax@_N_JH@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEB_NPEBH0PEA_J_N@Z
        282  119 0006E260 ??$ArgMinMax@_N_JHV?$function@$$A6A_N_N0@Z@std@@@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEB_NPEBH0PEA_JAEBV?$function@$$A6A_N_N0@Z@std@@@Z
        283  11A 0006FB90 ??$ArgMinMax@_N_J_J@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEB_NPEB_J0PEA_J_N@Z
        284  11B 0006E260 ??$ArgMinMax@_N_J_JV?$function@$$A6A_N_N0@Z@std@@@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEB_NPEB_J0PEA_JAEBV?$function@$$A6A_N_N0@Z@std@@@Z
        285  11C 0006FC20 ??$ArgMinMaxLastAxis@MH$00@optimized_ops@tflite@@YAXAEBVRuntimeShape@1@PEBM0PEAH@Z
        286  11D 0006FDA0 ??$ArgMinMaxLastAxis@MH$0A@@optimized_ops@tflite@@YAXAEBVRuntimeShape@1@PEBM0PEAH@Z
        287  11E 0006FF20 ??$ArgMinMaxLastAxis@M_J$00@optimized_ops@tflite@@YAXAEBVRuntimeShape@1@PEBM0PEA_J@Z
        288  11F 000700A0 ??$ArgMinMaxLastAxis@M_J$0A@@optimized_ops@tflite@@YAXAEBVRuntimeShape@1@PEBM0PEA_J@Z
        289  120 000032C0 ??$AssertThatExtraCapacityInPerChannelBuffersIsZeroInitialized@HC@detail@ruy@@YAXAEBV?$MulParams@HC@1@HH@Z
        290  121 000032C0 ??$AssertThatExtraCapacityInPerChannelBuffersIsZeroInitialized@HE@detail@ruy@@YAXAEBV?$MulParams@HE@1@HH@Z
        291  122 000032C0 ??$AssertThatExtraCapacityInPerChannelBuffersIsZeroInitialized@HF@detail@ruy@@YAXAEBV?$MulParams@HF@1@HH@Z
        292  123 000032C0 ??$AssertThatExtraCapacityInPerChannelBuffersIsZeroInitialized@HH@detail@ruy@@YAXAEBV?$MulParams@HH@1@HH@Z
        293  124 000032C0 ??$AssertThatExtraCapacityInPerChannelBuffersIsZeroInitialized@MM@detail@ruy@@YAXAEBV?$MulParams@MM@1@HH@Z
        294  125 00071C20 ??$Atan2@M@atan2@builtin@ops@tflite@@YA?AW4TfLiteStatus@@PEBUTfLiteTensor@@0PEAU5@@Z
        295  126 00071C20 ??$Atan2@M@atan2@custom@ops@tflite@@YA?AW4TfLiteStatus@@PEBUTfLiteTensor@@0PEAU5@@Z
        296  127 00071CD0 ??$Atan2@N@atan2@builtin@ops@tflite@@YA?AW4TfLiteStatus@@PEBUTfLiteTensor@@0PEAU5@@Z
        297  128 00071CD0 ??$Atan2@N@atan2@custom@ops@tflite@@YA?AW4TfLiteStatus@@PEBUTfLiteTensor@@0PEAU5@@Z
        298  129 0017F150 ??$AverageEval@$00@pooling@builtin@ops@tflite@@YA?AW4TfLiteStatus@@PEAUTfLiteContext@@PEAUTfLiteNode@@@Z
        299  12A 0017F5A0 ??$AverageEval@$0A@@pooling@builtin@ops@tflite@@YA?AW4TfLiteStatus@@PEAUTfLiteContext@@PEAUTfLiteNode@@@Z
        300  12B 0017F9F0 ??$AverageEvalFloat@$00@pooling@builtin@ops@tflite@@YA?AW4TfLiteStatus@@PEAUTfLiteContext@@PEAUTfLiteNode@@PEAUTfLitePoolParams@@PEAUOpData@0123@PEBUTfLiteTensor@@PEAU9@@Z
        301  12C 0017FB50 ??$AverageEvalFloat@$0A@@pooling@builtin@ops@tflite@@YA?AW4TfLiteStatus@@PEAUTfLiteContext@@PEAUTfLiteNode@@PEAUTfLitePoolParams@@PEAUOpData@0123@PEBUTfLiteTensor@@PEAU9@@Z
        302  12D 0017FCD0 ??$AverageEvalQuantizedInt16@$00@pooling@builtin@ops@tflite@@YA?AW4TfLiteStatus@@PEAUTfLiteContext@@PEAUTfLiteNode@@PEAUTfLitePoolParams@@PEAUOpData@0123@PEBUTfLiteTensor@@PEAU9@@Z
        303  12E 0017FCD0 ??$AverageEvalQuantizedInt16@$0A@@pooling@builtin@ops@tflite@@YA?AW4TfLiteStatus@@PEAUTfLiteContext@@PEAUTfLiteNode@@PEAUTfLitePoolParams@@PEAUOpData@0123@PEBUTfLiteTensor@@PEAU9@@Z
        304  12F 0017FE40 ??$AverageEvalQuantizedInt8@$00@pooling@builtin@ops@tflite@@YA?AW4TfLiteStatus@@PEAUTfLiteContext@@PEAUTfLiteNode@@PEAUTfLitePoolParams@@PEAUOpData@0123@PEBUTfLiteTensor@@PEAU9@@Z
        305  130 0017FFB0 ??$AverageEvalQuantizedInt8@$0A@@pooling@builtin@ops@tflite@@YA?AW4TfLiteStatus@@PEAUTfLiteContext@@PEAUTfLiteNode@@PEAUTfLitePoolParams@@PEAUOpData@0123@PEBUTfLiteTensor@@PEAU9@@Z
        306  131 00180120 ??$AverageEvalQuantizedUint8@$00@pooling@builtin@ops@tflite@@YA?AW4TfLiteStatus@@PEAUTfLiteContext@@PEAUTfLiteNode@@PEAUTfLitePoolParams@@PEAUOpData@0123@PEBUTfLiteTensor@@PEAU9@@Z
        307  132 00180280 ??$AverageEvalQuantizedUint8@$0A@@pooling@builtin@ops@tflite@@YA?AW4TfLiteStatus@@PEAUTfLiteContext@@PEAUTfLiteNode@@PEAUTfLitePoolParams@@PEAUOpData@0123@PEBUTfLiteTensor@@PEAU9@@Z
        308  133 00074E40 ??$BatchMatMul@CCH@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBC010PEAH@Z
        309  134 00075640 ??$BatchMatMul@CH@reference_ops@tflite@@YAXAEBUFullyConnectedParams@1@AEBVRuntimeShape@1@PEBC121PEAC@Z
        310  135 00075DD0 ??$BatchMatMul@F_J@reference_ops@tflite@@YAXAEBUFullyConnectedParams@1@AEBVRuntimeShape@1@PEBF121PEAF@Z
        311  136 000764A0 ??$BatchMatMul@MMM@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBM010PEAM@Z
        312  137 00081DA0 ??$BatchToSpaceND@C@optimized_ops@tflite@@YAXAEBVRuntimeShape@1@PEBC0PEBH020PEAC@Z
        313  138 00082050 ??$BatchToSpaceND@C@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBC0PEBH020PEAC@Z
        314  139 00081DA0 ??$BatchToSpaceND@E@optimized_ops@tflite@@YAXAEBVRuntimeShape@1@PEBE0PEBH020PEAE@Z
        315  13A 00082050 ??$BatchToSpaceND@E@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBE0PEBH020PEAE@Z
        316  13B 000822A0 ??$BatchToSpaceND@F@optimized_ops@tflite@@YAXAEBVRuntimeShape@1@PEBF0PEBH020PEAF@Z
        317  13C 00082570 ??$BatchToSpaceND@F@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBF0PEBH020PEAF@Z
        318  13D 000827C0 ??$BatchToSpaceND@H@optimized_ops@tflite@@YAXAEBVRuntimeShape@1@PEBH01010PEAH@Z
        319  13E 00082A90 ??$BatchToSpaceND@H@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBH01010PEAH@Z
        320  13F 000827C0 ??$BatchToSpaceND@M@optimized_ops@tflite@@YAXAEBVRuntimeShape@1@PEBM0PEBH020PEAM@Z
        321  140 00082A90 ??$BatchToSpaceND@M@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBM0PEBH020PEAM@Z
        322  141 00082CE0 ??$BatchToSpaceND@_J@optimized_ops@tflite@@YAXAEBVRuntimeShape@1@PEB_J0PEBH020PEA_J@Z
        323  142 00082FB0 ??$BatchToSpaceND@_J@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEB_J0PEBH020PEA_J@Z
        324  143 000EB250 ??$BiasAdd3D@M@optimized_ops@tflite@@YAXPEAMPEBMAEBVRuntimeShape@1@MM@Z
        325  144 00226C20 ??$BiasAdd@H@optimized_ops@tflite@@YAXPEAHPEBHHHHH@Z
        326  145 00226DC0 ??$BiasAdd@M@optimized_ops@tflite@@YAXPEAMPEBMHHHH@Z
        327  146 000654A0 ??$BinaryBroadcastFiveFold@P6AXHAEBUArithmeticParams@tflite@@PEBC1PEAC@ZP6AXH0C12@ZC@optimized_ops@tflite@@YAXAEBUArithmeticParams@1@AEBVRuntimeShape@1@PEBC121PEACP6AXH0223@ZP6AXH0C23@Z@Z
        328  147 000654A0 ??$BinaryBroadcastFiveFold@P6AXHAEBUArithmeticParams@tflite@@PEBE1PEAE@ZP6AXH0E12@ZE@optimized_ops@tflite@@YAXAEBUArithmeticParams@1@AEBVRuntimeShape@1@PEBE121PEAEP6AXH0223@ZP6AXH0E23@Z@Z
        329  148 0004F770 ??$BinaryBroadcastFiveFold@P6AXHAEBUArithmeticParams@tflite@@PEBM1PEAM@ZP6AXH0M12@ZM@optimized_ops@tflite@@YAXAEBUArithmeticParams@1@AEBVRuntimeShape@1@PEBM121PEAMP6AXH0223@ZP6AXH0M23@Z@Z
        330  149 0004FA10 ??$BinaryFunction@MMM@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBM010PEAMP6AMMM@Z@Z
        331  14A 0004FAA0 ??$BitAnd@F@gemmlowp@@YAFFF@Z
        332  14B 0004FAB0 ??$BitAnd@H@gemmlowp@@YAHHH@Z
        333  14C 0004FAC0 ??$BitNot@F@gemmlowp@@YAFF@Z
        334  14D 0004FAD0 ??$BitNot@H@gemmlowp@@YAHH@Z
        335  14E 0004FAE0 ??$BitXor@F@gemmlowp@@YAFFF@Z
        336  14F 0004FAF0 ??$BitXor@H@gemmlowp@@YAHHH@Z
        337  150 0008BBF0 ??$BitwiseXor@C@bitwise_xor@builtin@ops@tflite@@YACCC@Z
        338  151 0004FAE0 ??$BitwiseXor@F@bitwise_xor@builtin@ops@tflite@@YAFFF@Z
        339  152 0004FAF0 ??$BitwiseXor@H@bitwise_xor@builtin@ops@tflite@@YAHHH@Z
        340  153 00065740 ??$BroadcastAdd6DSlow@E@reference_ops@tflite@@YAXAEBUArithmeticParams@1@AEBVRuntimeShape@1@PEBE121PEAE@Z
        341  154 000658A0 ??$BroadcastAdd6DSlow@F$00@reference_ops@tflite@@YAXAEBUArithmeticParams@1@AEBVRuntimeShape@1@PEBF121PEAF@Z
        342  155 00065A10 ??$BroadcastAdd6DSlow@F@reference_ops@tflite@@YAXAEBUArithmeticParams@1@AEBVRuntimeShape@1@PEBF121PEAF@Z
        343  156 00065B70 ??$BroadcastAdd6DSlow@H$0A@@reference_ops@tflite@@YAXAEBUArithmeticParams@1@AEBVRuntimeShape@1@PEBH121PEAH@Z
        344  157 00065CE0 ??$BroadcastAdd6DSlow@M$0A@@reference_ops@tflite@@YAXAEBUArithmeticParams@1@AEBVRuntimeShape@1@PEBM121PEAM@Z
        345  158 00065E60 ??$BroadcastAdd6DSlow@_J$0A@@reference_ops@tflite@@YAXAEBUArithmeticParams@1@AEBVRuntimeShape@1@PEB_J121PEA_J@Z
        346  159 00097AA0 ??$BroadcastAdd@U?$RegisterBlock@H$00$00@gemmlowp@@U12@@gemmlowp@@YA?AU?$RegisterBlock@H$00$00@0@AEBU10@0@Z
        347  15A 00097AB0 ??$BroadcastAdd@U?$RegisterBlock@H$00$03@gemmlowp@@U12@@gemmlowp@@YA?AU?$RegisterBlock@H$00$03@0@AEBU10@0@Z
        348  15B 00097AE0 ??$BroadcastAdd@U?$RegisterBlock@H$00$03@gemmlowp@@U?$RegisterBlock@H$00$00@2@@gemmlowp@@YA?AU?$RegisterBlock@H$00$03@0@AEBU10@AEBU?$RegisterBlock@H$00$00@0@@Z
        349  15C 00097B10 ??$BroadcastAdd@U?$RegisterBlock@H$03$00@gemmlowp@@U12@@gemmlowp@@YA?AU?$RegisterBlock@H$03$00@0@AEBU10@0@Z
        350  15D 00097B30 ??$BroadcastAdd@U?$RegisterBlock@H$03$00@gemmlowp@@U?$RegisterBlock@H$00$00@2@@gemmlowp@@YA?AU?$RegisterBlock@H$03$00@0@AEBU10@AEBU?$RegisterBlock@H$00$00@0@@Z
        351  15E 00097B50 ??$BroadcastAdd@U?$RegisterBlock@H$03$03@gemmlowp@@U?$RegisterBlock@H$00$03@2@@gemmlowp@@YA?AU?$RegisterBlock@H$03$03@0@AEBU10@AEBU?$RegisterBlock@H$00$03@0@@Z
        352  15F 00097B90 ??$BroadcastAdd@U?$RegisterBlock@H$03$03@gemmlowp@@U?$RegisterBlock@H$03$00@2@@gemmlowp@@YA?AU?$RegisterBlock@H$03$03@0@AEBU10@AEBU?$RegisterBlock@H$03$00@0@@Z
        353  160 00097BD0 ??$BroadcastAdd@U?$RegisterBlock@H$07$00@gemmlowp@@U12@@gemmlowp@@YA?AU?$RegisterBlock@H$07$00@0@AEBU10@0@Z
        354  161 00097C00 ??$BroadcastAdd@U?$RegisterBlock@H$07$00@gemmlowp@@U?$RegisterBlock@H$00$00@2@@gemmlowp@@YA?AU?$RegisterBlock@H$07$00@0@AEBU10@AEBU?$RegisterBlock@H$00$00@0@@Z
        355  162 00097C30 ??$BroadcastAdd@U?$RegisterBlock@H$07$03@gemmlowp@@U?$RegisterBlock@H$00$03@2@@gemmlowp@@YA?AU?$RegisterBlock@H$07$03@0@AEBU10@AEBU?$RegisterBlock@H$00$03@0@@Z
        356  163 00097C80 ??$BroadcastAdd@U?$RegisterBlock@H$07$03@gemmlowp@@U?$RegisterBlock@H$07$00@2@@gemmlowp@@YA?AU?$RegisterBlock@H$07$03@0@AEBU10@AEBU?$RegisterBlock@H$07$00@0@@Z
        357  164 00065FD0 ??$BroadcastAddRecursiveDimensions@C@reference_integer_ops@tflite@@YAXAEBUArithmeticParams@1@HPEA_K11111PEBC2PEACP6AX0@ZP6ACCC0@Z@Z
        358  165 00066260 ??$BroadcastAddRecursiveDimensions@E@reference_ops@tflite@@YAXAEBUArithmeticParams@1@HPEA_K11111PEBE2PEAE@Z
        359  166 000665C0 ??$BroadcastAddRecursiveDimensions@F@reference_ops@tflite@@YAXAEBUArithmeticParams@1@HPEA_K11111PEBF2PEAF@Z
        360  167 00066920 ??$BroadcastAddRecursiveDimensions@F@reference_ops@tflite@@YAXHPEA_K00000FFPEBF1PEAF@Z
        361  168 00066BB0 ??$BroadcastAddRecursiveDimensions@H@reference_ops@tflite@@YAXHPEA_K00000HHPEBH1PEAH@Z
        362  169 00066E20 ??$BroadcastAddRecursiveDimensions@M@reference_ops@tflite@@YAXHPEA_K00000MMPEBM1PEAM@Z
        363  16A 000671E0 ??$BroadcastAddRecursiveDimensions@_J@reference_ops@tflite@@YAXHPEA_K00000_J1PEB_J2PEA_J@Z
        364  16B 0008CBA0 ??$BroadcastArgs@H@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBH010PEAH@Z
        365  16C 0008CC80 ??$BroadcastArgs@_J@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEB_J010PEA_J@Z
        366  16D 00067460 ??$BroadcastBinaryFunction4DSlow@C@reference_integer_ops@tflite@@YAXAEBUArithmeticParams@1@AEBVRuntimeShape@1@PEBC121PEACP6AX0@ZP6ACCC0@Z@Z
        367  16E 0008BC00 ??$BroadcastBinaryFunction4DSlow@CCC@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBC010PEACP6ACCC@Z@Z
        368  16F 0008BC00 ??$BroadcastBinaryFunction4DSlow@EEE@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBE010PEAEP6AEEE@Z@Z
        369  170 0008BEC0 ??$BroadcastBinaryFunction4DSlow@FFF@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBF010PEAFP6AFFF@Z@Z
        370  171 0008BEC0 ??$BroadcastBinaryFunction4DSlow@GGG@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBG010PEAGP6AGGG@Z@Z
        371  172 0008C190 ??$BroadcastBinaryFunction4DSlow@HHH@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBH010PEAHP6AHHH@Z@Z
        372  173 0008C190 ??$BroadcastBinaryFunction4DSlow@III@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBI010PEAIP6AIII@Z@Z
        373  174 0004FB00 ??$BroadcastBinaryFunction4DSlow@MMM@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEBM010PEAMP6AMMM@Z@Z
        374  175 0011DF40 ??$BroadcastBinaryFunction4DSlow@_J_J_J@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEB_J010PEA_JP6A_J_J3@Z@Z
        375  176 0008BC00 ??$BroadcastBinaryFunction4DSlow@_N_N_N@reference_ops@tflite@@YAXAEBVRuntimeShape@1@PEB_N010PEA_NP6A_N_N3@Z@Z
        376  177 00067460 ??$BroadcastBinaryFunction6DSlow@C@reference_integer_ops@tflite@@YAXAEBUArithmeticParams@1@AEBVRuntimeShape@1@PEBC121PEACP6AX0@ZP6ACCC0@Z@Z
        377  178 00266650 ??$BroadcastComparison4DSlowImpl@M$1??$EqualFn@M@reference_ops@tflite@@YA_NMM@Z@reference_ops@tflite@@YAXAEBUComparisonParams@1@AEBVRuntimeShape@1@PEBM121PEA_N@Z
        378  179 00266840 ??$BroadcastComparison4DSlowImpl@M$1??
```",1
New version of nVidia Windows GPU driver (576+) causing TF in WSL process to crash.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.19.0

### Custom code

No

### OS platform and distribution

WSL Ubuntu 24.04.1

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

any 12.x

### GPU model and memory

rtx 4090

### Current behavior?

With GPU driver 576.02, this happens:

If you set the environment variable with 
`os.environ[""CUDA_VISIBLE_DEVICES""] = """"`
to disable the CUDA GPU.
TensorFlow will fail with a ""free(): double free detected in tcache 2"" if you check GPU availability with 
`physical_devices = tf.config.list_physical_devices('GPU')`
and the whole process will crash (leading to a Jupyter kernel death).

Drivers of version 572.83 and lower work fine.
Tested with TF 2.18 and 2.19, both have the same behavior.
Also, tested with CUDA 12.5 and 12.6. Same behaviors, too.

On native Linux Ubuntu 24.04.1 with nvidia-drives-570-server, everything is fine.

### Standalone code to reproduce the issue

```shell
import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ[""CUDA_VISIBLE_DEVICES""] = """"

import tensorflow as tf

try:
    physical_devices = tf.config.list_physical_devices('GPU')
    print(physical_devices)
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
except:
    print(""gpu not found, gpu init error!"")
```

### Relevant log output

```shell
free(): double free detected in tcache 2

crash (core dumped)
```",2
Muting Tensorflow Lite logs,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.16

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

Android and iOS

### Python version

_No response_

### Bazel version

1.20

### GCC/compiler version

AppleClang 16

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

We would like to silence all logging from the TFLite binaries we compiled locally following the documentation found here https://ai.google.dev/edge/litert/build/android and here https://ai.google.dev/edge/litert/build/ios. For the Android build we extracted the .so binary from the .aar, and for the iOS build we built the TensorflowLiteC Static Framework. As it stands right now, we are unable to silence logs. 

I have tried implementing a definition in CMake for our TF module target
`target_compile_definitions(my_tf_app PRIVATE TF_CPP_MIN_LOG_LEVEL=4)`

I have tried setting the environment variable at runtime
`setenv(""TF_CPP_MIN_LOG_LEVEL"", ""4"", /*overwrite=*/1);`

And I have tried calling the logging API
`tflite::logging_internal::MinimalLogger::SetMinimumLogSeverity(
      tflite::logging_internal::LogSeverity::TFLITE_LOG_SILENT);`
Including the header
`#include ""tensorflow/lite/minimal_logging.h""`
But I am met with compilation issues that the symbols for `SetMinimumLogSeverity` are missing.

Employing all of these methods hasn't yielded silence from TFLite. How can I go about resolving this?


### Standalone code to reproduce the issue

```shell
See above.
```

### Relevant log output

```shell

```",1
GPU kernel for `tf.linalg.eig` / `tf.linalg.eigvals`,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 24.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuda 12.6 cudnn 9

### GPU model and memory

A5000 24GB

### Current behavior?

I've tried to distribute my computation bottleneck (large batches of diagonalization of non-Hermitian matrices) over all available cpu & gpu, and it turns out the distributed computation is even slower. Further check reveals that the `Eig` operation submitted to GPU automatically fallbacks to CPU.
I tested the `eig()` of `numpy`, `tensorflow`, `torch`, and `jax` (CuPy seems only support `eigh` and not `eig`), on both CPU and GPU (if applicable), among all tensorflow's eig runs the fastest on my machine, however only on CPU. Therefore I'm really interested in a fast eig that can run on GPU from tensorflow.

### Standalone code to reproduce the issue

```python
import numpy as np
import concurrent.futures as cf
import logging
from numpy.typing import DTypeLike
from typing import Sequence, Tuple, Union, Optional, Any

DeviceList  = Union[str, Sequence[str], None]
WeightList  = Optional[Sequence[float]]

def eig_batch_(array_of_matrices,
              devices : DeviceList = None,
              weights : WeightList = None,
              is_hermitian : bool = False,
              chop        : bool = False,
              dtype       : DTypeLike = None,
              ) -> Tuple[np.ndarray, np.ndarray]:
    """"""
    Batched eigen-decomposition with automatic backend *and* multi-device support.

    Preferred order of back-ends:
        1. TensorFlow
        2. PyTorch
        3. NumPy          (CPU fallback)

    Parameters
    ----------
    array_of_matrices : array-like, shape (..., N, N)
        Real or complex square matrices.
    devices : str | list[str] | None
        • str  -> single device string ('cpu', '/GPU:0', 'cuda:1', …)  
        • list -> multiple devices, will be used in parallel  
        • None -> auto-detect first GPU else CPU
    is_hermitian, chop : as before.

    Returns
    -------
    eigvals_np : np.ndarray, shape (..., N)
    eigvecs_np : np.ndarray, shape (..., N, N)
    """"""
    # ------------------------- 1. choose backend ----------------------------- #
    backend = None
    try:
        import tensorflow as tf
        backend = ""tf""
    except Exception:
        try:
            import torch
            backend = ""torch""
            logging.warning(""Tensorflow not available - using Torch."")
        except Exception:
            backend = ""numpy""
            logging.warning(""Tensorflow/Torch not available - using NumPy on CPU."")

    # ------------------------- 2. early exit for NumPy ----------------------- #
    if backend == ""numpy"":
        return _eig_numpy(array_of_matrices, is_hermitian, chop)

    # ------------------------- 3. normalise device list ---------------------- #
    if backend == ""tf"":
        import tensorflow as tf
        avail = [
            f""/{kind}:{i}""
            for kind in (""CPU"", ""GPU"")
            for i, _ in enumerate(tf.config.list_logical_devices(kind))
        ]
    else:  # torch
        import torch
        avail = [""cpu""] + [f""cuda:{i}"" 
                           for i in range(torch.cuda.device_count())]

    print(f""Input devices: {devices}"")
    if devices is None:
        devices = avail
    elif isinstance(devices, str):
        devices = [devices]
    else:
        devices = [d for d in devices if d in avail]
        if not devices:
            devices = avail
    print(f""Using devices: {devices}"")
    num_dev = len(devices)
    
    if weights is None:
        weights = np.ones(num_dev, dtype=float)
    else:
        if len(weights) != num_dev:
            raise ValueError(""len(weights) must equal num_dev"")
        if sum(weights) <= 0:
            raise ValueError(""weights must sum to a positive value"")
    
    # ------------------------- 4. flatten & shard ---------------------------- #
    A = np.asarray(array_of_matrices, dtype)
    batch = A.shape[:-2]
    m = A.shape[-1]
    n = int(np.prod(batch))
    flat_A = A.reshape(n, m, m)

    # Heuristic: if data size ≤ 1e7 elements, just run on the first device   
    if flat_A.size <= 10_000_000 or num_dev == 1:
        vals, vecs = _eig_on_device(flat_A, backend, devices[0],
                                    is_hermitian, chop)
    # Handling small batches
    elif n <= num_dev:
        def worker(i):
            return _eig_on_device(flat_A[i], backend, devices[i],
                                is_hermitian, chop)
        with cf.ThreadPoolExecutor(max_workers=n) as pool:
            parts = list(pool.map(worker, range(n)))
        vals, vecs = map(np.concatenate, zip(*parts))
    else:
        w = np.asarray(weights, dtype=float)
        w /= w.sum()
        chunk_sizes = np.floor(w * n).astype(int)
        # give leftover matrices (≤ num_dev) to the earliest devices
        remainder = n - chunk_sizes.sum()
        chunk_sizes[:remainder] += 1
        print(f""Chunk sizes: {chunk_sizes}"")
        offsets = np.concatenate(([0], np.cumsum(chunk_sizes)))
        
        def worker(i_dev):
            sl = slice(offsets[i_dev], offsets[i_dev+1])
            return _eig_on_device(flat_A[sl], backend, devices[i_dev],
                                is_hermitian, chop)
        
        with cf.ThreadPoolExecutor(max_workers=num_dev) as pool:
            parts = list(pool.map(worker, range(num_dev)))
        vals, vecs = map(np.concatenate, zip(*parts))
    
    return (vals.reshape(*batch, m),
            vecs.reshape(*batch, m, m))

def _eig_numpy(arr, is_hermitian, chop):
    tol = 1e-14 if arr.dtype in (np.float64, np.complex128) else 1e-7
    if not is_hermitian and chop:
        arr = np.where(np.abs(arr) < tol, 0.0, arr)
    vals, vecs = (np.linalg.eigh(arr) if is_hermitian
                  else np.linalg.eig(arr))
    return vals, vecs

def _eig_on_device(subarray, backend, device, is_hermitian, chop):
    """"""Run eigen-decomp on one backend/device, return NumPy arrays.""""""
    tol64, tol32 = 1e-14, 1e-7

    if backend == ""tf"":
        import tensorflow as tf
        with tf.device(device):
            T = tf.convert_to_tensor(subarray)
            if is_hermitian:
                vals, vecs = tf.linalg.eigh(T)
            else:
                if chop:
                    tol = tol64 if T.dtype in (tf.float64, tf.complex128) else tol32
                    T = tf.where(tf.abs(T) < tol, 0., T)
                vals, vecs = tf.linalg.eig(T)
        return vals.numpy(), vecs.numpy()

    else:   # torch
        import torch
        dev = torch.device(device)
        X = torch.as_tensor(subarray, device=dev)
        with torch.no_grad():
            if is_hermitian:
                vals, vecs = torch.linalg.eigh(X)
            else:
                if chop:
                    tol = tol64 if X.dtype in (torch.float64,
                                               torch.complex128) else tol32
                    X = torch.where(X.abs() < tol, torch.zeros_like(X), X)
                vals, vecs = torch.linalg.eig(X)
        return vals.cpu().numpy(), vecs.cpu().numpy()
    
import tensorflow as tf
tf.debugging.set_log_device_placement(True)

arr = np.random.rand(64, 64, 128, 128)

# prerun for compilation
eig_batch_(arr, is_hermitian=False, chop=True)
eig_batch_(arr, is_hermitian=False, chop=True, dtype=np.float32)

import time

t0 = time.perf_counter()
eig_batch_(arr, is_hermitian=False, chop=True)
print(f""Time taken: {time.perf_counter() - t0:.2f} seconds"")

t0 = time.perf_counter()
eig_batch_(arr, devices=['/CPU:0'],
is_hermitian=False, chop=True)
print(f""Time taken: {time.perf_counter() - t0:.2f} seconds"")

t0 = time.perf_counter()
eig_batch_(arr, devices=['/GPU:0'],
is_hermitian=False, chop=True)
print(f""Time taken: {time.perf_counter() - t0:.2f} seconds"")

t0 = time.perf_counter()
eig_batch_(arr, is_hermitian=False, chop=True, dtype=np.float32)
print(f""Time taken: {time.perf_counter() - t0:.2f} seconds"")

t0 = time.perf_counter()
eig_batch_(arr, devices=['/CPU:0'],
is_hermitian=False, chop=True, dtype=np.float32)
print(f""Time taken: {time.perf_counter() - t0:.2f} seconds"")

t0 = time.perf_counter()
eig_batch_(arr, devices=['/GPU:0'],
is_hermitian=False, chop=True, dtype=np.float32)
print(f""Time taken: {time.perf_counter() - t0:.2f} seconds"")
```

### Relevant log output

```shell
...
Using devices: ['/CPU:0', '/GPU:0', '/GPU:1']
Chunk sizes: [1366 1365 1365]
2025-04-25 17:35:45.253376: I tensorflow/core/common_runtime/eager/execute.cc:1746] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
2025-04-25 17:35:45.257518: I tensorflow/core/common_runtime/eager/execute.cc:1746] Executing op Abs in device /job:localhost/replica:0/task:0/device:CPU:0
2025-04-25 17:35:45.257632: I tensorflow/core/common_runtime/eager/execute.cc:1746] Executing op Abs in device /job:localhost/replica:0/task:0/device:GPU:0
2025-04-25 17:35:45.274676: I tensorflow/core/common_runtime/eager/execute.cc:1746] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:1
2025-04-25 17:35:45.275421: I tensorflow/core/common_runtime/eager/execute.cc:1746] Executing op Less in device /job:localhost/replica:0/task:0/device:CPU:0
2025-04-25 17:35:45.276583: I tensorflow/core/common_runtime/eager/execute.cc:1746] Executing op Abs in device /job:localhost/replica:0/task:0/device:GPU:1
2025-04-25 17:35:45.276877: I tensorflow/core/common_runtime/eager/execute.cc:1746] Executing op Less in device /job:localhost/replica:0/task:0/device:GPU:1
2025-04-25 17:35:45.277057: I tensorflow/core/common_runtime/eager/execute.cc:1746] Executing op SelectV2 in device /job:localhost/replica:0/task:0/device:GPU:1
2025-04-25 17:35:45.277800: I tensorflow/core/common_runtime/eager/execute.cc:1746] Executing op Less in device /job:localhost/replica:0/task:0/device:GPU:0
2025-04-25 17:35:45.278058: I tensorflow/core/common_runtime/eager/execute.cc:1746] Executing op SelectV2 in device /job:localhost/replica:0/task:0/device:GPU:0
2025-04-25 17:35:45.278145: I tensorflow/core/common_runtime/eager/execute.cc:1746] Executing op SelectV2 in device /job:localhost/replica:0/task:0/device:CPU:0
2025-04-25 17:35:45.284307: I tensorflow/core/common_runtime/eager/execute.cc:1746] Executing op Eig in device /job:localhost/replica:0/task:0/device:CPU:0
2025-04-25 17:35:45.291457: I tensorflow/core/common_runtime/eager/execute.cc:1746] Executing op Eig in device /job:localhost/replica:0/task:0/device:CPU:0
2025-04-25 17:35:45.296938: I tensorflow/core/common_runtime/eager/execute.cc:1746] Executing op Eig in device /job:localhost/replica:0/task:0/device:CPU:0
Time taken: 3.01 seconds
...
```",1
TensorFlow Lite Model Maker link is broken in this codelab,"https://codelabs.developers.google.com/codelabs/recognize-flowers-with-tensorflow-on-android/#0

The broken link in this codelab https://ai.google.dev/edge/litert/models/modify/model_maker/image_classification",0
Impossible to free GPU memory used by rank 0 tensors,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.15.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.5 LTS

### Mobile device

_No response_

### Python version

3.11.0rc1

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.4

### GPU model and memory

_No response_

### Current behavior?

Whenever a tensor of rank 0 is created, it's impossible to be removed from GPU memory.
I tried also tf.keras.backend.clear_session() with no success.

The code I present is just one case. The same issues happens whenever a rank 0 tensor is created as a collateral (i.e. in tf.zeros,, when building the learning rate in an optimizer from a numerical value, or, probably,, when creating the variable optimizer._iterations)

### Standalone code to reproduce the issue

```shell
This is one example of code that creates the issue. I refer to it as code 1 in the relevant log output section:
import gc
import tensorflow as tf
# I create a rank 0 tensor
test=tf.cast(1.0,tf.float32)
del test
gc.collect()
# I force an oom to see the memory dump (any suggestion to a more graceful way to get the same output?)
crash = tf.ones([1000, 1000, 1000, 1000, 1000], tf.float32)


The following is the code I ran to check how the clean memory looks like (any suggestions for a cleaner way to get the same output?). I refer to this as code 2, in the relevant log output section:
import tensorflow as tf
# I force an oom to see the memory dump (any suggestion to a more graceful way to get the same output?)
crash = tf.ones([1000, 1000, 1000, 1000, 1000], tf.float32)
```

### Relevant log output

```shell
The following is the dump from code 1. Note the extra allocation for size 4, compared to the subsequent dump from code 2:
2025-04-24 03:26:32.437963: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:63] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2025-04-24 03:26:32.437965: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4, 2
2025-04-24 03:26:32.437967: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1028, 1
2025-04-24 03:26:32.437970: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:97] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 33554432



The following is a clean tensorflow dump, from code 2:
2025-04-24 03:23:41.062128: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4, 1
2025-04-24 03:23:41.062129: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1028, 1
2025-04-24 03:23:41.062132: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:97] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 33554432
```",2
[TFLite] GPU delegation issues with Vision Transformer (ViT-base) in TFLite: dynamic shape & op support problems,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.20

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

Android 13

### Python version

3.9

### Bazel version

6.1

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hi TensorFlow Lite team,

I'm currently attempting to delegate a Vision Transformer model (ViT-base, https://huggingface.co/google/vit-base-patch16-224) to the GPU using TFLite, but I’ve encountered several issues related to dynamic shapes and operator support that are blocking the GPU delegation.

When attempting GPU delegation, the process **fails during the Subgraph::PrepareOpsStartingAt() step**.
(i used max_delegate_partition to 1000 for maximum delegation)

Specifically, the Reshape operator  prepare returns **kTfLiteOutputShapeNotKnown** during its op_prepare() phase.

Structurally, ViT models have fixed input shapes (e.g., 224x224x3), so most of the downstream shapes should be statically determined.
Q1: Why does TFLite treat the output shape as unknown in this context?

I also tried modifying the delegate partitioning to only allow compatible ops for GPU delegation.
Q2: Is it possible (and safe) to manually restrict GPU delegation to only shape-safe operators to avoid this issue? What would be the best way to do that? Would there be any consistency issues across CPU-GPU boundaries?

As an experiment, I forcibly bypassed the kTfLiteOutputShapeNotKnown return (i.e., returning kTfLiteOk unconditionally), and allowed a large max_delegate_partitions (e.g., 1000). This allowed delegation to proceed further, but it failed later in the Multi-Head Attention block—specifically on the fully connected (FC) layer. Specifically, at delegation/gpu/common/model_builder.cc/    
   FullyConnectedOperationParser(), at line 1351
    if (input->tensor.shape.c != attr.weights.shape.i) {
      return absl::UnimplementedError(
          ""Amount of input channels should match weights width"");
    }


Q3: Why would a standard FC layer be incompatible with GPU delegation? Is this expected? Is there any workaround or support planned?

Final Question
Is there a straightforward way to make ViT models more compatible with GPU delegation in TFLite (e.g., rewriting the model, using certain converters/options)? Or would it require modifying TFLite source code (e.g., how it handles dynamic shapes or op registration)?

I’d really appreciate any advice or pointers. Thank you!






### Standalone code to reproduce the issue

```shell
just benchmark code
```

### Relevant log output

```shell
INFO: Created TensorFlow Lite delegate for GPU.
INFO: GPU delegate created.
WARNING: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors (tensor#-1 is a dynamic-sized tensor).
ERROR: Failed to apply GPU delegate.
ERROR: Benchmarking failed.
--------- this is the first error from RESHAPE operator --------
```",1
The default value for skip_gradients_aggregation (False) of optimizers prevents freeing memory,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.15.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.5 LTS

### Mobile device

_No response_

### Python version

3.11.0rc1

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.4

### GPU model and memory

_No response_

### Current behavior?

I have a tf.function that performs a custom gradient descent.
If gradients aggregation is used in optimizer.apply_gradients, then it's impossible to clear the GPU memory from the variable on which the gradient descent is performed, and from the auxiliary variables created by the optimizer.

I tried everything I could find, including tf.keras.backend.clear_session()

### Standalone code to reproduce the issue

```shell
# Minimal example
class MinimalExample(tf.Module):
    def __init__(self):
        super().__init__()
        # Big variable, so it's noticeable
        self.var_v_f=tf.Variable(tf.zeros([10000000]))
        self.optimizer = tf.keras.optimizers.Adam(0.4)

    @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.float32)])
    def __call__(self, a):
        # Just running a gradient descent step from 0 towards a
        self.var_v_f.assign(tf.zeros_like(self.var_v_f))
        with tf.GradientTape() as tape:
            loss_f=tf.reduce_mean(tf.square(self.var_v_f-a))

        gradients_tuple_f = tape.gradient(loss_f, [self.var_v_f])

        # This is the line tha causes the problem
        self.optimizer.apply_gradients(zip(gradients_tuple_f, [self.var_v_f]))

        # If the following is used, instead of the above, it works
        # self.optimizer.apply_gradients(zip(gradients_tuple_f,[self.var_v_f]),skip_gradients_aggregation=True)
        return tf.identity(self.var_v_f)

# I run the function once so that the graph is built
minimal_example=MinimalExample()
minimal_example(1.0)

# Commodity code to save and load the model: it has the same behavior as running it directly
# tf.saved_model.save(minimal_example,'/tmp/minimal_example')
# minimal_example=tf.saved_model.load('/tmp/minimal_example')

del minimal_example

# necessary to clean the memory
gc.collect()

# I forcefully cause an OOM to see the memory dump (any suggestions to get the same output this more gracefully?)
crash = tf.ones([1000, 1000, 1000, 1000, 1000], tf.float32)
```

### Relevant log output

```shell
Momory dump with skip_gradients_aggregation=False (the default)
2025-04-24 03:05:27.667540: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:63] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2025-04-24 03:05:27.667542: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4, 5
2025-04-24 03:05:27.667544: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 8, 2
2025-04-24 03:05:27.667546: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1028, 1
2025-04-24 03:05:27.667548: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 40000000, 3
2025-04-24 03:05:27.667551: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:97] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 301989888

Memory dump with skip_gradients_aggregation=True:
2025-04-24 03:07:49.727901: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:63] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2025-04-24 03:07:49.727903: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4, 5
2025-04-24 03:07:49.727905: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 8, 1
2025-04-24 03:07:49.727907: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1028, 1
2025-04-24 03:07:49.727910: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:97] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 301989888

In the second case the big variable was deleted
```",2
Tensor Flow not working as expected,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.17.0

### Custom code

No

### OS platform and distribution

elementary OS 8.0 (ubuntu 24.04)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

My issue is similar to this: https://github.com/tensorflow/tensorflow/issues/79798
 However the solution of setting environment variable `ROCM_PATH` does not work for me. 

Specifying the GPU with `ROCR_VISIBLE_DEVICES` environment variable also doesn't work. 

I'm using this guide to install and test tensorflow : https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/native_linux/install-tensorflow.html

in the code, tensortest.py consists of the following: 

```
import tensorflow as tf
print(""TensorFlow version:"", tf.__version__)
mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
model = tf.keras.models.Sequential([ tf.keras.layers.Flatten(input_shape=(28, 28)), tf.keras.layers.Dense(128, activation='relu'), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(10)])
predictions = model(x_train[:1]).numpy()
tf.nn.softmax(predictions).numpy()
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
loss_fn(y_train[:1], predictions).numpy()
model.compile(optimizer='adam',loss=loss_fn, metrics=['accuracy'])
model.fit(x_train, y_train, epochs=5)
model.evaluate(x_test,  y_test, verbose=2)

```

### Standalone code to reproduce the issue

```shell
$ echo $ROCM_PATH

$ python3 tensortest.py
```

### Relevant log output

```shell
/opt/rocm-6.3.4

2025-04-21 20:38:06.386624: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
TensorFlow version: 2.17.0
/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
2025-04-21 20:38:07.657771: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-04-21 20:38:07.657809: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-04-21 20:38:09.689474: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-04-21 20:38:09.689515: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-04-21 20:38:09.689534: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-04-21 20:38:09.689552: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-04-21 20:38:09.690591: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-04-21 20:38:09.690618: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-04-21 20:38:09.690657: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-04-21 20:38:09.690674: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-04-21 20:38:09.690691: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-04-21 20:38:09.690707: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-04-21 20:38:09.690738: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-04-21 20:38:09.690757: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-04-21 20:38:09.690776: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-04-21 20:38:09.690795: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-04-21 20:38:09.690815: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-04-21 20:38:09.690827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15310 MB memory:  -> device: 0, name: Radeon RX 7900 GRE, pci bus id: 0000:03:00.0
2025-04-21 20:38:09.836462: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2025-04-21 20:38:09.836497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7660 MB memory:  -> device: 1, name: Radeon RX Vega, pci bus id: 0000:19:00.0
error: Failure when generating HSACO
error: Failure when generating HSACO
error: Failure when generating HSACO
error: Failure when generating HSACO
error: Failure when generating HSACO
error: Failure when generating HSACO
error: Failure when generating HSACO
error: Failure when generating HSACO
error: Failure when generating HSACO
error: Failure when generating HSACO
error: Failure when generating HSACO
error: Failure when generating HSACO
error: Failure when generating HSACO
2025-04-21 20:38:11.034032: E tensorflow/compiler/mlir/tools/kernel_gen/tf_framework_c_interface.cc:228] INTERNAL: Generating device code failed.
2025-04-21 20:38:11.034934: W tensorflow/core/framework/op_kernel.cc:1828] UNKNOWN: JIT compilation failed.
2025-04-21 20:38:11.034952: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: UNKNOWN: JIT compilation failed.
Traceback (most recent call last):
  File ""/home/praful/tensortest.py"", line 6, in <module>
    model = tf.keras.models.Sequential([ tf.keras.layers.Flatten(input_shape=(28, 28)), tf.keras.layers.Dense(128, activation='relu'), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(10)])
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/keras/src/models/sequential.py"", line 76, in __init__
    self._maybe_rebuild()
  File ""/usr/local/lib/python3.12/dist-packages/keras/src/models/sequential.py"", line 149, in _maybe_rebuild
    self.build(input_shape)
  File ""/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py"", line 230, in build_wrapper
    original_build_method(*args, **kwargs)
  File ""/usr/local/lib/python3.12/dist-packages/keras/src/models/sequential.py"", line 195, in build
    x = layer(x)
        ^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/random.py"", line 19, in _cast_seed
    seed = tf.cast(tf.math.floormod(seed, tf.int32.max - 1), dtype=""int32"")
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.UnknownError: {{function_node __wrapped__FloorMod_device_/job:localhost/replica:0/task:0/device:GPU:0}} JIT compilation failed. [Op:FloorMod] name: 

```",1
Out of memory when saving a tf.data.Dataset,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.19.0 (and 2.18.1)

### Custom code

No

### OS platform and distribution

Linux, Colab

### Mobile device

_No response_

### Python version

3.11 (Colab) and 3.12 (local)

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

None (CPU), 12.2 (local), 12.4 (Colab)

### GPU model and memory

_No response_

### Current behavior?

I want to use a keras_hub model to preprocess a dataset. This process is expensive, so I want to save the resulting dataset.
As everything else in my codebase relies on TensorFlow and Keras, it seems like using `tf.data.Dataset` should be the best option.
But, when saving the dataset, the process crashes with OOM.

1. Load a dataset with `tfds`.
2. Preprocess the dataset (with the `.map()` functionality
3. Call `ds.save() ` to save the dataset to disk <-- here, it crashes.

This works for smaller datasets. In the example code, use only 5000 elements (`.take(5000)` ), and the dataset is stored as intended.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1W-1vk3-TBnybhCdVt3Y2Y7itLpDAhMGZ?usp=sharing
```

### Relevant log output

```shell

```",2
TFLITE Memory Issue,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

org.tensorflow:tensorflow-lite-gpu-api:2.12.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

GPU and Graphic memory

### Current behavior?

package expo.modules.threesixtyplayer.aistillphotos

import android.content.Context
import android.graphics.Bitmap
import android.os.SystemClock
import android.view.Surface
import com.google.firebase.crashlytics.FirebaseCrashlytics
import expo.modules.threesixtyplayer.utils.Utils
import org.tensorflow.lite.gpu.CompatibilityList
import org.tensorflow.lite.support.image.ImageProcessor
import org.tensorflow.lite.support.image.TensorImage
import org.tensorflow.lite.task.core.BaseOptions
import org.tensorflow.lite.task.core.vision.ImageProcessingOptions
import org.tensorflow.lite.task.vision.classifier.Classifications
import org.tensorflow.lite.task.vision.classifier.ImageClassifier

/**
 * Venky on 28-Oct-2024
 * Modified by Venky on 11-Dec-2024
 * Modified by Venky on 16-Dec-2024
 */
class ImageClassifierHelper(
    private var threshold: Float = 0.5f,
    private var numThreads: Int = 4,
    private var maxResults: Int = 1,
    private var currentDelegate: Int = 0,
    private val context: Context,
    private val imageClassifierListener: ClassifierListener?
) {
    private var imageClassifier: ImageClassifier? = null

    init {
        setupImageClassifier()
    }

    fun clearImageClassifier() {
//        try {
//            Thread.sleep(100)
//            imageClassifier?.close()
//
//        } catch (rethrown: RuntimeException) {
//
//        }finally {
//            imageClassifier = null
//
//        }
        imageClassifier = null
    }


    private fun setupImageClassifier() {
        imageClassifier?.close()
        imageClassifier = null
//        classifierScope.launch {
        val optionsBuilder = ImageClassifier.ImageClassifierOptions.builder()
            .setScoreThreshold(threshold)
            .setMaxResults(maxResults)

        val baseOptionsBuilder = BaseOptions.builder()

        when (currentDelegate) {
            DELEGATE_CPU -> {
                // Default
                baseOptionsBuilder.setNumThreads(numThreads)

            }

            DELEGATE_GPU -> {
                val compatList = CompatibilityList()
                if (compatList.isDelegateSupportedOnThisDevice) {
                    // if the device has a supported GPU, add the GPU delegate
                    val delegateOptions = compatList.bestOptionsForThisDevice
                    baseOptionsBuilder.useGpu()
                    Utils.readLog(TAG, ""Using GPU delegate with options: $delegateOptions"")
                } else {
                    // if the GPU is not supported, run on 4 threads
                    baseOptionsBuilder.setNumThreads(4)
                }
            }

            DELEGATE_NNAPI -> {
                baseOptionsBuilder.setNumThreads(numThreads).useNnapi()
            }
        }

        optionsBuilder.setBaseOptions(baseOptionsBuilder.build())

        val modelName = ""reshaped_model_optimized_graph.tflite""

        try {
            imageClassifier =
                ImageClassifier.createFromFileAndOptions(
                    context,
                    modelName,
                    optionsBuilder.build()
                )
        } catch (e: IllegalStateException) {
            FirebaseCrashlytics.getInstance().recordException(e)
            Utils.readLog(AIStillphotosUtils::class.java.simpleName, e.message)

//            imageClassifierListener?.onError(
//                ""Image classifier failed to initialize. See error logs for details""
//            )
            Utils.readLog(TAG, ""TFLite failed to load model with error: "" + e.message)
        }
//        }
    }

    fun classify(image: Bitmap, rotation: Int) {
        if (imageClassifier == null) {
            setupImageClassifier()
        }

//        classifierScope.launch {
        try {
            // Inference time is the difference between the system time at the start and finish of the
            // process
            var inferenceTime = SystemClock.uptimeMillis()

            // Create preprocessor for the image.
            // See https://www.tensorflow.org/lite/inference_with_metadata/
            //            lite_support#imageprocessor_architecture
            val imageProcessor =
                ImageProcessor.Builder()
                    .build()

            // Preprocess the image and convert it into a TensorImage for classification.
            val tensorImage = imageProcessor.process(TensorImage.fromBitmap(image))

            val imageProcessingOptions = ImageProcessingOptions.builder()
                .setOrientation(getOrientationFromRotation(rotation))
                .build()

            val results = imageClassifier?.classify(tensorImage, imageProcessingOptions)
            inferenceTime = SystemClock.uptimeMillis() - inferenceTime
            imageClassifierListener?.onResults(
                results,
                inferenceTime
            )
        } catch (e: Exception){
            FirebaseCrashlytics.getInstance().recordException(e)
            e.printStackTrace()
            Utils.readLog(TAG,""Error Classify an image : ${e.message}"")
        }
//        }
    }

    // Receive the device rotation (Surface.x values range from 0->3) and return EXIF orientation
    // http://jpegclub.org/exif_orientation.html
    private fun getOrientationFromRotation(rotation: Int) : ImageProcessingOptions.Orientation {
        return when (rotation) {
            Surface.ROTATION_270 ->
                ImageProcessingOptions.Orientation.BOTTOM_RIGHT

            Surface.ROTATION_180 ->
                ImageProcessingOptions.Orientation.RIGHT_BOTTOM

            Surface.ROTATION_90 ->
                ImageProcessingOptions.Orientation.TOP_LEFT

            else ->
                ImageProcessingOptions.Orientation.RIGHT_TOP
        }
    }

    interface ClassifierListener {
        fun onError(error: String)
        fun onResults(
            results: List<Classifications>?,
            inferenceTime: Long
        )
    }

    companion object {
        const val DELEGATE_CPU = 0
        const val DELEGATE_GPU = 1
        const val DELEGATE_NNAPI = 2
        //private val classifierScope = CoroutineScope(newSingleThreadContext(""ClassifierGpuThread""))
        private const val TAG = ""ImageClassifierHelper""
    }
}

### Standalone code to reproduce the issue

```shell
We have a camera app whenever user go to camera preview screen and close and open again CPU and Graphic memory increase rapidly for each action it increase to 300mb this cause random crash in some mobiles.
i want to know how to release the all the resource and clear the cache memory .
```

### Relevant log output

atal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0xb400006e0aba8004 in tid 9504 (pool-39-thread-), pid 8360 (o.frontline_dev)
2025-04-18 19:06:49.124  9584-9584  DEBUG                   crash_dump64                         A  Cmdline: com.xciteauto.frontline_dev
2025-04-18 19:06:49.124  9584-9584  DEBUG                   crash_dump64                         A  pid: 8360, tid: 9504, name: pool-39-thread-  >>> com.xciteauto.frontline_dev <<<
2025-04-18 19:06:49.124  9584-9584  DEBUG                   crash_dump64                         A        #00 pc 00000000004a006c  /data/app/~~hWrWUSL7syYQlxizFmj9lw==/com.xciteauto.frontline_dev-GLR9GyZwTjgMYaOnjR5myA==/base.apk!libtask_vision_jni.so (offset 0xe50c000)
2025-04-18 19:06:49.124  9584-9584  DEBUG                   crash_dump64                         A        #01 pc 0000000000342aac  /data/app/~~hWrWUSL7syYQlxizFmj9lw==/com.xciteauto.frontline_dev-GLR9GyZwTjgMYaOnjR5myA==/base.apk!libtask_vision_jni.so (offset 0xe50c000)
2025-04-18 19:06:49.124  9584-9584  DEBUG                   crash_dump64                         A        #02 pc 0000000000341a38  /data/app/~~hWrWUSL7syYQlxizFmj9lw==/com.xciteauto.frontline_dev-GLR9GyZwTjgMYaOnjR5myA==/base.apk!libtask_vision_jni.so (offset 0xe50c000)
2025-04-18 19:06:49.124  9584-9584  DEBUG                   crash_dump64                         A        #03 pc 0000000000335ac8  /data/app/~~hWrWUSL7syYQlxizFmj9lw==/com.xciteauto.frontline_dev-GLR9GyZwTjgMYaOnjR5myA==/base.apk!libtask_vision_jni.so (offset 0xe50c000)
2025-04-18 19:06:49.124  9584-9584  DEBUG                   crash_dump64                         A        #04 pc 0000000000476d9c  /data/app/~~hWrWUSL7syYQlxizFmj9lw==/com.xciteauto.frontline_dev-GLR9GyZwTjgMYaOnjR5myA==/base.apk!libtask_vision_jni.so (offset 0xe50c000)
2025-04-18 19:06:49.124  9584-9584  DEBUG                   crash_dump64                         A        #05 pc 00000000004791fc  /data/app/~~hWrWUSL7syYQlxizFmj9lw==/com.xciteauto.frontline_dev-GLR9GyZwTjgMYaOnjR5myA==/base.apk!libtask_vision_jni.so (offset 0xe50c000)
2025-04-18 19:06:49.124  9584-9584  DEBUG                   crash_dump64                         A        #06 pc 000000000011b350  /data/app/~~hWrWUSL7syYQlxizFmj9lw==/com.xciteauto.frontline_dev-GLR9GyZwTjgMYaOnjR5myA==/base.apk!libtask_vision_jni.so (offset 0xe50c000)
2025-04-18 19:06:49.124  9584-9584  DEBUG                   crash_dump64                         A        #07 pc 0000000000051dc0  /data/app/~~hWrWUSL7syYQlxizFmj9lw==/com.xciteauto.frontline_dev-GLR9GyZwTjgMYaOnjR5myA==/base.apk!libtask_vision_jni.so (offset 0xe50c000)
2025-04-18 19:06:49.124  9584-9584  DEBUG                   crash_dump64                         A        #08 pc 000000000004a8f0  /data/app/~~hWrWUSL7syYQlxizFmj9lw==/com.xciteauto.frontline_dev-GLR9GyZwTjgMYaOnjR5myA==/base.apk!libtask_vision_jni.so (offset 0xe50c000) (Java_org_tensorflow_lite_task_vision_classifier_ImageClassifier_classifyNative+212)
2025-04-18 19:06:49.124  9584-9584  DEBUG                   crash_dump64                         A        #14 pc 000000000000706c  /data/app/~~hWrWUSL7syYQlxizFmj9lw==/com.xciteauto.frontline_dev-GLR9GyZwTjgMYaOnjR5myA==/base.apk (org.tensorflow.lite.task.vision.classifier.ImageClassifier.classify+0)
2025-04-18 19:06:49.124  9584-9584  DEBUG                   crash_dump64                         A        #20 pc 0000000000006fec  /data/app/~~hWrWUSL7syYQlxizFmj9lw==/com.xciteauto.frontline_dev-GLR9GyZwTjgMYaOnjR5myA==/base.apk (org.tensorflow.lite.task.vision.classifier.ImageClassifier.access$1000+0)
2025-04-18 19:06:49.124  9584-9584  DEBUG                   crash_dump64                         A        #26 pc 0000000000006c28  /data/app/~~hWrWUSL7syYQlxizFmj9lw==/com.xciteauto.frontline_dev-GLR9GyZwTjgMYaOnjR5myA==/base.apk (org.tensorflow.lite.task.vision.classifier.ImageClassifier$4.run+0)
2025-04-18 19:06:49.124  9584-9584  DEBUG                   crash_dump64                         A        #32 pc 0000000000006c0c  /data/app/~~hWrWUSL7syYQlxizFmj9lw==/com.xciteauto.frontline_dev-GLR9GyZwTjgMYaOnjR5myA==/base.apk (org.tensorflow.lite.task.vision.classifier.ImageClassifier$4.run+0)
2025-04-18 19:06:49.124  9584-9584  DEBUG                   crash_dump64                         A        #38 pc 0000000000007690  /data/app/~~hWrWUSL7syYQlxizFmj9lw==/com.xciteauto.frontline_dev-GLR9GyZwTjgMYaOnjR5myA==/base.apk (org.tensorflow.lite.task.vision.core.BaseVisionTaskApi.run+0)
2025-04-18 19:06:49.124  9584-9584  DEBUG                   crash_dump64                         A        #43 pc 000000000000710c  /data/app/~~hWrWUSL7syYQlxizFmj9lw==/com.xciteauto.frontline_dev-GLR9GyZwTjgMYaOnjR5myA==/base.apk (org.tensorflow.lite.task.vision.classifier.ImageClassifier.classify+0)
2025-04-18 19:06:49.124  9584-9584  DEBUG                   crash_dump64                         A        #48 pc 00000000002920a4  /data/data/com.xciteauto.frontline_dev/code_cache/.overlay/base.apk/classes2.dex (expo.modules.threesixtyplayer.aistillphotos.ImageClassifierHelper.classify+0)
2025-04-18 19:06:49.124  9584-9584  DEBUG                   crash_dump64                         A        #53 pc 000000000028ccc4  /data/data/com.xciteauto.frontline_dev/code_cache/.overlay/base.apk/classes2.dex (expo.modules.threesixtyplayer.aistillphotos.AiStillCameraActivity.detectObjects+0)
2025-04-18 19:06:49.124  9584-9584  DEBUG                   crash_dump64                         A        #58 pc 000000000028c668  /data/data/com.xciteauto.frontline_dev/code_cache/.overlay/base.apk/classes2.dex (expo.modules.threesixtyplayer.aistillphotos.AiStillCameraActivity.bindCameraUseCases$lambda$33$lambda$32+0)
2025-04-18 19:06:49.124  9584-9584  DEBUG                   crash_dump64                         A        #63 pc 000000000028bfac  /data/data/com.xciteauto.frontline_dev/code_cache/.overlay/base.apk/classes2.dex (expo.modules.threesixtyplayer.aistillphotos.AiStillCameraActivity.$r8$lambda$_UaM8Z5yKfQ9hcFVZavzt8naD5U+0)
2025-04-18 19:06:49.124  9584-9584  DEBUG                   crash_dump64                         A        #68 pc 0000000000285778  /data/data/com.xciteauto.frontline_dev/code_cache/.overlay/base.apk/classes2.dex (expo.modules.threesixtyplayer.aistillphotos.AiStillCameraActivity$$ExternalSyntheticLambda25.analyze+0)
2025-04-18 19:06:49.125  9584-9584  DEBUG                   crash_dump64                         A        #73 p

```shell

```",2
TensorFlow DLL failed to load with newer version of TF,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.19.0

### Custom code

Yes

### OS platform and distribution

Windows 11 64-bit

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

ImportError: Traceback (most recent call last):
  File ""C:\Users\adity\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 73, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

### Standalone code to reproduce the issue

```shell
NA
```

### Relevant log output

```shell

```",1
Notes on padding missing in documentation,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.16.1 

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The link for the `padding` argument in the [conv1d_transpose](https://www.tensorflow.org/api_docs/python/tf/nn/conv1d_transpose) documentation currently redirects to the top of the [tf.nn documentation page](https://www.tensorflow.org/api_docs/python/tf/nn), instead of the relevant section.

### Expected Behavior

The [link](https://www.tensorflow.org/api_docs/python/tf/nn#notes_on_padding_2) should redirect to the appropriate section that documents the `padding` argument, ideally pointing to the relevant source code for context:

https://github.com/tensorflow/tensorflow/blob/4dbae670e3a5c6e15c2dd9192ee229e56c84a3c7/tensorflow/python/ops/nn_ops.py#L17-L166


### Standalone code to reproduce the issue

```shell
Not applicable.
```

### Relevant log output

```shell

```",1
"The model.save_weights(filename) is bigger than model.save(filename, include_optimizer=False)","### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.19.0-rc0-6-ge36baa30292 2.19.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 24.10

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The model.save_weights(filename) is bigger than model.save(filename, include_optimizer=False).

When saving the weights of the model using the `model.save_weights(filename)` method, I expect that the size of the saved file should be similar or lower to the file saved by using `model.save(filename, include_optimizer=False)`. Because the weights should not include the optimizer states and parameters.

However this is not true, and the size is more similar to the size of the file obtained by `model.save(filename, include_optimizer=True)`.

My suspect is then that `model.save_weights` saves also some optimizer information.
This sound a little bit strange to me, but if so, should be possible to add the `include_optimizer` argument also in the  `model.save_weights` method, in order to save just the network weights?




### Standalone code to reproduce the issue

```shell
# create a rnn model with random data, fit and save it with different options
import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import SimpleRNN, Dense

# Generate random data
np.random.seed(42)
tf.random.set_seed(42)
x_train = np.random.random((100, 10, 5))  # 100 samples, 10 timesteps, 5 features
y_train = np.random.random((100, 1))      # 100 samples, 1 output

# Create a simple RNN model
model = Sequential([
    SimpleRNN(10, activation='relu', input_shape=(10, 5)),
    Dense(1)
])

model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(x_train, y_train, epochs=5, batch_size=10)

# Save the model_1
model.save('rnn_model_1.h5', include_optimizer=True)
# Save the model_2 without the optimizer state
model.save('rnn_model_2.h5', include_optimizer=False)
# Save just the weights
model.save_weights('rnn_model.weights.h5')

# check the size of the model files
model_1_size = os.path.getsize('rnn_model_1.h5')
model_2_size = os.path.getsize('rnn_model_2.h5')
model_weights_size = os.path.getsize('rnn_model.weights.h5')

print(""Model <model.save with include_optimizer=True> size (bytes):"", model_1_size)
print(""Model <model.save with include_optimizer=False> size (bytes):"", model_2_size)
print(""Model <model.save_weights> size (bytes):"", model_weights_size)
```

### Relevant log output

```shell
Model <model.save with include_optimizer=True> size (bytes): 27592
Model <model.save with include_optimizer=False> size (bytes): 18528
Model <model.save_weights> size (bytes): 24592
```",1
Why does the `.whl` file built from source not include the system version?,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.19

### Custom code

No

### OS platform and distribution

macOS 15.4

### Mobile device

None

### Python version

Exists in every version.

### Bazel version

6.5.0

### GCC/compiler version

Apple clang version 17.0.0 (clang-1700.0.13.3)

### CUDA/cuDNN version

None

### GPU model and memory

None

### Current behavior?

The `tensorflow` built from source does not include the system version, for example, `*-macosx_12_0_arm64.whl` it only has `*-macosx_arm64.whl`. You need to manually add the version number in order to install it, otherwise it will report that this `.whl` is not supported on this platform.

### Standalone code to reproduce the issue

```shell
conda create -n tensorflow-macos python=3.10
conda activate tensorflow-macos
./configure # Use all default options.
bazel build //tensorflow/tools/pip_package:wheel
pip install ./bazel-bin/tensorflow/tools/pip_package/wheel_house/*.whl
```

### Relevant log output

```shell
(tensorflow-macos) ➜  tensorflow-2.19.0 bazel build //tensorflow/tools/pip_package:wheel
Starting local Bazel server and connecting to it...
INFO: Reading 'startup' options from /Users/sunruiqi/Desktop/tensorflow-2.19.0/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=80
INFO: Reading rc options for 'build' from /Users/sunruiqi/Desktop/tensorflow-2.19.0/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /Users/sunruiqi/Desktop/tensorflow-2.19.0/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /Users/sunruiqi/Desktop/tensorflow-2.19.0/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/Users/sunruiqi/miniforge3/envs/tensorflow-macos/bin/python3 --action_env PYTHON_LIB_PATH=/Users/sunruiqi/miniforge3/envs/tensorflow-macos/lib/python3.10/site-packages --python_path=/Users/sunruiqi/miniforge3/envs/tensorflow-macos/bin/python3
INFO: Found applicable config definition build:short_logs in file /Users/sunruiqi/Desktop/tensorflow-2.19.0/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /Users/sunruiqi/Desktop/tensorflow-2.19.0/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:macos in file /Users/sunruiqi/Desktop/tensorflow-2.19.0/.bazelrc: --apple_platform_type=macos --copt=-DGRPC_BAZEL_BUILD --features=archive_param_file --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=no_tfrt
INFO: Found applicable config definition build:no_tfrt in file /Users/sunruiqi/Desktop/tensorflow-2.19.0/.bazelrc: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils
DEBUG: /private/var/tmp/_bazel_sunruiqi/3c594619420d75084e8668187493abc9/external/local_tsl/third_party/py/python_repo.bzl:87:10: 
=============================
Hermetic Python configuration:
Version: ""3.10""
Kind: """"
Interpreter: ""default"" (provided by rules_python)
Requirements_lock label: ""@python_version_repo//:requirements_lock_3_10.txt""
=====================================
INFO: Analyzed target //tensorflow/tools/pip_package:wheel (734 packages loaded, 45013 targets configured).
INFO: Found 1 target...
Target //tensorflow/tools/pip_package:wheel up-to-date:
  bazel-bin/tensorflow/tools/pip_package/wheel_house/tensorflow-2.19.0-cp310-cp310-macosx_arm64.whl
INFO: Elapsed time: 4848.673s, Critical Path: 235.42s
INFO: 23752 processes: 5286 internal, 18466 local.
INFO: Build completed successfully, 23752 total actions
(tensorflow-macos) ➜  tensorflow-2.19.0 pip install ./bazel-bin/tensorflow/tools/pip_package/wheel_house/*.whl
ERROR: tensorflow-2.19.0-cp310-cp310-macosx_arm64.whl is not a supported wheel on this platform.
```",1
'OK' is not a member of 'tsl::Status',"```
GPUInstrumentOp.cc: In lambda function: GPUInstrumentOp.cc:12:22: error: 'OK' is not a member of 'tsl::Status' {aka 'absl::lts_20230802::Status'} 
 12 |       return Status::OK();
    |                      ^~ GPUInstrumentOp.cc: At global scope: GPUInstrumentOp.cc:10:16: error: cannot convert '<lambda(tensorflow::shape_inference::InferenceContext*)>' to 'tensorflow::OpShapeInferenceFn' {aka 'std::function<absl::lts_20230802::Status(tensorflow::shape_inference::InferenceContext*)>'} 
 10 |     .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) { In file included from GPUInstrumentOp.cc:1: /usr/local/lib/python3.10/dist-packages/tensorflow/include/tensorflow/core/framework/op.h:272:54: note:   initializing argument 1 of 'tensorflow::register_op::OpDefBuilderWrapper& tensorflow::register_op::OpDefBuilderWrapper::SetShapeFn(tensorflow::OpShapeInferenceFn)'
272 |   OpDefBuilderWrapper& SetShapeFn(OpShapeInferenceFn fn) { 
    |                                   ~~~~~~~~~~~~~~~~~~~^~ 
```

### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.14

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

https://github.com/tensorflow/tensorflow/issues/71568#issue-2400655132
i have the same problem, some solution?
i tried to follow this guide https://www.tensorflow.org/guide/create_op?hl=it, the code only for cpu

### Standalone code to reproduce the issue

```shell

```

### Relevant log output

```shell

```",1
tf.pad (mode='reflect') silent failure with large input arrays,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.1

### Custom code

No

### OS platform and distribution

Red Hat Enterprise Linux 8.8

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I use tf.pad with mode = 'reflect' in some custom layers and noticed that model output can be corrupted, without any error handling, if the input to these layers is too large. If I instead use np.pad in these layers this corruption is not observed. I have not observed a pattern in the type of corruption coming out of the pad call. Sometimes it is constant output sometimes it seems like noise. 

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

arr = np.repeat(np.ones((500, 500))[..., None], 1000, axis=-1)

tf_out = tf.pad(arr, [[100, 100], [100, 100], [0, 0]], mode='reflect')
np_out = np.pad(arr, [[100, 100], [100, 100], [0, 0]], mode='reflect')

print('tf.pad == np.pad:', np.allclose(tf_out[:, :, 0], np_out[:, :, 0])) # true

arr = np.repeat(np.ones((500, 500))[..., None], 5000, axis=-1)

tf_out = tf.pad(arr, [[100, 100], [100, 100], [0, 0]], mode='reflect')
np_out = np.pad(arr, [[100, 100], [100, 100], [0, 0]], mode='reflect')

print('tf.pad == np.pad:', np.allclose(tf_out[:, :, 0], np_out[:, :, 0])) # false
```

### Relevant log output

```shell
None. This is a silent error.
```",1
(deprecated arguments) (deprecated arguments) (deprecated arguments),"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

N/A

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

E.g., https://www.tensorflow.org/api_docs/python/tf/function#features
> Compiles a function into a callable TensorFlow graph. (deprecated arguments) (deprecated arguments) (deprecated arguments)

### Standalone code to reproduce the issue

```shell
see https://www.tensorflow.org/api_docs/python/tf/function#features
```

### Relevant log output

```shell

```",1
`Aborted (core dumped)` in `tf.compat.v1.raw_ops.BlockLSTMGrad` and `tf.raw_ops.BlockLSTMGradV2`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.19.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered abortion issues with `tf.compat.v1.raw_ops.BlockLSTMGrad` and `tf.raw_ops.BlockLSTMGradV2` in tf-2.19.0. This bug can be used to trigger a denial of service attack. I have attached a [BUG 8: tf.compat.v1.raw_ops.BlockLSTMGrad and BUG 9: tf.raw_ops.BlockLSTMGradV2](https://colab.research.google.com/drive/1hIeUZYcxfySEUthZcwrlEQ6EwdXYiFi_?usp=sharing) for your reference.

### Bug Code for `tf.compat.v1.raw_ops.BlockLSTMGrad`

```shell
import os
import tensorflow as tf
import numpy as np
# Make sure any specific optimizations are disabled (if required)
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
seq_len_max = 10  # Using a valid sequence length
x = tf.saturate_cast(tf.random.uniform([10, 2, 16], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)
cs_prev = tf.saturate_cast(tf.random.uniform([2, 0], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)
h_prev = tf.saturate_cast(tf.random.uniform([2, 0], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)
w = tf.saturate_cast(tf.random.uniform([16, 0], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)
wci = tf.saturate_cast(tf.random.uniform([5], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)
wcf = tf.saturate_cast(tf.random.uniform([16], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)
wco = tf.saturate_cast(tf.random.uniform([13], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)
b = tf.saturate_cast(tf.random.uniform([0], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)
# Placeholder tensors for gradients which would also be empty
i = tf.saturate_cast(tf.random.uniform([10, 2, 0], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)
cs = tf.saturate_cast(tf.random.uniform([10, 2, 0], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)
f = tf.saturate_cast(tf.random.uniform([10, 2, 0], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)
o = tf.saturate_cast(tf.random.uniform([10, 2, 0], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)
ci = tf.saturate_cast(tf.random.uniform([10, 2, 0], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)
co = tf.saturate_cast(tf.random.uniform([10, 2, 0], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)
h = tf.saturate_cast(tf.random.uniform([10, 2, 0], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)
cs_grad = tf.saturate_cast(tf.random.uniform([2,0], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)
h_grad = tf.saturate_cast(tf.random.uniform([2,0], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)
# Attempt to call the BlockLSTMGrad op with empty tensors in critical inputs.
res = tf.compat.v1.raw_ops.BlockLSTMGrad(
    seq_len_max=seq_len_max,
    x=x,
    cs_prev=cs_prev,
    h_prev=h_prev,
    w=w,
    wci=wci,
    wcf=wcf,
    wco=wco,
    b=b,
    i=i,
    cs=cs,
    f=f,
    o=o,
    ci=ci,
    co=co,
    h=h,
    cs_grad=cs_grad,
    h_grad=h_grad,
    use_peephole=False
)
```
### Bug Code for `tf.raw_ops.BlockLSTMGradV2`
```shell
import os
import tensorflow as tf
# Make sure any specific optimizations are disabled (if required)
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
seq_len_max = 5
forget_bias = 112.66590343649887
use_peephole = False
# valid batch size and input size for x; other inputs with empty second dim
x = tf.saturate_cast(tf.random.uniform([seq_len_max, 2, 16], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)  # valid shape
cs_prev = tf.saturate_cast(tf.random.uniform([2, 0], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)  # empty second dim
h_prev = tf.saturate_cast(tf.random.uniform([2, 0], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)  # empty second dim
w = tf.saturate_cast(tf.random.uniform([16, 0], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)  # empty second dim
wci = tf.saturate_cast(tf.random.uniform([5], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)  # valid
wcf = tf.saturate_cast(tf.random.uniform([16], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)  # valid
wco = tf.saturate_cast(tf.random.uniform([13], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)  # valid
b = tf.saturate_cast(tf.random.uniform([0], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)  # empty
i = tf.saturate_cast(tf.random.uniform([seq_len_max, 2, 16], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)  # valid
cs = tf.saturate_cast(tf.random.uniform([seq_len_max, 2, 0], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)  # empty
f = tf.saturate_cast(tf.random.uniform([seq_len_max, 2, 16], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)  # valid
o = tf.saturate_cast(tf.random.uniform([seq_len_max, 2, 16], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)  # valid
ci = tf.saturate_cast(tf.random.uniform([seq_len_max, 2, 16], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)  # valid
co = tf.saturate_cast(tf.random.uniform([seq_len_max, 2, 0], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)  # empty
h = tf.saturate_cast(tf.random.uniform([seq_len_max, 2, 16], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)  # valid
cs_grad = tf.saturate_cast(tf.random.uniform([seq_len_max, 2, 16], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)  # valid
h_grad = tf.saturate_cast(tf.random.uniform([seq_len_max, 2, 16], minval=0, maxval=64, dtype=tf.int64), dtype=tf.half)  # valid
# Attempt to call the BlockLSTMGradV2 op with empty tensors in critical inputs.
res = tf.raw_ops.BlockLSTMGradV2(
    seq_len_max=seq_len_max,
    x=x,
    cs_prev=cs_prev,
    h_prev=h_prev,
    w=w,
    wci=wci,
    wcf=wcf,
    wco=wco,
    b=b,
    i=i,
    cs=cs,
    f=f,
    o=o,
    ci=ci,
    co=co,
    h=h,
    cs_grad=cs_grad,
    h_grad=h_grad,
    use_peephole=use_peephole,
)
```

### Relevant log output

```shell
Aborted (core dumped)
```",1
`Aborted (core dumped)` in `tf.raw_ops.DenseCountSparseOutput`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.19.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an abortion issue with tf.raw_ops.DenseCountSparseOutput in tf-2.19.0. This bug can be used to trigger a denial of service attack. I have attached a [BUG 7: tf.raw_ops.DenseCountSparseOutput](https://colab.research.google.com/drive/1hIeUZYcxfySEUthZcwrlEQ6EwdXYiFi_?usp=sharing) for your reference.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
input_size = 5000
values = np.random.randint(0, 2000, size=input_size).astype(np.int64)
weights = np.random.rand(input_size).astype(np.float32)
# Move to tf.Tensor
values_tensor = tf.constant(values)
weights_tensor = tf.constant(weights)
# Compute expected counts manually (using np.bincount)
expected_counts = np.bincount(values, weights=weights, minlength=2000)
# Call DenseCountSparseOutput
output_indices, output_values, output_shape = tf.raw_ops.DenseCountSparseOutput(
    values=values_tensor,
    weights=weights_tensor,
    binary_output=False,
    minlength=0,
    maxlength=2000
)
# Create an expected output from TensorFlow to compare
dense_output = tf.zeros(2000, dtype=weights_tensor.dtype)
for idx, value in zip(output_indices.numpy(), output_values.numpy()):
    dense_output = tf.tensor_scatter_nd_update(dense_output, [[idx]], [value])
```

### Relevant log output

```shell
Aborted (core dumped)
```",1
`No TPU platform registered` with tf.estimator and TPUv5,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.15.0; v2.15.0-0-g6887368d6d4

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.10.16

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I'm trying to use a TPUv5p to run training code which leverages tf.estimator which, if I understand correctly, is still supported in TF 2.15.
While this code seems to work with TPUv3, it's failing with a segfault on TPUv5 (errors noted below).

In case it's relevant, I am using TPUv5p via GKE. We have the following snippet in our Dockerfile for our custom TPU image:
```
RUN pip install https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/tf-2.15.0/tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl

# Download and install `libtpu`.
# You must save `libtpu.so` in the '/lib' directory of the container image.
RUN curl -L https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/1.9.0/libtpu.so -o /lib/libtpu.so

# TensorFlow training on TPU v5e requires the PJRT runtime. To enable the PJRT
# runtime, configure the following environment variables in your Dockerfile.
# For details, see https://cloud.google.com/tpu/docs/runtimes#tf-pjrt-support.
ENV NEXT_PLUGGABLE_DEVICE_USE_C_API=true
ENV TF_PLUGGABLE_DEVICE_LIBRARY_PATH=/lib/libtpu.so
```

I have verified that I can successfully access and use the TPU resource with other scripts which do not leverage tf.estimator e.g.
```py
import tensorflow as tf

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(""local"")
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)

strategy = tf.distribute.TPUStrategy(resolver)
print(""TPU strategy initialized successfully."")

with strategy.scope():
    a = tf.constant([[1.0, 2.0], [3.0, 4.0]])
    b = tf.constant([[1.0, 1.0], [0.0, 1.0]])
    c = tf.matmul(a, b)
    print(""TPU Computation Result:"", c.numpy())
```

I expected that Estimator falling under the [""compatibility guarantees""](https://www.tensorflow.org/versions/r2.15/api_docs/python/tf/estimator/Estimator) would imply that I should be able to use the Estimator API with TPUv5. Is there something I am doing wrong or is compatibility broken here? I'm hoping to avoid the lift of migrating a lot of code off of tf.Estimator at the moment.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# Ensure compatibility with TensorFlow 1.x (TPUEstimator requires session-based execution)
tf.compat.v1.disable_eager_execution()

# Detect and initialize the TPU
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=""local"")
tf.tpu.experimental.initialize_tpu_system(resolver)

# TPU RunConfig
tpu_config = tf.compat.v1.estimator.tpu.RunConfig(
    cluster=resolver,
    tpu_config=tf.compat.v1.estimator.tpu.TPUConfig(iterations_per_loop=100)
)

# Model function for TPUEstimator
def model_fn(features, labels, mode, params):
    """"""Simple Linear Model for TPUEstimator""""""
    net = tf.compat.v1.layers.dense(features, 1)  # Single-layer model

    loss = tf.compat.v1.losses.mean_squared_error(labels, net)

    if mode == tf.estimator.ModeKeys.TRAIN:
        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01)
        optimizer = tf.compat.v1.tpu.CrossShardOptimizer(optimizer)  # TPU-optimized optimizer
        train_op = optimizer.minimize(loss, tf.compat.v1.train.get_global_step())
        return tf.compat.v1.estimator.tpu.TPUEstimatorSpec(mode, loss=loss, train_op=train_op)

    return tf.compat.v1.estimator.tpu.TPUEstimatorSpec(mode, loss=loss)

# Create TPUEstimator
estimator = tf.compat.v1.estimator.tpu.TPUEstimator(
    model_fn=model_fn,
    config=tpu_config,
    train_batch_size=128
)

# Input function with synthetic data (random numbers)
def input_fn(params):
    dataset = tf.data.Dataset.from_tensor_slices((
        tf.random.normal([1000, 10]),  # 1000 samples, 10 features each
        tf.random.normal([1000, 1])   # 1000 labels
    ))
    dataset = dataset.batch(params[""batch_size""], drop_remainder=True).repeat()
    return dataset

# Train the model (no need for a bucket)
estimator.train(input_fn=input_fn, steps=100)

print(""Training completed!"")
```

### Relevant log output

```shell
INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.
INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.
2025-04-02 20:43:22.238222: I external/local_xla/xla/stream_executor/tpu/tpu_platform_interface.cc:78] No TPU platform registered. Waiting 1 second and trying again... (4 tries left)

[2025-04-02, 20:43:32 UTC] {kubernetes_operator.py:1073} INFO - 2025-04-02 20:43:23.238376: I external/local_xla/xla/stream_executor/tpu/tpu_platform_interface.cc:78] No TPU platform registered. Waiting 1 second and trying again... (3 tries left)
2025-04-02 20:43:24.238518: I external/local_xla/xla/stream_executor/tpu/tpu_platform_interface.cc:78] No TPU platform registered. Waiting 1 second and trying again... (2 tries left)
2025-04-02 20:43:25.238650: I external/local_xla/xla/stream_executor/tpu/tpu_platform_interface.cc:78] No TPU platform registered. Waiting 1 second and trying again... (1 tries left)
2025-04-02 20:43:26.238789: I external/local_xla/xla/stream_executor/tpu/tpu_platform_interface.cc:75] No TPU platform found.
2025-04-02 20:43:26.238839: E external/local_xla/xla/stream_executor/tpu/tpu_transfer_manager_interface.cc:28] Unable to retrieve registered TPU platform.
2025-04-02 20:43:26.238934: E external/local_xla/xla/stream_executor/tpu/tpu_transfer_manager_interface.cc:28] Unable to retrieve registered TPU platform.
2025-04-02 20:43:26.239866: E external/local_xla/xla/stream_executor/tpu/tpu_transfer_manager_interface.cc:28] Unable to retrieve registered TPU platform.
2025-04-02 20:43:26.239920: E external/local_xla/xla/stream_executor/tpu/tpu_transfer_manager_interface.cc:28] Unable to retrieve registered TPU platform.
2025-04-02 20:43:26.240501: E external/local_xla/xla/stream_executor/tpu/tpu_transfer_manager_interface.cc:28] Unable to retrieve registered TPU platform.
2025-04-02 20:43:26.240641: E external/local_xla/xla/stream_executor/tpu/tpu_transfer_manager_interface.cc:28] Unable to retrieve registered TPU platform.
2025-04-02 20:43:26.241022: E external/local_xla/xla/stream_executor/tpu/tpu_transfer_manager_interface.cc:28] Unable to retrieve registered TPU platform.
2025-04-02 20:43:26.241395: E external/local_xla/xla/stream_executor/tpu/tpu_transfer_manager_interface.cc:28] Unable to retrieve registered TPU platform.
Fatal Python error: Fatal Python error: Fatal Python error: Segmentation faultSegmentation faultSegmentation fault

# followed by a big dump of information:
Thread 0x0000795d3389f6c0 (most recent call first):
  File ""/usr/local/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1478 in _call_tf_sessionrun
  File ""/usr/local/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1385 in _run_fn
  File ""/usr/local/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1402 in _do_call
  File ""/usr/local/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1395 in _do_run
  File ""/usr/local/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1215 in _run
  File ""/usr/local/lib/python3.10/site-packages/tensorflow/python/client/session.py
Extension modules: ""numpy.core._multiarray_umath
Extension modules: , line numpy.core._multiarray_umath972 in run
  File , , ""numpy.core._multiarray_testsnumpy.core._multiarray_tests/usr/local/lib/python3.10/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 548 in _run_outfeed
  File ""/usr/local/lib/python3.10/threading.py, , ""numpy.linalg._umath_linalgnumpy.linalg._umath_linalg, line 953 in run
, , numpy.fft._pocketfft_internal  File numpy.fft._pocketfft_internal"", /usr/local/lib/python3.10/threading.pynumpy.random._common, ""numpy.random._common, line 1016 in _bootstrap_inner
  File ""/usr/local/lib/python3.10/threading.py"", line , 973numpy.random.bit_generator in _bootstrap, https://symbolize.stripped_domain/r/?trace=
, 
numpy.random.bit_generator7984f558d70c,numpy.random._bounded_integersThread 0x, 7985a65e704f,0000795d340a06c0numpy.random._bounded_integers, 7984f53896bf, (most recent call first):
numpy.random._mt199377984f558cd58,,   File , 7984f558d193,numpy.random._mt19937""numpy.random.mtrand7984f558db9c,/usr/local/lib/python3.10/site-packages/tensorflow/python/client/session.py, ""7985024436af,, numpy.random.mtrand, line 7985024430c0,numpy.random._philox1478, 798502ed998a, in 7985a66341f4, numpy.random._philox_call_tf_sessionrun&map=numpy.random._pcg64
,   File  
numpy.random._pcg64, ""numpy.random._sfc64, /usr/local/lib/python3.10/site-packages/tensorflow/python/client/session.py*** SIGSEGV (@(nil)), see go/stacktraces#s15 received by PID 151 (TID 2817) on cpu 160; stack trace: ***
numpy.random._sfc64"", , numpy.random._generator, line numpy.random._generator1385 in _run_fn
  File ""/usr/local/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1402 in _do_call
  File ""/usr/local/lib/python3.10/site-packages/tensorflow/python/client/session.py, "", google.protobuf.pyext._message, line google.protobuf.pyext._message1395 in _do_run
  File , , ""tensorflow.python.framework.fast_tensor_utiltensorflow.python.framework.fast_tensor_util/usr/local/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1215 in _run
  File ""/usr/local/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 972 in run
  File ""/usr/local/lib/python3.10/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 535 in _run_infeed
  File ""/usr/local/lib/python3.10/threading.py"", line 953 in run
  File ""/usr/local/lib/python3.10/threading.py"", line 1016 in _bootstrap_inner
  File ""/usr/local/lib/python3.10/threading.py"", line 973 in _bootstrap

Thread 0x00007978c8ff96c0, ,  (most recent call first):
charset_normalizer.mdcharset_normalizer.md  File "", /usr/local/lib/python3.10/threading.pyrequests.packages.charset_normalizer.md, , ""requests.packages.charset_normalizer.mdrequests.packages.chardet.md, line , 320, requests.packages.chardet.md in h5py._errorswait, , 
h5py._errorsh5py.defs  File "", , /usr/local/lib/python3.10/site-packages/tensorflow/python/summary/writer/event_file_writer.pyh5py.defsh5py._objects"", line , , 261h5py._objectsh5py.h5 in get
, h5py.h5,   File , h5py.utils""h5py.utils, /usr/local/lib/python3.10/site-packages/tensorflow/python/summary/writer/event_file_writer.py, h5py.h5t""h5py.h5t, , line h5py.h5s, 204h5py.h5s,  in h5py.h5ac, runh5py.h5ac
,   File , h5py.h5p""h5py.h5p/usr/local/lib/python3.10/threading.py, , ""h5py.h5rh5py.h5r, line 1016 in _bootstrap_inner, 
, h5py._proxy  File h5py._proxy, "", h5py._conv/usr/local/lib/python3.10/threading.pyh5py._conv, "", h5py.h5z, line h5py.h5z973,  in , h5py.h5a_bootstraph5py.h5a
, 
h5py.h5d, Thread 0xh5py.h5d, 00007985a64c9b80h5py.h5ds (most recent call first):
,   File h5py.h5ds, ""h5py.h5g, /usr/local/lib/python3.10/site-packages/tensorflow/python/client/session.pyh5py.h5i"", line 1478, , h5py.h5g in h5py.h5o_call_tf_sessionrun, , 
h5py.h5ih5py.h5f  File "", , /usr/local/lib/python3.10/site-packages/tensorflow/python/client/session.pyh5py.h5oh5py.h5fd"", , line , h5py.h5f1385h5py.h5pl in , _run_fn, h5py.h5fd
h5py.h5l,   File h5py.h5pl"", /usr/local/lib/python3.10/site-packages/tensorflow/python/client/session.py, h5py._selector""h5py.h5l, line , , 1402h5py._selectorscipy._lib._ccallback_c in _do_call
  File "", , /usr/local/lib/python3.10/site-packages/tensorflow/python/client/session.pyscipy.sparse._sparsetoolsscipy._lib._ccallback_c"", line , , 1395_csparsetoolsscipy.sparse._sparsetools in , _do_run, scipy.sparse._csparsetools
_csparsetools  File , ""scipy.sparse.linalg._isolve._iterative, /usr/local/lib/python3.10/site-packages/tensorflow/python/client/session.pyscipy.sparse._csparsetools"", , line scipy.linalg._fblas, 1215scipy.sparse.linalg._isolve._iterative in , _run, scipy.linalg._flapack
scipy.linalg._fblas,   File , scipy.linalg._cythonized_array_utils""scipy.linalg._flapack, /usr/local/lib/python3.10/site-packages/tensorflow/python/client/session.py, scipy.linalg._flinalg""scipy.linalg._cythonized_array_utils, , line scipy.linalg._solve_toeplitz972 in , run, scipy.linalg._flinalg
scipy.linalg._matfuncs_sqrtm_triu,   File , scipy.linalg._solve_toeplitz""scipy.linalg.cython_lapack/usr/local/lib/python3.10/site-packages/tensorflow/python/training/monitored_session.py, "", scipy.linalg._matfuncs_sqrtm_triu, line scipy.linalg.cython_blas, 1228scipy.linalg.cython_lapack,  in , scipy.linalg._matfuncs_expmrunscipy.linalg.cython_blas
, ,   File scipy.linalg._decomp_updatescipy.linalg._matfuncs_expm"", , /usr/local/lib/python3.10/site-packages/tensorflow/python/training/monitored_session.pyscipy.sparse.linalg._dsolve._superluscipy.linalg._decomp_update"", line 1464,  in , scipy.sparse.linalg._eigen.arpack._arpackrunscipy.sparse.linalg._dsolve._superlu
,   File , scipy.sparse.csgraph._tools""scipy.sparse.linalg._eigen.arpack._arpack/usr/local/lib/python3.10/site-packages/tensorflow/python/training/monitored_session.py, ""scipy.sparse.csgraph._shortest_path, , line scipy.sparse.csgraph._tools1397,  in scipy.sparse.csgraph._traversal, run, scipy.sparse.csgraph._shortest_path
scipy.sparse.csgraph._min_spanning_tree,   File scipy.sparse.csgraph._traversal, ""scipy.sparse.csgraph._flow/usr/local/lib/python3.10/site-packages/tensorflow/python/training/monitored_session.py, , ""scipy.sparse.csgraph._min_spanning_treescipy.sparse.csgraph._matching, line 1307,  in , scipy.sparse.csgraph._flowrun, scipy.sparse.csgraph._reordering
scipy.sparse.csgraph._matching  File "", /usr/local/lib/python3.10/site-packages/tensorflow/python/training/monitored_session.py, scipy.sparse.csgraph._reordering""jaxlib.cpu_feature_guard, line 778 in , runjaxlib.cpu_feature_guard
  File ""/usr/local/lib/python3.10/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1535 in _train_with_estimator_spec
  File ""/usr/local/lib/python3.10/site-packages/tensorflow_estimator/python/estimator/estimator.py"", , , line pandas._libs.tslibs.dtypespandas._libs.tslibs.dtypes1219, ,  in pandas._libs.tslibs.basepandas._libs.tslibs.base_train_model_default
,   File , pandas._libs.tslibs.np_datetime""pandas._libs.tslibs.np_datetime/usr/local/lib/python3.10/site-packages/tensorflow_estimator/python/estimator/estimator.py, "", pandas._libs.tslibs.nattype, line pandas._libs.tslibs.nattype, 1188pandas._libs.tslibs.timezones in , _train_modelpandas._libs.tslibs.timezones
,   File pandas._libs.tslibs.ccalendar"", /usr/local/lib/python3.10/site-packages/tensorflow_estimator/python/estimator/estimator.pypandas._libs.tslibs.ccalendar, ""pandas._libs.tslibs.tzconversion, , line pandas._libs.tslibs.tzconversion360,  in pandas._libs.tslibs.strptime, train, pandas._libs.tslibs.strptime
pandas._libs.tslibs.fields  File , , pandas._libs.tslibs.fields""pandas._libs.tslibs.timedeltas/usr/local/lib/python3.10/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py, "", pandas._libs.tslibs.timedeltas, line pandas._libs.tslibs.timestamps3096,  in pandas._libs.tslibs.timestamps, trainpandas._libs.properties
, pandas._libs.properties  File , ""pandas._libs.tslibs.offsets, /usr/src/t2t/tensor2tensor/utils/trainer_lib.pypandas._libs.tslibs.offsets, ""pandas._libs.tslibs.parsing, line , 438pandas._libs.tslibs.parsing,  in pandas._libs.tslibs.conversiontrain, 
, pandas._libs.tslibs.conversion  File pandas._libs.tslibs.period, ""pandas._libs.tslibs.period/usr/src/t2t/tensor2tensor/bin/t2t_trainer.py, "", pandas._libs.tslibs.vectorized, line , pandas._libs.tslibs.vectorized355pandas._libs.ops_dispatch in , execute_schedulepandas._libs.ops_dispatch, 
pandas._libs.missing  File , , ""pandas._libs.missingpandas._libs.hashtable/usr/src/t2t/tensor2tensor/bin/t2t_trainer.py, , ""pandas._libs.algospandas._libs.hashtable, line 405 in , main, pandas._libs.interval
pandas._libs.algos  File , ""pandas._libs.tslib, /usr/src/t2t/tensor2tensor/bin/t2t-trainerpandas._libs.interval"", , line pandas._libs.lib26,  in , pandas._libs.tslibmainpandas._libs.hashing
,   File pandas._libs.lib, ""pyarrow.lib/usr/local/lib/python3.10/site-packages/absl/app.py, ""pandas._libs.hashing, , line pandas._libs.ops, 261pyarrow.lib,  in pandas._libs.arrays, _run_mainpandas._libs.ops, 
pandas._libs.index  File , ""pandas._libs.arrays, /usr/local/lib/python3.10/site-packages/absl/app.py, pandas._libs.join""pandas._libs.index, line , , 316pandas._libs.sparsepandas._libs.join in run, , 
pandas._libs.sparsepyarrow._compute  File "", , /usr/local/lib/python3.10/site-packages/tensorflow/python/platform/app.pypyarrow._computepandas._libs.reduction"", , line , pandas._libs.reduction36pandas._libs.indexing,  in pandas._libs.indexing, run, pandas._libs.internals
pandas._libs.internals  File , , ""pandas._libs.writerspandas._libs.writers/usr/src/t2t/tensor2tensor/bin/t2t-trainer, , ""pandas._libs.window.aggregationspandas._libs.window.aggregations, line 31, ,  in pandas._libs.window.indexerspandas._libs.window.indexers<module>, , 
pandas._libs.reshapepandas._libs.reshape, , pandas._libs.groupbypandas._libs.groupby, , pandas._libs.testingpandas._libs.testing, , pandas._libs.parserspandas._libs.parsers, , pandas._libs.jsonpandas._libs.json
Extension modules: , , numpy.core._multiarray_umathscipy.ndimage._nd_imagescipy.ndimage._nd_image, , , scipy.special._ufuncs_cxxscipy.special._ufuncs_cxxnumpy.core._multiarray_tests, , scipy.special._ufuncsscipy.special._ufuncs, , scipy.special._specfunscipy.special._specfun, , numpy.linalg._umath_linalgscipy.special._comb, , scipy.special._combnumpy.fft._pocketfft_internal, scipy.special._ellip_harm_2, , scipy.special._ellip_harm_2numpy.random._common, _ni_label, _ni_label, , scipy.ndimage._ni_labelscipy.ndimage._ni_label, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, , numpy.random._generatorgrpc._cython.cygrpc, grpc._cython.cygrpc, , _cffi_backend_cffi_backend, google.protobuf.pyext._message, , multidict._multidict, multidict._multidicttensorflow.python.framework.fast_tensor_util, yarl._quoting_c, yarl._quoting_c, propcache._helpers_c, , propcache._helpers_caiohttp._http_writer, aiohttp._http_writer, , , charset_normalizer.mdaiohttp._http_parseraiohttp._http_parser, , , requests.packages.charset_normalizer.mdaiohttp._websocket.maskaiohttp._websocket.mask, requests.packages.chardet.md, , aiohttp._websocket.reader_c, aiohttp._websocket.reader_ch5py._errors, , frozenlist._frozenlistfrozenlist._frozenlist, h5py.defs, , , h5py._objectsregex._regexregex._regex, h5py.h5, , , yaml._yamlyaml._yamlh5py.utils, h5py.h5t, h5py.h5s, , , h5py.h5acsqlalchemy.cprocessorssqlalchemy.cprocessors, , , h5py.h5psqlalchemy.cutilssqlalchemy.cutils, , , h5py.h5rsqlalchemy.cresultproxysqlalchemy.cresultproxy, h5py._proxy, , numpy.linalg.lapack_lite, numpy.linalg.lapack_liteh5py._conv, scipy.spatial._ckdtree, , , scipy.spatial._ckdtreeh5py.h5zscipy._lib.messagestream, , , scipy._lib.messagestreamscipy.spatial._qhullh5py.h5a, , scipy.spatial._qhull, scipy.spatial._voronoi, h5py.h5dscipy.spatial._voronoi, scipy.spatial._distance_wrap, , , scipy.spatial._distance_wraph5py.h5dsscipy.spatial._hausdorff, , scipy.spatial._hausdorffh5py.h5g, , scipy.spatial.transform._rotation, scipy.spatial.transform._rotation, h5py.h5iscipy.optimize._minpack2, scipy.optimize._minpack2, , h5py.h5oscipy.optimize._group_columns, scipy.optimize._group_columns, , , h5py.h5fscipy.optimize._trlib._trlibscipy.optimize._trlib._trlib, h5py.h5fd, , , scipy.optimize._lbfgsbscipy.optimize._lbfgsbh5py.h5pl, _moduleTNC, , , h5py.h5l_moduleTNCscipy.optimize._moduleTNC, , h5py._selectorscipy.optimize._moduleTNC, scipy.optimize._cobyla, , , scipy._lib._ccallback_cscipy.optimize._cobylascipy.optimize._slsqp, , , scipy.sparse._sparsetoolsscipy.optimize._slsqpscipy.optimize._minpack, , , scipy.optimize._minpack_csparsetoolsscipy.optimize._lsq.givens_elimination, scipy.optimize._lsq.givens_elimination, , , scipy.sparse._csparsetoolsscipy.optimize._zerosscipy.optimize._zeros, , scipy.sparse.linalg._isolve._iterativescipy.optimize.__nnls, , , scipy.optimize.__nnlsscipy.linalg._fblasscipy.optimize._highs.cython.src._highs_wrapper, scipy.optimize._highs.cython.src._highs_wrapper, , , scipy.linalg._flapackscipy.optimize._highs._highs_wrapperscipy.optimize._highs._highs_wrapper, , , scipy.linalg._cythonized_array_utilsscipy.optimize._highs.cython.src._highs_constantsscipy.optimize._highs.cython.src._highs_constants, , , scipy.linalg._flinalgscipy.optimize._highs._highs_constantsscipy.optimize._highs._highs_constants, , scipy.linalg._solve_toeplitz, scipy.linalg._interpolativescipy.linalg._interpolative, scipy.linalg._matfuncs_sqrtm_triu, , , scipy.optimize._bglu_densescipy.optimize._bglu_densescipy.linalg.cython_lapack, scipy.optimize._lsap, , scipy.optimize._lsapscipy.linalg.cython_blas, scipy.optimize._direct, , scipy.optimize._direct, scipy.linalg._matfuncs_expm, scipy.integrate._odepack, scipy.integrate._odepackscipy.linalg._decomp_update, , , scipy.integrate._quadpackscipy.integrate._quadpackscipy.sparse.linalg._dsolve._superlu, , , scipy.integrate._vodescipy.integrate._vodescipy.sparse.linalg._eigen.arpack._arpack, scipy.integrate._dop, , , scipy.integrate._dopscipy.sparse.csgraph._toolsscipy.integrate._lsoda, , scipy.sparse.csgraph._shortest_pathscipy.integrate._lsoda, , scipy.special.cython_specialscipy.sparse.csgraph._traversal, scipy.special.cython_special, , , scipy.stats._statsscipy.sparse.csgraph._min_spanning_treescipy.stats._stats, , , scipy.stats.beta_ufuncscipy.sparse.csgraph._flowscipy.stats.beta_ufunc, , , scipy.stats._boost.beta_ufuncscipy.sparse.csgraph._matchingscipy.stats._boost.beta_ufunc, , , scipy.stats.binom_ufuncscipy.sparse.csgraph._reorderingscipy.stats.binom_ufunc, , scipy.stats._boost.binom_ufuncscipy.stats._boost.binom_ufunc, , , jaxlib.cpu_feature_guardscipy.stats.nbinom_ufuncscipy.stats.nbinom_ufunc, scipy.stats._boost.nbinom_ufunc, , scipy.stats._boost.nbinom_ufuncscipy.stats.hypergeom_ufunc, , pandas._libs.tslibs.dtypesscipy.stats.hypergeom_ufunc, scipy.stats._boost.hypergeom_ufunc, , , pandas._libs.tslibs.basescipy.stats._boost.hypergeom_ufuncscipy.stats.ncf_ufunc, , , pandas._libs.tslibs.np_datetimescipy.stats.ncf_ufuncscipy.stats._boost.ncf_ufunc, , pandas._libs.tslibs.nattypescipy.stats._boost.ncf_ufunc, , scipy.interpolate._fitpack, pandas._libs.tslibs.timezones, scipy.interpolate._fitpackscipy.interpolate.dfitpack, pandas._libs.tslibs.ccalendar, , scipy.interpolate.dfitpack, scipy.interpolate._bsplpandas._libs.tslibs.tzconversion, , scipy.interpolate._bsplscipy.interpolate._ppoly, pandas._libs.tslibs.strptime, , scipy.interpolate._ppoly, scipy.interpolate.interpndpandas._libs.tslibs.fields, , , scipy.interpolate.interpndscipy.interpolate._rbfinterp_pythranpandas._libs.tslibs.timedeltas, , scipy.interpolate._rbfinterp_pythranscipy.stats._biasedurn, , pandas._libs.tslibs.timestamps, scipy.stats._biasedurnscipy.stats._levy_stable.levyst, , pandas._libs.properties, scipy.stats._levy_stable.levystscipy._lib._uarray._uarray, , pandas._libs.tslibs.offsetsscipy._lib._uarray._uarray, , scipy.stats._hypotests_pythran, pandas._libs.tslibs.parsingscipy.stats._hypotests_pythran, , scipy.stats._statlib, pandas._libs.tslibs.conversionscipy.stats._statlib, , , scipy.stats._mvnpandas._libs.tslibs.periodscipy.stats._mvn, , , scipy.stats._sobolpandas._libs.tslibs.vectorizedscipy.stats._sobol, , , scipy.stats._qmc_cypandas._libs.ops_dispatchscipy.stats._qmc_cy, , , pandas._libs.missingscipy.stats.unuran_wrapperscipy.stats.unuran_wrapper, scipy.stats._unuran.unuran_wrapper, , scipy.stats._unuran.unuran_wrapperpandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, , psycopg2._psycopg, psycopg2._psycopgpandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pyarrow.lib, pandas._libs.ops, , srsly.ujson.ujson, srsly.ujson.ujsonpandas._libs.arrays, srsly.msgpack._packer, , pandas._libs.index, srsly.msgpack._packersrsly.msgpack._unpacker, , pandas._libs.joinsrsly.msgpack._unpacker, , cymem.cymempandas._libs.sparse, , cymem.cymem, preshed.maps, pyarrow._computepreshed.maps, , , blis.cypandas._libs.reductionblis.cy, , pandas._libs.indexing, blis.pyblis.py, , , pandas._libs.internalsthinc.linalgthinc.linalg, , pandas._libs.writersmurmurhash.mrmr, , , murmurhash.mrmrpandas._libs.window.aggregationsthinc.neural._aligned_alloc, thinc.neural._aligned_alloc, , , pandas._libs.window.indexersthinc.neural.opsthinc.neural.ops, , pandas._libs.reshapethinc.neural.optimizers, , pandas._libs.groupbythinc.neural.optimizers, spacy.symbols, pandas._libs.testing, , spacy.symbolsspacy.strings, pandas._libs.parsers, , spacy.strings, spacy.attrspandas._libs.json, spacy.attrs, , spacy.lexeme, spacy.lexeme, scipy.ndimage._nd_imagespacy.tokens.morphanalysis, , spacy.tokens.morphanalysisscipy.special._ufuncs_cxx, , , spacy.parts_of_speechspacy.parts_of_speechscipy.special._ufuncs, , , spacy.tokens.tokenspacy.tokens.tokenscipy.special._specfun, , , spacy.tokens.spanspacy.tokens.spanscipy.special._comb, , , spacy.tokens._retokenizespacy.tokens._retokenizescipy.special._ellip_harm_2, , spacy.tokens.doc, spacy.tokens.doc, _ni_labelpreshed.bloom, preshed.bloom, , scipy.ndimage._ni_labelspacy.vectors, spacy.vectors, thinc.linear.linear, , thinc.linear.linearspacy.vocab, , , spacy.vocabspacy.morphologygrpc._cython.cygrpc, , spacy.morphologyspacy.syntax.stateclass, , , _cffi_backendspacy.syntax.stateclassthinc.extra.search, , thinc.extra.searchspacy.syntax._beam_utils, multidict._multidict, , spacy.syntax._beam_utils, spacy.syntax.transition_system, yarl._quoting_c, spacy.syntax.transition_systemspacy.syntax.nonproj, , , spacy.goldpropcache._helpers_cspacy.syntax.nonproj, , , spacy.syntax.arc_eageraiohttp._http_writerspacy.gold, , , spacy.syntax._parser_modelaiohttp._http_parserspacy.syntax.arc_eager, , , spacy.syntax.nn_parseraiohttp._websocket.maskspacy.syntax._parser_model, , , spacy.syntax.neraiohttp._websocket.reader_c, spacy.syntax.nn_parserspacy.tokenizer, , , frozenlist._frozenlistspacy.syntax.nerspacy.matcher.matcher, , regex._regexspacy.tokenizer, , , spacy.matcher.phrasematcherspacy.matcher.matcheryaml._yaml, , spacy.matcher.dependencymatcherspacy.matcher.phrasematcher, , , spacy.kbspacy.matcher.dependencymatchersqlalchemy.cprocessors, , , spacy.pipeline.pipesspacy.kbsqlalchemy.cutils, , spacy.pipeline.morphologizer, spacy.pipeline.pipessqlalchemy.cresultproxy, , spacy.pipeline.morphologizer, preshed.counternumpy.linalg.lapack_lite, preshed.counter, scipy.spatial._ckdtree, scipy._lib.messagestream (total:  (total: , 240240scipy.spatial._qhull))
, 
scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.optimize._minpack2https://symbolize.stripped_domain/r/?trace=https://symbolize.stripped_domain/r/?trace=, 7985a6635eec,7985a6635eec,scipy.optimize._group_columns7985a65e704f,7985a65e704f,7984f53896bf,, 7984f53896bf,7984f558cd58,scipy.optimize._trlib._trlib7984f558cd58,, 7984f558d193,7984f558d193,scipy.optimize._lbfgsb7984f558db9c,7984f558db9c,, 7985024436af,7985024436af,_moduleTNC7985024430c0,7985024430c0,, 798502ed998a,798502ed998a,scipy.optimize._moduleTNC7985a66341f47985a66341f4&map=, &map= 
scipy.optimize._cobyla 
, scipy.optimize._slsqpE0402 20:43:26.245979    2832 process_state.cc:1112] RAW: Signal 11 raised at PC: 0x7985a6635eec while already in FailureSignalHandler!
, E0402 20:43:26.245986    2857 process_state.cc:1112] RAW: Signal 11 raised at PC: 0x7985a6635eec while already in FailureSignalHandler!
E0402 20:43:26.245995    2832 process_state.cc:1116] RAW: tid: 2832 raised new signal (old_tid: 2817)
scipy.optimize._minpackE0402 20:43:26.246004    2857 process_state.cc:1116] RAW: tid: 2857 raised new signal (old_tid: 2817)
, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize.__nnls, scipy.optimize._highs.cython.src._highs_wrapper, scipy.optimize._highs._highs_wrapper, scipy.optimize._highs.cython.src._highs_constants, scipy.optimize._highs._highs_constants, scipy.linalg._interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.special.cython_special, scipy.stats._stats, scipy.stats.beta_ufunc, scipy.stats._boost.beta_ufunc, scipy.stats.binom_ufunc, scipy.stats._boost.binom_ufunc, scipy.stats.nbinom_ufunc, scipy.stats._boost.nbinom_ufunc, scipy.stats.hypergeom_ufunc, scipy.stats._boost.hypergeom_ufunc, scipy.stats.ncf_ufunc, scipy.stats._boost.ncf_ufunc, scipy.interpolate._fitpack, scipy.interpolate.dfitpack, scipy.interpolate._bspl, scipy.interpolate._ppoly, scipy.interpolate.interpnd, scipy.interpolate._rbfinterp_pythran, scipy.stats._biasedurn, scipy.stats._levy_stable.levyst, scipy._lib._uarray._uarray, scipy.stats._hypotests_pythran, scipy.stats._statlib, scipy.stats._mvn, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats.unuran_wrapper, scipy.stats._unuran.unuran_wrapper, psycopg2._psycopg, srsly.ujson.ujson, srsly.msgpack._packer, srsly.msgpack._unpacker, cymem.cymem, preshed.maps, blis.cy, blis.py, thinc.linalg, murmurhash.mrmr, thinc.neural._aligned_alloc, thinc.neural.ops, thinc.neural.optimizers, spacy.symbols, spacy.strings, spacy.attrs, spacy.lexeme, spacy.tokens.morphanalysis, spacy.parts_of_speech, spacy.tokens.token, spacy.tokens.span, spacy.tokens._retokenize, spacy.tokens.doc, preshed.bloom, spacy.vectors, thinc.linear.linear, spacy.vocab, spacy.morphology, spacy.syntax.stateclass, thinc.extra.search, spacy.syntax._beam_utils, spacy.syntax.transition_system, spacy.syntax.nonproj, spacy.gold, spacy.syntax.arc_eager, spacy.syntax._parser_model, spacy.syntax.nn_parser, spacy.syntax.ner, spacy.tokenizer, spacy.matcher.matcher, spacy.matcher.phrasematcher, spacy.matcher.dependencymatcher, spacy.kb, spacy.pipeline.pipes, spacy.pipeline.morphologizer, preshed.counter (total: 240)
https://symbolize.stripped_domain/r/?trace=7985a6635eec,7985a65e704f,7984f53896bf,7984f558cd58,7984f558d193,7984f558db9c,7985024436af,7985024430c0,798502ed998a,7985a66341f4&map= 
E0402 20:43:26.246868    2845 process_state.cc:1112] RAW: Signal 11 raised at PC: 0x7985a6635eec while already in FailureSignalHandler!
E0402 20:43:26.246881    2845 process_state.cc:1116] RAW: tid: 2845 raised new signal (old_tid: 2817)
PC: @     0x7984f558d70c  (unknown)  tensorflow::StreamExecutorTransferOpImpl::TransferLiteralFromOutfeed()
    @     0x7984e78214f9        928  (unknown)
    @     0x7985a65e7050       3520  (unknown)
    @     0x7984f53896c0        560  tensorflow::TpuOutfeedDequeueTupleOp<>::DoWork()
    @     0x7984f558cd59        112  tensorflow::TpuTransferAsyncOpKernelBase::RunTransferWithOrdinal()
    @     0x7984f558d194         32  tensorflow::TpuTransferAsyncOpKernel::RunTransfer()
    @     0x7984f558db9d        304  std::_Function_handler<>::_M_invoke()
    @     0x7985024436b0        128  Eigen::ThreadPoolTempl<>::WorkerLoop()
    @     0x7985024430c1         48  std::__invoke_impl<>()
    @     0x798502ed998b         64  tsl::(anonymous namespace)::PThread::ThreadFn()
    @     0x7985a66341f5  (unknown)  (unknown)
https://symbolize.stripped_domain/r/?trace=7984f558d70c,7984e78214f8,7985a65e704f,7984f53896bf,7984f558cd58,7984f558d193,7984f558db9c,7985024436af,7985024430c0,798502ed998a,7985a66341f4&map=5edeb7d86db111100e979a74159a3982:7984d7c00000-7984e7a40ba0 
E0402 20:43:26.340216    2817 coredump_hook.cc:447] RAW: Remote crash data gathering hook invoked.
E0402 20:43:26.340228    2817 client.cc:272] RAW: Coroner client retries enabled (b/136286901), will retry for up to 30 sec.
E0402 20:43:26.340234    2817 coredump_hook.cc:542] RAW: Sending fingerprint to remote end.
E0402 20:43:26.340257    2817 coredump_hook.cc:551] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
E0402 20:43:26.340267    2817 coredump_hook.cc:603] RAW: Dumping core locally.

[2025-04-02, 20:43:42 UTC] {kubernetes_operator.py:1073} INFO - https://symbolize.stripped_domain/r/?trace=https://symbolize.stripped_domain/r/?trace=https://symbolize.stripped_domain/r/?trace=7984f558d67a,7984f558d67a,https://symbolize.stripped_domain/r/?trace=7984f558d67a,7985a65e704f,7985a65e704f,7984f558d67a,7985a65e704f,7984f5380864,7984f5380864,7985a65e704f,7984f5380864,7984f558cd58,7984f558cd58,7984f558d193,7984f5380864,7984f558cd58,7984f558d193,7984f558db9c,7984f558cd58,7984f558d193,7984f558db9c,7985024436af,7984f558d193,7984f558db9c,7985024436af,7985024430c0,7984f558db9c,7985024436af,7985024430c0,798502ed998a,7985024436af,7985a66341f47985024430c0,798502ed998a,7985a66341f4&map=7985024430c0,&map=798502ed998a,798502ed998a,7985a66341f47985a66341f4 
 
&map=&map= 
 
E0402 20:43:38.682990    2813 process_state.cc:1112] RAW: Signal 11 raised at PC: 0x7984f558d67a while already in FailureSignalHandler!
E0402 20:43:38.682981    2833 process_state.cc:1112] RAW: Signal 11 raised at PC: 0x7984f558d67a while already in FailureSignalHandler!
E0402 20:43:38.682979    2865 process_state.cc:1112] RAW: Signal 11 raised at PC: 0x7984f558d67a while already in FailureSignalHandler!
E0402 20:43:38.682989    2853 process_state.cc:1112] RAW: Signal 11 raised at PC: 0x7984f558d67a while already in FailureSignalHandler!
E0402 20:43:38.683019    2813 process_state.cc:1116] RAW: tid: 2813 raised new signal (old_tid: 2817)
E0402 20:43:38.683021    2833 process_state.cc:1116] RAW: tid: 2833 raised new signal (old_tid: 2817)
E0402 20:43:38.683025    2865 process_state.cc:1116] RAW: tid: 2865 raised new signal (old_tid: 2817)
E0402 20:43:38.683027    2853 process_state.cc:1116] RAW: tid: 2853 raised new signal (old_tid: 2817)
E0402 20:43:39.549462    2817 process_state.cc:808] RAW: Raising signal 11 with default behavior
```",2
maybe calculate bug in adam optimator .,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

master

### Custom code

No

### OS platform and distribution

Centos

### Mobile device

-

### Python version

-

### Bazel version

-

### GCC/compiler version

-

### CUDA/cuDNN version

-

### GPU model and memory

-

### Current behavior?

We compare a model within torch and tensorflow, then I found a calculate bug in adam optimator .

### Standalone code to reproduce the issue

```shell
I want same result with torch or adam paper .
```

### Relevant log output

```shell

```",1
Error on WSL - Attempting to register factory for plugin cuDNN when one has already been registered,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.19.0

### Custom code

No

### OS platform and distribution

Ubuntu 24.04.1 LTS on WSL version: 2.4.11.0

### Mobile device

_No response_

### Python version

3.10.4

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Installed with `pip install tensorflow[and-cuda]`, when running:

```
python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""
```

I get the following output:

```
2025-04-07 10:49:32.614704: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1744015772.625537 3982776 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1744015772.628814 3982776 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1744015772.637017 3982776 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744015772.637038 3982776 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744015772.637040 3982776 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744015772.637042 3982776 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-04-07 10:49:32.639667: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
```


nvidia-smi

```
Mon Apr  7 10:50:11 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.04             Driver Version: 538.78       CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Quadro RTX 5000                On  | 00000000:01:00.0 Off |                  N/A |
| N/A   49C    P8              13W / 110W |      0MiB / 16384MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
```

### Standalone code to reproduce the issue

```shell
python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""
```

### Relevant log output

```shell

```",1
Convolution FP32 in oneDNN Changed from gemm:acl to gemm:ref in TensorFlow 2.16,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.19

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In TensorFlow version 2.16 (and later), the convolution operation implementation for FP32 data type in oneDNN was changed from gemm:acl to gemm:ref. However, this change has resulted in performance degradation compared to TensorFlow version 2.15, where gemm:acl was used.

System Information:

- TensorFlow Version: 2.16
- Previous Working Version: 2.15
- oneDNN Version: 3.2.1
- Hardware: Aarch64
- Operating System: Ubuntu 22.04

### Standalone code to reproduce the issue

```shell
- In TensorFlow 2.15, the convolution operation for FP32 data type was routed through gemm:acl in oneDNN, which provided better performance.
- In TensorFlow 2.16 (and later), the implementation was changed to use gemm:ref, leading to a noticeable performance drop.
```

### Relevant log output

```shell

```",1
NaN loss on multi-GPU MirroredStrategy since tf 2.16,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

starting from 2.16 ([and-cuda] versions)

### Custom code

Yes

### OS platform and distribution

Rocky Linux 8.9

### Mobile device

Rocky Linux 8.9

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

A100 - 80Gb

### Current behavior?

Starting from tensorflow version 2.16, models that train perfectly fine on a single A100 GPU will quickly devolve into an inf and then NaN loss on multiple GPUs. This is not the case for tensorflow version 2.15.
This is not an isolated issue on my machine, as multiple users of our package which is built on tensorflow report the same exact issue (https://github.com/aertslab/CREsted/issues/100).
Also, there's multiple mentions on here (for example https://github.com/tensorflow/tensorflow/issues/87432 and https://github.com/tensorflow/tensorflow/issues/62915) without a real resolution pertaining to the same issue. It's clearly a real issue as I've seen it happening on multiple different machines by multiple different users and it has a big impact since it stops us from scaling our models with tensorflow. 

A couple of things I tried that didn't work:
- Clipping the gradients
- Lowering the learning rate
- Ensuring no empty batches get created


### Standalone code to reproduce the issue

```shell
import tensorflow as tf


def create_model():
    """"""Create a larger and more complex model for demonstration purposes.""""""
    model = tf.keras.Sequential(
        [
            tf.keras.layers.InputLayer(input_shape=(128, 128, 3)),
            tf.keras.layers.Conv2D(128, 3, padding=""same"", activation=""relu""),
            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Conv2D(256, 3, padding=""same"", activation=""relu""),
            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Conv2D(512, 3, padding=""same"", activation=""relu""),
            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Conv2D(1024, 3, padding=""same"", activation=""relu""),
            tf.keras.layers.GlobalAveragePooling2D(),
            tf.keras.layers.Dense(2048, activation=""relu""),
            tf.keras.layers.Dropout(0.5),
            tf.keras.layers.Dense(2048, activation=""relu""),
            tf.keras.layers.Dropout(0.5),
            tf.keras.layers.Dense(1000, activation=""softmax""),
        ]
    )
    return model


def generate_synthetic_data(batch_size, num_batches):
    """"""Generate synthetic data for training and validation.""""""
    for _ in range(num_batches):
        images = tf.random.normal((batch_size, 128, 128, 3))
        labels = tf.random.uniform((batch_size,), maxval=10, dtype=tf.int32)
        yield images, labels


def train_multi_gpu(batch_size, epochs):
    """"""Train the model using multiple GPUs.""""""
    # Detect and initialize GPUs
    gpus_found = tf.config.list_physical_devices(""GPU"")

    strategy = tf.distribute.MirroredStrategy()
    print(""Number of replica devices in use: {}"".format(strategy.num_replicas_in_sync))
    print(""Number of GPUs available: {}"".format(len(gpus_found)))

    assert len(gpus_found) >= 1, ""Training requires at least 1 GPU""

    strategy = tf.distribute.MirroredStrategy()
    global_batch_size = batch_size * strategy.num_replicas_in_sync

    # Create a synthetic dataset
    train_dataset = tf.data.Dataset.from_generator(
        lambda: generate_synthetic_data(global_batch_size, 500),
        output_signature=(
            tf.TensorSpec(shape=(global_batch_size, 128, 128, 3), dtype=tf.float32),
            tf.TensorSpec(shape=(global_batch_size,), dtype=tf.int32),
        ),
    ).repeat()

    with strategy.scope():
        model = create_model()
        model.compile(
            optimizer=""adam"", loss=tf.keras.losses.SparseCategoricalCrossentropy()
        )

        model.fit(train_dataset, epochs=epochs, steps_per_epoch=500)


if __name__ == ""__main__"":
    train_multi_gpu(batch_size=64, epochs=20)
```

### Relevant log output

```shell
Loss: NaN
```",1
"gpu_prim.h: no instance of overloaded function ""cub::ThreadLoadVolatilePointer"" matches the specified type","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

r2.19

### Custom code

No

### OS platform and distribution

Alma Linux 9.5

### Mobile device

_No response_

### Python version

3.12

### Bazel version

6.5.0

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.8.0

### GPU model and memory

_No response_

### Current behavior?

Compiling against CUDA 12.8.0 gives an error:

```
./tensorflow/core/kernels/gpu_prim.h(48): error: no instance of overloaded function ""cub::ThreadLoadVolatilePointer"" matches the specified type
                            Eigen::half ThreadLoadVolatilePointer<Eigen::half>(
```

[Commit 5467ee9](https://github.com/tensorflow/tensorflow/commit/5467ee993e1d3e4709c1e99f3a15a978325ae536) for XLA needs to be adapted and applied to tensorflow/core/kernels/gpu_prim.h.


### Standalone code to reproduce the issue

```shell
`env HERMETIC_CUDA_VERSION=1.8.0 ./configure`
`bazel build`
```

### Relevant log output

```shell
./tensorflow/core/kernels/gpu_prim.h(48): error: no instance of overloaded function ""cub::ThreadLoadVolatilePointer"" matches the specified type
                            Eigen::half ThreadLoadVolatilePointer<Eigen::half>(
```",1
Fail to build on Ubuntu 24.10,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

master

### Custom code

No

### OS platform and distribution

Ubuntu 24.10

### Mobile device

_No response_

### Python version

3.12

### Bazel version

7.4.1

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.6

### GPU model and memory

_No response_

### Current behavior?

bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tf_nightly --config=cuda_wheel --copt=-Wno-gnu-offsetof-extensions

The error message is as follows:

clang failed: error executing CppCompile command (from target @@upb//:upb) /usr/lib/llvm-19/bin/clang -MD -MF bazel-out/k8-opt/bin/external/upb/_objs/upb/upb.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/upb/_objs/upb/upb.pic.o' -iquote external/upb -iquote ... (remaining 43 arguments skipped)
external/upb/upb/upb.c:192:10: error: defining a type within 'offsetof' is a C23 extension [-Werror,-Wc23-extensions]
  192 |   n &= ~(upb_alignof(upb_arena) - 1);
      |          ^~~~~~~~~~~~~~~~~~~~~~
external/upb/upb/upb.c:183:37: note: expanded from macro 'upb_alignof'
  183 | #define upb_alignof(type) offsetof (struct { char c; type member; }, member)
      |                                     ^~~~~~
/usr/lib/llvm-19/lib/clang/19/include/__stddef_offsetof.h:16:43: note: expanded from macro 'offsetof'
   16 | #define offsetof(t, d) __builtin_offsetof(t, d)
      |                                           ^
1 error generated.


### Standalone code to reproduce the issue

```shell
No code
```

### Relevant log output

```shell

```",1
Starlark Transition Error: Unreadable type AutoValue_ExecutionInfoModifier for build setting //command_line_option:modify_execution_info,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

master branch commit 9ebac72

### Custom code

Yes

### OS platform and distribution

Win10

### Mobile device

_No response_

### Python version

3.12.8

### Bazel version

6.5.0

### GCC/compiler version

Microsoft Visual Studio 2022（v17.13.4）

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am part of the MSVC testing team at Microsoft. We build popular open source projects to test the compiler for any issues. During our regular update of recent commits, we found that when building TensorFlow with Bazel, we encountered the following error, causing the build to fail:
```
ERROR: C:/gitp/tensorflow/tensorflow/tensorflow/tensorflow.bzl:2986:5: before calling _local_exec_transition_impl: Input build setting //command_line_option:modify_execution_info is of type class com.google.devtools.build.lib.analysis.config.AutoValue_ExecutionInfoModifier, which is unreadable in Starlark. Please submit a feature request.
```

It looks like before calling _local_exec_transition_impl, the build setting //command_line_option:modify_execution_info is passed a type AutoValue_ExecutionInfoModifier which Starlark cannot read. This directly blocks the subsequent Starlark transformation and build process.
From the error message, the TensorFlow build rule passes an internal type that Starlark cannot resolve when calling _local_exec_transition_impl. The error message suggests submitting a feature request. I'm not sure if this is a limitation of Bazel or an adjustable part of the TensorFlow build rule. Do I need to change TensorFlow's build configuration to avoid this problem, or do I need to wait for improvements to Bazel's support for this type?

### Standalone code to reproduce the issue

```shell
I'm just building tensorflow, so I can't provide this
```

### Relevant log output

```shell
##[command] Command #0 (Output in ""Build.log""):  set VSCMD_SKIP_SENDTELEMETRY=1 & ""C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\Tools\VsDevCmd.bat"" -host_arch=amd64 -arch=amd64 & set _CL_= /Bcapture_repro C:\output\Tensorflow\preprocessed_repro_build & set _LINK_= /onfailrepro:C:\output\Tensorflow\link_repro_build
**********************************************************************
** Visual Studio 2022 Developer Command Prompt v17.13.4
** Copyright (c) 2022 Microsoft Corporation
**********************************************************************
##[debug] Command #0 exited with code [0].
##[command] Command #1 (Output in ""Build.log""):  set PATH=C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;%PATH%
##[debug] Command #1 exited with code [0].
##[command] Command #2 (Output in ""Build.log""):  cd /d C:\gitP\tensorflow\tensorflow\build_amd64
##[debug] Command #2 exited with code [0].
##[command] Command #3 (Output in ""Build.log""):  set BAZEL_VC=C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC
##[debug] Command #3 exited with code [0].
##[command] Command #4 (Output in ""Build.log""):  set BAZEL_VC_FULL_VERSION=14.43.34808
##[debug] Command #4 exited with code [0].
##[command] Command #5 (Output in ""Build.log""):  set PATH=C:\tools\msys64\usr\bin;%path%
##[debug] Command #5 exited with code [0].
##[command] Command #6 (Output in ""Build.log""):  bazel --output_user_root C:\bazelTemp build --jobs 16 --config=opt --local_ram_resources=16384  --subcommands //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tf_nightly --repo_env=TF_PYTHON_VERSION=3.12 2>&1
Extracting Bazel installation...
Starting local Bazel server and connecting to it...
INFO: Reading 'startup' options from c:\gitp\tensorflow\tensorflow\.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=0 --terminal_columns=80
INFO: Reading rc options for 'build' from c:\gitp\tensorflow\tensorflow\.bazelrc:
  Inherited 'common' options: --announce_rc --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility --noenable_bzlmod --noincompatible_enable_cc_toolchain_resolution --noincompatible_enable_android_toolchain_resolution --experimental_repo_remote_exec --java_runtime_version=remotejdk_21
INFO: Options provided by the client:
  'build' options: --python_path=C:/Python312/python.exe
INFO: Reading rc options for 'build' from c:\gitp\tensorflow\tensorflow\.bazelrc:
  'build' options: --repo_env=ML_WHEEL_TYPE=snapshot --repo_env=ML_WHEEL_BUILD_DATE= --repo_env=ML_WHEEL_VERSION_SUFFIX= --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --host_features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2
INFO: Reading rc options for 'build' from c:\gitp\tensorflow\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Python312/python.exe --action_env PYTHON_LIB_PATH=C:/Python312/Lib/site-packages --python_path=C:/Python312/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true
INFO: Found applicable config definition build:short_logs in file c:\gitp\tensorflow\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file c:\gitp\tensorflow\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:opt in file c:\gitp\tensorflow\tensorflow\.tf_configure.bazelrc: --copt=/arch:AVX --host_copt=/arch:AVX
INFO: Found applicable config definition build:windows in file c:\gitp\tensorflow\tensorflow\.bazelrc: --copt=/W0 --host_copt=/W0 --copt=/Zc:__cplusplus --host_copt=/Zc:__cplusplus --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --features=compiler_param_file --features=archive_param_file --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --copt=-D_ENABLE_EXTENDED_ALIGNED_STORAGE --host_copt=-D_ENABLE_EXTENDED_ALIGNED_STORAGE --enable_runfiles --nobuild_python_zip --dynamic_mode=off --cxxopt=/std:c++17 --host_cxxopt=/std:c++17 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/Zc:preprocessor --host_copt=/Zc:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --config=no_tfrt
INFO: Found applicable config definition build:monolithic in file c:\gitp\tensorflow\tensorflow\.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false
INFO: Found applicable config definition build:no_tfrt in file c:\gitp\tensorflow\tensorflow\.bazelrc: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils
Loading: 
Loading: 
Loading: 
DEBUG: C:/bazeltemp/yasrpdtv/external/local_xla/third_party/py/python_repo.bzl:87:10: 
=============================
Hermetic Python configuration:
Version: ""3.12""
Kind: """"
Interpreter: ""default"" (provided by rules_python)
Requirements_lock label: ""@python_version_repo//:requirements_lock_3_12.txt""
=====================================
Loading: 
Loading: 
Loading: 
Loading: 
Loading: 
Loading: 
Loading: 
Loading: 
Loading: 
Loading: 
Loading: 
Loading: 
Loading: 
Loading: 
Loading: 
Loading: 
Loading: 
Loading: 
Loading: 0 packages loaded
Analyzing: target //tensorflow/tools/pip_package:wheel (1 packages loaded, 0 targets configured)
Analyzing: target //tensorflow/tools/pip_package:wheel (6 packages loaded, 16 targets configured)
Analyzing: target //tensorflow/tools/pip_package:wheel (7 packages loaded, 18 targets configured)
Analyzing: target //tensorflow/tools/pip_package:wheel (90 packages loaded, 59 targets configured)
Analyzing: target //tensorflow/tools/pip_package:wheel (93 packages loaded, 59 targets configured)
Analyzing: target //tensorflow/tools/pip_package:wheel (93 packages loaded, 59 targets configured)
Analyzing: target //tensorflow/tools/pip_package:wheel (104 packages loaded, 71 targets configured)
Analyzing: target //tensorflow/tools/pip_package:wheel (120 packages loaded, 274 targets configured)
Analyzing: target //tensorflow/tools/pip_package:wheel (199 packages loaded, 623 targets configured)
Analyzing: target //tensorflow/tools/pip_package:wheel (241 packages loaded, 1306 targets configured)
ERROR: C:/gitp/tensorflow/tensorflow/tensorflow/tensorflow.bzl:2986:5: before calling _local_exec_transition_impl: Input build setting //command_line_option:modify_execution_info is of type class com.google.devtools.build.lib.analysis.config.AutoValue_ExecutionInfoModifier, which is unreadable in Starlark. Please submit a feature request.
ERROR: C:/gitp/tensorflow/tensorflow/tensorflow/python/platform/BUILD:27:25: Errors encountered while applying Starlark transition
INFO: Repository go_sdk instantiated at:
  C:/gitp/tensorflow/tensorflow/WORKSPACE:72:14: in <toplevel>
  C:/gitp/tensorflow/tensorflow/tensorflow/workspace0.bzl:135:20: in workspace
  C:/bazeltemp/yasrpdtv/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps
  C:/bazeltemp/yasrpdtv/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains
  C:/bazeltemp/yasrpdtv/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk
Repository rule _go_download_sdk defined at:
  C:/bazeltemp/yasrpdtv/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>
INFO: Repository pypi__pip instantiated at:
  C:/gitp/tensorflow/tensorflow/WORKSPACE:33:25: in <toplevel>
  C:/bazeltemp/yasrpdtv/external/local_xla/third_party/py/python_init_repositories.bzl:23:20: in python_init_repositories
  C:/bazeltemp/yasrpdtv/external/rules_python/python/private/py_repositories.bzl:70:14: in py_repositories
  C:/bazeltemp/yasrpdtv/external/rules_python/python/private/pypi/deps.bzl:133:14: in pypi_deps
  C:/bazeltemp/yasrpdtv/external/bazel_tools/tools/build_defs/repo/utils.bzl:233:18: in maybe
Repository rule http_archive defined at:
  C:/bazeltemp/yasrpdtv/external/bazel_tools/tools/build_defs/repo/http.bzl:372:31: in <toplevel>
ERROR: Analysis of target '//tensorflow/tools/pip_package:wheel' failed; build aborted: 
INFO: Elapsed time: 171.190s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (262 packages loaded, 4048 targets configured)
##[debug] Command #6 exited with code [1].
##[error] Detected error code [1].
```",1
Error in nested inputs recurrent neural networks when using layer.RNN,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v2.18.0-rc2-4-g6550e4bd802

### Custom code

No

### OS platform and distribution

WSL2 Ubuntu 22.04.1 LTS

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I've been having problems building a custom RNN that takes nested inputs. I've replicated the error with [this](https://www.tensorflow.org/guide/keras/working_with_rnns#rnns_with_listdict_inputs_or_nested_inputs)  documentation example of a custom nested inputs RNN. While the nested RNN cell works fine, when using the `layers.RNN` wrapper it fails, it seems that input_shape is not correctly passed to the nested cell build method.

While I've found the problem with version 2.18.0, I've also tested 2.16.2 and tf.nightly and it keeps happening, it can also be replicated in Colab.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import tensorflow.keras as keras


class NestedCell(keras.layers.Layer):
    def __init__(self, unit_1, unit_2, unit_3, **kwargs):
        self.unit_1 = unit_1
        self.unit_2 = unit_2
        self.unit_3 = unit_3
        self.state_size = [tf.TensorShape([unit_1]), tf.TensorShape([unit_2, unit_3])]
        self.output_size = [tf.TensorShape([unit_1]), tf.TensorShape([unit_2, unit_3])]
        super().__init__(**kwargs)

    def build(self, input_shapes):
        # expect input_shape to contain 2 items, [(batch, i1), (batch, i2, i3)]
        i1 = input_shapes[0][1]
        i2 = input_shapes[1][1]
        i3 = input_shapes[1][2]

        self.kernel_1 = self.add_weight(
            shape=(i1, self.unit_1), initializer=""uniform"", name=""kernel_1""
        )
        self.kernel_2_3 = self.add_weight(
            shape=(i2, i3, self.unit_2, self.unit_3),
            initializer=""uniform"",
            name=""kernel_2_3"",
        )

    def call(self, inputs, states):
        # inputs should be in [(batch, input_1), (batch, input_2, input_3)]
        # state should be in shape [(batch, unit_1), (batch, unit_2, unit_3)]
        input_1, input_2 = tf.nest.flatten(inputs)
        s1, s2 = states

        output_1 = tf.matmul(input_1, self.kernel_1)
        output_2_3 = tf.einsum(""bij,ijkl->bkl"", input_2, self.kernel_2_3)
        state_1 = s1 + output_1
        state_2_3 = s2 + output_2_3

        output = (output_1, output_2_3)
        new_states = (state_1, state_2_3)

        return output, new_states

    def get_config(self):
        return {""unit_1"": self.unit_1, ""unit_2"": self.unit_2, ""unit_3"": self.unit_3}


unit_1 = 10
unit_2 = 20
unit_3 = 30

i1 = 32
i2 = 64
i3 = 32
batch_size = 64
num_batches = 10
timestep = 50

cell = NestedCell(unit_1, unit_2, unit_3)
rnn = keras.layers.RNN(cell)

input_1 = keras.Input((None, i1))
input_2 = keras.Input((None, i2, i3))

outputs = rnn((input_1, input_2))

model = keras.models.Model([input_1, input_2], outputs)

model.compile(optimizer=""adam"", loss=""mse"", metrics=[""accuracy""])
```

### Relevant log output

```shell
2025-04-01 19:19:49.186743: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1743527989.197181   20234 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1743527989.200246   20234 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-01 19:19:49.210967: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
  File ""/home/user/projects/dl_stash/reproducing_bug.py"", line 67, in <module>
    outputs = rnn((input_1, input_2))
              ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/user/projects/dl_stash/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/user/projects/dl_stash/reproducing_bug.py"", line 18, in build
    i2 = input_shapes[1][1]
         ~~~~~~~~~~~~^^^
IndexError: tuple index out of range
```",1
TypeError in site-packages path detection when sys.path includes PosixPath (e.g. Streamlit),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

Version: 2.16.1

### Custom code

Yes

### OS platform and distribution

CentOS Stream release 9

### Mobile device

_No response_

### Python version

3.11.5 

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When running TensorFlow inside a Streamlit app, the following error occurs:


### Standalone code to reproduce the issue

```shell
I could not create a minimal reproducible example that triggers the issue 100% of the time, but this error consistently occurs in my actual project environment.

It seems to happen when TensorFlow is used inside a Streamlit app, and `sys.path` contains `PosixPath` objects.

Here’s a simplified version of the code I run:

import streamlit as st
import tensorflow as tf

st.write(tf.__version__)
```

### Relevant log output

```shell
TypeError: argument of type 'PosixPath' is not iterable
```",1
TensorFlow issue with data generator used for training a Keras LSTM autoencoder,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.5 LTS

### Mobile device

_No response_

### Python version

Python 3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am trying to build a model which is a LSTM-autoencoder using TensorFlow. The model generates the training data using 'tf.data.Dataset'.  The original dimension of the data which is loaded from a .mat file is `[79266,1001]`, I ran the code and then I got an error message saying : Training failed: None values not supported. I have tried to use the minimum batch size and I reduced the size of the data to check if the problem is related to memory load but still the issue is happening even for very small data and batch sizes. In the code I replaced the loading command with a random data generation just for the purposes of reproducing the error.



### Standalone code to reproduce the issue

```shell
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Force CPU execution
import sys
import tensorflow as tf
import os.path
import numpy as np
import scipy.io
import time
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.callbacks import Callback, ModelCheckpoint

# Ultra-safe hyperparameters
SPATIAL_SUBSAMPLE = 400  # Now taking every 400th spatial point (~198 points)
TEMPORAL_SUBSAMPLE = 50  # Taking every 50th snapshot (~20 timesteps)
SEQ_LENGTH = 1           # Single timestep sequences
LATENT_DIM = 1           # 1D latent space
BATCH_SIZE = 1           # Minimum batch size

def load_and_validate():
    """"""Load with maximum subsampling""""""
    U_COM = tf.random.normal(shape = [79266,1001])
    
    # Aggressive subsampling
    U_subsampled = U_COM[::SPATIAL_SUBSAMPLE, ::TEMPORAL_SUBSAMPLE]
    print(f""Subsampled shape: {U_subsampled.shape} (spatial × temporal)"")
    
    # Validation
    assert U_subsampled.shape[1] >= SEQ_LENGTH, ""Not enough timesteps""
    assert not np.isnan(U_subsampled).any(), ""NaN values detected""
    
    # Normalize
    U_min, U_max = np.min(U_subsampled), np.max(U_subsampled)
    return 2 * (U_subsampled - U_min) / (U_max - U_min) - 1

# Load with cleanup
tf.keras.backend.clear_session()
U_norm = load_and_validate()

# Create single-timestep sequences
sequences = U_norm[:, :, np.newaxis]  # Shape: (spatial, timesteps, 1)
print(f""Sequences shape: {sequences.shape}"")

# Create dataset
dataset = tf.data.Dataset.from_tensor_slices(sequences)
dataset = dataset.batch(BATCH_SIZE)

# Verify
sample = next(iter(dataset))
print(f""Sample batch shape: {sample.shape}"")

# Micro LSTM model
def build_micro_model():
    inputs = tf.keras.Input(shape=(sequences.shape[1], 1))
    x = layers.LSTM(2)(inputs)  # Only 2 units
    x = layers.Dense(LATENT_DIM)(x)
    x = layers.RepeatVector(sequences.shape[1])(x)
    outputs = layers.LSTM(1, return_sequences=True)(x)
    return tf.keras.Model(inputs, outputs)

model = build_micro_model()
model.compile(optimizer='adam', loss='mse')
model.summary()

# Training
os.makedirs('Tests', exist_ok=True)
try:
    history = model.fit(
        dataset,
        epochs=3,  # Very few epochs
        verbose=2
    )
    print(""Training completed successfully!"")
except Exception as e:
    print(f""Training failed: {str(e)}"")
    print(""\nThis should never happen with these settings."")
    print(""Please verify your input data structure."")
```

### Relevant log output

```shell
2025-03-31 13:08:33.404751: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-03-31 13:08:33.405142: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2025-03-31 13:08:33.407488: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2025-03-31 13:08:33.414088: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1743419313.425745   24852 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1743419313.428932   24852 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-31 13:08:33.440581: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-03-31 13:08:35.116129: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)
Subsampled shape: (199, 21) (spatial × temporal)
Sequences shape: (199, 21, 1)
Sample batch shape: (1, 21, 1)
Model: ""functional""
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ input_layer (InputLayer)        │ (None, 21, 1)          │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lstm (LSTM)                     │ (None, 2)              │            32 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense (Dense)                   │ (None, 1)              │             3 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ repeat_vector (RepeatVector)    │ (None, 21, 1)          │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lstm_1 (LSTM)                   │ (None, 21, 1)          │            12 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 47 (188.00 B)
 Trainable params: 47 (188.00 B)
 Non-trainable params: 0 (0.00 B)
Epoch 1/3
Training failed: None values not supported.

This should never happen with these settings.
Please verify your input data structure.
```",1
TensorFlow with CUDA: RTX 5xxx series isn't supported (CUDA_ERROR_INVALID_HANDLE),"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

WSL2

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.8

### GPU model and memory

5070TI

### Current behavior?

hi guys. i upgraded my GPU to 5070ti and found that tensorflow is still not working with new nvidia cards

honestly i knew that can be problems. but its been 3 month since nvidia released new gpu, i thought now should be no prob already.

any idea how long it will take until at least there will be a nightly version which will fix the problem?
like 1-2 weeks or half a year ? :)

or maybe someone found an easy way to fix it?








### Standalone code to reproduce the issue

```shell
import tensorflow as tf

model = tf.keras.Sequential([
    tf.keras.Input(shape=(20,)),
    tf.keras.layers.Dense(10),
    tf.keras.layers.Dense(1)
])
```

### Relevant log output

```shell
2025-03-31 14:23:13.091967: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-03-31 14:23:13.734399: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1743398598.268271   13190 gpu_device.cc:2429] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
W0000 00:00:1743398598.270049   13190 gpu_device.cc:2429] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
I0000 00:00:1743398598.408238   13190 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13123 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 5070 Ti, pci bus id: 0000:01:00.0, compute capability: 12.0
2025-03-31 14:23:18.851409: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleLoadData(&module, data)' failed with 'CUDA_ERROR_INVALID_PTX'

2025-03-31 14:23:18.851466: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleGetFunction(&function, module, kernel_name)' failed with 'CUDA_ERROR_INVALID_HANDLE'

2025-03-31 14:23:18.851502: W tensorflow/core/framework/op_kernel.cc:1843] INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'
2025-03-31 14:23:18.851526: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'
Traceback (most recent call last):
  File ""/mnt/c/Users/alex/desktop/test111.py"", line 3, in <module>
    model = tf.keras.Sequential([
  File ""/home/alexflame/.local/lib/python3.10/site-packages/keras/src/models/sequential.py"", line 76, in __init__
    self._maybe_rebuild()
  File ""/home/alexflame/.local/lib/python3.10/site-packages/keras/src/models/sequential.py"", line 149, in _maybe_rebuild
    self.build(input_shape)
  File ""/home/alexflame/.local/lib/python3.10/site-packages/keras/src/layers/layer.py"", line 229, in build_wrapper
    original_build_method(*args, **kwargs)
  File ""/home/alexflame/.local/lib/python3.10/site-packages/keras/src/models/sequential.py"", line 195, in build
    x = layer(x)
  File ""/home/alexflame/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/alexflame/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/core.py"", line 152, in convert_to_tensor
    return tf.cast(x, dtype)
tensorflow.python.framework.errors_impl.InternalError: {{function_node __wrapped__Cast_device_/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' [Op:Cast] name:
```",1
`Aborted` in `tf.raw_ops.BatchMatMulV2`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.19.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an abortion issue with tf.raw_ops.BatchMatMulV2 in tf-2.19.0. This bug can be used to trigger a denial of service attack. I have attached a [gist](https://colab.research.google.com/drive/1hIeUZYcxfySEUthZcwrlEQ6EwdXYiFi_?usp=sharing) for your reference.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import time
def test_bug():
    tf.config.experimental_run_functions_eagerly(True)
    A = tf.fill((100, 100), 1e-45)
    B = tf.random.uniform((100, 100), minval=0, maxval=1, dtype=tf.float32)
    for _ in range(10):
        tf.raw_ops.BatchMatMulV2(x=A[tf.newaxis, ...], y=B[tf.newaxis, ...])
    n_iter = 300
    times = []
    X = A
    for i in range(n_iter):
        t0 = time.time()
        X = tf.raw_ops.BatchMatMulV2(x=X[tf.newaxis, ...], y=B[tf.newaxis, ...])
        t1 = time.time()
        times.append(t1 - t0)
    early_avg = sum(times[:50]) / 50
    late_avg  = sum(times[-50:]) / 50
if __name__ == ""__main__"":
    test_bug()
```

### Relevant log output

```shell
Aborted
```",1
"Memory leak in tfp.math.minimize, reproducible from two tutorial notebooks.","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.18.0

### Custom code

No

### OS platform and distribution

M2 Mac MacOS  Sequoia 15.2

### Mobile device

_No response_

### Python version

3.11.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I recently filed this as an issue in tensorflow/probability but perhaps I should file it here instead:

Python version: 3.11.6
tensorflow==2.18.0
    # via
    #   -r requirements.in
    #   tf-keras
tensorflow-probability==0.24.0
    # via -r requirements.in
tf-keras==2.18.0
    # via -r requirements.in


I am trying to perform an analysis built on the multiple changepoint detection and Bayesian model selection code found in this tutorial notebook, specifically the ""Unknown number of states"" portion: https://www.tensorflow.org/probability/examples/Multiple_changepoint_detection_and_Bayesian_model_selection

In the process of running the analysis for multiple datasets I observed that the memory used increases with each successive analysis, by about 20 MB per run in my code. I confirmed that this occurs with the original tutorial notebook as well to ensure that there is not an error specific to my code. The easiest way to reproduce is to download the notebook, execute all cells, then rerun from the ""Unknown number of states"" section. I have tried adding `tf_keras.backend.clear_session()` between each run, deleting all created objects before rerunning, and running garbage collection, but nothing has released the ~20MB of memory that is allocated with each `tfp.math.minimize` run. The example I show in the standalone code below is from the ""Known number of states"" example, which has about 12 MiB increase in memory with each run.

I see a similar issue with the [Probabilistic PCA tutorial notebook](https://www.tensorflow.org/probability/examples/Probabilistic_PCA). Memory increase with each run is ~3 MiB per run.

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf
import tf_keras
import tensorflow_probability as tfp
from tensorflow_probability import distributions as tfd

import os
import psutil
import gc


true_rates = [40, 3, 20, 50]
true_durations = [10, 20, 5, 35]

observed_counts = tf.concat(
    [tfd.Poisson(rate).sample(num_steps)
     for (rate, num_steps) in zip(true_rates, true_durations)], axis=0)

num_states = 4
initial_state_logits = tf.zeros([num_states]) # uniform distribution

daily_change_prob = 0.05
transition_probs = tf.fill([num_states, num_states],
                           daily_change_prob / (num_states - 1))
transition_probs = tf.linalg.set_diag(transition_probs,
                                      tf.fill([num_states],
                                              1 - daily_change_prob))

trainable_log_rates = tf.Variable(
  tf.math.log(tf.reduce_mean(observed_counts)) +
  tf.random.stateless_normal([num_states], seed=(42, 42)),
  name='log_rates')

for i in range(20):
    tf_keras.backend.clear_session()
    gc.collect()
    hmm = tfd.HiddenMarkovModel(
      initial_distribution=tfd.Categorical(
          logits=initial_state_logits),
      transition_distribution=tfd.Categorical(probs=transition_probs),
      observation_distribution=tfd.Poisson(log_rate=trainable_log_rates),
      num_steps=len(observed_counts))
    
    rate_prior = tfd.LogNormal(5, 5)
    
    def log_prob():
     return (tf.reduce_sum(rate_prior.log_prob(tf.math.exp(trainable_log_rates))) +
             hmm.log_prob(observed_counts))
    
    losses = tfp.math.minimize(
        lambda: -log_prob(),
        optimizer=tf_keras.optimizers.legacy.Adam(learning_rate=0.1),
        num_steps=100)

    print(f""[#{i+1}] Memory usage: {psutil.Process(os.getpid()).memory_info().rss / 1024 **2} MiB"")
    del hmm, losses
```

### Relevant log output

```shell
[#1] Memory usage: 583.109375 MiB
[#2] Memory usage: 596.3125 MiB
[#3] Memory usage: 608.5 MiB
[#4] Memory usage: 620.40625 MiB
[#5] Memory usage: 629.28125 MiB
[#6] Memory usage: 639.65625 MiB
[#7] Memory usage: 648.78125 MiB
[#8] Memory usage: 662.71875 MiB
[#9] Memory usage: 673.234375 MiB
[#10] Memory usage: 688.53125 MiB
[#11] Memory usage: 699.40625 MiB
[#12] Memory usage: 710.140625 MiB
[#13] Memory usage: 719.234375 MiB
[#14] Memory usage: 728.609375 MiB
[#15] Memory usage: 736.859375 MiB
[#16] Memory usage: 746.84375 MiB
[#17] Memory usage: 758.03125 MiB
[#18] Memory usage: 767.5625 MiB
[#19] Memory usage: 777.453125 MiB
[#20] Memory usage: 785.0625 MiB
```",1
Build against TF 2.19: llvm/ADT/ArrayRef.h: No such file or directory,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.19.0

### Custom code

Yes

### OS platform and distribution

Linux

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

```
  In file included from /usr/local/lib/python3.11/dist-packages/tensorflow/include/xla/stream_executor/stream_executor.h:40,
                   from /usr/local/lib/python3.11/dist-packages/tensorflow/include/xla/service/stream_pool.h:23,
                   from /usr/local/lib/python3.11/dist-packages/tensorflow/include/xla/service/service_executable_run_options.h:25,
                   from /usr/local/lib/python3.11/dist-packages/tensorflow/include/xla/service/executable.h:40,
                   from /usr/local/lib/python3.11/dist-packages/tensorflow/include/xla/service/compiler.h:40,
                   from /usr/local/lib/python3.11/dist-packages/tensorflow/include/xla/service/backend.h:34,
                   from /usr/local/lib/python3.11/dist-packages/tensorflow/include/xla/service/allocation_tracker.h:30,
                   from /usr/local/lib/python3.11/dist-packages/tensorflow/include/xla/service/service.h:33,
                   from /usr/local/lib/python3.11/dist-packages/tensorflow/include/xla/client/client.h:32,
                   from /usr/local/lib/python3.11/dist-packages/tensorflow/include/tensorflow/compiler/tf2xla/xla_expression.h:21,
                   from /usr/local/lib/python3.11/dist-packages/tensorflow/include/tensorflow/compiler/tf2xla/xla_compiler.h:28,
                   from /usr/local/lib/python3.11/dist-packages/tensorflow/include/tensorflow/compiler/tf2xla/xla_op_kernel.h:23,
                   from /tmp/pip-install-x2yl8oev/horovod_4633c6601fd0413bb51391e014b651b0/horovod/tensorflow/xla_mpi_ops.cc:25:
  /usr/local/lib/python3.11/dist-packages/tensorflow/include/xla/stream_executor/gpu/tma_metadata.h:25:10: fatal error: llvm/ADT/ArrayRef.h: No such file or directory
     25 | #include ""llvm/ADT/ArrayRef.h""
        |          ^~~~~~~~~~~~~~~~~~~~~
  compilation terminated.
```

Building Horovod against TF 2.19 reports this error. It is related to #83257, which added a reference to llvm, but llvm is not contained in the pip wheel:

https://github.com/tensorflow/tensorflow/blob/a817e56cb950b432e2abb27ad11582fb5a326107/tensorflow/tools/pip_package/build_pip_package.py#L122

Reproduce: https://gist.github.com/njzjz/ab88fffd3cfbb534dfb1e11ec4deb040

### Standalone code to reproduce the issue

```shell
# https://gist.github.com/njzjz/ab88fffd3cfbb534dfb1e11ec4deb040
pip install tensorflow==2.19
HOROVOD_WITH_TENSORFLOW=1 HOROVOD_WITHOUT_PYTORCH=1 pip -v install horovod
```

### Relevant log output

```shell
[ 99%] Building CXX object horovod/tensorflow/CMakeFiles/tensorflow.dir/xla_mpi_ops.cc.o
  cd /tmp/pip-install-x2yl8oev/horovod_4633c6601fd0413bb51391e014b651b0/build/temp.linux-x86_64-cpython-311/RelWithDebInfo/horovod/tensorflow && /usr/bin/c++ -DEIGEN_MPL2_ONLY=1 -DHAVE_GLOO=1 -DHAVE_MPI=1 -DTENSORFLOW_VERSION=2019000000 -Dtensorflow_EXPORTS -I/tmp/pip-install-x2yl8oev/horovod_4633c6601fd0413bb51391e014b651b0/third_party/HTTPRequest/include -I/tmp/pip-install-x2yl8oev/horovod_4633c6601fd0413bb51391e014b651b0/third_party/boost/assert/include -I/tmp/pip-install-x2yl8oev/horovod_4633c6601fd0413bb51391e014b651b0/third_party/boost/config/include -I/tmp/pip-install-x2yl8oev/horovod_4633c6601fd0413bb51391e014b651b0/third_party/boost/core/include -I/tmp/pip-install-x2yl8oev/horovod_4633c6601fd0413bb51391e014b651b0/third_party/boost/detail/include -I/tmp/pip-install-x2yl8oev/horovod_4633c6601fd0413bb51391e014b651b0/third_party/boost/iterator/include -I/tmp/pip-install-x2yl8oev/horovod_4633c6601fd0413bb51391e014b651b0/third_party/boost/lockfree/include -I/tmp/pip-install-x2yl8oev/horovod_4633c6601fd0413bb51391e014b651b0/third_party/boost/mpl/include -I/tmp/pip-install-x2yl8oev/horovod_4633c6601fd0413bb51391e014b651b0/third_party/boost/parameter/include -I/tmp/pip-install-x2yl8oev/horovod_4633c6601fd0413bb51391e014b651b0/third_party/boost/predef/include -I/tmp/pip-install-x2yl8oev/horovod_4633c6601fd0413bb51391e014b651b0/third_party/boost/preprocessor/include -I/tmp/pip-install-x2yl8oev/horovod_4633c6601fd0413bb51391e014b651b0/third_party/boost/static_assert/include -I/tmp/pip-install-x2yl8oev/horovod_4633c6601fd0413bb51391e014b651b0/third_party/boost/type_traits/include -I/tmp/pip-install-x2yl8oev/horovod_4633c6601fd0413bb51391e014b651b0/third_party/boost/utility/include -I/tmp/pip-install-x2yl8oev/horovod_4633c6601fd0413bb51391e014b651b0/third_party/lbfgs/include -I/tmp/pip-install-x2yl8oev/horovod_4633c6601fd0413bb51391e014b651b0/third_party/gloo -I/tmp/pip-install-x2yl8oev/horovod_4633c6601fd0413bb51391e014b651b0/third_party/flatbuffers/include -isystem /usr/lib/x86_64-linux-gnu/openmpi/include -isystem /usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -isystem /usr/local/lib/python3.11/dist-packages/tensorflow/include -I/usr/local/lib/python3.11/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=1 --std=c++17 -DEIGEN_MAX_ALIGN_BYTES=64  -pthread -fPIC -Wall -ftree-vectorize -mf16c -mavx -mfma -O3 -g -DNDEBUG -std=c++17 -fPIC -MD -MT horovod/tensorflow/CMakeFiles/tensorflow.dir/xla_mpi_ops.cc.o -MF CMakeFiles/tensorflow.dir/xla_mpi_ops.cc.o.d -o CMakeFiles/tensorflow.dir/xla_mpi_ops.cc.o -c /tmp/pip-install-x2yl8oev/horovod_4633c6601fd0413bb51391e014b651b0/horovod/tensorflow/xla_mpi_ops.cc
  In file included from /usr/local/lib/python3.11/dist-packages/tensorflow/include/xla/stream_executor/stream_executor.h:40,
                   from /usr/local/lib/python3.11/dist-packages/tensorflow/include/xla/service/stream_pool.h:23,
                   from /usr/local/lib/python3.11/dist-packages/tensorflow/include/xla/service/service_executable_run_options.h:25,
                   from /usr/local/lib/python3.11/dist-packages/tensorflow/include/xla/service/executable.h:40,
                   from /usr/local/lib/python3.11/dist-packages/tensorflow/include/xla/service/compiler.h:40,
                   from /usr/local/lib/python3.11/dist-packages/tensorflow/include/xla/service/backend.h:34,
                   from /usr/local/lib/python3.11/dist-packages/tensorflow/include/xla/service/allocation_tracker.h:30,
                   from /usr/local/lib/python3.11/dist-packages/tensorflow/include/xla/service/service.h:33,
                   from /usr/local/lib/python3.11/dist-packages/tensorflow/include/xla/client/client.h:32,
                   from /usr/local/lib/python3.11/dist-packages/tensorflow/include/tensorflow/compiler/tf2xla/xla_expression.h:21,
                   from /usr/local/lib/python3.11/dist-packages/tensorflow/include/tensorflow/compiler/tf2xla/xla_compiler.h:28,
                   from /usr/local/lib/python3.11/dist-packages/tensorflow/include/tensorflow/compiler/tf2xla/xla_op_kernel.h:23,
                   from /tmp/pip-install-x2yl8oev/horovod_4633c6601fd0413bb51391e014b651b0/horovod/tensorflow/xla_mpi_ops.cc:25:
  /usr/local/lib/python3.11/dist-packages/tensorflow/include/xla/stream_executor/gpu/tma_metadata.h:25:10: fatal error: llvm/ADT/ArrayRef.h: No such file or directory
     25 | #include ""llvm/ADT/ArrayRef.h""
        |          ^~~~~~~~~~~~~~~~~~~~~
  compilation terminated.
  gmake[2]: *** [horovod/tensorflow/CMakeFiles/tensorflow.dir/build.make:513: horovod/tensorflow/CMakeFiles/tensorflow.dir/xla_mpi_ops.cc.o] Error 1
  gmake[2]: *** Waiting for unfinished jobs....
```",2
colab tpu initialization,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I was unable to initialize tpu in colab, it seems that the system failed to detect the tpu.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1mGQ7_-EeCtFnrrtUW6mYhyFx8k06MCTz?usp=sharing
```

### Relevant log output

```shell
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.11/dist-packages/tensorflow/python/tpu/tpu_strategy_util.py in initialize_tpu_system_impl(cluster_resolver, tpu_cluster_resolver_cls)
    138       with ops.device(tpu._tpu_system_device_name(job)):  # pylint: disable=protected-access
--> 139         output = _tpu_init_fn()
    140       context.async_wait()

4 frames
/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs)
    152       filtered_tb = _process_traceback_frames(e.__traceback__)
--> 153       raise e.with_traceback(filtered_tb) from None
    154     finally:

/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     58     raise core._status_to_exception(e) from None
---> 59   except TypeError as e:
     60     keras_symbolic_tensors = [x for x in inputs if _is_keras_symbolic_tensor(x)]

InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [tpu_embedding_config="""", is_global_init=false, compilation_failure_closes_chips=false, embedding_config="""", enable_whole_mesh_compilations=false, tpu_cancellation_closes_chips=2]
Registered devices: [CPU]
Registered kernels:
  <no registered kernels>

	 [[ConfigureDistributedTPU]] [Op:__inference__tpu_init_fn_4]

During handling of the above exception, another exception occurred:

NotFoundError                             Traceback (most recent call last)
<ipython-input-2-1b8a8e7ba345> in <cell line: 0>()
      3 tf.config.experimental_connect_to_cluster(resolver)
      4 # This is the TPU initialization code that has to be at the beginning.
----> 5 tf.tpu.experimental.initialize_tpu_system(resolver)
      6 print(""All devices: "", tf.config.list_logical_devices('TPU'))

/usr/local/lib/python3.11/dist-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py in initialize_tpu_system(cluster_resolver)
     70     NotFoundError: If no TPU devices found in eager mode.
     71   """"""
---> 72   return tpu_strategy_util.initialize_tpu_system_impl(
     73       cluster_resolver, TPUClusterResolver)
     74 

/usr/local/lib/python3.11/dist-packages/tensorflow/python/tpu/tpu_strategy_util.py in initialize_tpu_system_impl(cluster_resolver, tpu_cluster_resolver_cls)
    140       context.async_wait()
    141     except errors.InvalidArgumentError as e:
--> 142       raise errors.NotFoundError(
    143           None, None,
    144           ""TPUs not found in the cluster. Failed in initialization: ""

NotFoundError: TPUs not found in the cluster. Failed in initialization: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [tpu_embedding_config="""", is_global_init=false, compilation_failure_closes_chips=false, embedding_config="""", enable_whole_mesh_compilations=false, tpu_cancellation_closes_chips=2]
Registered devices: [CPU]
Registered kernels:
  <no registered kernels>

	 [[ConfigureDistributedTPU]] [Op:__inference__tpu_init_fn_4]
```",1
fastspeech2 中文中训练,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v2.9.0-rc2-42-g8a20d54a3c1 2.9.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.5 LTS

### Mobile device

Ubuntu 22.04.5 LTS

### Python version

Python 3.7.12

### Bazel version

_No response_

### GCC/compiler version

gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0

### CUDA/cuDNN version

nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2021 NVIDIA Corporation Built on Thu_Nov_18_09:45:30_PST_2021 Cuda compilation tools, release 11.5, V11.5.119 Build cuda_11.5.r11.5/compiler.30672275_0

### GPU model and memory

 NVIDIA-SMI 535.230.02             Driver Version: 535.230.02   CUDA Version: 12.2  NVIDIA GeForce RTX 4090

### Current behavior?

我通过[TensorFlowTTS](https://github.com/TensorSpeech/TensorFlowTTS)项目，训练了一个自己的音色模型，但是合成的音频声音很奇怪。以下是我用同一段文字在自己训练的模型和huggingface下载的模型【tensorspeech/tts-fastspeech2-baker-ch】合成的语音的对比。melgan用的是同一个，是从huggingface下载的【tensorspeech/tts-mb_melgan-baker-ch】。难道melgan也要自己重新训练吗？

![Image](https://github.com/user-attachments/assets/55561285-248a-450d-b9b1-f5180d261d31)
用我生成的模型合成出来的音频能微弱的听到声音，但是很小声。不知道是什么问题导致的。

### Standalone code to reproduce the issue

```shell
import soundfile as sf
import numpy as np

import tensorflow as tf
import sys
sys.path.append(""."")

from tensorflow_tts.inference import AutoConfig
from tensorflow_tts.inference import AutoProcessor
from tensorflow_tts.inference import TFAutoModel

mel_cfg = AutoConfig.from_pretrained(""/data/benben/TensorFlowTTS-master/my_test/mel.yml"")
mb_melgan = TFAutoModel.from_pretrained(""/data/benben/TensorFlowTTS-master/my_test/mel.h5"", config=mel_cfg)

config1 = AutoConfig.from_pretrained(""examples/fastspeech2/conf/fastspeech2.baker.v2.yaml"")

# fastspeech2 = TFAutoModel.from_pretrained(""dump_mandarin-fm/exp/train.fastspeech2.baker.v2/checkpoints/model-10000.h5"", config=config1)
# processor = AutoProcessor.from_pretrained(""dump_mandarin-fm/baker_mapper.json"")

fastspeech2 = TFAutoModel.from_pretrained(""my_test/model.h5"", config=config1)
processor = AutoProcessor.from_pretrained(""my_test/processor.json"")

text = ""通用在去年第三季度每天售出逾二点七万辆新车，业界关注的是转型创业企业生态圈运营商之后""
text = ""通用在去年第三季度每天售出逾二点七万辆新车""

input_ids = processor.text_to_sequence(text, inference=True)

print(""***********"",input_ids)

mel_before, mel_after, duration_outputs, _, _ = fastspeech2.inference(
    input_ids=tf.expand_dims(tf.convert_to_tensor(input_ids, dtype=tf.int32), 0),
    speaker_ids=tf.convert_to_tensor([0], dtype=tf.int32),
    speed_ratios=tf.convert_to_tensor([1.0], dtype=tf.float32),
    f0_ratios =tf.convert_to_tensor([1.0], dtype=tf.float32),
    energy_ratios =tf.convert_to_tensor([1.0], dtype=tf.float32),
)


print(mel_before)
print(mel_after)
# melgan inference (mel-to-wav)
audio = mb_melgan.inference(mel_after)[0, :, 0]

# save to file
sf.write('./audio.wav', audio, 24000, ""PCM_16"")
# sf.write('./audio.wav', audio, 24000)
```

### Relevant log output

```shell

```",1
Add support for SeparableConv2DTranspose (Depthwise Conv2DTranspose),"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.16.1

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Feature Request

TensorFlow currently lacks a SeparableConv2DTranspose operation, which is essential for efficient depthwise transposed convolutions. PyTorch already supports this easily via the groups parameter in ConvTranspose2d. However, TensorFlow’s Conv2DTranspose does not have a groups argument, making it impossible to achieve the same functionality.

Workarounds

Currently, the only way to approximate this functionality in TensorFlow is:
	•	Manually splitting input channels and applying Conv2DTranspose separately (inefficient and slow).

Proposed Solution
	•	Either add a groups parameter to Conv2DTranspose, similar to PyTorch,
	•	Or implement a native SeparableConv2DTranspose API to complement SeparableConv2D.

Use Case

Separable transposed convolutions are useful for:
	•	Efficient upsampling in lightweight CNN architectures.
	•	Mobile and edge computing models requiring optimized computations.

Would be great to have this officially supported in TensorFlow! Thanks.

#12001 
Previous discussion has been closed which is not able to reopen, so I create a new one for it.

### Standalone code to reproduce the issue

```shell
None
```

### Relevant log output

```shell

```",1
NotImplementedError: StreamingModel.call() not implemented,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.18.0

### Custom code

Yes

### OS platform and distribution

windows 11

### Mobile device

_No response_

### Python version

3.12.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When I try to compile and run my custom Keras model (StreamingModel), I encounter the following error:
`NotImplementedError: Exception encountered when calling StreamingModel.call().
Could not automatically infer the output shape / dtype of 'streaming_model' (of type StreamingModel).
Either the `StreamingModel.call()` method is incorrect, or you need to implement the `StreamingModel.compute_output_spec() / compute_output_shape()` method.
Error encountered: Model StreamingModel does not have a `call()` method implemented.`
Expected Behavior
I expect the StreamingModel to correctly implement the call() method and infer the output shape/dtype automatically, or allow me to manually specify it using compute_output_spec() or compute_output_shape().


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# Define CircularBufferLayer
class CircularBufferLayer(tf.keras.layers.Layer):
    def __init__(self, num_features, buffer_size, stride, **kwargs):
        super().__init__(**kwargs)
        self.num_features = num_features
        self.buffer_size = buffer_size
        self.stride = stride
        self.gradient_scale = 0.1
        self.buffer = self.add_weight(name='buffer', shape=(1, buffer_size, num_features),
                                      initializer='zeros', trainable=False, dtype=tf.float32)
        self.call_count = self.add_weight(name='call_count', shape=(), initializer='zeros',
                                          dtype=tf.int32, trainable=False)
        self.total_call_count = self.add_weight(name='total_call_count', shape=(), initializer='zeros',
                                                dtype=tf.int32, trainable=False)

    def call(self, inputs):
        scaled_input = tf.multiply(inputs, self.gradient_scale)
        new_buffer = tf.concat([scaled_input, self.buffer[:, :-1]], axis=1)
        self.buffer.assign(new_buffer)
        return self.buffer

# Define StreamingModel
class StreamingModel(tf.keras.Model):
    def call(self, inputs):
        x, _ = super().call(inputs)  # Assume another branch is truncated
        return x

# Instantiate the model
buffer_layer = CircularBufferLayer(num_features=64, buffer_size=10, stride=1)
model = StreamingModel()

# Define input shape
input_shape = (None, 10, 64)  # (batch_size, sequence_length, num_features)
inputs = tf.keras.Input(shape=input_shape[1:])  # Ignore batch_size
outputs = model(inputs)

# Build the model
model = tf.keras.Model(inputs=inputs, outputs=outputs)

# Compile the model
model.compile(
    optimizer='adam',  # Use Adam optimizer
    loss='mse',       # Use mean squared error as the loss function
    metrics=['mae']   # Use mean absolute error as the evaluation metric
)
```

### Relevant log output

```shell
2025-03-19 16:09:45.545622: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-03-19 16:09:46.283611: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-03-19 16:09:48.149403: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING:tensorflow:From D:\project\DLComplier\.venv\Lib\site-packages\keras\src\backend\tensorflow\core.py:216: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Traceback (most recent call last):
  File ""D:\project\DLComplier\.venv\statiblity.py"", line 37, in <module>
    outputs = model(inputs)
              ^^^^^^^^^^^^^
  File ""D:\project\DLComplier\.venv\Lib\site-packages\keras\src\utils\traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""D:\project\DLComplier\.venv\statiblity.py"", line 27, in call
    x, _ = super().call(inputs)  # Assume another branch is truncated
           ^^^^^^^^^^^^^^^^^^^^
NotImplementedError: Exception encountered when calling StreamingModel.call().

Could not automatically infer the output shape / dtype of 'streaming_model' (of type StreamingModel). Either the `StreamingModel.call()` method is incorrect, or you need to implement the `StreamingModel.compute_output_spec() / compute_output_shape()` method. Error encountered:

Model StreamingModel does not have a `call()` method implemented.

Arguments received by StreamingModel.call():
  • args=('<KerasTensor shape=(None, 10, 64), dtype=float32, sparse=False, name=keras_tensor>',)
  • kwargs=<class 'inspect._empty'>
```",1
How to adapt TFRA with Tensorflow Parameter Strategy V2,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

all OS

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

tensorflow recomendation addon 's keras + Ps has been broken for sometime, following are the issues so far
https://github.com/tensorflow/recommenders-addons/issues/182
https://github.com/tensorflow/recommenders-addons/issues/401
https://github.com/tensorflow/recommenders-addons/issues/365

Specifically, the incompatibility stems from changes introduced by TensorFlow’s ParameterServerStrategy. Two issues have been identified so far, though there may be more:

- Changes in ParameterServerStrategy.extended.worker_devices return values – This issue has already been addressed in https://github.com/tensorflow/recommenders-addons/pull/488

- Graph creation and variable placement differences – ParameterServerStrategy creates the computation graph on the PS and then distributes it to workers. TFRA,  uses proxy variables on each worker. These proxy variables wrapped in DistributedVariableWrapper and inherit from DistributedVariable, and tfra rely on DistributedVariable to get the correct device placement i.e. ._get_on_device_or_primary method,  however, this  only returns device 0, i.e. /job:worker/replica:0/task:0/device:GPU:0 , under ParameterServerStrategy, so all other workers keep crashing.

I wonder what will be correct path to fix the TensorFlow’s ParameterServerStrategy + TFRA + keras? 
I had explored PerWorkerVariable 
to get a PerWorkerVariable, we need to use something like 
`variables.Variable(initial_value=(),
  shape=shape, dtype=dtype, name=name, per_worker_de_variable=True)`
      this is not clean for complex classes, i.e.
`class TrainableWrapper(resource_variable_ops.ResourceVariable):`
`class ShadowVariable(EmbeddingWeights, TrainableWrapper):`
      to make ShadowVariable a PerWorkerVariable,   new classes needed with full duplication 
`
class TrainableWrapperPerWorker(PerWorkerVariable):`
`class ShadowVariablePerWorker(EmbeddingWeights, TrainableWrapperPerWorker):
`
is this the right path?
     



### Standalone code to reproduce the issue

```shell
any of the code in the issues
https://github.com/tensorflow/recommenders-addons/issues/182
https://github.com/tensorflow/recommenders-addons/issues/401
https://github.com/tensorflow/recommenders-addons/issues/365
```

### Relevant log output

```shell
RROR:tensorflow:Worker /job:worker/replica:0/task:1 failed with InvalidArgumentError():/job:worker/replica:0/task:0/device:GPU:0 unknown device.
Additional GRPC error information from remote target /job:worker/replica:0/task:1 while calling /tensorflow.eager.EagerService/Enqueue:
:{""created"":""@1742438917.320488513"",""description"":""Error received from peer ipv4:127.0.0.1:2232"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""/job:worker/replica:0/task:0/device:GPU:0 unknown device."",""grpc_status"":3} [Op:__inference_train_function_2473]
E0320 02:48:37.325277 139838372021824 cluster_coordinator.py:949] Worker /job:worker/replica:0/task:1 failed with InvalidArgumentError():/job:worker/replica:0/task:0/device:GPU:0 unknown device.
Additional GRPC error information from remote target /job:worker/replica:0/task:1 while calling /tensorflow.eager.EagerService/Enqueue:
:{""created"":""@1742438917.320488513"",""description"":""Error received from peer ipv4:127.0.0.1:2232"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""/job:worker/replica:0/task:0/device:GPU:0 unknown device."",""grpc_status"":3} [Op:__inference_train_function_2473]
INFO:tensorflow:[Worker 1] Putting back a closure after it failed.
I0320 02:48:37.325551 139838372021824 cluster_coordinator.py:1077] [Worker 1] Putting back a closure after it failed.
INFO:tensorflow:[Worker 1] Clearing all resources.
I0320 02:48:37.325744 139838372021824 cluster_coordinator.py:1065] [Worker 1] Clearing all resources.
INFO:tensorflow:Cluster now being recovered.
I0320 02:48:37.325959 139838355236416 cluster_coordinator.py:991] Cluster now being recovered.
2025-03-20 02:48:37.326268: W tensorflow/core/common_runtime/eager/context_distributed_manager.cc:694] Device filters can only be specified when initializing the cluster. Any changes in device filters are ignored when updating the server def.
INFO:tensorflow:Cluster successfully recovered.
I0320 02:48:37.334258 139838355236416 cluster_coordinator.py:997] Cluster successfully recovered.
INFO:tensorflow:Worker /job:worker/replica:0/task:1 has been recovered.
I0320 02:48:37.334562 139838372021824 cluster_coordinator.py:964] Worker /job:worker/replica:0/task:1 has been recovered.
INFO:tensorflow:Worker /job:worker/replica:0/task:1 calling on_recovery_fn
I0320 02:48:37.334733 139838372021824 cluster_coordinator.py:967] Worker /job:worker/replica:0/task:1 calling on_recovery_fn
INFO:tensorflow:[Worker 1] calling _on_worker_recovery
I0320 02:48:37.334853 139838372021824 cluster_coordinator.py:1103] [Worker 1] calling _on_worker_recovery
2025-03-20 02:48:37.361068: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:553] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.
2025-03-20 02:48:37.362102: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:553] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.
```",2
Compilation error（The call method of the StreamingModel class is not implemented correctly）,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.18.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Is this a new compilation error？The call method of the StreamingModel class is not implemented correctly, causing TensorFlow/Karas to be unable to infer the output shape and data type of the model.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# 定义 CircularBufferLayer
class CircularBufferLayer(tf.keras.layers.Layer):
    def __init__(self, num_features, buffer_size, stride, **kwargs):
        super().__init__(**kwargs)
        self.num_features = num_features
        self.buffer_size = buffer_size
        self.stride = stride
        self.gradient_scale = 0.1
        self.buffer = self.add_weight(name='buffer', shape=(1, buffer_size, num_features),
                                      initializer='zeros', trainable=False, dtype=tf.float32)
        self.call_count = self.add_weight(name='call_count', shape=(), initializer='zeros',
                                          dtype=tf.int32, trainable=False)
        self.total_call_count = self.add_weight(name='total_call_count', shape=(), initializer='zeros',
                                                dtype=tf.int32, trainable=False)

    def call(self, inputs):
        scaled_input = tf.multiply(inputs, self.gradient_scale)
        new_buffer = tf.concat([scaled_input, self.buffer[:, :-1]], axis=1)
        self.buffer.assign(new_buffer)
        return self.buffer

# 定义 StreamingModel
class StreamingModel(tf.keras.Model):
    def call(self, inputs):
        x, _ = super().call(inputs)  # 假设另一个分支被截断
        return x

# 实例化模型
buffer_layer = CircularBufferLayer(num_features=64, buffer_size=10, stride=1)
model = StreamingModel()

# 定义输入形状
input_shape = (None, 10, 64)  # (batch_size, sequence_length, num_features)
inputs = tf.keras.Input(shape=input_shape[1:])  # 忽略 batch_size
outputs = model(inputs)

# 构建模型
model = tf.keras.Model(inputs=inputs, outputs=outputs)

# 编译模型
model.compile(
    optimizer='adam',  # 使用 Adam 优化器
    loss='mse',       # 使用均方误差作为损失函数
    metrics=['mae']   # 使用平均绝对误差作为评估指标
)

# 准备训练数据
x_train = tf.random.normal((100, 10, 64))  # 100 个样本，每个样本的形状为 (10, 64)
y_train = tf.random.normal((100, 10, 64))  # 100 个样本，每个样本的形状为 (10, 64)

# 训练模型
history = model.fit(
    x_train, y_train,
    batch_size=32,  # 批量大小
    epochs=10,      # 训练轮数
    validation_split=0.2  # 使用 20% 的数据作为验证集
)

# 保存模型
model.save('streaming_model.h5')
```

### Relevant log output

```shell
D:\project\DLComplier\.venv\Scripts\python.exe D:\project\DLComplier\.venv\statiblity.py 
2025-03-19 16:09:45.545622: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-03-19 16:09:46.283611: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-03-19 16:09:48.149403: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING:tensorflow:From D:\project\DLComplier\.venv\Lib\site-packages\keras\src\backend\tensorflow\core.py:216: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Traceback (most recent call last):
  File ""D:\project\DLComplier\.venv\statiblity.py"", line 37, in <module>
    outputs = model(inputs)
              ^^^^^^^^^^^^^
  File ""D:\project\DLComplier\.venv\Lib\site-packages\keras\src\utils\traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""D:\project\DLComplier\.venv\statiblity.py"", line 27, in call
    x, _ = super().call(inputs)  # 假设另一个分支被截断
           ^^^^^^^^^^^^^^^^^^^^
NotImplementedError: Exception encountered when calling StreamingModel.call().

Could not automatically infer the output shape / dtype of 'streaming_model' (of type StreamingModel). Either the `StreamingModel.call()` method is incorrect, or you need to implement the `StreamingModel.compute_output_spec() / compute_output_shape()` method. Error encountered:

Model StreamingModel does not have a `call()` method implemented.

Arguments received by StreamingModel.call():
  • args=('<KerasTensor shape=(None, 10, 64), dtype=float32, sparse=False, name=keras_tensor>',)
  • kwargs=<class 'inspect._empty'>
```",1
'LNK1120: 12 unresolved externals' when building and using TFLite C library with CMake on Windows,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes (using current master)

### Source

source

### TensorFlow version

tf2.19.0

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I want to build the tflite c library on windows. Therefore I looked at the guide https://ai.google.dev/edge/litert/build/cmake#build_litert_c_library and did the following steps:

```shell
git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
cd tensorflow_src
git checkout v2.19.0
cd ..
mkdir tflite_build
cd tflite_build
cmake ../tensorflow_src/tensorflow/lite/c
cmake --build . -j
```

As output I got as expected the `tensorflowlite_c.dll` and `tensorflowlite_c.lib` files. I created a sample project for testing the library, where the code is mentioned below. 
I was able to build the library on Ubuntu 22.04. and run the example described below. 

CMake Version: `3.25.1`
Windows SDK version `10.0.22621.0`

### Standalone code to reproduce the issue

```cpp
#include <cstdio>

#include <tensorflow/lite/c/c_api.h>
#define TFLITE_MINIMAL_CHECK(x)                              \
  if (!(x)) {                                                \
    fprintf(stderr, ""Error at %s:%d\n"", __FILE__, __LINE__); \
    exit(1);                                                 \
  }

int main(int argc, char* argv[]) {
  fprintf(stdout, ""Hello TensorFlow Lite!\n"");
  TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();
  TFLITE_MINIMAL_CHECK(options != nullptr);
  TfLiteModel* model = TfLiteModelCreateFromFile(""C:/Users/user1/models/docker/testingeq.tflite"");
  TFLITE_MINIMAL_CHECK(model != nullptr);
  TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);
  TFLITE_MINIMAL_CHECK(interpreter != nullptr);

  return 0;
}
```

```cmake
cmake_minimum_required(VERSION 3.16)
project(minimal)

set(CMAKE_CXX_STANDARD 17)
set(CMAKE_INSTALL_RPATH ""$ORIGIN"")
set(CMAKE_INSTALL_RPATH_USE_LINK_PATH TRUE)
set(CMAKE_EXPORT_COMPILE_COMMANDS ON CACHE INTERNAL """")

get_filename_component(TensorflowLite_DIR ""${CMAKE_SOURCE_DIR}/deps/tensorflow_src/"" ABSOLUTE)
set(TensorflowLite_INCLUDE_DIR ""${TensorflowLite_DIR}"")
set(TensorflowLite_LIBRARIES ""${TensorflowLite_DIR}/bin/"")

if(MSVC)
    set(TENSORFLOWLITE_C ""${TensorflowLite_LIBRARIES}/tensorflowlite_c.lib"")
    set(TENSORFLOWLITE_SHARED ""${TensorflowLite_LIBRARIES}/tensorflowlite_c.dll"")
elseif(${TARGET_AARCH64})
    set(TENSORFLOWLITE_C ""${TensorflowLite_LIBRARIES_AARCH64}/libtensorflowlite_c.so"")
    set(TensorflowLite_SHARED ""${TensorflowLite_LIBRARIES_AARCH64}/libtensorflowlite_c.so"")
else()
    set(TENSORFLOWLITE_C ""${TensorflowLite_LIBRARIES}/libtensorflowlite_c.so"")
    set(TENSORFLOWLITE_SHARED ""${TensorflowLite_LIBRARIES}/libtensorflowlite_c.so"")
endif()

include_directories(${TensorflowLite_INCLUDE_DIR})

add_executable(minimal minimal.cc)
target_link_libraries(minimal ${TENSORFLOWLITE_C})
```

### Relevant log output

```shell
[main] Building folder: c:/Users/user1/dev/tf_example/build ALL_BUILD
[build] Starting build
[proc] Executing command: ""C:\Program Files\CMake\bin\cmake.EXE"" --build c:/Users/user1/dev/tf_example/build --config Debug --target ALL_BUILD -j 14 --
[build] MSBuild version 17.13.9+e0f243f1e for .NET Framework
[build] 
[build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteOperatorCreate referenced in function ""void __cdecl `dynamic initializer for 'TfLiteRegistrationExternalCreate''(void)"" (??__ETfLiteRegistrationExternalCreate@@YAXXZ) [C:\Users\user1\dev\tf_example\build\minimal.vcxproj]
[build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteOperatorDelete referenced in function ""void __cdecl `dynamic initializer for 'TfLiteRegistrationExternalDelete''(void)"" (??__ETfLiteRegistrationExternalDelete@@YAXXZ) [C:\Users\user1\dev\tf_example\build\minimal.vcxproj]
[build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteOperatorGetBuiltInCode referenced in function ""void __cdecl `dynamic initializer for 'TfLiteRegistrationExternalGetBuiltInCode''(void)"" (??__ETfLiteRegistrationExternalGetBuiltInCode@@YAXXZ) [C:\Users\user1\dev\tf_example\build\minimal.vcxproj]
[build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteOperatorGetCustomName referenced in function ""void __cdecl `dynamic initializer for 'TfLiteRegistrationExternalGetCustomName''(void)"" (??__ETfLiteRegistrationExternalGetCustomName@@YAXXZ) [C:\Users\user1\dev\tf_example\build\minimal.vcxproj]
[build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteOperatorGetVersion referenced in function ""void __cdecl `dynamic initializer for 'TfLiteRegistrationExternalGetVersion''(void)"" (??__ETfLiteRegistrationExternalGetVersion@@YAXXZ) [C:\Users\user1\dev\tf_example\build\minimal.vcxproj]
[build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteOperatorSetInit referenced in function ""void __cdecl `dynamic initializer for 'TfLiteRegistrationExternalSetInit''(void)"" (??__ETfLiteRegistrationExternalSetInit@@YAXXZ) [C:\Users\user1\dev\tf_example\build\minimal.vcxproj]
[build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteOperatorSetFree referenced in function ""void __cdecl `dynamic initializer for 'TfLiteRegistrationExternalSetFree''(void)"" (??__ETfLiteRegistrationExternalSetFree@@YAXXZ) [C:\Users\user1\dev\tf_example\build\minimal.vcxproj]
[build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteOperatorSetPrepare referenced in function ""void __cdecl `dynamic initializer for 'TfLiteRegistrationExternalSetPrepare''(void)"" (??__ETfLiteRegistrationExternalSetPrepare@@YAXXZ) [C:\Users\user1\dev\tf_example\build\minimal.vcxproj]
[build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteOperatorSetInvoke referenced in function ""void __cdecl `dynamic initializer for 'TfLiteRegistrationExternalSetInvoke''(void)"" (??__ETfLiteRegistrationExternalSetInvoke@@YAXXZ) [C:\Users\user1\dev\tf_example\build\minimal.vcxproj]
[build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteModelCreateFromFile referenced in function main [C:\Users\user1\dev\tf_example\build\minimal.vcxproj]
[build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteInterpreterOptionsCreate referenced in function main [C:\Users\user1\dev\tf_example\build\minimal.vcxproj]
[build] minimal.obj : error LNK2019: unresolved external symbol __imp_TfLiteInterpreterCreate referenced in function main [C:\Users\user1\dev\tf_example\build\minimal.vcxproj]
[build] C:\Users\user1\dev\tf_example\build\Debug\minimal.exe : fatal error LNK1120: 12 unresolved externals [C:\Users\user1\dev\tf_example\build\minimal.vcxproj]
[proc] The command: ""C:\Program Files\CMake\bin\cmake.EXE"" --build c:/Users/user1/dev/tf_example/build --config Debug --target ALL_BUILD -j 14 -- exited with code: 1
[driver] Build completed: 00:00:02.951
[build] Build finished with exit code 1
```",2
TensorFlow on RTX 5090,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.20.0.dev20250314

### Custom code

No

### OS platform and distribution

Windows 11 - WSL2 - Ubuntu 22.04.5 LTS

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

7.4.1

### GCC/compiler version

gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0

### CUDA/cuDNN version

CUDA Version: 12.8

### GPU model and memory

RTX 5090 32GB

### Current behavior?

I had hoped that tensorflow would work on the RTX 5090 at all. It does not, sadly. I tried building from source but that didn't work either. I tried running the environment script but that didn't work either. At least bash is my primary programming language, so I was able to tidy that one up here:

https://github.com/tensorflow/tensorflow/pull/89271

But I wasn't able to get tensorflow running. I had a similar issue with PyTorch, which needed to use CUDA 12.8.* to work on the Blackwell cards, but no dice with the nightly build of tensorflow. Below is my test and the output, and under that is the `tf_env.txt` from my patched script.

It may be helpful to know that nvidia themselves seem to have it running here:

https://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes/rel-25-02.html

But I get the same errors that this other guy does when I try it out:

https://www.reddit.com/r/tensorflow/comments/1iutjoj/tensorflow_2501_cuda_128_rtx_5090_on_wsl2_cuda/

This conversation was another one I found that may be helpful, according to these guys, you need to support CUDA 12.8.1 to support Blackwell (aka the RTX 50## series cards):

https://discuss.ai.google.dev/t/building-tensorflow-from-source-for-rtx5000-gpu-series/65171/15


```

(tfnightie) mitch@win11ml:~/stable_diff
$ cat tfnightie/test_2.py
import tensorflow as tf
import time

# Check if TensorFlow sees the GPU
print(""TensorFlow version:"", tf.__version__)
print(""Available GPUs:"", tf.config.experimental.list_physical_devices('GPU'))

# Matrix multiplication test
shape = (5000, 5000)
a = tf.random.normal(shape)
b = tf.random.normal(shape)

# Time execution on GPU
with tf.device('/GPU:0'):
    print(""Running on GPU..."")
    start_time = time.time()
    c = tf.matmul(a, b)
    tf.print(""Matrix multiplication (GPU) done."")
    print(""Execution time (GPU):"", time.time() - start_time, ""seconds"")

# Time execution on CPU for comparison
with tf.device('/CPU:0'):
    print(""Running on CPU..."")
    start_time = time.time()
    c = tf.matmul(a, b)
    tf.print(""Matrix multiplication (CPU) done."")
    print(""Execution time (CPU):"", time.time() - start_time, ""seconds"")




(tfnightie) mitch@win11ml:~/stable_diff
$ python tfnightie/test_2.py
2025-03-14 21:35:33.400099: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
TensorFlow version: 2.20.0-dev20250314
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1742009735.413544  326199 gpu_device.cc:2429] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
W0000 00:00:1742009735.417720  326199 gpu_device.cc:2429] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
I0000 00:00:1742009735.572153  326199 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 29043 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 5090, pci bus id: 0000:09:00.0, compute capability: 12.0
2025-03-14 21:35:36.969440: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleLoadData(&module, data)' failed with 'CUDA_ERROR_INVALID_PTX'

2025-03-14 21:35:36.969480: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleGetFunction(&function, module, kernel_name)' failed with 'CUDA_ERROR_INVALID_HANDLE'

2025-03-14 21:35:36.969505: W tensorflow/core/framework/op_kernel.cc:1843] INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'
2025-03-14 21:35:36.969533: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'
Traceback (most recent call last):
  File ""/home/mitch/stable_diff/tfnightie/test_2.py"", line 10, in <module>
    a = tf.random.normal(shape)
  File ""/home/mitch/.virtualenvs/tfnightie/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/mitch/.virtualenvs/tfnightie/lib/python3.10/site-packages/tensorflow/python/framework/ops.py"", line 6027, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InternalError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' [Op:Mul] name:
```

Also, while nvidia's site says that the Compute Capability of the RTX5090 is ""10.0"", the card itself seems to report ""12.0"". I am not so sure that info will be helpful, but it spun me for a loop:

```

$ cat <<EOF > card_details.cu
> #include <cuda_runtime.h>
#include <iostream>

int main() {
    cudaDeviceProp prop;
    int device;

    cudaGetDevice(&device); // Get the current device ID
    cudaGetDeviceProperties(&prop, device); // Get device properties

    size_t free_mem, total_mem;
    cudaMemGetInfo(&free_mem, &total_mem); // Get VRAM usage

    std::cout << ""GPU Name: "" << prop.name << std::endl;
    std::cout << ""Compute Capability: "" << prop.major << ""."" << prop.minor << std::endl;
    std::cout << ""VRAM Usage: "" << (total_mem - free_mem) / (1024 * 1024) << "" MB / "" << total_mem / (1024 * 1024) << "" MB"" << std::endl;

    return 0;
}
EOF



$ nvcc card_details.cu -o card_details && ./card_details
nvcc warning : Support for offline compilation for architectures prior to '<compute/sm/lto>_75' will be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
GPU Name: NVIDIA GeForce RTX 5090
Compute Capability: 12.0
VRAM Usage: 1763 MB / 32606 MB
```


# tf_env.txt
```

== check python ====================================================
python version: 3.10.12
python branch:
python build version: ('main', 'Feb  4 2025 14:57:36')
python compiler version: GCC 11.4.0
python implementation: CPython


== check os platform ===============================================
os: Linux
os kernel version: #1 SMP Tue Nov 5 00:21:55 UTC 2024
os release version: 5.15.167.4-microsoft-standard-WSL2
os platform: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35
freedesktop os release: {'NAME': 'Ubuntu', 'ID': 'ubuntu', 'PRETTY_NAME': 'Ubuntu 22.04.5 LTS', 'VERSION_ID': '22.04', 'VERSION': '22.04.5 LTS (Jammy Jellyfish)', 'VERSION_CODENAME': 'jammy', 'ID_LIKE': 'debian', 'HOME_URL': 'https://www.ubuntu.com/', 'SUPPORT_URL': 'https://help.ubuntu.com/', 'BUG_REPORT_URL': 'https://bugs.launchpad.net/ubuntu/', 'PRIVACY_POLICY_URL': 'https://www.ubuntu.com/legal/terms-and-policies/privacy-policy', 'UBUNTU_CODENAME': 'jammy'}
mac version: ('', ('', '', ''), '')
uname: uname_result(system='Linux', node='win11ml', release='5.15.167.4-microsoft-standard-WSL2', version='#1 SMP Tue Nov 5 00:21:55 UTC 2024', machine='x86_64')
architecture: ('64bit', 'ELF')
machine: x86_64

== are we in docker ================================================
No

== c++ compiler ====================================================
/usr/bin/c++
c++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Copyright (C) 2021 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== check pips ======================================================
numpy                   2.1.3
protobuf                5.29.3
tf_nightly              2.20.0.dev20250314

== check for virtualenv ============================================
Running inside a virtual environment.

== tensorflow import ===============================================
2025-03-14 21:02:48.002965: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1742007769.198398  317963 gpu_device.cc:2429] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
W0000 00:00:1742007769.202246  317963 gpu_device.cc:2429] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
I0000 00:00:1742007769.355021  317963 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 29043 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 5090, pci bus id: 0000:09:00.0, compute capability: 12.0

tf.version.VERSION = 2.20.0-dev20250314
tf.version.GIT_VERSION = v1.12.1-123444-g07ff428d432
tf.version.COMPILER_VERSION = Ubuntu Clang 18.1.8 (++20240731024944+3b5b5c1ec4a3-1~exp1~20240731145000.144)

Sanity check: <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>
libcudnn not found

== env =============================================================
LD_LIBRARY_PATH /usr/local/cuda-12.8/lib64:
DYLD_LIBRARY_PATH is unset

== nvidia-smi ======================================================
Fri Mar 14 21:02:52 2025
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.124.06             Driver Version: 572.70         CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 5090        On  |   00000000:09:00.0 Off |                  N/A |
|  0%   43C    P1             78W /  600W |    2115MiB /  32607MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              31      G   /Xwayland                             N/A      |
|    0   N/A  N/A              35      G   /Xwayland                             N/A      |
+-----------------------------------------------------------------------------------------+

== cuda libs =======================================================
/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudart.so.11.8.89
/usr/local/cuda-12.8/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-12.8/targets/x86_64-linux/lib/libcudart.so.12.8.90

== tensorflow installation =========================================
tensorflow not found

== tf_nightly installation =========================================
Name: tf_nightly
Version: 2.20.0.dev20250314
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /home/mitch/.virtualenvs/tfnightie/lib/python3.10/site-packages
Required-by:

== python version ==================================================
(major, minor, micro, releaselevel, serial)
(3, 10, 12, 'final', 0)

== bazel version ===================================================
Bazelisk version: v1.25.0
Build label: 7.4.1
Build time: Mon Nov 11 21:24:53 2024 (1731360293)
Build timestamp: 1731360293
Build timestamp as int: 1731360293
```

### Standalone code to reproduce the issue

```shell
Try running anything with an RTX 5090. My test script is above.
```

### Relevant log output

```shell

```",1
Getting AttributeError when running tf.ones_like,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Linux 22.04

### Mobile device

N\A

### Python version

3.9

### Bazel version

N\A

### GCC/compiler version

N\A

### CUDA/cuDNN version

N\A

### GPU model and memory

N\A

### Current behavior?

Getting AttributeError when running tf.ones_like.

### Standalone code to reproduce the issue

```shell
def test_ones_like_with_name(self):
        # Test with name
        input_tensor = tf.constant([1, 2, 3])
        result = tf.ones_like(input_tensor, name=""test_ones_like"")
        self.assertEqual(result.op.name, ""test_ones_like"")
```

### Relevant log output

```shell
Traceback (most recent call last):
  File ""/home/user/projects/api_guided_testgen/out/bug_detect_gpt4o/exec/basic_rag_apidoc/tf/tf.ones_like.py"", line 66, in test_ones_like_with_name
    self.assertEqual(result.op.name, ""test_ones_like"")
  File ""/home/user/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 444, in __getattr__
    self.__getattribute__(name)
  File ""/home/user/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 1305, in op
    raise AttributeError(
AttributeError: Tensor.op is undefined when eager execution is enabled.
```",1
The API documentation of tf.no_op specifies that tf.no_op() should return a TensorFlow Operation.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

N\A

### Mobile device

N\A

### Python version

3.9

### Bazel version

N\A

### GCC/compiler version

N\A

### CUDA/cuDNN version

N\A

### GPU model and memory

N\A

### Current behavior?

The API documentation of tf.no_op specifies that tf.no_op() should return a TensorFlow Operation.  The tests expect that calling tf.no_op() returns an instance of tf.Operation (and that its name property can be set by providing a name, e.g., ""my_no_op""). The error message indicates that tf.no_op() is returning None rather than an Operation (None has no attribute ""name""), causing the tests to fail. Given that the test suite is properly written according to the API's specification and the expected behavior, the issue stems from the source code implementation.

### Standalone code to reproduce the issue

```shell
def test_no_op_name(self):
        """"""Test that a no_op can be created with a specific name.""""""
        no_op = tf.no_op(name=""my_no_op"")
        self.assertEqual(no_op.name, ""my_no_op"")



   def test_no_op_creation(self):
        """"""Test that a no_op can be created without errors.""""""
        try:
            no_op = tf.no_op()
            self.assertIsInstance(no_op, tf.Operation)
        except Exception as e:
            self.fail(f""tf.no_op raised an exception: {e}"")
```

### Relevant log output

```shell
Traceback (most recent call last):
  File ""/home/user/projects/api_guided_testgen/out/bug_detect_gpt4o/exec/basic_rag_apidoc/tf/tf.no_op.py"", line 17, in test_no_op_name
    self.assertEqual(no_op.name, ""my_no_op"")
AttributeError: 'NoneType' object has no attribute 'name'



Traceback (most recent call last):
  File ""/home/user/projects/api_guided_testgen/out/bug_detect_gpt4o/exec/basic_rag_apidoc/tf/tf.no_op.py"", line 12, in test_no_op_creation
    self.fail(f""tf.no_op raised an exception: {e}"")
AssertionError: tf.no_op raised an exception: None is not an instance of <class 'tensorflow.python.framework.ops.Operation'>
```",1
tf.keras.losses.SparseCategoricalCrossentropy has logical/Doc bug,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

API documentation of tf.keras.losses.SparseCategoricalCrossentropy mentions that one of the parameters can be None, but the implementation does not check None, it checks 'none' which is a string.

### Standalone code to reproduce the issue

```shell
def test_reduction_none(self):
        # Test with reduction set to None
        y_true = np.array([0, 1, 2])
        y_pred = np.array([[0.9, 0.05, 0.05],
                           [0.05, 0.9, 0.05],
                           [0.05, 0.05, 0.9]])
        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(reduction=None)
        loss = loss_fn(y_true, y_pred).numpy()
        expected_loss = -np.log([0.9, 0.9, 0.9])
        np.testing.assert_almost_equal(loss, expected_loss, decimal=5)
```

### Relevant log output

```shell
Traceback (most recent call last):
  File ""/home/user/projects/api_guided_testgen/out/bug_detect_gpt4o/exec/basic_rag_apidoc/tf/tf.keras.losses.SparseCategoricalCrossentropy.py"", line 49, in test_reduction_none
    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(reduction=None)
  File ""/home/user/anaconda3/lib/python3.8/site-packages/keras/losses.py"", line 1026, in __init__
    super().__init__(
  File ""/home/user/anaconda3/lib/python3.8/site-packages/keras/losses.py"", line 262, in __init__
    super().__init__(reduction=reduction, name=name)
  File ""/home/user/anaconda3/lib/python3.8/site-packages/keras/losses.py"", line 93, in __init__
    losses_utils.ReductionV2.validate(reduction)
  File ""/home/user/anaconda3/lib/python3.8/site-packages/keras/utils/losses_utils.py"", line 88, in validate
    raise ValueError(
ValueError: Invalid Reduction Key: None. Expected keys are ""('auto', 'none', 'sum', 'sum_over_batch_size')""
```",1
I get an invalid shape error when running tf.keras.losses.BinaryCrossentropy,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Linux 22.04

### Mobile device

N\A

### Python version

3.9

### Bazel version

N\A

### GCC/compiler version

N\A

### CUDA/cuDNN version

N\A

### GPU model and memory

N\A

### Current behavior?

I get an invalid shape error when running ```tf.keras.losses.BinaryCrossentropy```

### Standalone code to reproduce the issue

```shell
def test_binary_crossentropy_invalid_inputs(self):
        # Test with invalid inputs
        y_true = np.array([0, 1, 0, 1], dtype=np.float32)
        y_pred = np.array([0.1, 0.9, 0.2], dtype=np.float32)  # Mismatched shape
        
        bce = tf.keras.losses.BinaryCrossentropy()
        with self.assertRaises(ValueError):
            bce(y_true, y_pred)
```

### Relevant log output

```shell
Traceback (most recent call last):
  File ""/home/user/projects/api_guided_testgen/out/bug_detect_gpt4o/exec/basic_rag_apidoc/tf/tf.keras.losses.BinaryCrossentropy.py"", line 70, in test_binary_crossentropy_invalid_inputs
    bce(y_true, y_pred)
  File ""/home/user/anaconda3/lib/python3.8/site-packages/keras/losses.py"", line 152, in __call__
    losses = call_fn(y_true, y_pred)
  File ""/home/user/anaconda3/lib/python3.8/site-packages/keras/losses.py"", line 284, in call
    return ag_fn(y_true, y_pred, **self._fn_kwargs)
  File ""/home/user/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/user/anaconda3/lib/python3.8/site-packages/keras/losses.py"", line 2176, in binary_crossentropy
    backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),
  File ""/home/user/anaconda3/lib/python3.8/site-packages/keras/backend.py"", line 5688, in binary_crossentropy
    bce = target * tf.math.log(output + epsilon())
tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [4] vs. [3] [Op:Mul]
```",1
AssertionError when calling tf.keras.layers.Conv2DTranspose,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Linux 22.4

### Mobile device

N\A

### Python version

3.9

### Bazel version

N\A

### GCC/compiler version

N\A

### CUDA/cuDNN version

N\A

### GPU model and memory

N\A

### Current behavior?

When running ```tf.keras.layers.Conv2DTranspose```, I get AssertionError. The minimum reproducing example is attached.

### Standalone code to reproduce the issue

```shell
def test_kernel_initializer():
        # Test with a custom kernel initializer
        model = tf.keras.Sequential([
            tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, kernel_initializer='ones', input_shape=(4, 4, 1))
        ])
        input_data = np.ones((1, 4, 4, 1), dtype=np.float32)
        output_data = model(input_data)
        expected_output = np.full((1, 6, 6, 1), 9.0)  # Since kernel is initialized with ones
        np.testing.assert_array_almost_equal(output_data.numpy(), expected_output)
```

### Relevant log output

```shell
Traceback (most recent call last):
  File ""/home/user/projects/api_guided_testgen/out/bug_detect_gpt4o/exec/basic_rag_apidoc/tf/tf.keras.layers.Conv2DTranspose.py"", line 74, in test_kernel_initializer
    np.testing.assert_array_almost_equal(output_data.numpy(), expected_output)
  File ""/home/user/anaconda3/lib/python3.8/site-packages/numpy/testing/_private/utils.py"", line 1046, in assert_array_almost_equal
    assert_array_compare(compare, x, y, err_msg=err_msg, verbose=verbose,
  File ""/home/user/anaconda3/lib/python3.8/site-packages/numpy/testing/_private/utils.py"", line 844, in assert_array_compare
    raise AssertionError(msg)
AssertionError: 
Arrays are not almost equal to 6 decimals

Mismatched elements: 32 / 36 (88.9%)
Max absolute difference: 8.
Max relative difference: 0.88888889
 x: array([[[[1.],
         [2.],
         [3.],...
 y: array([[[[9.],
         [9.],
         [9.],...

----------------------------------------------------------------------
Ran 8 tests in 0.155s

FAILED (failures=2)
```",1
Getting 0 nodes delegated while using TFLite C Library with TFLite Flex Delegate,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

Tensroflow 2.18.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 24.04

### Mobile device

_No response_

### Python version

3.12.3

### Bazel version

6.5.0

### GCC/compiler version

13.3.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am using the TFLite C library to generate model inference C code for an edge device.
In my use case, I require certain operations, such as tf.unpack with shape (0, 2), which are supported by TensorFlow but not currently available in TFLite.

### How I built the TFLite C Library
I found a solution article on the Google AI Edge website ([here](https://ai.google.dev/edge/litert/build/cmake)), which provides instructions on building the TFLite C library using CMake. I followed these steps and successfully built the library without any errors.

As a result, I obtained `libtensorflowlite_c.so` and placed it in `/usr/local/lib`, which is the system library directory on Ubuntu.

### How I built the Tensorflow Flex Delegate library
Similarly, there is an article [here](https://ai.google.dev/edge/litert/models/ops_select#cc) that explains how to build `libtensorflowlite_flex.so.` I followed the instructions provided and successfully built the library without encountering any issues.

To verify the build, I ran the command:
```bash
strings /usr/local/lib/libtensorflowlite_flex.so | grep Unpack
```
This command displayed several symbols containing Unpack, which indicates that the build was successful, I think.
<details>
<summary>Click to show output</summary>

```
UnpackOp<CPUDevice, ::tensorflow::uint64>
UnpackOp<CPUDevice, ::int64_t>
UnpackOp<CPUDevice, ::tensorflow::uint32>
UnpackOp<CPUDevice, ::tensorflow::uint16>
UnpackOp<CPUDevice, ::tensorflow::int16>
UnpackOp<CPUDevice, ::tensorflow::uint8>
UnpackOp<CPUDevice, ::tensorflow::int8>
UnpackOp<CPUDevice, ::tensorflow::int32>
UnpackOp<CPUDevice, Eigen::half>
UnpackOp<CPUDevice, ::tensorflow::bfloat16>
UnpackOp<CPUDevice, ::tensorflow::complex64>
UnpackOp<CPUDevice, ::tensorflow::complex128>
UnpackOp<CPUDevice, ::tensorflow::tstring>
UnpackOp<CPUDevice, ::tensorflow::ResourceHandle>
UnpackOp<CPUDevice, ::tensorflow::Variant>
UnpackOp<CPUDevice, ::tensorflow::float8_e5m2>
UnpackOp<CPUDevice, ::tensorflow::float8_e4m3fn>
TensorArrayUnpackOrScatterOp<CPUDevice, ::tensorflow::uint64, true >
TensorArrayUnpackOrScatterOp<CPUDevice, ::tensorflow::uint64, false >
TensorArrayUnpackOrScatterOp<CPUDevice, ::int64_t, true >
TensorArrayUnpackOrScatterOp<CPUDevice, ::int64_t, false >
TensorArrayUnpackOrScatterOp<CPUDevice, ::tensorflow::uint32, true >
TensorArrayUnpackOrScatterOp<CPUDevice, ::tensorflow::uint32, false >
TensorArrayUnpackOrScatterOp<CPUDevice, ::tensorflow::uint16, true >
TensorArrayUnpackOrScatterOp<CPUDevice, ::tensorflow::uint16, false >
TensorArrayUnpackOrScatterOp<CPUDevice, ::tensorflow::int16, true >
TensorArrayUnpackOrScatterOp<CPUDevice, ::tensorflow::int16, false >
TensorArrayUnpackOrScatterOp<CPUDevice, ::tensorflow::uint8, true >
TensorArrayUnpackOrScatterOp<CPUDevice, ::tensorflow::uint8, false >
TensorArrayUnpackOrScatterOp<CPUDevice, ::tensorflow::int8, true >
TensorArrayUnpackOrScatterOp<CPUDevice, ::tensorflow::int8, false >
TensorArrayUnpackOrScatterOp<CPUDevice, ::tensorflow::int32, true >
TensorArrayUnpackOrScatterOp<CPUDevice, ::tensorflow::int32, false >
TensorArrayUnpackOrScatterOp<CPUDevice, Eigen::half, true >
TensorArrayUnpackOrScatterOp<CPUDevice, Eigen::half, false >
TensorArrayUnpackOrScatterOp<CPUDevice, ::tensorflow::bfloat16, true >
TensorArrayUnpackOrScatterOp<CPUDevice, ::tensorflow::bfloat16, false >
TensorArrayUnpackOrScatterOp<CPUDevice, float, true >
TensorArrayUnpackOrScatterOp<CPUDevice, float, false >
TensorArrayUnpackOrScatterOp<CPUDevice, double, true >
TensorArrayUnpackOrScatterOp<CPUDevice, double, false >
TensorArrayUnpackOrScatterOp<CPUDevice, ::tensorflow::complex64, true >
TensorArrayUnpackOrScatterOp<CPUDevice, ::tensorflow::complex64, false >
TensorArrayUnpackOrScatterOp<CPUDevice, ::tensorflow::complex128, true >
TensorArrayUnpackOrScatterOp<CPUDevice, ::tensorflow::complex128, false >
TensorArrayUnpackOrScatterOp<CPUDevice, bool, true >
TensorArrayUnpackOrScatterOp<CPUDevice, bool, false >
TensorArrayUnpackOrScatterOp<CPUDevice, ::tensorflow::tstring, true >
TensorArrayUnpackOrScatterOp<CPUDevice, ::tensorflow::tstring, false >
TensorArrayUnpackOrScatterOp<CPUDevice, ::tensorflow::ResourceHandle, true >
TensorArrayUnpackOrScatterOp<CPUDevice, ::tensorflow::ResourceHandle, false >
TensorArrayUnpackOrScatterOp<CPUDevice, ::tensorflow::Variant, true >
TensorArrayUnpackOrScatterOp<CPUDevice, ::tensorflow::Variant, false >
'(new gtl::FlatSet<string>{""ArgMax"", ""ArgMin"", ""AudioSpectrogram"", ""AvgPool"", ""BatchMatMul"", ""BatchMatMulV2"", ""BatchNormWithGlobalNormalization"", ""BatchToSpace"", ""BatchToSpaceND"", ""Bincount"", ""BroadcastArgs"", ""BroadcastGradientArgs"", ""Bucketize"", ""CTCBeamSearchDecoder"", ""CTCGreedyDecoder"", ""CTCLoss"", ""CompareAndBitpack"", ""ComplexAbs"", ""Concat"", ""ConcatOffset"", ""ConcatV2"", ""Conv2D"", ""Copy"", ""CopyHost"", ""Cross"", ""CudnnRNN"", ""CudnnRNNBackprop"", ""CudnnRNNBackpropV2"", ""CudnnRNNBackpropV3"", ""CudnnRNNCanonicalToParams"", ""CudnnRNNCanonicalToParamsV2"", ""CudnnRNNParamsSize"", ""CudnnRNNParamsToCanonical"", ""CudnnRNNParamsToCanonicalV2"", ""CudnnRNNV2"", ""CudnnRNNV3"", ""CumProd"", ""CumSum"", ""DebugNanCount"", ""DebugNumericSummary"", ""DecodeProtoV2"", ""DecodeWav"", ""DeepCopy"", ""DepthToSpace"", ""Dequantize"", ""Diag"", ""DiagPart"", ""EditDistance"", ""Empty"", ""EncodeProtoV2"", ""EncodeWav"", ""ExtractImagePatches"", ""ExtractVolumePatches"", ""Fill"", ""Gather"", ""GatherNd"", ""GatherV2"", ""HistogramFixedWidth"", ""InvertPermutation"", ""IsInf"", ""IsNan"", ""Isfinite"", ""LinSpace"", ""LowerBound"", ""MatMul"", ""MatrixDiag"", ""MatrixDiagPart"", ""MatrixDiagPartV2"", ""MatrixDiagV2"", ""Mfcc"", ""Multinomial"", ""OneHot"", ""Pack"", ""ParameterizedTruncatedNormal"", ""PopulationCount"", ""RandomGamma"", ""RandomPoisson"", ""RandomPoissonV2"", ""RandomStandardNormal"", ""RandomUniform"", ""RandomUniformInt"", ""Range"", ""Rank"", ""RequantizationRange"", ""Requantize"", ""ReverseSequence"", ""Shape"", ""ShapeN"", ""Size"", ""SpaceToBatch"", ""SpaceToBatchND"", ""SpaceToDepth"", ""SparseMatMul"", ""Split"", ""SplitV"", ""TruncatedNormal"", ""Unique"", ""UniqueV2"", ""UniqueWithCounts"", ""UniqueWithCountsV2"", ""Unpack"", ""UnravelIndex"", ""UpperBound"", ""Where""})' Must be non NULL
TensorArrayUnpack
Unpack
UnpackOp<CPUDevice, float>
UnpackOp<CPUDevice, double>
UnpackOp<CPUDevice, bool>
UnpackOp<CPUDevice, int32>
UnpackOp<CPUDevice, int64>
UnpackGrad
tfg.Unpack
N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEmEE
N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceElEE
N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEjEE
N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEtEE
N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEsEE
N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEhEE
N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEaEE
N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEiEE
N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceENS1_4halfEEE
N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceENS1_8bfloat16EEE
N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEfEE
N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEdEE
N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceESt7complexIfEEE
N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceESt7complexIdEEE
N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEbEE
N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEN3tsl7tstringEEE
N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceENS_14ResourceHandleEEE
N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceENS_7VariantEEE
N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEN9ml_dtypes15float8_internal11float8_e5m2EEE
N10tensorflow8UnpackOpIN5Eigen16ThreadPoolDeviceEN9ml_dtypes15float8_internal13float8_e4m3fnEEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEmLb1EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEmLb0EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceElLb1EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceElLb0EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEjLb1EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEjLb0EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEtLb1EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEtLb0EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEsLb1EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEsLb0EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEhLb1EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEhLb0EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEaLb1EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEaLb0EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEiLb1EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEiLb0EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceENS1_4halfELb1EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceENS1_4halfELb0EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceENS1_8bfloat16ELb1EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceENS1_8bfloat16ELb0EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEfLb1EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEfLb0EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEdLb1EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEdLb0EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceESt7complexIfELb1EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceESt7complexIfELb0EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceESt7complexIdELb1EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceESt7complexIdELb0EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEbLb1EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEbLb0EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEN3tsl7tstringELb1EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceEN3tsl7tstringELb0EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceENS_14ResourceHandleELb1EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceENS_14ResourceHandleELb0EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceENS_7VariantELb1EEE
N10tensorflow28TensorArrayUnpackOrScatterOpIN5Eigen16ThreadPoolDeviceENS_7VariantELb0EEE
Unpack
``` 
</details>

### How I use C Library Function to load model and delegate library
I have check the model that some ops do have `Flex` prefix in the op names.
![Image](https://github.com/user-attachments/assets/dfe4fb5b-0d81-4aec-aef5-7a624779f2a5)

It was converted using the following settings:
```python
converter = tf.lite.TFLiteConverter.from_saved_model(model_folder)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
# converter.target_spec.supported_types = [tf.float16]
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.SELECT_TF_OPS,  tf.lite.OpsSet.TFLITE_BUILTINS, 
]
converter.allow_custom_ops = True
converter.legalize_custom_tensor_list_ops = True
converter._experimental_lower_tensor_list_ops = False 
converter.experimental_enable_resource_variables = True
tflite_model = converter.convert()
```

Then, I use the code below to load the model and delegate operations that are not supported by TFLite to the Flex library.
However, it always displays a message indicating that no nodes have been delegated by the Flex library.
```bash
INFO: Created TensorFlow Lite delegate for select TF ops.
2025-03-14 17:43:31.412379: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 623 nodes with 0 partitions.

ModifyGraphWithDelegate Ok
----------Load model successfully!----------
```

How should I resolve this issue?
I would really appreciate any insights or suggestions. Thanks in advance for your help!

### Standalone code to reproduce the issue

```shell
void tflite_load(const string model_path, map<string, TfLiteSignatureRunner*>& sig_map) {
    // Load model
    TfLiteModel* model = TfLiteModelCreateFromFile(model_path.c_str());
    if (model == nullptr) {
        cerr << ""Failed to load model."" << endl;
        return;
    }

    // Create Interpreter
    TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();
    TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);
    TfLiteInterpreterOptionsDelete(options);

    auto hdll = SharedLibrary::LoadLibrary(""libtensorflowlite_flex.so"");
    if (hdll == nullptr) {
        cerr << ""Load failed:"" << dlerror() << endl;
        return;
    }
    auto TF_AcquireFlexDelegate = reinterpret_cast<Interpreter::TfLiteDelegatePtr(*)()>(SharedLibrary::GetLibrarySymbol(hdll, ""TF_AcquireFlexDelegate""));
    if (TF_AcquireFlexDelegate == NULL) {
        cerr << ""TF_AcquireFlexDelegate couldn't be run: "" << dlerror() << endl;
        return;
    }
     
    std::unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate*)> delegate = TF_AcquireFlexDelegate();
    auto TfLiteStatus = TfLiteInterpreterModifyGraphWithDelegate(interpreter, delegate.get());
    if(TfLiteStatus==0)
        cout << ""ModifyGraphWithDelegate Ok"" << endl;

    cout << ""----------Load model successfully!----------"" << endl;
}
```

### Relevant log output

```shell

```",1
API Doc Error for tf.train.ClusterSpec API,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Mac

### Mobile device

N/A

### Python version

3.9

### Bazel version

N/A

### GCC/compiler version

N/A

### CUDA/cuDNN version

N/A

### GPU model and memory

N/A

### Current behavior?

The test conversion expects the resulting ClusterDef to have its first job named ""worker"" (matching the insertion order of the original dictionary). However, the test reveals that the first job ends up being ""ps"", which deviates from the API’s expected behavior (as evidenced in the provided examples API doc).

### Standalone code to reproduce the issue

```shell
import unittest
import tensorflow as tf

class TestClusterSpec(unittest.TestCase):

    def test_cluster_def_conversion(self):
        # Test conversion to ClusterDef
        cluster_dict = {
            ""worker"": [""worker0.example.com:2222"", ""worker1.example.com:2222""],
            ""ps"": [""ps0.example.com:2222""]
        }
        cluster_spec = tf.train.ClusterSpec(cluster_dict)
        cluster_def = cluster_spec.as_cluster_def()
        
        self.assertEqual(len(cluster_def.job), 2)
        self.assertEqual(cluster_def.job[0].name, ""worker"")
        self.assertEqual(cluster_def.job[1].name, ""ps"")

if __name__ == '__main__':
    unittest.main()
```

### Relevant log output

```shell
======================================================================
FAIL: test_cluster_def_conversion (__main__.TestClusterSpec)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/sjh/test.py"", line 16, in test_cluster_def_conversion
    self.assertEqual(cluster_def.job[0].name, ""worker"")
AssertionError: 'ps' != 'worker'
- ps
+ worker


----------------------------------------------------------------------
Ran 1 test in 0.000s

FAILED (failures=1)
```",1
Gradients are zero when clipping values in function definition,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18; 2.10

### Custom code

Yes

### Python version

3.10

### Current behavior?

I encountered a strange issue where gradients are zero when clipping-related TensorFlow Ops are defined in functions.

For context, I was implementing a numerically stable version of sigmoid where the inputs are clipped to `[-10, 10]`. However this bug can be reproduced with other functions. 

In the provided sample code, I define a truncated version of f(x) = x^2 where the function `truncate_domain` is used to clip the input tensors. However gradients only work as expected when the clipping occurs inside `GradientTape` and not when it's part of the function definition.

Reproduced on TensorFlow 2.10 with GPU, and latest 2.18 with CPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

def square1(x):
    return x**2

def truncate_domain(x, lb, ub):
    # Truncate for numerical stability
    x = tf.clip_by_value(x, lb, ub)
    #x = tf.math.maximum(x, lb)
    #x = tf.math.minimum(x, ub)
    return x

def square2(x):
    x = truncate_domain(x, -10., 10.)
    return x**2

def square3(x):
    x = truncate_domain(x, -10., 10.)
    return square1(x)

# Note: All 3 square functions are exactly mathematically equivalent when x in [-10, 10].

## Checking gradients

x = tf.constant(15., dtype=tf.float32)
with tf.GradientTape() as tape:
    tape.watch(x)
    x = truncate_domain(x, -10., 10.)
    l = square1(x)
grad1 = tape.gradient(l, x)
print(l)
print(grad1)
# >> tf.Tensor(100.0, shape=(), dtype=float32)
# >> tf.Tensor(20.0, shape=(), dtype=float32)

x = tf.constant(15., dtype=tf.float32)
with tf.GradientTape() as tape:
    tape.watch(x)
    l = square2(x)
grad2 = tape.gradient(l, x)
print(l)
print(grad2)
# >> tf.Tensor(100.0, shape=(), dtype=float32)
# >> tf.Tensor(0.0, shape=(), dtype=float32)

x = tf.constant(15., dtype=tf.float32)
with tf.GradientTape() as tape:
    tape.watch(x)
    l = square3(x)
grad3 = tape.gradient(l, x)
print(l)
print(grad3)
# >> tf.Tensor(100.0, shape=(), dtype=float32)
# >> tf.Tensor(0.0, shape=(), dtype=float32)

# Note: Gradients for square2 and square3 are 0, even though they are mathematically and (Python-) syntactically equivalent to square1's expected behavior
```

### Relevant log output

```shell

```",0
Doc Error: tf.boolean_mask API,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Ubuntu

### Mobile device

_No response_

### Python version

3.9

### Bazel version

N/A

### GCC/compiler version

N/A

### CUDA/cuDNN version

N/A

### GPU model and memory

N/A

### Current behavior?

The API documentation states that the mask parameter must be a Boolean tensor, but the implementation does not fully enforce this. In the test test_mask_with_different_dtype, an int32 mask is passed, and while a TypeError is expected, no error is raised.

The expectation is that the documentation should mention that non-Boolean can be handled in the API implementation.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
import unittest

class TestBooleanMask(unittest.TestCase):


    def test_mask_with_different_dtype(self):
        # Test with a mask of different dtype (int)
        tensor = tf.constant([1, 2, 3, 4, 5])
        mask = tf.constant([1, 0, 1, 0, 1], dtype=tf.int32)
        with self.assertRaises(TypeError):
            tf.boolean_mask(tensor, mask)

if __name__ == '__main__':
    unittest.main()
```

### Relevant log output

```shell

```",1
Update Docs to latest release,"### Issue type

Documentation Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.19.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The website at https://www.tensorflow.org/api_docs/python currently hosts docs for TensorFlow version 2.16.1, released March 8, 2024 (over 1 year ago)

The current release version of TensorFlow is 2.19.0.

Will the website be updated for the current TensorFlow release?

Or is it possible to build the docs locally with only open-source tools?

### Standalone code to reproduce the issue

```shell
Visit https://www.tensorflow.org/api_docs/python
```

### Relevant log output

```shell

```",1
Fixed-point Softmax() calls exp_on_negative_values() twice,"Fixed-point `Softmax()` calls `exp_on_negative_values()` twice per element: once to calculate the value of the exp of each element
https://github.com/tensorflow/tensorflow/blob/4266e9ab53bb3f7fca0dd816b965b82862b69b4e/tensorflow/lite/kernels/internal/reference/softmax.h#L131
and another one a few lines earlier to calculate the sum of all exps
https://github.com/tensorflow/tensorflow/blob/4266e9ab53bb3f7fca0dd816b965b82862b69b4e/tensorflow/lite/kernels/internal/reference/softmax.h#L109-L110
The `exp_on_negative_values()` function can be rather slow, taking most of the time of `Softmax()` (which can be especially concerning in embedded implementations), so calling it twice per element makes the whole `Softmax()` function be about twice as slow as necessary.

Could this be optimized so that it is only called once per element?  Store the exp results in a temporary array (for example, reusing `output_data`), computing the sum as they're stored, and then applying the scaling to the precomputed exp results rather than computing them again.
(`SoftmaxInt16()` seems to follow a similar approach.)",1
cuSteamSynchronize take tons of time,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.14

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 24.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.6

### GPU model and memory

4070S

### Current behavior?

I am trying to benchmark the inference.  A simple code as 
```
// Configure a `CallableHandle` that feeds from and fetches to a device.
Status SetupCallable(std::unique_ptr<tensorflow::Session>& session,
                     std::vector<tensorflow::TensorInfo>& input_info,
                     std::vector<tensorflow::TensorInfo>& output_info,
                     const string& device_name,
                     bool input_from_device,
                     bool output_to_host,
                     tensorflow::Session::CallableHandle* handle) {
  tensorflow::CallableOptions opts;
  for (const auto& info : input_info) {
    const string& name = info.name();
    opts.add_feed(name);
    if (input_from_device) {
      opts.mutable_feed_devices()->insert({name, device_name});
    }
  }
  for (const auto& info : output_info) {
    const string& name = info.name();
    opts.add_fetch(name);
    if (!output_to_host) {
      opts.mutable_fetch_devices()->insert({name, device_name});
    }
  }
  opts.set_fetch_skip_sync(true);
  return session->MakeCallable(opts, handle);
}
```
The inference is run by
```
start_time = std::chrono::steady_clock::now();
      TFTRT_ENSURE_OK(
        bundle.session->RunCallable(handle, inputs_device, &outputs, nullptr));
      
      // Sync, as `set_fetch_skip_sync(false)` is currently not implemented
      TFTRT_ENSURE_OK(device->Sync());
      end_time = std::chrono::steady_clock::now();
```
The profile is collected by
`nsys profile -w true -t cuda,nvtx,cudnn,cublas -f true -x true -o profile_c /opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/examples/image_classification/MiniBatch/mini_tftrt --model_path=""./resnet50_saved_model_RT"" --batch_size=64 --output_to_host=False`

And I found that cuSteamSynchronize takes most of time, as shown below:

[profile_c.zip](https://github.com/user-attachments/files/19120108/profile_c.zip)

![Image](https://github.com/user-attachments/assets/10df2c47-a950-4f22-8f2a-bda6eb40ada4)

I think the real computation is done and the GPU is wasting its time. Is that right? I don't see any other kernel working. So I try to skip that Sync by setting `opts.set_fetch_skip_sync(true);`.
However, the  cuSteamSynchronize  is still on there. No mater whether `device->Sync()` is used or not.  The time consumption and cuSteamSynchronize  are always unchanged even I set the output to host.

Here is the code and readme to reproduce the issue.

[MiniBatch2.zip](https://github.com/user-attachments/files/19120105/MiniBatch2.zip)

### Standalone code to reproduce the issue

```shell
See the zip file at the end.
```

### Relevant log output

```shell

```",1
tf.experimental.dlpack.to_dlpack() becomes performance bottleneck,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.9.0

### Custom code

Yes

### OS platform and distribution

Rocky Linux 8.6

### Mobile device

_No response_

### Python version

3.9.7

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA-11.7.0/cuDNN-8.4.1.50

### GPU model and memory

NVIDIA A100-SXM4-40GB

### Current behavior?

Hi, my code uses CuPy and TensorFlow, where CuPy is used for data preprocessing and postprocessing, and TensorFlow is responsible for loading models and inference. For the conversion between CuPy arrays and TF tensors, we use DLpack: https://docs.cupy.dev/en/stable/user_guide/interoperability.html#dlpack.

However, it seems that `tf.experimental.dlpack.to_dlpack()` has become a performance bottleneck because I need to convert the inference result (TF tensor) into a CuPy array for post-processing. And a single conversion takes about **0.13 seconds** but converting from a CuPy array to a TF tensor (`tf.experimental.dlpack.from_dlpack()`) takes less than **0.01 seconds**. The entire calculation process needs to be carried out in a loop, which usually requires hundreds of iterations, so the data conversion time will be magnified.

So I want to know how to reduce the time taken by `tf.experimental.dlpack.to_dlpack()` if there are any suggestions. Thanks in advance :)

### Standalone code to reproduce the issue

```shell
tf_tensor= tf_model(tf.experimental.dlpack.from_dlpack(CuPy_array.toDlpack()), training=False)

cp.from_dlpack(tf.experimental.dlpack.to_dlpack(tf_tensor))
```

### Relevant log output

```shell

```",1
Tensorflow does not recognize GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18.0

### Custom code

No

### OS platform and distribution

WSL Linux Ubuntu 24.04

### Mobile device

_No response_

### Python version

3.12.0

### Bazel version

_No response_

### GCC/compiler version

13.3.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

RTX A4000 16GB

### Current behavior?

First, I installed Anaconda3-2025.10.1-Linux using wget and the shell script. 

### 1.  Install Anaconda Commands:

```
wget https://repo.anaconda.com/archive/Anaconda3-2024.10-1-Linux-x86_64.sh
bash Anaconda3-2024.10-1-Linux-x86_64.sh
```

I selected yes on the location in which it generally installs.
Anaconda was installed in /home/user/anaconda3. 
I also allowed anaconda to update my shell profile by selecting yes. 
I rebooted the shell and went to step 2. 

### 2. I made sure my drivers were up to date. 

I ran the nvidia-smi command to check this and saw the latest driver for my NVIDIA RTX A4000 GPUs. The image below shows the output with the most up to date drivers. 

![Image](https://github.com/user-attachments/assets/89e0f560-be70-4f80-ba24-85cb5a7fc469)

### 3. I installed the latest cuda toolkit using the following commands. 

```
sudo apt-key del 7fa2af80
wget https://developer.download.nvidia.com/compute/cuda/12.8.0/local_installers/cuda_12.8.0_570.86.10_linux.run
sudo sh cuda_12.8.0_570.86.10_linux.run
```

### 4. I set my path to the cuda toolkit and the lib64 in my .bashrc file

I put the at the bottom of the file and rebooted the shell 

```
export PATH=/usr/local/cuda-12.8/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/cuda-12.8/lib64:$PATH
```

### 5. I install cuDNN
I downloaded the tar file for linux [here](https://developer.nvidia.com/rdp/cudnn-archive). It does seem that there is a file for 24.04, but the tar file did work when I installed it. 
```
tar -xvf cudnn-linux-x86_64-8.9.7.29_cuda12-archive.tar.xz
```
This unraveled a lot of files on my machine. I followed the installation instructions [here](https://docs.nvidia.com/deeplearning/cudnn/archives/cudnn-850/install-guide/)

specifically, I did the following: 

```
sudo cp cudnn-*-archive/include/cudnn*.h /usr/local/cuda/include 
sudo cp -P cudnn-*-archive/lib/libcudnn* /usr/local/cuda/lib64 
sudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn*
```

When I run nvcc -V, I get: 

![Image](https://github.com/user-attachments/assets/31ec4ac5-aaf9-4532-95da-2e3794c69ba7)

### 6 I install a virtual environment and activate it. 

```
conda create -n tf python==3.12
conda activate tf
python3 -m pip install tensorflow[and-cuda]
```

Then, when I check for GPU support, I get errors and my GPU is not recognized:

### 7 The errors are shown below: 

```
python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""
```

Errors: 

```
2025-03-05 15:55:46.327193: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-03-05 15:55:46.338383: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741211746.350376   17653 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741211746.354146   17653 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-05 15:55:46.365820: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-03-05 15:55:48.179254: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: RESOURCE_EXHAUSTED: Failed call to cuInit: CUDA_ERROR_OUT_OF_MEMORY: out of memory
```

What am I doing wrong? 

### Standalone code to reproduce the issue

```shell
I am getting a CUDA_ERROR_OUT_OF_MEMORY upon install and my GPU is not being listed.
```

### Relevant log output

```shell

```",1
Tensorflow with C++ Builder 12,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

libtensorflow-cpu-windows-x86_64.zip

### Custom code

Yes

### OS platform and distribution

windows 10

### Mobile device

N/A

### Python version

N/A

### Bazel version

N/A

### GCC/compiler version

N/A

### CUDA/cuDNN version

_No response_

### GPU model and memory

N/A

### Current behavior?

this simple c file should compile:
bcc64 test6.c -D__NO_INLINE -DWIN64 -I ""N:\DOWNLOADS\FASTEST\26WORKS\TOTEST\include"" -L ""N:\DOWNLOADS\FASTEST\26WORKS\TOTEST\lib"" -l""tensorflow"" -v

```
#include <tensorflow/c/c_api.h>
//#pragma comment(lib, ""tensorflow.lib"") 
//#pragma link ""tensorflow.lib""
int main() {
    // Your code using TensorFlow's C API functions
    TF_Graph* graph = TF_NewGraph();
    // More TensorFlow code...
    return 0;
}
```

but it does not:
Embarcadero C++ 7.70 for Win64 Copyright (c) 2012-2024 Embarcadero Technologies, Inc.
test6.c:
Turbo Incremental Link64 6.99 Copyright (c) 1997-2024 Embarcadero Technologies, Inc.
Error: Unresolved external 'TF_NewGraph' referenced from C:\USERS\USER\APPDATA\LOCAL\TEMP\TEST6-AF147F.O

if I add:
#pragma comment(lib, ""tensorflow.lib"") 

I got invalid object file tensorflow.dll

### Standalone code to reproduce the issue

```shell
#include <tensorflow/c/c_api.h>
//#pragma comment(lib, ""tensorflow.lib"") ; DOES NOT WORK
//#pragma link ""tensorflow.lib""          ; DOES NOT WORK
int main() {
    // Your code using TensorFlow's C API functions
    TF_Graph* graph = TF_NewGraph();
    // More TensorFlow code...
    return 0;
}
```

### Relevant log output

```shell
Embarcadero C++ 7.70 for Win64 Copyright (c) 2012-2024 Embarcadero Technologies, Inc.
test6.c:
Turbo Incremental Link64 6.99 Copyright (c) 1997-2024 Embarcadero Technologies, Inc.
Error: Unresolved external 'TF_NewGraph' referenced from C:\USERS\USER\APPDATA\LOCAL\TEMP\TEST6-AF147F.O
```",1
"`tf.compat.v1.linalg.set_diag` aborts with ""Check failed: d < dims() (2 vs. 2)""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.20.0-dev20250225

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04 LTS 

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an aborted issue in TensorFlow when I used API tf.compat.v1.linalg.set_diag . I have confirmed that below code would crash on tf-nightly 2.20.0-dev20250225 (nightly-build).

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
input_tensor = tf.random.uniform([5, 4, 4, 4], dtype=tf.float32)
input = tf.identity(input_tensor)
diagonal_0_0 = 2.0
diagonal_0_1 = 3.0
diagonal_0_2 = 4.0
diagonal_0_3 = 5.0
diagonal_0 = [diagonal_0_0, diagonal_0_1, diagonal_0_2, diagonal_0_3]
diagonal_1_0 = -1.0
diagonal_1_1 = -2.0
diagonal_1_2 = -3.0
diagonal_1_3 = -4.0
diagonal_1 = [diagonal_1_0, diagonal_1_1, diagonal_1_2, diagonal_1_3]
diagonal = [diagonal_0, diagonal_1]
name = 'set_diag'
k_0 = 0
k_1 = 1
k = [k_0, k_1]
align = 'LEFT_RIGHT'
out = tf.compat.v1.linalg.set_diag(input=input, diagonal=diagonal, name=name, k=k, align=align)
```

### Relevant log output

```shell
2025-03-02 02:25:05.721633: F tensorflow/core/framework/tensor_shape.cc:359] Check failed: d < dims() (2 vs. 2)
Aborted (core dumped)
```",2
`tf.config.threading.set_intra_op_parallelism_threads` can cause a crash,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.20.0-dev20250225

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04 LTS

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `aborted issue` in TensorFlow when I used API `tf.config.threading.set_intra_op_parallelism_threads` . I have confirmed that below code would crash on `tf-nightly 2.20.0-dev20250225` (nightly-build).

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
num_streams = -1
tf.config.threading.set_intra_op_parallelism_threads(num_streams)

x = np.array([[1, 2], [3, 4]], dtype=np.float32)
y = tf.cast(x, dtype=tf.int32)
```

### Relevant log output

```shell
2025-02-26 14:41:57.365578: F external/local_xla/xla/tsl/platform/threadpool.cc:126] Check failed: num_threads >= 1 (1 vs. -1)
Aborted (core dumped)
```",2
Disabling GPU delegate for particular tflite nodes,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 18.04 ARM

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The OpenCL GPU delegate on our device doesn't support floor operations causing the delegation for the entire graph to fail. 

```
INFO: Loaded OpenCL library with dlopen.
ERROR: No selector for floor
ERROR: Falling back to OpenGL
```

Is there anyway I can disable delegation specifically for the nodes involving floor ops so that it doesn't tank my entire delegation? 

### Standalone code to reproduce the issue

```shell
N/A
```

### Relevant log output

```shell

```",1
Could you kindly update CUDA version in official Tensorflow containers?,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.18.0, 2.19.0rc0

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Can you please update official Tensorflow Docker containers starting version 2.18.0? It is supposed to require CUDA > 12.5. However, the official containers still use 12.3 and does not work. We can technically pull the container and modify. But official containers should include the correct dependencies. Thank you!

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.config.list_physical_devices()
```

### Relevant log output

```shell
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1740763678.575642 2218885 gpu_device.cc:2340] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]
```",1
error handling in list_devices() not present,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.12.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I noticed that there is a lack of error handling in the test_utils.py or device_lib.py to handle improper device query failures.  If _pywrap_device_lib.list_devices() is called when GPU drivers are not updated/missing, it could lead to Tensorflow crashing out without handling the error.  This can also happen for undefined device_lib in gpu_device_name() or if no GPU is detected.  
If viable, I would like to be assigned this issue to work on.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.client import device_lib

def list_local_devices():
    """"""List the available devices""""""
    from tensorflow.core.protobuf import device_attributes_pb2

    def _convert(pb_str):
        """"""Convert serialized device attribute protobuf to a readable format.""""""
        m = device_attributes_pb2.DeviceAttributes()
        m.ParseFromString(pb_str)
        return m

    return [_convert(s) for s in tf.compat.v1.Session().list_devices()] # Might return none

def gpu_device_name():
    """"""Returns the name of a GPU device""""""
    for x in device_lib.list_local_devices():  # Might return none
        if x.device_type == ""GPU"":
            return tf.compat.as_str(x.name)  # Might return none
    return """"


def test_gpu():
    """"""Test the GPU functions""""""
    devices = list_local_devices()  # Where problem might occur
    for device in devices:
        print(f""Device found: {device}"")
    gpu_name = gpu_device_name()  # Where problem might occur
    print(f""GPU Name: {gpu_name}"")

if __name__ == ""__main__"":
    test_gpu()
```

### Relevant log output

```shell
ImportError: cannot import name 'device_attributes_pb2' from 'tensorflow.core.protobuf'
```",2
Tensorflow Website Out-of-date,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2

### Custom code

Yes

### OS platform and distribution

Windows 11 24H2

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In tensorflow.org/install, everything is just so out of date.
It started when i'm reading the chinese version of the install page (https://www.tensorflow.org/install?hl=zh-cn):
which is:
<html>
<body>
<!--StartFragment--><p style=""box-sizing: inherit; margin: 16px 0px; padding: 0px; color: rgb(32, 33, 36); font-family: Roboto, &quot;Noto Sans&quot;, &quot;Noto Sans JP&quot;, &quot;Noto Sans KR&quot;, &quot;Noto Naskh Arabic&quot;, &quot;Noto Sans Thai&quot;, &quot;Noto Sans Hebrew&quot;, &quot;Noto Sans Bengali&quot;, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">我们在以下 64 位系统上测试过 TensorFlow 并且这些系统支持 TensorFlow：</p><div class=""devsite-table-wrapper"" style=""box-sizing: inherit; margin-top: ; margin-right: ; margin-bottom: 0px; margin-left: ; padding: 0px; overflow: auto; color: rgb(32, 33, 36); font-family: Roboto, &quot;Noto Sans&quot;, &quot;Noto Sans JP&quot;, &quot;Noto Sans KR&quot;, &quot;Noto Naskh Arabic&quot;, &quot;Noto Sans Thai&quot;, &quot;Noto Sans Hebrew&quot;, &quot;Noto Sans Bengali&quot;, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">
Python 3.6–3.9
--
</div><!--EndFragment-->
</body>
</html>

See the problem? it said: TensorFlow is tested and supported on the following 64-bit systems: Python 3.6-3.9
But 3.6 is EOS like millions of years ago, so i scrolled down and saw the page last updated date: 2021-08-25.
But this may be that the people are lazy to translate them to chinese, so i changed my language to english.
Better but it said TF supports Python 3.8-3.11, which is also not true. The date last updated for this is 2023-03-24, almost two years ago.

So, my question is: why is this so out of date? Like this is supposed to be the official website for tensorflow!
In the main page (tensorflow.org), it said: TF 2.18 released (which is true), but the [API docs version is](https://www.tensorflow.org/api_docs) actually v2.16.1, with a date-last-updated of 2024-09-30.

This is just so annoying. When people use a google search on 'Tensorflow', the first thing that they see is going to be the website, not the Github repo itself, so please update it.
(Okay, if you're just too lazy to stay updated just post a message on the screen saying ""This page is no longer maintained so see the repo (link to repo)"" something like that)

### Standalone code to reproduce the issue

```shell
No code.
```

### Relevant log output

```shell

```",1
"tensorflow cannot detect GPU with cuda 12.8, torch can","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.13

### Custom code

No

### OS platform and distribution

Linux CentOS 7

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuda 12.8 cudnn 9.7.1

### GPU model and memory

v100

### Current behavior?

I have tried python 3.8, 3.11, none of the can detect GPU, but pytorch and tf-nightly is working well.
Whenever I ask tensorflow to list GPU available, it gave me this message:

> [GCC 11.2.0] :: Anaconda, Inc. on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
 "", len(tf.config.list_physical_devices('GPU')))2025-02-26 14:28:49.474758: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2025-02-26 14:28:49.528867: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
```

### Relevant log output

```shell

```",1
[TOSA] NameError: name 'ExperimentalTFLiteToTosaBytecode' is not defined,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.19.0rc

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

This function call should not return a `NameError`. It should either do not exist, or should be removed.

### Standalone code to reproduce the issue

```shell
from tensorflow.python.pywrap_mlir import experimental_tflite_to_tosa_bytecode
experimental_tflite_to_tosa_bytecode("""", """")
```

### Relevant log output

```shell
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/tmp/env/lib/python3.12/site-packages/tensorflow/python/pywrap_mlir.py"", line 123, in experimental_tflite_to_tosa_bytecode
    return ExperimentalTFLiteToTosaBytecode(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NameError: name 'ExperimentalTFLiteToTosaBytecode' is not defined. Did you mean: 'experimental_tflite_to_tosa_bytecode'?

```",1
Cuda 12.8 support,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.19.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 24.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

0.6.5

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.8/9.7.0

### GPU model and memory

5090

### Current behavior?

Not supported. Hermetic cuda download 12.5

### Standalone code to reproduce the issue

```shell
compile with hermetic cuda. XLA is not updated in tensorflow
```

### Relevant log output

```shell

```",1
A heap oob write in TensorArray.write,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.18.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.5

### Mobile device

_No response_

### Python version

Python 3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In a model, if there is a lambda layer containing the TensorArray.write operation, the model service will crash after sending data for inference.



### Standalone code to reproduce the issue

```shell
In a python code, we can varify the vulnerability quickly with the following poc.

import tensorflow as tf

@tf.function()
def foo():
  ta = tf.TensorArray(tf.float32, size=10,dynamic_size=True, clear_after_read=False)
  ta = ta.write(tf.cast(0xffffffff, tf.int32),1)
  return ta.read(1)

a=foo()
print(a)
```

### Relevant log output

```shell
## crash

Thread 1 ""python"" received signal SIGSEGV, Segmentation fault.
0x00007fffebfa5560 in tensorflow::TensorListSetItem::Compute(tensorflow::OpKernelContext*) () from /home/test/ai/keras-h5/tf-venv/lib/python3.10/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2
(gdb) bt
#0  0x00007fffebfa5560 in tensorflow::TensorListSetItem::Compute(tensorflow::OpKernelContext*) ()
   from /home/test/tf-venv/lib/python3.10/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2
#1  0x00007ffff563d0b9 in tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::Process(tensorflow::SimplePropagatorState::TaggedNode const&, long) ()
   from /home/test/tf-venv/lib/python3.10/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2
#2  0x00007ffff566c844 in std::_Function_handler<void (std::function<void ()>), tensorflow::GraphRunner::Run(tensorflow::Graph*, tensorflow::FunctionLibraryRuntime*, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*)::$_1>::_M_invoke(std::_Any_data const&, std::function<void ()>&&) ()
   from /home/test/tf-venv/lib/python3.10/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2
#3  0x00007ffff563bbd7 in tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode> >*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*) ()
   from /home/test/tf-venv/lib/python3.10/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2
#4  0x00007ffff563eb8e in tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::NodeDone(absl::lts_20230802::Status const&, absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode> >*, tensorflow::NodeExecStatsInterface*, tensorflow::SimplePropagatorState::TaggedNodeR--Type <RET> for more, q to quit, c to continue without paging--q
Quit
(gdb) x/i $rip
=> 0x7fffebfa5560 <_ZN10tensorflow17TensorListSetItem7ComputeEPNS_15OpKernelContextE+576>:	lock decq 0x8(%rbx)
(gdb) i r rbx
rbx            0x151               337
(gdb) x/8gx 0x5555577db870-0x20
0x5555577db850:	0x0000555559a09268	0x0000015555d278d8
0x5555577db860:	0x0000000000000001	【0x0000000000000151】--->rbx
0x5555577db870:	0x0000000000000000	0x0001003000000003
0x5555577db880:	0x0000000000000000	0x0000000000000000

## oob write
It will write a Tensor before the vector address(0x5555577db870).


# r12 is the start of vector, r15 is the index and r14 is the src Tensor which will be written to the index
0x7fffebfa550b <_ZN10tensorflow17TensorListSetItem7ComputeEPNS_15OpKernelContextE+491>:	lea    (%r12,%r15,1),%rbx
0x7fffebfa550f <_ZN10tensorflow17TensorListSetItem7ComputeEPNS_15OpKernelContextE+495>:	movzbl 0xd(%r14),%r13d

(gdb) x/4gx $r14
0x5555599e3170:	0x00005555599ea3f8	0x0000015558132bf8
0x5555599e3180:	0x0000000000000001	0x00005555596e7370
(gdb) i r r12
r12            0x5555577db870      93825028438128
(gdb) i r r15
r15            0xffffffffffffffe0  -32
#before writing
(gdb) x/4gx $rbx
0x5555577db850:	0x0000000000000000	0x0000000000000000
0x5555577db860:	0x00007fffaf2fb1e0	0x0000000000000151

#copy contents of r14 to an oob address rbx
0x7fffebfa5514 <_ZN10tensorflow17TensorListSetItem7ComputeEPNS_15OpKernelContextE+500>:	mov    0x10(%r14),%rax
0x7fffebfa5518 <_ZN10tensorflow17TensorListSetItem7ComputeEPNS_15OpKernelContextE+504>:	mov    %rax,0x10(%r12,%r15,1)
...
0x7fffebfa552c <_ZN10tensorflow17TensorListSetItem7ComputeEPNS_15OpKernelContextE+524>:	vmovups (%r14),%xmm0
0x7fffebfa5531 <_ZN10tensorflow17TensorListSetItem7ComputeEPNS_15OpKernelContextE+529>:	vmovups %xmm0,(%rbx)

#after writing
(gdb) x/4gx 0x5555577db870-0x20
0x5555577db850:	0x00005555599ea3f8	0x0000015558132bf8
0x5555577db860:	0x0000000000000001	0x0000000000000151
```",2
[RNN] Conversion of model containing GRU layer to quantized TFLite causes Segmentation Fault,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 22.04
- TensorFlow installation (pip package or built from source): both pip package and built from source
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.18

### 2. Code

import os
os.environ[""TF_USE_LEGACY_KERAS""] = ""1""
import tensorflow as tf
import numpy as np
import tf_keras as keras

def gru_model():
    """"""Factory method for gru model.""""""
    gru_input = keras.layers.Input(batch_input_shape=(1, 1, 64), 
                                   name=""gru_input"")
    gru_state_in = keras.layers.Input(batch_input_shape=(1, 128), 
                                      name=""gru_state_in"")
    gru_output, gru_state_out = keras.layers.GRU(128,
                                                 activation=""tanh"",
                                                 recurrent_activation=""sigmoid"",
                                                 use_bias=True,
                                                 return_sequences=False,
                                                 bias_initializer=""random_uniform"",
                                                 return_state=True)([gru_input, gru_state_in])

    keras_model = keras.Model(inputs=[gru_input, gru_state_in], outputs=[gru_output, gru_state_out])
    return keras_model
model = gru_model()
train_inputs = [tf.random.uniform((1, 1, 64), minval=-1.0, maxval=32767.0/32768.0, dtype=tf.dtypes.float32),
                tf.random.uniform((1, 128), minval=-1.0, maxval=32767.0/32768.0, dtype=tf.dtypes.float32)]
train_outputs = tf.random.uniform((1, 128), minval=-1.0, maxval=32767.0/32768.0, dtype=tf.dtypes.float32)
model.compile(optimizer='adam', loss='mse', metrics=['mse'])
model.fit([train_inputs], train_outputs, batch_size=1, epochs=1)
model.summary()
model.save(""ticket_gru.keras"")

print(""Converting to floating point Tensorflow Lite"")
converter_fp = tf.lite.TFLiteConverter.from_keras_model(model)
converter_fp.optimizations = [tf.lite.Optimize.DEFAULT]
converter_fp.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]
tflite_model_fp = converter_fp.convert()
with open(""ticket_gru_fp.tflite"",""wb"") as f: f.write(tflite_model_fp)
print(""Success converting to floating point Tensorflow Lite containing WHILE operator in \
subgraph 0 and cell implementation in subgraph 1"")

print(""Converting to quantized Tensorflow Lite"")
def representative_data():
  for _ in range(10):
    yield {model.inputs[0].name:tf.random.uniform((1, 1, 64), minval=-1.0, maxval=32767.0/32768.0, dtype=tf.dtypes.float32, seed=None, name=None),
           model.inputs[1].name:tf.random.uniform((1, 128), minval=-1.0, maxval=32767.0/32768.0, dtype=tf.dtypes.float32, seed=None, name=None)}
converter_q = tf.lite.TFLiteConverter.from_keras_model(model)
converter_q.optimizations = [tf.lite.Optimize.DEFAULT]
converter_q.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8,
tf.lite.OpsSet.TFLITE_BUILTINS]
converter_q.inference_input_type = tf.int8
converter_q.inference_output_type = tf.int8
converter_q._experimental_disable_per_channel = True
converter_q.representative_dataset = representative_data
print(""Call to convert method of the converter_q will cause SEGFAULT during model calibration"")
tflite_model_q = converter_q.convert()
print(""This line is never reached"")
with open(""ticket_gru_8_8.tflite"",""wb"") as f: f.write(tflite_model_q)

### 3. Failure after conversion

Converter fails with throwing Segmentation fault. The fault is tracked down to the Calibration phase where the representative dataset is fead to the floating point model inferred by the interpreter instantiated by the converter. The main subgraph processing causes nested subgraph processing as a part of the WHILE operator invoke. All the operator in the nested subgraph are invoked, but the return from the model invoke ends with Segmentation Fault. The segmentation fault can be traced down to Tensorflow 2.14. Tensorflow 2.13 handles the quantized conversion properly.
Conversion to floating point TFLite model works fine, and the floating point inference with same data that is used as representative dataset in quantized conversion does complete without problems
  
### 4. (optional) RNN conversion support

Yes - RNN conversion support. Prefixed in the title as [RNN]

### 5. (optional) Any other info / logs

2025-02-18 11:45:02.889542: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2025-02-18 11:45:04.329812: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)
1/1 [==============================] - ETA: 0s - loss: 1.0199 - gru_loss: 0.5099 - gru_1_loss: 0.5099 - gru_mse: 0.5099 - gru_1_mse: 1/1 [==============================] - 1s 1s/step - loss: 1.0199 - gru_loss: 0.5099 - gru_1_loss: 0.5099 - gru_mse: 0.5099 - gru_1_mse: 0.5099
Model: ""model""
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 gru_input (InputLayer)      [(1, 1, 64)]                 0         []                            
                                                                                                  
 gru_state_in (InputLayer)   [(1, 128)]                   0         []                            
                                                                                                  
 gru (GRU)                   [(1, 128),                   74496     ['gru_input[0][0]',           
                              (1, 128)]                              'gru_state_in[0][0]']        
                                                                                                  
==================================================================================================
Total params: 74496 (291.00 KB)
Trainable params: 74496 (291.00 KB)
Non-trainable params: 0 (0.00 Byte)
__________________________________________________________________________________________________
Converting to floating point Tensorflow Lite
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1739875508.650404  896323 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1739875508.650452  896323 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
2025-02-18 11:45:08.650993: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp96nzf981
2025-02-18 11:45:08.656672: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }
2025-02-18 11:45:08.656711: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmp96nzf981
I0000 00:00:1739875508.691084  896323 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled
2025-02-18 11:45:08.698338: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.
2025-02-18 11:45:08.768960: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmp96nzf981
2025-02-18 11:45:08.807201: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 156213 microseconds.
2025-02-18 11:45:09.144839: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2025-02-18 11:45:09.264954: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3893] Estimated count of arithmetic ops: 0.174 M  ops, equivalently 0.087 M  MACs
Success converting to floating point Tensorflow Lite containing WHILE operator in subgraph 0 and cell implementation in subgraph 1
Converting to quantized Tensorflow Lite
Call to convert method of the converter_q will cause SEGFAULT during model calibration
/opt/samba/nxf35112/keras2_experiments/venv/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.
  warnings.warn(
W0000 00:00:1739875512.496385  896323 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1739875512.496430  896323 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
2025-02-18 11:45:12.496633: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpfx3q8bul
2025-02-18 11:45:12.501629: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }
2025-02-18 11:45:12.501670: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpfx3q8bul
2025-02-18 11:45:12.542409: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.
2025-02-18 11:45:12.609623: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpfx3q8bul
2025-02-18 11:45:12.649789: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 153160 microseconds.
2025-02-18 11:45:13.053211: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3893] Estimated count of arithmetic ops: 0.174 M  ops, equivalently 0.087 M  MACs
Segmentation fault

",2
[CUDA] Check failed work_element_count >= 0,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Ubuntu 24.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuDNN 90300

### GPU model and memory

H100 80G

### Current behavior?

F0000 00:00:1739864447.179429 1679389 gpu_launch_config.h:129] Check failed: work_element_count >= 0 (-1881720796 vs. 0)
*** Check failure stack trace: ***
    @     0x7fffec867bc4  absl::lts_20230802::log_internal::LogMessage::SendToLog()
    @     0x7fffec8679c4  absl::lts_20230802::log_internal::LogMessage::Flush()
    @     0x7fffec867fe9  absl::lts_20230802::log_internal::LogMessageFatal::~LogMessageFatal()
    @     0x7fffe33017d4  tensorflow::functor::TransformFilter<>::operator()()
    @     0x7fffe1896f56  tensorflow::LaunchConvOpImpl<>()
    @     0x7fffe1a03561  tensorflow::Conv2DOp<>::Compute()
    @     0x7ffff51474d5  tensorflow::BaseGPUDevice::Compute()
    @     0x7ffff51f8f48  tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run()
    @     0x7ffff51b8724  tensorflow::FunctionLibraryRuntimeImpl::RunSync()
    @     0x7ffff51c5730  tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync()
    @     0x7ffff51cbb7d  tensorflow::ProcessFunctionLibraryRuntime::RunSync()
    @     0x7fffdcd3ddf0  tensorflow::KernelAndDeviceFunc::Run()
    @     0x7fffdcce98d6  tensorflow::EagerKernelExecute()
    @     0x7fffdccf33b0  tensorflow::ExecuteNode::Run()
    @     0x7fffdcd39244  tensorflow::EagerExecutor::SyncExecute()
    @     0x7fffdcce925b  tensorflow::(anonymous namespace)::EagerLocalExecute()
    @     0x7fffdcce6929  tensorflow::DoEagerExecute()
    @     0x7fffdccea660  tensorflow::EagerExecute()
    @     0x7fffdc3209f7  tensorflow::EagerOperation::Execute()
    @     0x7fffdcd37943  tensorflow::CustomDeviceOpHandler::Execute()
    @     0x7fffd9b38cf5  TFE_Execute
    @     0x7fffee231efa  TFE_Py_FastPathExecute_C()
    @     0x7fffedad6893  pybind11::detail::argument_loader<>::call<>()
    @     0x7fffedad67cf  pybind11::cpp_function::initialize<>()::{lambda()#1}::__invoke()
    @     0x7fffedab08df  pybind11::cpp_function::dispatcher()
    @           0x54d2d4  cfunction_call


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from keras import layers
import os
os.environ[""TF_DISABLE_RZ_CHECK""] = ""1""
os.environ[""TF_GPU_ALLOCATOR""] = ""cuda_malloc_async""
tf.keras.backend.set_image_data_format('channels_first')
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)

tensor = tf.random.uniform([100, 100, 100])
model = layers.Conv1D(kernel_size=3, strides=98, filters=8044155, groups=1)
model(tensor)
```

### Relevant log output

```shell

```",2
[CUDA] cudaErrorInvalidConfiguration detected by compute-sanitizer,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.8.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 24.04

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

90300

### GPU model and memory

H100 80G

### Current behavior?

compute-sanitizer reports Program hit cudaErrorInvalidConfiguration

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from keras import layers
import os
os.environ[""TF_DISABLE_RZ_CHECK""] = ""1""
os.environ[""TF_GPU_ALLOCATOR""] = ""cuda_malloc_async""
tf.keras.backend.set_image_data_format('channels_first')
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)

tensor = tf.random.uniform([2, 388814, 2])
model = layers.Conv1D(filters=2, kernel_size=1, strides=2, groups=2)
model(tensor)
```

### Relevant log output

```shell
========= COMPUTE-SANITIZER
========= Program hit cudaErrorInvalidConfiguration (error 9) due to ""invalid configuration argument"" on CUDA API call to cudaLaunchKernelExC.
=========     Saved host backtrace up to driver entry point at error
=========     Host Frame: [0x4aa4a5]
=========                in /lib/x86_64-linux-gnu/libcuda.so.1
=========     Host Frame: [0x1d1b509]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_engines_precompiled.so.9
=========     Host Frame: [0x15f94a1]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_engines_precompiled.so.9
=========     Host Frame: [0x1461f45]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_engines_precompiled.so.9
=========     Host Frame: [0x14627e9]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_engines_precompiled.so.9
=========     Host Frame: [0xfefdb1]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_engines_precompiled.so.9
=========     Host Frame: [0xff0704]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_engines_precompiled.so.9
=========     Host Frame: [0x4a33ad]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_engines_precompiled.so.9
=========     Host Frame: [0x4a3888]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_engines_precompiled.so.9
=========     Host Frame: [0x4c4a94]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_engines_precompiled.so.9
=========     Host Frame:cudnn::backend::execute(cudnnContext*, cudnn::backend::ExecutionPlan const&, cudnn::backend::VariantPack&) [0x134c78]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_graph.so.9
=========     Host Frame:cudnnBackendExecute [0x134d8e]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_graph.so.9
=========     Host Frame: [0x8d5cf]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_graph.so.9
=========     Host Frame:cuda_graph_util::CudaGraphInfo::init(void*, cudnn::backend::OperationSetFinalizedMode_t) [0x868e5]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_graph.so.9
=========     Host Frame:cudnn::backend::cudnnBackendGetMetadataFromGraph(void*) [0x125fac]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_graph.so.9
=========     Host Frame: [0x153281]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_heuristic.so.9
=========     Host Frame: [0x1a4291]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_heuristic.so.9
=========     Host Frame: [0x1a593a]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../../nvidia/cudnn/lib/libcudnn_heuristic.so.9
=========     Host Frame: [0x151dea]
```",1
[CUDA] illegal memory read in ShuffleInTensor3Simple,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 24.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuDNN 90300

### GPU model and memory

H100 80G

### Current behavior?

compute-sanitizer detects invalid memory read.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from keras import layers
import os
os.environ[""TF_DISABLE_RZ_CHECK""] = ""1""
os.environ[""TF_GPU_ALLOCATOR""] = ""cuda_malloc_async""
tf.keras.backend.set_image_data_format('channels_first')
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)

tensor = tf.random.uniform([1, 79768, 2])
model = layers.Conv1D(filters=65470, kernel_size=1, strides=32827, groups=1)
model(tensor)
```

### Relevant log output

```shell
========= COMPUTE-SANITIZER
========= Invalid __global__ read of size 4 bytes
=========     at void tensorflow::functor::ShuffleInTensor3Simple<float, (int)2, (int)1, (int)0, (bool)0>(int, const T1 *, tensorflow::functor::Dimension<(int)3>, T1 *)+0x630
=========     by thread (64,0,0) in block (84,0,0)
=========     Address 0x32f9d8d188 is out of bounds
=========     and is 5,845,702,776 bytes before the nearest allocation at 0x3456472a00 of size 20,889,643,840 bytes
=========     Saved host backtrace up to driver entry point at kernel launch time
=========     Host Frame: [0x37f187]
=========                in /lib/x86_64-linux-gnu/libcuda.so.1
=========     Host Frame: [0x15a13]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../../nvidia/cuda_runtime/lib/libcudart.so.12
=========     Host Frame:cudaLaunchKernel [0x75750]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../../nvidia/cuda_runtime/lib/libcudart.so.12
=========     Host Frame:absl::lts_20230802::Status tensorflow::GpuLaunchKernel<int, float const*, tensorflow::functor::Dimension<3>, float*, int, float const*, tensorflow::functor::Dimension<3>, float*>(void (*)(int, float const*, tensorflow::functor::Dimension<3>, float*), dim3, dim3, unsigned long, CUstream_st*, int, float const*, tensorflow::functor::Dimension<3>, float*) [0x2b5018e4]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2
=========     Host Frame:tensorflow::functor::TransformFilter<Eigen::GpuDevice, float, int, 4>::operator()(Eigen::GpuDevice const&, tensorflow::FilterTensorFormat, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float, 4, 1, int>, 16, Eigen::MakePointer>) [0x2b5015ff]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2
=========     Host Frame:void tensorflow::LaunchConvOpImpl<float>(tensorflow::OpKernelContext*, bool, tensorflow::Tensor const&, tensorflow::Tensor const&, absl::lts_20230802::InlinedVector<long, 3ul, std::allocator<long> > const&, absl::lts_20230802::InlinedVector<long, 3ul, std::allocator<long> > const&, tensorflow::Padding const&, std::vector<long, std::allocator<long> > const&, tensorflow::TensorFormat, tensorflow::Tensor*) [0x29a96f55]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2
=========     Host Frame:tensorflow::Conv2DOp<Eigen::GpuDevice, float>::Compute(tensorflow::OpKernelContext*) [0x29c03560]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2
=========     Host Frame:tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) [0x6d474d4]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2
=========     Host Frame:tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) [0x6df8f47]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2
=========     Host Frame:tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) [0x6db8723]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2
=========     Host Frame:tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const [0x6dc572f]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2
=========     Host Frame:tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) const [0x6dcbb7c]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2
=========     Host Frame:tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) [0x24f3ddef]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2
=========     Host Frame:tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*> > const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) [0x24ee98d5]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2
=========     Host Frame:tensorflow::ExecuteNode::Run() [0x24ef33af]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2
=========     Host Frame:tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) [0x24f39243]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2
=========     Host Frame:tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) [0x24ee925a]
```",1
[CUDA] illegal memory write in ShuffleInTensor3Simple,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.8.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 24.04

### Mobile device

_No response_

### Python version

3.12.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuDNN: 90300

### GPU model and memory

H100 80G

### Current behavior?

compute-sanitizer reports Illegal Memory Access.

Expect: no IMA should occur.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from keras import layers
import os
os.environ[""TF_DISABLE_RZ_CHECK""] = ""1""
tf.keras.backend.set_image_data_format('channels_first')
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)
tensor = tf.random.uniform([2, 1, 392366])
model = layers.Conv1D(filters=2147483647, kernel_size=1, strides=392366, groups=1)
model(tensor)
```

### Relevant log output

```shell
========= COMPUTE-SANITIZER
========= Invalid __global__ write of size 4 bytes
=========     at void tensorflow::functor::ShuffleInTensor3Simple<float, (int)2, (int)1, (int)0, (bool)0>(int, const T1 *, tensorflow::functor::Dimension<(int)3>, T1 *)+0x660
=========     by thread (128,0,0) in block (8,0,0)
=========     Address 0x2e9c2ff600 is out of bounds
=========     and is 517 bytes after the nearest allocation at 0x2c9c2ff400 of size 8,589,934,588 bytes
=========     Saved host backtrace up to driver entry point at kernel launch time
=========     Host Frame: [0x37f187]
=========                in /lib/x86_64-linux-gnu/libcuda.so.1
=========     Host Frame: [0x15a13]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../../nvidia/cuda_runtime/lib/libcudart.so.12
=========     Host Frame:cudaLaunchKernel [0x75750]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../../nvidia/cuda_runtime/lib/libcudart.so.12
=========     Host Frame:absl::lts_20230802::Status tensorflow::GpuLaunchKernel<int, float const*, tensorflow::functor::Dimension<3>, float*, int, float const*, tensorflow::functor::Dimension<3>, float*>(void (*)(int, float const*, tensorflow::functor::Dimension<3>, float*), dim3, dim3, unsigned long, CUstream_st*, int, float const*, tensorflow::functor::Dimension<3>, float*) [0x2b5018e4]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2
=========     Host Frame:tensorflow::functor::TransformFilter<Eigen::GpuDevice, float, int, 4>::operator()(Eigen::GpuDevice const&, tensorflow::FilterTensorFormat, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float, 4, 1, int>, 16, Eigen::MakePointer>) [0x2b5015ff]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2
=========     Host Frame:void tensorflow::LaunchConvOpImpl<float>(tensorflow::OpKernelContext*, bool, tensorflow::Tensor const&, tensorflow::Tensor const&, absl::lts_20230802::InlinedVector<long, 3ul, std::allocator<long> > const&, absl::lts_20230802::InlinedVector<long, 3ul, std::allocator<long> > const&, tensorflow::Padding const&, std::vector<long, std::allocator<long> > const&, tensorflow::TensorFormat, tensorflow::Tensor*) [0x29a96f55]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2
=========     Host Frame:tensorflow::Conv2DOp<Eigen::GpuDevice, float>::Compute(tensorflow::OpKernelContext*) [0x29c03560]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2
=========     Host Frame:tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) [0x6d474d4]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2
=========     Host Frame:tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) [0x6df8f47]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2
=========     Host Frame:tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) [0x6db8723]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2
=========     Host Frame:tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const [0x6dc572f]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2
=========     Host Frame:tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) const [0x6dcbb7c]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2
=========     Host Frame:tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) [0x24f3ddef]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2
=========     Host Frame:tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*> > const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) [0x24ee98d5]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2
=========     Host Frame:tensorflow::ExecuteNode::Run() [0x24ef33af]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2
=========     Host Frame:tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) [0x24f39243]
=========                in /opt/ext/jwnhy/miniconda3/envs/tf218/lib/python3.12/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2
=========     Host Frame:tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) [0x24ee925a]
```",1
"`tf.raw_ops.LookupTableExportV2` aborts with ""Check failed: dtype() == expected_dtype (9 vs. 1) float expected, got int64""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly 2.19.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04 LTS

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

encountered an `aborted issue` in TensorFlow when I used API `tf.raw_ops.LookupTableExportV2` . I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20250207` (nightly-build).
Please find the [gist](https://colab.research.google.com/drive/1f7eQ9lklrmTGdQmI7hVckLDP2IJa14Wq?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
keys = tf.constant([0, 1, 2], dtype=tf.int64)
values = tf.constant([1, 2, 3], dtype=tf.float32)  
table = tf.lookup.StaticHashTable(tf.lookup.KeyValueTensorInitializer(keys, values), -1)
export_keys, export_values = tf.raw_ops.LookupTableExportV2(table_handle=table.resource_handle, Tkeys=tf.int64, Tvalues=tf.int64)
```

### Relevant log output

```shell
2025-02-17 12:21:39.561442: F tensorflow/core/framework/tensor.cc:857] Check failed: dtype() == expected_dtype (9 vs. 1) float expected, got int64
Aborted (core dumped)
```",2
"`tf.experimental.numpy.swapaxes` aborts with ""Check failed: d >= 0 (0 vs. -2)""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly 2.19.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04 LTS

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `aborted issue` in TensorFlow when I used API `tf.experimental.numpy.swapaxes` . I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20250207` (nightly-build)
Please find the [gist](https://colab.research.google.com/drive/1o9nDugvyDbKqmh8h-E5qR3S7U4kbNdaX?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
import tensorflow
import tensorflow as tf
import numpy
import numpy as np

input_data = np.arange(24).reshape(2, 3, 4)
output_data = tf.experimental.numpy.swapaxes(input_data, -5, 2)
```

### Relevant log output

```shell
2025-02-17 12:30:19.854628: F tensorflow/core/framework/tensor_shape.cc:358] Check failed: d >= 0 (0 vs. -2)
Aborted (core dumped)
```",2
Memory leak when compiling tfp.util.TransformedVariable since TF 2.14 (worked fine before!),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.14+ (present in 2.14, 2.16, 2.18; not an issue in 2.11, 2.12, 2.13)

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Upgrading a [GPflow](https://www.gpflow.org/)-based workflow to use TF 2.14 is showing memory leaks where none previously occurred. Preliminary investigations connected this to the compilation of GPflow models: in the code snippet below the memory usage increases over time (by around 1MB per iteration for a total of ~200MB) when run with TF 2.14, 2.16 or 2.18, but not when run with TF 2.11, 2.12 or 2.13.

Raising as a TF bug as code that was previously executing fine is now appearing to leak memory, though there is also a chance that this is an issue in GPflow. Any suggestions for helping identify the leaked memory would be very welcome!


### Standalone code to reproduce the issue

```shell
import gc
import gpflow
import keras.backend
import numpy as np
import os
import psutil
import tensorflow as tf

# define simple GPflow model
X = np.array([[0.865], [0.666], [0.804], [0.771], [0.147], [0.866], [0.007], [0.026], [0.171], [0.889], [0.243], [0.028]])
Y = np.array([[1.57], [3.48], [3.12], [3.91], [3.07], [1.35], [3.80], [3.82], [3.49], [1.30], [4.00], [3.82]])
model = gpflow.models.GPR((X, Y), kernel=gpflow.kernels.SquaredExponential())

# repeatedly compile and evaluate the model's log marginal likelihood
for i in range(200):
    # garbage collect to remove some of the noise
    keras.backend.clear_session(); gc.collect()
    # compile and evaluate closure
    tf.function(model.log_marginal_likelihood)()
    # track memory usage
    print(f""[#{i+1}] Memory usage: {psutil.Process(os.getpid()).memory_info().rss / 1024 **2} MiB"")
```

### Relevant log output

```shell

```",1
GPU and CPU utilization dropping to 0% during long training runs,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.18.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 24.04.1 LTS

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA 2.14

### GPU model and memory

_No response_

### Current behavior?

When running the attached test script for longer, at some point during the training GPU and CPU utilization will both fall to 0%, although neither VRAM nor RAM are exhausted.
The training on my NVIDIA L40S vGPU slows from ~35ms/step with batch size 256 to minutes per step.
Training speed only recovers on reboot. Larger batch sizes will make the error occurr later during training.

I am unsure if this is an issue of my machine/the vGPU configuration.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

print(""TensorFlow version:"", tf.__version__)
print(""CPU devices:"", tf.config.list_physical_devices(""CPU""))
print(""GPU devices:"", tf.config.list_physical_devices(""GPU""))
print(""Using cuDNN:"", tf.test.is_built_with_cuda())

gpus = tf.config.list_physical_devices(""GPU"")
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)


with tf.device(""GPU:0""):
    num_samples = 18_000
    input_length = 480
    input_channels = 1

    X = tf.random.normal((num_samples, input_length, input_channels), dtype=tf.float32)
    Y = tf.random.normal((num_samples, input_length, input_channels), dtype=tf.float32)

    input = tf.keras.layers.Input(shape=(input_length, input_channels))
    conv = tf.keras.layers.Conv1D(
        filters=256,
        kernel_size=5,
        strides=1,
        padding='same',
        activation=None,
        input_shape=(input_length, input_channels)
    )(input)
    pool = tf.keras.layers.AveragePooling1D(
        pool_size=2,
        strides=2
    )(conv)
    deconv = tf.keras.layers.Conv1DTranspose(
        filters=256,
        kernel_size=4,
        strides=2,
        padding='same',
        activation=None
    )(pool)
    dense = tf.keras.layers.Dense(2048*2, activation='tanh')(deconv)
    out = tf.keras.layers.Dense(input_channels, activation='linear')(dense)

    model = tf.keras.models.Model(
        inputs=input,
        outputs=out
    )  # type: ignore
    model.summary()

    model.compile(optimizer='adam', loss='mse', jit_compile=True)

    dataset = tf.data.Dataset.from_tensor_slices((X, Y))
    dataset = dataset.batch(256).prefetch(tf.data.AUTOTUNE)

    model.fit(dataset, epochs=500, verbose=1)
```

### Relevant log output

```shell
2025-02-16 06:35:01.114446: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-02-16 06:35:01.824571: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1739687702.107514    2941 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1739687702.197108    2941 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-02-16 06:35:02.874809: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
TensorFlow version: 2.18.0
CPU devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]
GPU devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
Using cuDNN: True
I0000 00:00:1739687707.790282    2941 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5117 MB memory:  -> device: 0, name: NVIDIA L40S-8C, pci bus id: 0000:03:04.0, compute capability: 8.9
2025-02-16 06:35:07.832778: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck.
2025-02-16 06:35:07.838416: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck.
/home/mokro1/ca-nilm/venv/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
2025-02-16 06:35:07.899661: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing Cast input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck.
2025-02-16 06:35:07.903310: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck.
2025-02-16 06:35:09.228058: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck.
2025-02-16 06:35:09.228711: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck.
2025-02-16 06:35:09.230774: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing StatelessRandomGetKeyCounter input #0 was expected to be on /job:localhost/replica:0/task:0/device:CPU:0 but is actually on /job:localhost/replica:0/task:0/device:GPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck.
2025-02-16 06:35:09.249462: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck.
2025-02-16 06:35:09.260795: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing Cast input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck.
2025-02-16 06:35:09.263365: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck.
2025-02-16 06:35:09.264361: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck.
2025-02-16 06:35:09.265277: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing StatelessRandomGetKeyCounter input #0 was expected to be on /job:localhost/replica:0/task:0/device:CPU:0 but is actually on /job:localhost/replica:0/task:0/device:GPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck.
2025-02-16 06:35:09.274980: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing Cast input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck.
2025-02-16 06:35:09.276006: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck.
2025-02-16 06:35:09.276518: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck.
2025-02-16 06:35:09.276878: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing StatelessRandomGetKeyCounter input #0 was expected to be on /job:localhost/replica:0/task:0/device:CPU:0 but is actually on /job:localhost/replica:0/task:0/device:GPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck.
2025-02-16 06:35:09.285657: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing Cast input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck.
2025-02-16 06:35:09.286505: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck.
2025-02-16 06:35:09.286793: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing _EagerConst input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck.
2025-02-16 06:35:09.287169: W tensorflow/core/common_runtime/eager/execute.cc:197] before computing StatelessRandomGetKeyCounter input #0 was expected to be on /job:localhost/replica:0/task:0/device:CPU:0 but is actually on /job:localhost/replica:0/task:0/device:GPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck.
```",1
`gradient_checker.compute_gradient` can cause a crash,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly 2.19.0-dev20241025

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `Floating point exception` issue in TensorFlow when I used API `gradient_checker.compute_gradient`. I have confirmed that below code would crash on tf-nightly 2.19.0-dev20241025 (nightly-build).
Please find the [gist](https://colab.research.google.com/drive/1Ow6DQI7g-s7LpKUM1fy8OXbNcfw1ZH7r?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
import math

from absl.testing import parameterized
import numpy as np
import tensorflow as tf
from tensorflow.python.framework import constant_op
from tensorflow.python.framework import dtypes
from tensorflow.python.framework import errors_impl
from tensorflow.python.framework import tensor_shape
from tensorflow.python.framework import test_util
from tensorflow.python.ops import gen_nn_ops
from tensorflow.python.ops import gradient_checker
from tensorflow.python.ops import gradients_impl
from tensorflow.python.ops import nn_ops
import tensorflow.python.ops.nn_grad  # pylint: disable=unused-import
from tensorflow.python.platform import test
from tensorflow.python.platform import tf_logging
from tensorflow.python.util.compat import collections_abc
from tensorflow.python.eager import context
def DtypesToTest(use_gpu):
  # double datatype is currently not supported for convolution ops
  # on the ROCm platform
  optional_float64 = [] if test.is_built_with_rocm() else [dtypes.float64]
  if use_gpu:
    if not test_util.GpuSupportsHalfMatMulAndConv():
      return optional_float64 + [dtypes.float32]
    else:
      # It is important that float32 comes before float16 here,
      # as we will be using its gradients as reference for fp16 gradients.
      return optional_float64 + [dtypes.float32, dtypes.float16]
  else:
    return optional_float64 + [dtypes.float32, dtypes.float16, dtypes.bfloat16]
def _ConstructAndTestGradientForConfig(
    batch, input_shape, filter_shape, in_depth, out_depth, stride,
    padding, test_input, data_format, use_gpu):
  input_planes, input_rows, input_cols = input_shape
  filter_planes, filter_rows, filter_cols = filter_shape
  input_shape = [batch, input_planes, input_rows, input_cols, in_depth]
  filter_shape = [
      filter_planes, filter_rows, filter_cols, in_depth, out_depth
  ]
  if isinstance(stride, collections_abc.Iterable):
    strides = [1] + list(stride) + [1]
  else:
    strides = [1, stride, stride, stride, 1]
  if padding == ""VALID"":
    output_planes = int(
        math.ceil((input_planes - filter_planes + 1.0) / strides[1]))
    output_rows = int(
        math.ceil((input_rows - filter_rows + 1.0) / strides[2]))
    output_cols = int(
        math.ceil((input_cols - filter_cols + 1.0) / strides[3]))
  else:
    output_planes = int(math.ceil(float(input_planes) / strides[1]))
    output_rows = int(math.ceil(float(input_rows) / strides[2]))
    output_cols = int(math.ceil(float(input_cols) / strides[3]))
  output_shape = [batch, output_planes, output_rows, output_cols, out_depth]
  input_size = 1
  for x in input_shape:
    input_size *= x
  filter_size = 1
  for x in filter_shape:
    filter_size *= x
  input_data = [x * 1.0 / input_size for x in range(0, input_size)]
  filter_data = [x * 1.0 / filter_size for x in range(0, filter_size)]
  for data_type in DtypesToTest(use_gpu=use_gpu):
    # TODO(mjanusz): Modify gradient_checker to also provide max relative
    # error and synchronize the tolerance levels between the tests for forward
    # and backward computations.
    if data_type == dtypes.float64:
      tolerance = 1e-8
    elif data_type == dtypes.float32:
      tolerance = 5e-3
    elif data_type == dtypes.float16:
      tolerance = 5e-3 if test.is_built_with_rocm() else 1e-3
    elif data_type == dtypes.bfloat16:
      tolerance = 1e-2
    sess = tf.compat.v1.Session()
    with sess.as_default():
      orig_input_tensor = constant_op.constant(
          input_data, shape=input_shape, dtype=data_type, name=""input"")
      filter_tensor = constant_op.constant(
          filter_data, shape=filter_shape, dtype=data_type, name=""filter"")
      if data_format == ""NCDHW"":
        input_tensor = test_util.NHWCToNCHW(orig_input_tensor)
        new_strides = test_util.NHWCToNCHW(strides)
      else:
        input_tensor = orig_input_tensor
        new_strides = strides
      conv = nn_ops.conv3d(
          input_tensor,
          filter_tensor,
          new_strides,
          padding,
          data_format=data_format,
          name=""conv"")
      jacob_t, jacob_n = gradient_checker.compute_gradient(
          orig_input_tensor, input_shape, conv, output_shape)

with context.graph_mode():
  _ConstructAndTestGradientForConfig(data_format=""NDHWC"",use_gpu=False,batch=2, input_shape=(3, 7, 6), filter_shape=(3, 3, 3), in_depth=2, out_depth=0, stride=3, padding='VALID', test_input=True)
```

### Relevant log output

```shell
Fatal Python error: Floating point exception
```",2
TPU nan issue,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Linux (google colab default)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In the 2.18.0 Tensorflow version, occur only when using TPU:
- At the middle of any epoch, loss turns out to be nan for the rest of the epoch 

### Standalone code to reproduce the issue

```shell
Shortest way: connect and connect to the colab tutorial on TPU, i.e. https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/tpu.ipynb#scrollTo=Tce3stUlHN0L
If the issue hasn't been fixed, the training loop will produce nan loss
```

### Relevant log output

```shell
Epoch 1/5
300/300 ━━━━━━━━━━━━━━━━━━━━ 22s 47ms/step - loss: nan - sparse_categorical_accuracy: nan - val_loss: nan - val_sparse_categorical_accuracy: nan
Epoch 2/5
300/300 ━━━━━━━━━━━━━━━━━━━━ 9s 30ms/step - loss: nan - sparse_categorical_accuracy: nan - val_loss: nan - val_sparse_categorical_accuracy: nan
Epoch 3/5
300/300 ━━━━━━━━━━━━━━━━━━━━ 9s 30ms/step - loss: nan - sparse_categorical_accuracy: nan - val_loss: nan - val_sparse_categorical_accuracy: nan
Epoch 4/5
231/300 ━━━━━━━━━━━━━━━━━━━━ 1s 23ms/step - loss: nan - sparse_categorical_accuracy: nan
```",1
"`tf.summary_ops.write` aborts with ""Check failed: 1 == NumElements() (1 vs. 4)""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly 2.19.0-dev20250207

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `aborted issue` in TensorFlow when I used API `tf.summary_ops.write` . I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build).
Please find the [gist](https://colab.research.google.com/drive/13pz-hZNK_BaOnbYo_9CT2YMdoncPXEA3?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import summary_ops_v2 as summary_ops
from tensorflow.python.ops import variables
writer = summary_ops.create_file_writer_v2(""/tmp"")
mystep = variables.Variable(1, dtype=dtypes.int64)
with writer.as_default(step=[3, 0, 0, 2]):
    summary_ops.write('tag', 1.0)
```

### Relevant log output

```shell
2025-02-09 04:47:35.482562: F tensorflow/core/framework/tensor.cc:865] Check failed: 1 == NumElements() (1 vs. 4)Must have a one element tensor
Aborted (core dumped)
```",2
"`tf.summary_ops.run_metadata_graphs` aborts with ""Check failed: 1 == NumElements() (1 vs. 4)""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly 2.19.0-dev20250207

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `aborted issue` in TensorFlow when I used API `tf.summary_ops.run_metadata_graphs` . I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build).
Please find the [gist](https://colab.research.google.com/drive/1qCZb2sPj2l79IecGA1tz9CywtNlevAjf?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
from tensorflow.python.ops import summary_ops_v2 as summary_ops
from tensorflow.core.protobuf import config_pb2
writer = summary_ops.create_file_writer_v2(""/tmp"")
meta = config_pb2.RunMetadata()
with writer.as_default([3, 0, 0, 2]):
    summary_ops.run_metadata_graphs(name='my_name', data=meta)
```

### Relevant log output

```shell
2025-02-09 04:39:36.666278: F tensorflow/core/framework/tensor.cc:865] Check failed: 1 == NumElements() (1 vs. 4)Must have a one element tensor
Aborted (core dumped)
```",2
"`io_ops.restore_v2` aborts with ""Check failed: size >= 0 (0 vs. -3) ""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.19.0-dev20250207

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `aborted issue` in TensorFlow when I used API `tf.io_ops.restore_v2` . I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20250207` (nightly-build).
Please find the [gist](https://colab.research.google.com/drive/17XwNF4WI3HqVJkLwk2VQjcTDAV42-6Bl?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
from tensorflow.python.framework import dtypes
from tensorflow.python.framework import ops
from tensorflow.python.ops import io_ops

dtype = dtypes.uint4
with ops.Graph().as_default():
    op = io_ops.restore_v2('model', ['var1', 'var2'], ['', '-3 4 0,1:-'], [dtype, dtype])
```

### Relevant log output

```shell
2025-02-09 04:25:19.857968: F tensorflow/core/framework/tensor_shape.cc:413] Check failed: size >= 0 (0 vs. -3) 
Aborted (core dumped)
```",2
Bug sur TensorFlow 2.13 multi-GPU : freeze lors du fitting,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Macos 15.3 (worker 0) Macos 12.7.6(worker 1)

### Mobile device

_No response_

### Python version

3.8.20

### Bazel version

...

### GCC/compiler version

16.0.0 (apple M3) 14.0.0 (intel iris)

### CUDA/cuDNN version

_No response_

### GPU model and memory

Apple M3 and Intel Iris Graphics 6100

### Current behavior?

When I train a TensorFlow model on several GPUs (with tf.distribute.MultiWorkerMirroredStrategy), the execution is blocked at  #Build the model under the strategy

No error message is displayed, but the process no longer progresses after

•	 I followed the recommendations of the official documentation, but the problem persists.

### Standalone code to reproduce the issue

```shell
import json
import os
import numpy as np
import tensorflow as tf

TF_CONFIG = {
    ""cluster"": {
        ""worker"": [""192.168.0.68:12345"", ""192.168.0.68:12346""]
    },
    ""task"": {""type"": ""worker"", ""index"": 0}  # Modifier index pour chaque worker
}

os.environ[""TF_CONFIG""] = json.dumps(TF_CONFIG)

# Manually Load the MNIST dataset
data = np.load(""mnist.npz"")
x_train, y_train = data[""x_train""], data[""y_train""]
x_test, y_test = data[""x_test""], data[""y_test""]

# Normalize images
x_train, x_test = x_train / 255.0, x_test / 255.0

# Add a dimension to match TensorFlow's expectations
x_train = x_train[..., np.newaxis]
x_test = x_test[..., np.newaxis]

# Define the distribution strategy
strategy = tf.distribute.MultiWorkerMirroredStrategy()

# Build the model under the strategy
with strategy.scope():
    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPooling2D((2, 2)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])

    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))

test_loss, test_acc = model.evaluate(x_test, y_test)
print(f""Test accuracy: {test_acc:.4f}"")
```

### Relevant log output

```shell
2025-02-08 12:29:20.630247: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3
2025-02-08 12:29:20.630286: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB
2025-02-08 12:29:20.630293: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB
2025-02-08 12:29:20.630324: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2025-02-08 12:29:20.630338: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
2025-02-08 12:29:20.631249: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2025-02-08 12:29:20.631259: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
2025-02-08 12:29:20.632347: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:449] Started server with target: grpc://192.168.0.68:12345
2025-02-08 12:29:20.637550: I tensorflow/tsl/distributed_runtime/coordination/coordination_service.cc:535] /job:worker/replica:0/task:0 has connected to coordination service. Incarnation: 14287335759644278642
2025-02-08 12:29:20.637654: I tensorflow/tsl/distributed_runtime/coordination/coordination_service_agent.cc:298] Coordination agent has successfully connected.
2025-02-08 12:29:37.728182: I tensorflow/tsl/distributed_runtime/coordination/coordination_service.cc:535] /job:worker/replica:0/task:1 has connected to coordination service. Incarnation: 15227140312468372989
```",1
unexpected import during stub creation from mypy-protobuf,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

master

### Custom code

No

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.12.8

### Bazel version

6.5.0

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I'm doing a stub distribution for tensorflow, and when I used mypy-protobuf to create stubs for .proto files, it resulted in an import that I didn't expect


The format it should create is:
```python
import tensorflow.tsl.protobuf.histogram_pb2
```
but it does
```python
import xla.tsl.protobuf.histogram_pb2
```

I don't really understand how they structure the configurations to compile from Bazel

Is there a setting I'm missing, or should I make the change manually?

Is there a way for tensorflow to automatically create the stubs files from the compiled proto files?

Here I leave you the code to speed up your analysis.


third_party\xla\xla\tsl\protobuf\histogram.proto
```proto
syntax = ""proto3"";

package tensorflow;

option cc_enable_arenas = true;
option java_multiple_files = true;
option java_package = ""org.tensorflow.framework"";
option go_package = ""github.com/google/tsl/tsl/go/core/protobuf/summary_go_proto"";

// Serialization format for histogram module in
// tsl/lib/histogram/histogram.h
message HistogramProto {
  double min = 1;
  double max = 2;
  double num = 3;
  double sum = 4;
  double sum_squares = 5;

  // Parallel arrays encoding the bucket boundaries and the bucket values.
  // bucket(i) is the count for the bucket i.  The range for
  // a bucket is:
  //   i == 0:  -DBL_MAX .. bucket_limit(0)
  //   i != 0:  bucket_limit(i-1) .. bucket_limit(i)
  repeated double bucket_limit = 6 [packed = true];
  repeated double bucket = 7 [packed = true];
}
```

tensorflow\core\framework\summary.proto
```proto
syntax = ""proto3"";

package tensorflow;

import public ""xla/tsl/protobuf/histogram.proto"";

import ""tensorflow/core/framework/tensor.proto"";

option cc_enable_arenas = true;
option java_outer_classname = ""SummaryProtos"";
option java_multiple_files = true;
option java_package = ""org.tensorflow.framework"";
option go_package = ""github.com/tensorflow/tensorflow/tensorflow/go/core/framework/summary_go_proto"";

// Metadata associated with a series of Summary data
message SummaryDescription {
  // Hint on how plugins should process the data in this series.
  // Supported values include ""scalar"", ""histogram"", ""image"", ""audio""
  string type_hint = 1;
}

// A SummaryMetadata encapsulates information on which plugins are able to make
// use of a certain summary value.
message SummaryMetadata {
  message PluginData {
    // The name of the plugin this data pertains to.
    string plugin_name = 1;

    // The content to store for the plugin. The best practice is for this to be
    // a binary serialized protocol buffer.
    bytes content = 2;
  }

  // Data that associates a summary with a certain plugin.
  PluginData plugin_data = 1;

  // Display name for viewing in TensorBoard.
  string display_name = 2;

  // Longform readable description of the summary sequence. Markdown supported.
  string summary_description = 3;

  // Class of data stored in this time series. Required for compatibility with
  // TensorBoard's generic data facilities (`DataProvider`, et al.). This value
  // imposes constraints on the dtype and shape of the corresponding tensor
  // values. See `DataClass` docs for details.
  DataClass data_class = 4;
}

enum DataClass {
  // Unknown data class, used (implicitly) for legacy data. Will not be
  // processed by data ingestion pipelines.
  DATA_CLASS_UNKNOWN = 0;
  // Scalar time series. Each `Value` for the corresponding tag must have
  // `tensor` set to a rank-0 tensor of type `DT_FLOAT` (float32).
  DATA_CLASS_SCALAR = 1;
  // Tensor time series. Each `Value` for the corresponding tag must have
  // `tensor` set. The tensor value is arbitrary, but should be small to
  // accommodate direct storage in database backends: an upper bound of a few
  // kilobytes is a reasonable rule of thumb.
  DATA_CLASS_TENSOR = 2;
  // Blob sequence time series. Each `Value` for the corresponding tag must
  // have `tensor` set to a rank-1 tensor of bytestring dtype.
  DATA_CLASS_BLOB_SEQUENCE = 3;
}

// A Summary is a set of named values to be displayed by the
// visualizer.
//
// Summaries are produced regularly during training, as controlled by
// the ""summary_interval_secs"" attribute of the training operation.
// Summaries are also produced at the end of an evaluation.
message Summary {
  message Image {
    // Dimensions of the image.
    int32 height = 1;
    int32 width = 2;
    // Valid colorspace values are
    //   1 - grayscale
    //   2 - grayscale + alpha
    //   3 - RGB
    //   4 - RGBA
    //   5 - DIGITAL_YUV
    //   6 - BGRA
    int32 colorspace = 3;
    // Image data in encoded format.  All image formats supported by
    // image_codec::CoderUtil can be stored here.
    bytes encoded_image_string = 4;
  }

  message Audio {
    // Sample rate of the audio in Hz.
    float sample_rate = 1;
    // Number of channels of audio.
    int64 num_channels = 2;
    // Length of the audio in frames (samples per channel).
    int64 length_frames = 3;
    // Encoded audio data and its associated RFC 2045 content type (e.g.
    // ""audio/wav"").
    bytes encoded_audio_string = 4;
    string content_type = 5;
  }

  message Value {
    // This field is deprecated and will not be set.
    string node_name = 7;

    // Tag name for the data. Used by TensorBoard plugins to organize data. Tags
    // are often organized by scope (which contains slashes to convey
    // hierarchy). For example: foo/bar/0
    string tag = 1;

    // Contains metadata on the summary value such as which plugins may use it.
    // Take note that many summary values may lack a metadata field. This is
    // because the FileWriter only keeps a metadata object on the first summary
    // value with a certain tag for each tag. TensorBoard then remembers which
    // tags are associated with which plugins. This saves space.
    SummaryMetadata metadata = 9;

    // Value associated with the tag.
    oneof value {
      float simple_value = 2;
      bytes obsolete_old_style_histogram = 3;
      Image image = 4;
      HistogramProto histo = 5;
      Audio audio = 6;
      TensorProto tensor = 8;
    }
  }

  // Set of values for the summary.
  repeated Value value = 1;
}
```



### Standalone code to reproduce the issue

```shell
Commands to recreate

>>> git clone https://github.com/tensorflow/tensorflow.git
>>> cd tensorflow
>>> mkdir out
>>> protoc tensorflow/core/framework/summary.proto --mypy_out=out/ -I=""."" -I=""third_party/xla/""
```

### Relevant log output

```shell

```",1
Issue created for Rollback of PR #82210: Align num_threads parameter validation with documentation,"Merged PR #82210 is rolled back in d9039b966f5b2468b37a2e7d04208fe3713d2e0d.
    Please follow up with the reviewer and close this issue once its resolved.",0
inconsistent result of ```tf.raw_ops.Rsqrt``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 (google colab)

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

result of ``tf.raw_ops.Rsqrt``` is inconsistent between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

x_0 = tf.constant([[[1.2031, 0.2168], [0.1177, 0.6758]], 
                    [[1.8203, 0.9688], [0.8828, 0.0767]]], dtype=tf.bfloat16)

with tf.device('CPU:0'):
  result_cpu = tf.raw_ops.Rsqrt(x=x_0)
  print(""Output on CPU:"", result_cpu)

with tf.device('GPU:0'):
  result_gpu = tf.raw_ops.Rsqrt(x=x_0)
  print(""Output on GPU:"", result_gpu)

max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:"", is_consistent.numpy())
```

### Relevant log output

```shell
Output on CPU: tf.Tensor(
[[[0.910156 2.14062]
  [2.92188 1.21875]]

 [[0.742188 1.01562]
  [1.0625 3.60938]]], shape=(2, 2, 2), dtype=bfloat16)

Output on GPU: tf.Tensor(
[[[0.914062 2.15625]
  [2.90625 1.21875]]

 [[0.738281 1.01562]
  [1.0625 3.60938]]], shape=(2, 2, 2), dtype=bfloat16)

Max absolute difference: 0.015625

Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False
```",1
inconsistent result of ```tf.raw_ops.Tan``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

```tf.raw_ops.Tan``` produces inconsistent results between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

real = tf.constant([1.5634333], dtype=tf.float32)
imag = tf.constant([0.020735], dtype=tf.float32)

complex_tensor = tf.complex(real, imag)

with tf.device('/CPU:0'):
    result_cpu = tf.raw_ops.Tan(x=complex_tensor)
    print(result_cpu)

with tf.device('/GPU:0'):
    result_gpu = tf.raw_ops.Tan(x=complex_tensor)
    print(result_gpu)

##Comparing whole complex numbers
max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_cons = tf.experimental.numpy.allclose(result_cpu, result_gpu, rtol=1e-6,  atol=1e-5)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6:"", is_cons.numpy())

##Comparing by parts
real_part_cpu = tf.math.real(result_cpu)
real_part_gpu = tf.math.real(result_gpu)
real_part_diff = tf.reduce_max(tf.abs(real_part_cpu - real_part_gpu)).numpy()
real_part_cons = tf.experimental.numpy.allclose(real_part_cpu, real_part_gpu, rtol=1e-6,  atol=1e-5)

imag_part_cpu = tf.math.imag(result_cpu)
imag_part_gpu = tf.math.imag(result_gpu)
imag_part_diff = tf.reduce_max(tf.abs(imag_part_cpu - imag_part_gpu)).numpy()
imag_part_cons = tf.experimental.numpy.allclose(imag_part_cpu, imag_part_gpu, rtol=1e-6,  atol=1e-5)

print(""Real parts absolute difference:"", real_part_diff)
print(""Real parts Consistency check with atol=1e-5 and rtol=1e-6:"", real_part_cons.numpy())

print(""Imag parts absolute difference:"", imag_part_diff)
print(""Imag parts Consistency check with atol=1e-5 and rtol=1e-6:"", imag_part_cons.numpy())
```

### Relevant log output

```shell
tf.Tensor([15.205576+42.834145j], shape=(1,), dtype=complex64)
tf.Tensor([15.205605+42.834225j], shape=(1,), dtype=complex64)

Max absolute difference: 8.5064334e-05
Consistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6: False

Real parts absolute difference: 2.861023e-05
Real parts Consistency check with atol=1e-5 and rtol=1e-6: False

Imag parts absolute difference: 8.010864e-05
Imag parts Consistency check with atol=1e-5 and rtol=1e-6: False
```",1
inconsistent result of ```tf.raw_ops.LogSoftmax``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 (google colab)

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

result of ``tf.raw_ops.LogSoftmax``` is inconsistent between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

logits = tf.constant([[0.0664, -2.3906]], dtype=tf.bfloat16)

with tf.device('CPU:0'):
  result_cpu = tf.raw_ops.LogSoftmax(logits=logits)
  print(""Output on CPU:"", result_cpu)

with tf.device('GPU:0'):
  result_gpu = tf.raw_ops.LogSoftmax(logits=logits)
  print(""Output on GPU:"", result_gpu)

max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:"", is_consistent.numpy())
```

### Relevant log output

```shell
Output on CPU: tf.Tensor([[-0.0825195 -2.53125]], shape=(1, 2), dtype=bfloat16)

Output on GPU: tf.Tensor([[-0.0825195 -2.54688]], shape=(1, 2), dtype=bfloat16)

Max absolute difference: 0.015625

Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False
```",1
inconsistent result of ```tf.raw_ops.L2Loss``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 (google colab)

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

result of ```tw.raw_ops.L2Loss``` is inconsistent between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

t = tf.constant([
    [[0.9922, -1.4922], 
     [0.0376,  0.1504], 
     [0.6172,  1.2266]],

    [[-0.1387,  1.3047], 
     [0.3535, -0.0471], 
     [0.0437,  0.2637]]
], dtype=tf.bfloat16)

with tf.device('CPU:0'):
  result_cpu = tf.raw_ops.L2Loss(t=t)
  print(""Output on CPU:"", result_cpu)

with tf.device('GPU:0'):
  result_gpu = tf.raw_ops.L2Loss(t=t)
  print(""Output on GPU:"", result_gpu)

max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:"", is_consistent.numpy())
```

### Relevant log output

```shell
Output on CPU: tf.Tensor(3.51562, shape=(), dtype=bfloat16)

Output on GPU: tf.Tensor(3.53125, shape=(), dtype=bfloat16)

Max absolute difference: 0.015625

Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False
```",1
TF wheel shouldn't be built with CUDA dependencies,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18

### Custom code

No

### OS platform and distribution

CentOS Stream 9

### Mobile device

_No response_

### Python version

3.9

### Bazel version

6.5.0

### GCC/compiler version

11.5.0

### CUDA/cuDNN version

12.6.1/8.9.7.29

### GPU model and memory

2080 Ti 11GB

### Current behavior?

Build fails with following error:

Error in fail: TF wheel shouldn't be built with CUDA dependencies. Please provide `--config=cuda_wheel` for bazel build command. If you absolutely need to add CUDA dependencies, provide `--@local_config_cuda//cuda:override_include_cuda_libs=true`.

Current driver:

`
NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8
`

Build command:

`
bazel build --config=cuda --local_cpu_resources=HOST_CPUS*.8 //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow
`

What I missed?

### Standalone code to reproduce the issue

```shell
You have bazel 6.5.0 installed.
Please specify the location of python. [Default is /usr/bin/python3]:


Found possible Python library paths:
  /usr/lib/python3.9/site-packages
  /usr/lib64/python3.9/site-packages
  /usr/local/lib/python3.9/site-packages
  /usr/local/lib64/python3.9/site-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3.9/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the hermetic CUDA version you want to use or leave empty to use the default version. 12.6.1


Please specify the hermetic cuDNN version you want to use or leave empty to use the default version.


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 7.5


Please specify the local CUDA path you want to use or leave empty to use the default version.


Please specify the local CUDNN path you want to use or leave empty to use the default version.


Please specify the local NCCL path you want to use or leave empty to use the default version.


Do you want to use clang as CUDA compiler? [Y/n]: n
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/lib64/ccache/gcc]:


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]:


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]:
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).
        --config=monolithic     # Config for mostly static monolithic build.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=nogcp          # Disable GCP support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished

```

### Relevant log output

```shell
ERROR: /usr/src/tensorflow/tensorflow/tools/pip_package/BUILD:278:9: in tf_wheel rule //tensorflow/tools/pip_package:wheel:
Traceback (most recent call last):
        File ""/usr/src/tensorflow/tensorflow/tools/pip_package/utils/tf_wheel.bzl"", line 72, column 13, in _tf_wheel_impl
                fail(""TF wheel shouldn't be built with CUDA dependencies."" +
Error in fail: TF wheel shouldn't be built with CUDA dependencies. Please provide `--config=cuda_wheel` for bazel build command. If you absolutely need to add CUDA dependencies, provide `--@local_config_cuda//cuda:override_include_cuda_libs=true`.
ERROR: /usr/src/tensorflow/tensorflow/tools/pip_package/BUILD:278:9: Analysis of target '//tensorflow/tools/pip_package:wheel' failed
ERROR: Analysis of target '//tensorflow/tools/pip_package:wheel' failed; build aborted:
INFO: Elapsed time: 189.679s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (790 packages loaded, 56723 targets configured)
```",1
inconsistent result of ```tf.raw_ops.BiasAddGrad``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

```tf.raw_ops.BiasAddGrad``` produces inconsistent results between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

out_backprop = tf.constant([
    [
        [
            [[ 0.2207,  2.1094], [-0.3730, -1.0625], [ 1.7031,  0.7148]], 
            [[ 1.5078, -0.6719], [-0.6367,  0.5039], [-2.3281,  0.5078]]
        ],
        [
            [[-0.3574,  0.0461], [ 2.3750, -2.9688], [-0.5703, -2.0156]],
            [[ 0.8125,  1.7656], [-0.9570,  0.6250], [-0.6914, -0.4746]]
        ],
        [
            [[-0.3750, -0.7383], [ 0.3691,  0.4570], [ 1.1641,  0.2715]],
            [[-1.2969, -0.9844], [-0.4863,  1.0938], [-1.4297,  0.8086]]
        ]
    ],
    [
        [
            [[ 0.3730,  0.8477], [-0.3887,  1.2266], [ 0.0859, -0.5742]],
            [[-0.7383, -0.2432], [-0.7578, -0.8281], [-0.1660, -0.9336]]
        ],
        [
            [[ 1.4297,  0.6797], [-1.6172,  0.4941], [-0.3047, -0.3711]],
            [[-0.6250, -0.7617], [ 0.9453,  0.1064], [ 1.4062, -2.9531]]
        ],
        [
            [[-1.4297, -0.1387], [ 0.0625,  1.0469], [-0.1953,  1.6406]],
            [[-0.3047,  0.5117], [ 1.8125,  1.1797], [-0.8789, -0.4688]]
        ]
    ]
], dtype=tf.bfloat16)

with tf.device('CPU:0'):
  result_cpu = tf.raw_ops.BiasAddGrad(out_backprop=out_backprop, data_format=""NCHW"")
  print(""BiasAddGrad Output on CPU:"", result_cpu)

with tf.device('GPU:0'):
  result_gpu = tf.raw_ops.BiasAddGrad(out_backprop=out_backprop, data_format=""NCHW"")
  print(""BiasAddGrad Output on GPU:"", result_gpu)

max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:"", is_consistent.numpy())
```

### Relevant log output

```shell
BiasAddGrad Output on CPU: tf.Tensor([0.09375 -3.96875 1.70312], shape=(3,), dtype=bfloat16)

BiasAddGrad Output on GPU: tf.Tensor([0.078125 -4 1.70312], shape=(3,), dtype=bfloat16)

Max absolute difference: 0.03125
Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False
```",1
inconsistent result of ```tf.raw_ops.BatchMatMulV2``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

getting inconsistent results of ```tf.raw_ops.BatchMatMulV2``` between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

x_0 = tf.constant([
    [[[[ -1.3594, -0.3027], [-1.4141,  0.2969]],
      [[ -0.9141,  1.7812], [ 1.2266,  0.8594]]],

     [[[  0.8359, -0.9414], [-1.7969, -0.7461]],
      [[  0.3164,  0.3691], [ 0.7656,  0.2354]]]],

    [[[[ -0.5898,  1.3516], [ 0.4902, -0.1045]],
      [[ -0.1099,  1.5078], [ 0.2852, -0.0957]]],

     [[[-0.9883,  1.3203], [-0.2715, -1.7578]],
      [[ -0.1602, -0.4336], [-0.6875, -0.4492]]]]
], dtype=tf.bfloat16)

y = tf.constant([
    [[[  0.6836, -0.6562], [-0.5508, -0.8438]], 
     [[  1.6094, -0.9883], [-0.1318,  1.1094]]],

    [[[  0.4062, -1.1094], [-0.7188, -1.7578]], 
     [[ -1.0391, -0.6602], [ 0.8359, -0.6562]]]
], dtype=tf.bfloat16) 

with tf.device('CPU:0'):
    result_cpu = tf.raw_ops.BatchMatMulV2(
        x=x_0, 
        y=y,
    )
    print(result_cpu)

with tf.device('GPU:0'):
    result_gpu = tf.raw_ops.BatchMatMulV2(
        x=x_0, 
        y=y,
    )
    print(result_gpu)


max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:"", is_consistent.numpy())
```

### Relevant log output

```shell
tf.Tensor(
[[[[[-0.761719 1.14062]
    [-1.125 0.675781]]

   [[-1.70312 2.875]
    [1.85938 -0.257812]]]


  [[[1.01562 0.726562]
    [-0.193359 3.29688]]

   [[-0.0201416 -0.449219]
    [-0.597656 -0.65625]]]]



 [[[[-1.14062 -0.75]
    [0.392578 -0.233398]]

   [[-0.375 1.78125]
    [0.470703 -0.386719]]]


  [[[-1.34375 -1.21875]
    [1.14844 3.39062]]

   [[-0.195312 0.388672]
    [0.337891 0.746094]]]]], shape=(2, 2, 2, 2, 2), dtype=bfloat16)

tf.Tensor(
[[[[[-0.761719 1.14844]
    [-1.13281 0.675781]]

   [[-1.70312 2.875]
    [1.85938 -0.259766]]]


  [[[1.01562 0.726562]
    [-0.193359 3.3125]]

   [[-0.0201416 -0.451172]
    [-0.597656 -0.660156]]]]



 [[[[-1.14844 -0.753906]
    [0.392578 -0.233398]]

   [[-0.375 1.78125]
    [0.470703 -0.388672]]]


  [[[-1.35156 -1.22656]
    [1.15625 3.39062]]

   [[-0.196289 0.390625]
    [0.337891 0.75]]]]], shape=(2, 2, 2, 2, 2), dtype=bfloat16)


Max absolute difference: 0.015625

Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False
```",1
inconsistent result of ```tf.image.adjust_hue``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

getting inconsistent results of ```tf.image.adjust_hue``` between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

images = tf.constant([
    [[ 1.9720840,  2.1302242, -0.1902120],
     [ 0.6557856, -1.3016001,  1.1452782]],
    
    [[-2.2193234,  0.3198028,  0.9568117],
     [-0.3937407, -0.0503466, -0.3693791]]
], dtype=tf.float32)

delta = tf.constant(-0.7441734, dtype=tf.float32)

with tf.device('CPU:0'):
    adjusted_cpu = tf.image.adjust_hue(images, delta)
    print(""Adjusted Hue on CPU:\n"", adjusted_cpu)

with tf.device('GPU:0'):
    adjusted_gpu = tf.image.adjust_hue(images, delta)
    print(""Adjusted Hue on GPU:\n"", adjusted_gpu)


is_consistent = tf.experimental.numpy.allclose(adjusted_cpu, adjusted_gpu, atol=1e-5, rtol=1e-6)

max_abs_diff = tf.reduce_max(tf.abs(adjusted_cpu - adjusted_gpu)).numpy()

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6:"", is_consistent.numpy())
```

### Relevant log output

```shell
Adjusted Hue on CPU:
 tf.Tensor(
[[[-0.190212    2.1302242   1.2092681 ]
  [ 1.1452782  -0.48211157 -1.3016001 ]]

 [[ 0.11679006 -2.2193234   0.9568117 ]
  [-0.3937407  -0.25841027 -0.0503466 ]]], shape=(2, 2, 3), dtype=float32)

Adjusted Hue on GPU:
 tf.Tensor(
[[[-0.19021209  2.1302242   1.209268  ]
  [ 1.1452781  -0.48211193 -1.3016001 ]]

 [[ 0.11678863 -2.2193234   0.95681167]
  [-0.0503466  -0.0503466  -0.0503466 ]]], shape=(2, 2, 3), dtype=float32)

Max absolute difference: 0.3433941
Consistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6: False
```",1
"TF 2.18 with GPU does not detect GPU, Cannot dlopen some GPU libraries, in a container","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

Linux Centos 7.9, RHEL 8, RHEL 9

### Mobile device

_No response_

### Python version

3.11.0rc1

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

550.90.07

### GPU model and memory

_No response_

### Current behavior?

After discussing this on the [Apptainer Git](https://github.com/apptainer/apptainer/issues/2706#issuecomment-2613253071) we determined the latest TF-GPU running 2.18.0 does not register any GPUs. Older versions like 2.7.1-gpu work just fine.

`apptainer run --nv  /apps/Miniforge/lib/python3.12/site-packages/containers/tensorflow/tensorflow/latest-gpu/tensorflow-tensorflow-latest-gpu-sha256\:1f16fbd9be8bb84891de12533e332bbd500511caeb5cf4db501dbe39d422f9c7.sif python`


```
import tensorflow as tf
2025-01-24 15:03:27.629215: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1737749008.639844   35316 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1737749008.847756   35316 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-24 15:03:31.499335: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.

```
```
print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
W0000 00:00:1737749068.599039   35316 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
Num GPUs Available:  0

```
```
>>> print(tf.__version__)
2.18.0
```


### Standalone code to reproduce the issue

```shell
shpc install tensorflow/tensorflow:latest-gpu

or

apptainer pull docker://tensorflow/tensorflow:latest-gpu

apptainer run --nv  /apps/Miniforge/lib/python3.12/site-packages/containers/tensorflow/tensorflow/latest-gpu/tensorflow-tensorflow-latest-gpu-sha256\:1f16fbd9be8bb84891de12533e332bbd500511caeb5cf4db501dbe39d422f9c7.sif python

python
import tensorflow as tf
print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
```

### Relevant log output

```shell
apptainer --debug exec --nv  /apps/Miniforge/lib/python3.12/site-packages/containers/tensorflow/tensorflow/latest-gpu/tensorflow-tensorflow-latest-gpu-sha256:1f16fbd9be8bb84891de12533e332bbd500511caeb5cf4db501dbe39d422f9c7.sif python
DEBUG   [U=0,P=1355208]    persistentPreRun()            Apptainer version: 1.3.6-1
DEBUG   [U=0,P=1355208]    persistentPreRun()            Parsing configuration file /etc/apptainer/apptainer.conf
DEBUG   [U=0,P=1355208]    SetBinaryPath()               Setting binary path to /usr/libexec/apptainer/bin:/usr/share/Modules/bin:/usr/local/sbin:/sbin:/bin:/usr/sbin:/usr/bin:/opt/TurboVNC/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
DEBUG   [U=0,P=1355208]    SetBinaryPath()               Using that path for all binaries
DEBUG   [U=0,P=1355208]    handleConfDir()               /root/.apptainer already exists. Not creating.
DEBUG   [U=0,P=1355208]    handleRemoteConf()            Ensuring file permission of 0600 on /root/.apptainer/remote.yaml
DEBUG   [U=0,P=1355208]    setUmask()                    Saving umask 0002 for propagation into container
DEBUG   [U=0,P=1355208]    checkEncryptionKey()          Checking for encrypted system partition
DEBUG   [U=0,P=1355208]    Init()                        Image format detection
DEBUG   [U=0,P=1355208]    Init()                        Check for sandbox image format
DEBUG   [U=0,P=1355208]    Init()                        sandbox format initializer returned: not a directory image
DEBUG   [U=0,P=1355208]    Init()                        Check for sif image format
DEBUG   [U=0,P=1355208]    Init()                        sif image format detected
VERBOSE [U=0,P=1355208]    SetGPUConfig()                'always use nv = yes' found in apptainer.conf
DEBUG   [U=0,P=1355208]    setNVLegacyConfig()           Using legacy binds for nv GPU setup
VERBOSE [U=0,P=1355208]    NvidiaIpcsPath()              persistenced socket /var/run/nvidia-persistenced/socket not found
DEBUG   [U=0,P=1355208]    findOnPath()                  Found ""ldconfig"" at ""/sbin/ldconfig""
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SHELL environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SUDO_GID environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding HISTCONTROL environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding no_proxy environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding HOSTNAME environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding HISTSIZE environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SBATCH_PARTITION environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding GUESTFISH_OUTPUT environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SLURM_PARTITION environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SUDO_COMMAND environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SUDO_USER environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LMOD_DIR environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding PWD environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LOGNAME environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding MODULESHOME environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding MANPATH environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding GUESTFISH_RESTORE environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding __MODULES_SHARE_MANPATH environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SSH_ASKPASS environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LANG environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LS_COLORS environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LMOD_SETTARG_FULL_SUPPORT environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding GUESTFISH_PS1 environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding https_proxy environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LMOD_VERSION environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding MODULEPATH_ROOT environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LMOD_PKG environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding TERM environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LESSOPEN environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding USER environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding NO_PROXY environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding MODULES_RUN_QUARANTINE environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LOADEDMODULES environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SHLVL environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding BASH_ENV environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LMOD_sys environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding HTTPS_PROXY environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding GUESTFISH_INIT environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding HTTP_PROXY environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding http_proxy environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding S_COLORS environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding __MODULES_LMINIT environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding which_declare environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding XDG_DATA_DIRS environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding MODULEPATH environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding SUDO_UID environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding LMOD_CMD environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding MAIL environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding MODULES_CMD environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding BASH_FUNC_ml%% environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding BASH_FUNC_which%% environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding BASH_FUNC_module%% environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding BASH_FUNC_scl%% environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding BASH_FUNC__module_raw%% environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding _ environment variable
VERBOSE [U=0,P=1355208]    SetContainerEnv()             Not forwarding APPTAINER_DEBUG environment variable
DEBUG   [U=0,P=1355208]    SetContainerEnv()             Forwarding USER_PATH environment variable
VERBOSE [U=0,P=1355208]    SetContainerEnv()             Setting HOME=/root
VERBOSE [U=0,P=1355208]    SetContainerEnv()             Setting PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
DEBUG   [U=0,P=1355208]    InitImageDrivers()            Skipping installing fuseapps image driver because running as root
DEBUG   [U=0,P=1355208]    SetuidMountAllowed()          Kernel squashfs mount allowed because running as root
DEBUG   [U=0,P=1355208]    init()                        Use starter binary /usr/libexec/apptainer/bin/starter
VERBOSE [U=0,P=1355208]    print()                       Set messagelevel to: 5
VERBOSE [U=0,P=1355208]    init()                        Starter initialization
VERBOSE [U=0,P=1355208]    is_suid()                     Check if we are running as setuid: 0
DEBUG   [U=0,P=1355208]    read_engine_config()          Read engine configuration
DEBUG   [U=0,P=1355208]    init()                        Wait completion of stage1
DEBUG   [U=0,P=1355224]    set_parent_death_signal()     Set parent death signal to 9
VERBOSE [U=0,P=1355224]    init()                        Spawn stage 1
DEBUG   [U=0,P=1355224]    func1()                       executablePath is /usr/libexec/apptainer/bin/starter
DEBUG   [U=0,P=1355224]    func1()                       starter was not relocated from /usr/libexec
DEBUG   [U=0,P=1355224]    func1()                       Install prefix is /usr
DEBUG   [U=0,P=1355224]    startup()                     apptainer runtime engine selected
VERBOSE [U=0,P=1355224]    startup()                     Execute stage 1
DEBUG   [U=0,P=1355224]    StageOne()                    Entering stage 1
DEBUG   [U=0,P=1355224]    InitImageDrivers()            Skipping installing fuseapps image driver because running as root
DEBUG   [U=0,P=1355224]    prepareRootCaps()             Root full capabilities
DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/proc/sys/fs/binfmt_misc"" as autofs mount point
DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/home"" as autofs mount point
DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/share"" as autofs mount point
DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/misc"" as autofs mount point
DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/net"" as autofs mount point
DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/mnt/smb/locker"" as autofs mount point
DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/mnt/smb/labshare"" as autofs mount point
DEBUG   [U=0,P=1355224]    prepareAutofs()               Found ""/mnt/smb/staging"" as autofs mount point
DEBUG   [U=0,P=1355224]    prepareAutofs()               Could not keep file descriptor for bind path /etc/localtime: no mount point
DEBUG   [U=0,P=1355224]    prepareAutofs()               Could not keep file descriptor for bind path /etc/hosts: no mount point
DEBUG   [U=0,P=1355224]    prepareAutofs()               Could not keep file descriptor for home directory /root: no mount point
DEBUG   [U=0,P=1355224]    prepareAutofs()               Could not keep file descriptor for current working directory /root: no mount point
DEBUG   [U=0,P=1355224]    Init()                        Image format detection
DEBUG   [U=0,P=1355224]    Init()                        Check for sandbox image format
DEBUG   [U=0,P=1355224]    Init()                        sandbox format initializer returned: not a directory image
DEBUG   [U=0,P=1355224]    Init()                        Check for sif image format
DEBUG   [U=0,P=1355224]    Init()                        sif image format detected
DEBUG   [U=0,P=1355224]    setSessionLayer()             Using overlay because it is not disabled
DEBUG   [U=0,P=1355224]    PrepareConfig()               image driver is 
VERBOSE [U=0,P=1355208]    wait_child()                  stage 1 exited with status 0
DEBUG   [U=0,P=1355208]    cleanup_fd()                  Close file descriptor 4
DEBUG   [U=0,P=1355208]    cleanup_fd()                  Close file descriptor 5
DEBUG   [U=0,P=1355208]    cleanup_fd()                  Close file descriptor 6
DEBUG   [U=0,P=1355208]    init()                        Set child signal mask
DEBUG   [U=0,P=1355208]    init()                        Create socketpair for master communication channel
DEBUG   [U=0,P=1355208]    init()                        Create RPC socketpair for communication between stage 2 and RPC server
VERBOSE [U=0,P=1355208]    init()                        Spawn master process
DEBUG   [U=0,P=1355230]    set_parent_death_signal()     Set parent death signal to 9
VERBOSE [U=0,P=1355230]    create_namespace()            Create mount namespace
VERBOSE [U=0,P=1355208]    enter_namespace()             Entering in mount namespace
DEBUG   [U=0,P=1355208]    enter_namespace()             Opening namespace file ns/mnt
VERBOSE [U=0,P=1355230]    create_namespace()            Create mount namespace
VERBOSE [U=0,P=1355231]    init()                        Spawn RPC server
DEBUG   [U=0,P=1355208]    func1()                       executablePath is /usr/libexec/apptainer/bin/starter
DEBUG   [U=0,P=1355208]    func1()                       starter was not relocated from /usr/libexec
DEBUG   [U=0,P=1355208]    func1()                       Install prefix is /usr
DEBUG   [U=0,P=1355231]    func1()                       executablePath is /usr/libexec/apptainer/bin/starter
DEBUG   [U=0,P=1355231]    func1()                       starter was not relocated from /usr/libexec
DEBUG   [U=0,P=1355231]    func1()                       Install prefix is /usr
DEBUG   [U=0,P=1355208]    startup()                     apptainer runtime engine selected
VERBOSE [U=0,P=1355208]    startup()                     Execute master process
DEBUG   [U=0,P=1355231]    startup()                     apptainer runtime engine selected
VERBOSE [U=0,P=1355231]    startup()                     Serve RPC requests
DEBUG   [U=0,P=1355208]    InitImageDrivers()            Skipping installing fuseapps image driver because running as root
DEBUG   [U=0,P=1355208]    setupSessionLayout()          Using Layer system: overlay
DEBUG   [U=0,P=1355208]    setupOverlayLayout()          Creating overlay SESSIONDIR layout
DEBUG   [U=0,P=1355208]    addRootfsMount()              Mount rootfs in read-only mode
DEBUG   [U=0,P=1355208]    addRootfsMount()              Image type is 4096
DEBUG   [U=0,P=1355208]    addRootfsMount()              Mounting block [squashfs] image: /share/apps/Miniforge/lib/python3.12/site-packages/containers/tensorflow/tensorflow/latest-gpu/tensorflow-tensorflow-latest-gpu-sha256:1f16fbd9be8bb84891de12533e332bbd500511caeb5cf4db501dbe39d422f9c7.sif
DEBUG   [U=0,P=1355208]    addKernelMount()              Checking configuration file for 'mount proc'
DEBUG   [U=0,P=1355208]    addKernelMount()              Adding proc to mount list
VERBOSE [U=0,P=1355208]    addKernelMount()              Default mount: /proc:/proc
DEBUG   [U=0,P=1355208]    addKernelMount()              Checking configuration file for 'mount sys'
DEBUG   [U=0,P=1355208]    addKernelMount()              Adding sysfs to mount list
VERBOSE [U=0,P=1355208]    addKernelMount()              Default mount: /sys:/sys
DEBUG   [U=0,P=1355208]    addDevMount()                 Checking configuration file for 'mount dev'
DEBUG   [U=0,P=1355208]    addDevMount()                 Adding dev to mount list
VERBOSE [U=0,P=1355208]    addDevMount()                 Default mount: /dev:/dev
DEBUG   [U=0,P=1355208]    addHostMount()                Not mounting host file systems per configuration
VERBOSE [U=0,P=1355208]    addBindsMount()               Found 'bind path' = /etc/localtime, /etc/localtime
VERBOSE [U=0,P=1355208]    addBindsMount()               Found 'bind path' = /etc/hosts, /etc/hosts
DEBUG   [U=0,P=1355208]    addHomeStagingDir()           Staging home directory (/root) at /var/lib/apptainer/mnt/session/root
DEBUG   [U=0,P=1355208]    addHomeMount()                Adding home directory mount [/var/lib/apptainer/mnt/session/root:/root] to list using layer: overlay
DEBUG   [U=0,P=1355208]    addTmpMount()                 Checking for 'mount tmp' in configuration file
DEBUG   [U=0,P=1355208]    addScratchMount()             Not mounting scratch directory: Not requested
DEBUG   [U=0,P=1355208]    addLibsMount()                Checking for 'user bind control' in configuration file
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libOpenCL.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libOpenGL.so.0 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-cfg.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libEGL.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-eglcore.so.550.120 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-ml.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvcuvid.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-gtk3.so.550.120 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-glvkspirv.so.550.120 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libcudadebugger.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLESv2.so.2 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-ptxjitcompiler.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLESv2.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGL.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLX_nvidia.so.0 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLESv1_CM.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-tls.so.550.120 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLdispatch.so.0 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-encode.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libOpenCL.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-ptxjitcompiler.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-encode.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-nvvm.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-glsi.so.550.120 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-opticalflow.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-opencl.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-egl-wayland.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLX.so.0 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvoptix.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-gpucomp.so.550.120 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGL.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLESv2_nvidia.so.2 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLESv1_CM_nvidia.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-nvvm.so.4 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libEGL.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-gtk2.so.550.120 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-ml.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-fbc.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-fbc.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvcuvid.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-opticalflow.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLX.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLESv1_CM.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-cfg.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libcuda.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libvdpau_nvidia.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-rtcore.so.550.120 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libcuda.so.1 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libOpenGL.so to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libEGL_nvidia.so.0 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libnvidia-glcore.so.550.120 to mount list
DEBUG   [U=0,P=1355208]    addLibsMount()                Add library /lib64/libGLdispatch.so to mount list
DEBUG   [U=0,P=1355208]    addFilesMount()               Checking for 'user bind control' in configuration file
DEBUG   [U=0,P=1355208]    addFilesMount()               Adding file /bin/nvidia-persistenced:/usr/bin/nvidia-persistenced to mount list
DEBUG   [U=0,P=1355208]    addFilesMount()               Adding file /bin/nvidia-cuda-mps-control:/usr/bin/nvidia-cuda-mps-control to mount list
DEBUG   [U=0,P=1355208]    addFilesMount()               Adding file /bin/nvidia-cuda-mps-server:/usr/bin/nvidia-cuda-mps-server to mount list
DEBUG   [U=0,P=1355208]    addFilesMount()               Adding file /bin/nvidia-smi:/usr/bin/nvidia-smi to mount list
DEBUG   [U=0,P=1355208]    addFilesMount()               Adding file /bin/nvidia-debugdump:/usr/bin/nvidia-debugdump to mount list
DEBUG   [U=0,P=1355208]    addResolvConfMount()          Adding /etc/resolv.conf to mount list
VERBOSE [U=0,P=1355208]    addResolvConfMount()          Default mount: /etc/resolv.conf:/etc/resolv.conf
DEBUG   [U=0,P=1355208]    addHostnameMount()            Skipping hostname mount, not virtualizing UTS namespace on user request
DEBUG   [U=0,P=1355208]    create()                      Mount all
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting tmpfs to /var/lib/apptainer/mnt/session
DEBUG   [U=0,P=1355208]    mountImage()                  Mounting loop device /dev/loop0 to /var/lib/apptainer/mnt/session/rootfs of type squashfs
DEBUG   [U=0,P=1355208]    createCwdDir()                Using /root as current working directory
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting overlay to /var/lib/apptainer/mnt/session/final
DEBUG   [U=0,P=1355208]    mountGeneric()                Unmounting and remounting overlay
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final
DEBUG   [U=0,P=1355208]    setPropagationMount()         Set RPC mount propagation flag to SLAVE
VERBOSE [U=0,P=1355208]    Passwd()                      Checking for template passwd file: /var/lib/apptainer/mnt/session/rootfs/etc/passwd
VERBOSE [U=0,P=1355208]    Passwd()                      Creating passwd content
VERBOSE [U=0,P=1355208]    Passwd()                      Creating template passwd file and injecting user data: /var/lib/apptainer/mnt/session/rootfs/etc/passwd
DEBUG   [U=0,P=1355208]    addIdentityMount()            Adding /etc/passwd to mount list
VERBOSE [U=0,P=1355208]    addIdentityMount()            Default mount: /etc/passwd:/etc/passwd
VERBOSE [U=0,P=1355208]    Group()                       Checking for template group file: /var/lib/apptainer/mnt/session/rootfs/etc/group
VERBOSE [U=0,P=1355208]    Group()                       Creating group content
DEBUG   [U=0,P=1355208]    addIdentityMount()            Adding /etc/group to mount list
VERBOSE [U=0,P=1355208]    addIdentityMount()            Default mount: /etc/group:/etc/group
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /dev to /var/lib/apptainer/mnt/session/final/dev
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /etc/localtime to /var/lib/apptainer/mnt/session/final/etc/localtime
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/etc/localtime
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /etc/hosts to /var/lib/apptainer/mnt/session/final/etc/hosts
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/etc/hosts
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /proc to /var/lib/apptainer/mnt/session/final/proc
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/proc
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting sysfs to /var/lib/apptainer/mnt/session/final/sys
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /root to /var/lib/apptainer/mnt/session/root
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/root
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /var/lib/apptainer/mnt/session/root to /var/lib/apptainer/mnt/session/final/root
DEBUG   [U=0,P=1355208]    func1()                       Container /tmp resolves to ""/tmp""
DEBUG   [U=0,P=1355208]    func1()                       Container /var/tmp resolves to ""/var/tmp""
VERBOSE [U=0,P=1355208]    func1()                       Default mount: /tmp:/tmp
VERBOSE [U=0,P=1355208]    func1()                       Default mount: /var/tmp:/var/tmp
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /tmp to /var/lib/apptainer/mnt/session/final/tmp
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/tmp
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /var/tmp to /var/lib/apptainer/mnt/session/final/var/tmp
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/var/tmp
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libOpenCL.so to /var/lib/apptainer/mnt/session/libs/libOpenCL.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libOpenCL.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libOpenGL.so.0 to /var/lib/apptainer/mnt/session/libs/libOpenGL.so.0
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libOpenGL.so.0
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-cfg.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidia-cfg.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-cfg.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libEGL.so.1 to /var/lib/apptainer/mnt/session/libs/libEGL.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libEGL.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-eglcore.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidia-eglcore.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-eglcore.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-ml.so to /var/lib/apptainer/mnt/session/libs/libnvidia-ml.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-ml.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvcuvid.so.1 to /var/lib/apptainer/mnt/session/libs/libnvcuvid.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvcuvid.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-gtk3.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidia-gtk3.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-gtk3.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-glvkspirv.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidia-glvkspirv.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-glvkspirv.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libcudadebugger.so.1 to /var/lib/apptainer/mnt/session/libs/libcudadebugger.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libcudadebugger.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLESv2.so.2 to /var/lib/apptainer/mnt/session/libs/libGLESv2.so.2
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLESv2.so.2
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-ptxjitcompiler.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidia-ptxjitcompiler.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-ptxjitcompiler.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLESv2.so to /var/lib/apptainer/mnt/session/libs/libGLESv2.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLESv2.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGL.so to /var/lib/apptainer/mnt/session/libs/libGL.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGL.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLX_nvidia.so.0 to /var/lib/apptainer/mnt/session/libs/libGLX_nvidia.so.0
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLX_nvidia.so.0
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLESv1_CM.so.1 to /var/lib/apptainer/mnt/session/libs/libGLESv1_CM.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLESv1_CM.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-tls.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidia-tls.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-tls.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLdispatch.so.0 to /var/lib/apptainer/mnt/session/libs/libGLdispatch.so.0
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLdispatch.so.0
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-encode.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidia-encode.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-encode.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libOpenCL.so.1 to /var/lib/apptainer/mnt/session/libs/libOpenCL.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libOpenCL.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-ptxjitcompiler.so to /var/lib/apptainer/mnt/session/libs/libnvidia-ptxjitcompiler.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-ptxjitcompiler.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-encode.so to /var/lib/apptainer/mnt/session/libs/libnvidia-encode.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-encode.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-nvvm.so to /var/lib/apptainer/mnt/session/libs/libnvidia-nvvm.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-nvvm.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-glsi.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidia-glsi.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-glsi.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-opticalflow.so to /var/lib/apptainer/mnt/session/libs/libnvidia-opticalflow.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-opticalflow.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-opencl.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidia-opencl.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-opencl.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-egl-wayland.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidia-egl-wayland.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-egl-wayland.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLX.so.0 to /var/lib/apptainer/mnt/session/libs/libGLX.so.0
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLX.so.0
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvoptix.so.1 to /var/lib/apptainer/mnt/session/libs/libnvoptix.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvoptix.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-gpucomp.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidia-gpucomp.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-gpucomp.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGL.so.1 to /var/lib/apptainer/mnt/session/libs/libGL.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGL.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLESv2_nvidia.so.2 to /var/lib/apptainer/mnt/session/libs/libGLESv2_nvidia.so.2
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLESv2_nvidia.so.2
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLESv1_CM_nvidia.so.1 to /var/lib/apptainer/mnt/session/libs/libGLESv1_CM_nvidia.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLESv1_CM_nvidia.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-nvvm.so.4 to /var/lib/apptainer/mnt/session/libs/libnvidia-nvvm.so.4
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-nvvm.so.4
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libEGL.so to /var/lib/apptainer/mnt/session/libs/libEGL.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libEGL.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-gtk2.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidia-gtk2.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-gtk2.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-ml.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidia-ml.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-ml.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-fbc.so to /var/lib/apptainer/mnt/session/libs/libnvidia-fbc.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-fbc.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-fbc.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidia-fbc.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-fbc.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvcuvid.so to /var/lib/apptainer/mnt/session/libs/libnvcuvid.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvcuvid.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-opticalflow.so.1 to /var/lib/apptainer/mnt/session/libs/libnvidia-opticalflow.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-opticalflow.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLX.so to /var/lib/apptainer/mnt/session/libs/libGLX.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLX.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLESv1_CM.so to /var/lib/apptainer/mnt/session/libs/libGLESv1_CM.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLESv1_CM.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-cfg.so to /var/lib/apptainer/mnt/session/libs/libnvidia-cfg.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-cfg.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libcuda.so to /var/lib/apptainer/mnt/session/libs/libcuda.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libcuda.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libvdpau_nvidia.so to /var/lib/apptainer/mnt/session/libs/libvdpau_nvidia.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libvdpau_nvidia.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-rtcore.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidia-rtcore.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-rtcore.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libcuda.so.1 to /var/lib/apptainer/mnt/session/libs/libcuda.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libcuda.so.1
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libOpenGL.so to /var/lib/apptainer/mnt/session/libs/libOpenGL.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libOpenGL.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libEGL_nvidia.so.0 to /var/lib/apptainer/mnt/session/libs/libEGL_nvidia.so.0
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libEGL_nvidia.so.0
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libnvidia-glcore.so.550.120 to /var/lib/apptainer/mnt/session/libs/libnvidia-glcore.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libnvidia-glcore.so.550.120
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /lib64/libGLdispatch.so to /var/lib/apptainer/mnt/session/libs/libGLdispatch.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/libs/libGLdispatch.so
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /var/lib/apptainer/mnt/session/libs to /var/lib/apptainer/mnt/session/final/.singularity.d/libs
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/.singularity.d/libs
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /bin/nvidia-persistenced to /var/lib/apptainer/mnt/session/final/usr/bin/nvidia-persistenced
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/usr/bin/nvidia-persistenced
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /bin/nvidia-cuda-mps-control to /var/lib/apptainer/mnt/session/final/usr/bin/nvidia-cuda-mps-control
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/usr/bin/nvidia-cuda-mps-control
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /bin/nvidia-cuda-mps-server to /var/lib/apptainer/mnt/session/final/usr/bin/nvidia-cuda-mps-server
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/usr/bin/nvidia-cuda-mps-server
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /bin/nvidia-smi to /var/lib/apptainer/mnt/session/final/usr/bin/nvidia-smi
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/usr/bin/nvidia-smi
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /bin/nvidia-debugdump to /var/lib/apptainer/mnt/session/final/usr/bin/nvidia-debugdump
DEBUG   [U=0,P=1355208]    mountGeneric()                Remounting /var/lib/apptainer/mnt/session/final/usr/bin/nvidia-debugdump
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /var/lib/apptainer/mnt/session/etc/resolv.conf to /var/lib/apptainer/mnt/session/final/etc/resolv.conf
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /var/lib/apptainer/mnt/session/etc/passwd to /var/lib/apptainer/mnt/session/final/etc/passwd
DEBUG   [U=0,P=1355208]    mountGeneric()                Mounting /var/lib/apptainer/mnt/session/etc/group to /var/lib/apptainer/mnt/session/final/etc/group
VERBOSE [U=0,P=1355208]    addCwdMount()                 /root found within container
DEBUG   [U=0,P=1355208]    create()                      Chroot into /var/lib/apptainer/mnt/session/final
DEBUG   [U=0,P=1355231]    Chroot()                      Hold reference to host / directory
DEBUG   [U=0,P=1355231]    Chroot()                      Called pivot_root on /var/lib/apptainer/mnt/session/final
DEBUG   [U=0,P=1355231]    Chroot()                      Change current directory to host / directory
DEBUG   [U=0,P=1355231]    Chroot()                      Apply slave mount propagation for host / directory
DEBUG   [U=0,P=1355231]    Chroot()                      Called unmount(/, syscall.MNT_DETACH)
DEBUG   [U=0,P=1355231]    Chroot()                      Changing directory to / to avoid getpwd issues
DEBUG   [U=0,P=1355208]    create()                      Chdir into / to avoid errors
VERBOSE [U=0,P=1355230]    wait_child()                  rpc server exited with status 0
DEBUG   [U=0,P=1355230]    init()                        Set container privileges
DEBUG   [U=0,P=1355230]    apply_privileges()            Effective capabilities:   0x000001ffffffffff
DEBUG   [U=0,P=1355230]    apply_privileges()            Permitted capabilities:   0x000001ffffffffff
DEBUG   [U=0,P=1355230]    apply_privileges()            Bounding capabilities:    0x000001ffffffffff
DEBUG   [U=0,P=1355230]    apply_privileges()            Inheritable capabilities: 0x000001ffffffffff
DEBUG   [U=0,P=1355230]    apply_privileges()            Ambient capabilities:     0x000001ffffffffff
DEBUG   [U=0,P=1355230]    apply_privileges()            Set user ID to 0
DEBUG   [U=0,P=1355230]    set_parent_death_signal()     Set parent death signal to 9
DEBUG   [U=0,P=1355230]    func1()                       executablePath is /usr/libexec/apptainer/bin/starter
DEBUG   [U=0,P=1355230]    func1()                       executablePath does not exist, assuming default prefix
DEBUG   [U=0,P=1355230]    startup()                     apptainer runtime engine selected
VERBOSE [U=0,P=1355230]    startup()                     Execute stage 2
DEBUG   [U=0,P=1355230]    StageTwo()                    Entering stage 2
DEBUG   [U=0,P=1355230]    StartProcess()                Setting umask in container to 0002
DEBUG   [U=0,P=1355230]    func4()                       Not exporting ""BASH_FUNC__module_raw%%"" to container environment: invalid key
DEBUG   [U=0,P=1355230]    func4()                       Not exporting ""BASH_FUNC_ml%%"" to container environment: invalid key
DEBUG   [U=0,P=1355230]    func4()                       Not exporting ""BASH_FUNC_module%%"" to container environment: invalid key
DEBUG   [U=0,P=1355230]    func4()                       Not exporting ""BASH_FUNC_scl%%"" to container environment: invalid key
DEBUG   [U=0,P=1355230]    func4()                       Not exporting ""BASH_FUNC_which%%"" to container environment: invalid key
DEBUG   [U=0,P=1355230]    sylogBuiltin()                Sourcing /.singularity.d/env/01-base.sh
DEBUG   [U=0,P=1355230]    sylogBuiltin()                Sourcing /.singularity.d/env/10-docker2singularity.sh
DEBUG   [U=0,P=1355230]    sylogBuiltin()                Sourcing /.singularity.d/env/90-environment.sh
DEBUG   [U=0,P=1355230]    sylogBuiltin()                Sourcing /.singularity.d/env/94-appsbase.sh
DEBUG   [U=0,P=1355230]    sylogBuiltin()                Sourcing /.singularity.d/env/95-apps.sh
DEBUG   [U=0,P=1355230]    sylogBuiltin()                Sourcing /.singularity.d/env/99-base.sh
DEBUG   [U=0,P=1355230]    sylogBuiltin()                Sourcing /.singularity.d/env/99-runtimevars.sh
DEBUG   [U=0,P=1355230]    sylogBuiltin()                Running action command exec
DEBUG   [U=0,P=1355208]    PostStartProcess()            Post start process
Python 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
```",1
TensorFlow warning shows whenever importing it,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.18.0-rc2-4-g6550e4bd802

### Custom code

No

### OS platform and distribution

Linux Ubuntu 24.10 x86_64

### Mobile device

_No response_

### Python version

3.12.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA:12.6

### GPU model and memory

_No response_

### Current behavior?

`cuFFT`, `cuDNN` and `cuBLAS` warnings showing after importing `TensorFlow`

- OS: Ubuntu 24.10 x86_64
- Host: G5 5590
- Kernel: 6.11.0-13-generic
- CPU: Intel i7-9750H (12) @ 4.500GHz
- GPU: NVIDIA GeForce GTX 1650 Mobile / Max-Q
- GPU: Intel CoffeeLake-H GT2 [UHD Graphics 630]
> whenever running the following code it gives that warning also it outputs the predicted output but after the warning:
```python
import tensorflow as tf
print(tf.config.list_physical_devices('GPU'))
```
> output:
```
2025-01-23 21:08:06.468437: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1737659286.484845  763412 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1737659286.489647  763412 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-23 21:08:06.505984: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.

[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
```
> also i know that for that warning (`cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.`) i must rebuild the tensor-flow from binaries enabling the AVX2 and FMA instructions but what about the others?

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
print(tf.config.list_physical_devices('GPU'))
```

### Relevant log output

```shell

```",1
"Tutorial ""Multi-worker training with Keras"" fails to complete","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v1.12.1-120353-gc5bd67bc56f 2.19.0-dev20250107

### Custom code

No

### OS platform and distribution

Debian 6.1.123-1 (2025-01-02) x86_64 GNU/Linux

### Mobile device

_No response_

### Python version

Python 3.12.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Following the tutorial everything goes well until you start the second worker. Then the below failure occures.

2025-01-20 07:19:35.283801: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-01-20 07:19:35.290192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1737379175.297785    4595 cuda_dnn.cc:8501] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1737379175.300054    4595 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-20 07:19:35.307721: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-01-20 07:19:36.510476: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2025-01-20 07:19:36.510494: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:167] env: CUDA_VISIBLE_DEVICES=""-1""
2025-01-20 07:19:36.510499: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] CUDA_VISIBLE_DEVICES is set to -1 - this hides all GPUs from CUDA
2025-01-20 07:19:36.510501: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module
2025-01-20 07:19:36.510505: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: michael
2025-01-20 07:19:36.510507: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: michael
2025-01-20 07:19:36.510562: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 565.77.0
2025-01-20 07:19:36.510572: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 565.77.0
2025-01-20 07:19:36.510574: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:291] kernel version seems to match DSO: 565.77.0
2025-01-20 07:19:36.519175: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:637] Initializing CoordinationService
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1737379176.519611    4595 grpc_server_lib.cc:465] Started server with target: grpc://localhost:12345
2025-01-20 07:19:36.524874: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:378] /job:worker/replica:0/task:0 has connected to coordination service. Incarnation: 4677280066871850635
2025-01-20 07:19:36.524894: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:816] Waiting for 1/2 tasks to connect.
2025-01-20 07:19:36.524898: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:819] Example stragglers:
/job:worker/replica:0/task:1
I0000 00:00:1737379176.525022    4595 coordination_service_agent.cc:369] Coordination agent has successfully connected.
2025-01-20 07:22:27.996664: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:378] /job:worker/replica:0/task:1 has connected to coordination service. Incarnation: 13530699364709055870
2025-01-20 07:22:27.996686: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:816] Waiting for 0/2 tasks to connect.
/home/chad/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.
  warnings.warn(
2025-01-20 07:22:28.461733: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.
Traceback (most recent call last):
  File ""/home/chad/Documents/McCueFiles/NeuralNetworks/TensorFlowProject/TensorFlowDocExample/main.py"", line 21, in <module>
    multi_worker_model.fit(multi_worker_dataset, epochs=3, steps_per_epoch=70)
  File ""/home/chad/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/chad/anaconda3/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py"", line 108, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: Attempt to convert a value (PerReplica:{
  0: <tf.Tensor: shape=(64, 28, 28), dtype=float32, numpy=
array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       ...,

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>
}) with an unsupported type (<class 'tensorflow.python.distribute.values.PerReplica'>) to a Tensor.

### Standalone code to reproduce the issue

```shell
python main.py &> job_1.log
```

### Relevant log output

```shell
2025-01-20 07:19:35.283801: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-01-20 07:19:35.290192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1737379175.297785    4595 cuda_dnn.cc:8501] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1737379175.300054    4595 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-20 07:19:35.307721: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-01-20 07:19:36.510476: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2025-01-20 07:19:36.510494: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:167] env: CUDA_VISIBLE_DEVICES=""-1""
2025-01-20 07:19:36.510499: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] CUDA_VISIBLE_DEVICES is set to -1 - this hides all GPUs from CUDA
2025-01-20 07:19:36.510501: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module
2025-01-20 07:19:36.510505: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: michael
2025-01-20 07:19:36.510507: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: michael
2025-01-20 07:19:36.510562: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 565.77.0
2025-01-20 07:19:36.510572: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 565.77.0
2025-01-20 07:19:36.510574: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:291] kernel version seems to match DSO: 565.77.0
2025-01-20 07:19:36.519175: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:637] Initializing CoordinationService
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1737379176.519611    4595 grpc_server_lib.cc:465] Started server with target: grpc://localhost:12345
2025-01-20 07:19:36.524874: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:378] /job:worker/replica:0/task:0 has connected to coordination service. Incarnation: 4677280066871850635
2025-01-20 07:19:36.524894: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:816] Waiting for 1/2 tasks to connect.
2025-01-20 07:19:36.524898: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:819] Example stragglers:
/job:worker/replica:0/task:1
I0000 00:00:1737379176.525022    4595 coordination_service_agent.cc:369] Coordination agent has successfully connected.
2025-01-20 07:22:27.996664: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:378] /job:worker/replica:0/task:1 has connected to coordination service. Incarnation: 13530699364709055870
2025-01-20 07:22:27.996686: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:816] Waiting for 0/2 tasks to connect.
/home/chad/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.
  warnings.warn(
2025-01-20 07:22:28.461733: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.
Traceback (most recent call last):
  File ""/home/chad/Documents/McCueFiles/NeuralNetworks/TensorFlowProject/TensorFlowDocExample/main.py"", line 21, in <module>
    multi_worker_model.fit(multi_worker_dataset, epochs=3, steps_per_epoch=70)
  File ""/home/chad/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/chad/anaconda3/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py"", line 108, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: Attempt to convert a value (PerReplica:{
  0: <tf.Tensor: shape=(64, 28, 28), dtype=float32, numpy=
array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       ...,

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>
}) with an unsupported type (<class 'tensorflow.python.distribute.values.PerReplica'>) to a Tensor.
```",1
Cannot use GpuDelegate - java.lang.IllegalArgumentException: Internal error: Cannot create interpreter,"Hi, I get ""Cannot use GpuDelegate - java.lang.IllegalArgumentException: Internal error: Cannot create interpreter"" when attempting to use GpuDelegate

I have seen a couple of issue related to this but all seems to be abandoned. I have created a repo replicating the issue.  You can see the config at [https://github.com/NLLAPPS/WhisperOffline/blob/d075a84aa42adc4a05a02ce64b0fcda9416f0e8c/app/src/main/java/com/whispertflite/engine/WhisperEngineJava.java#L114](https://github.com/NLLAPPS/WhisperOffline/blob/d075a84aa42adc4a05a02ce64b0fcda9416f0e8c/app/src/main/java/com/whispertflite/engine/WhisperEngineJava.java#L114)

**System information**
- Android Device information: Samsung S23
- TensorFlow Lite in Play Services SDK version : 16.4.0
- Google Play Services version: 24.50.34

**Standalone code to reproduce the issue**
Clone and run project from [https://github.com/NLLAPPS/WhisperOffline/](https://github.com/NLLAPPS/WhisperOffline/)

**Any other info / logs**
`Created TensorFlow Lite delegate for GPU.
Created interpreter.
Created interpreter.
java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: 
at com.google.android.gms.tflite.NativeInterpreterWrapper.createInterpreter(Native Method)
at com.google.android.gms.tflite.NativeInterpreterWrapper.zzs(com.google.android.gms:play-services-tflite-java@@16.4.0:34)
at com.google.android.gms.tflite.NativeInterpreterWrapper.<init>(com.google.android.gms:play-services-tflite-java@@16.4.0:14)
at com.google.android.gms.tflite.zzd.<init>(com.google.android.gms:play-services-tflite-java@@16.4.0:3)
at com.google.android.gms.tflite.InterpreterFactoryImpl.create(com.google.android.gms:play-services-tflite-java@@16.4.0:4)
at org.tensorflow.lite.InterpreterApi.create(InterpreterApi.java:373)
at com.whispertflite.engine.WhisperEngineJava.loadModel(WhisperEngineJava.java:131)
at com.whispertflite.engine.WhisperEngineJava.initialize(WhisperEngineJava.java:44)
at com.whispertflite.asr.WhisperJava.loadModel(WhisperJava.java:72)
at com.whispertflite.asr.WhisperJava.loadModel(WhisperJava.java:67)
at com.whispertflite.MainActivity.initModel(MainActivity.java:247)
at com.whispertflite.MainActivity.lambda$onCreate$3$com-whispertflite-MainActivity(MainActivity.java:155)
at com.whispertflite.MainActivity$$ExternalSyntheticLambda0.run(D8$$SyntheticClass:0)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:644)
at java.lang.Thread.run(Thread.java:1012)`
",1
Force TF to log GPU memory allocation,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.18.0

### Custom code

No

### OS platform and distribution

Debian 12

### Mobile device

_No response_

### Python version

3.11.2

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2

### GPU model and memory

RTX 3060 12Gb

### Current behavior?

If I ran out of GPU memory I get error saying

`Allocator (GPU_0_bfc) ran out of memory trying to allocate 9.78GiB`

But if allocation succeeded there is no real way to know what the allocator did/doing.. I know that you can use nvidia-smi etc but this is an indirect way to estimate very crudely whats going on.

What I believe would be very beneficial (because I dont know about you guys but I run out of memory so very often and I cant affort 128Gb card) - is to be able to force TF to log every and single one GPU memory allocation. 

So I can see whats going on my healthy models and whats in the failing.",1
Aborted  in `tf.raw_ops.RaggedGather`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific inputs, `tf.raw_ops.RaggedGather` triggers crash.

### Standalone code to reproduce the issue

```shell
params_nested_splits = tf.constant(0, shape=[3], dtype=tf.int64)
params_dense_values = tf.constant(1, shape=[0], dtype=tf.float32)
indices = tf.constant(0, shape=[], dtype=tf.int64)
OUTPUT_RAGGED_RANK = 1
PARAMS_RAGGED_RANK = 1

tf.raw_ops.RaggedGather(
    params_nested_splits=[params_nested_splits],
    params_dense_values=params_dense_values,
    indices=indices,
    OUTPUT_RAGGED_RANK=1,
    name=None
)
```

### Relevant log output

```shell
2025-01-18 09:30:00.549762: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (9 vs. 1) float expected, got int64
Aborted (core dumped)
```",2
Segmentation fault (core dumped) in `RaggedTensorToTensor`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific inputs, `tf.raw_ops.RaggedTensorToTensor` triggers crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

shape = tf.constant(-1, shape=[], dtype=tf.int64)
values = tf.constant(0, shape=[0], dtype=tf.int32)
default_value = tf.constant(0, shape=[], dtype=tf.int32)
row_partition_tensors = tf.constant([0, 1, 6], shape=[3], dtype=tf.int64)
row_partition_types = [""ROW_SPLITS""]

tf.raw_ops.RaggedTensorToTensor(
    shape=shape,
    values=values,
    default_value=default_value,
    row_partition_tensors=[row_partition_tensors],
    row_partition_types=row_partition_types)
```

### Relevant log output

```shell
Segmentation fault (core dumped)
```",2
Windows libtensorflow size increased 4x with 2.17,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.17+

### Custom code

No

### OS platform and distribution

Windows x86_64

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Libtensorflow.dll for windows was 238MB with 2.16.2, is 909MB with 2.17.0, and is 931MB with 2.18.0.

At the same time the linux versions haven't changed significantly.

I would not expect the tensorflow binaries on windows to be over twice the size of linux, nor for the size to increase so much without any notice in the release notes.

I've tried to look through the bazel configs but there's nothing obvious to me which would cause this!

### 2.16.2 
versions_2.16.2_libtensorflow-cpu-windows-x86_64 238MB
versions_2.16.2_libtensorflow-cpu-linux-x86_64 422MB

### 2.17.0
 versions_2.17.0_libtensorflow-cpu-windows-x86_64 909MB
versions_2.17.0_libtensorflow-cpu-linux-x86_64 412MB

### 2.18.0
versions_2.18.0_libtensorflow-cpu-windows-x86_64 931MB

### Standalone code to reproduce the issue

```shell
Libtensorflow is provided by google via the GCS buckets documented here https://www.tensorflow.org/install/lang_c
```

### Relevant log output

```shell

```",1
Seg Fault when iterate dataset created from data service,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Segfault when trying to iterate dataset get from data service.

### Standalone code to reproduce the issue

```shell
# start the data service file start_dataservice.py

import tensorflow as tf

dispatcher = tf.data.experimental.service.DispatchServer(
    tf.data.experimental.service.DispatcherConfig(port=50050), start=True
)
dispatcher_address = dispatcher.target.split(""://"")[1]
worker = tf.data.experimental.service.WorkerServer(
    tf.data.experimental.service.WorkerConfig(dispatcher_address=dispatcher_address), start=True
)
print(""Starting Worker"")
worker.join()

# test file test_dataset_service.py
import tensorflow as tf
import numpy as np


flags = tf.compat.v1.app.flags

flags.DEFINE_bool(""local"", False, ""Run data service in process"")
flags.DEFINE_bool(""distribute"", False, ""Run data service in distributed_epoch mode"")
FLAGS = flags.FLAGS


def local_service():
    print(""Starting Local Service"")
    dispatcher = tf.data.experimental.service.DispatchServer(
        tf.data.experimental.service.DispatcherConfig(port=50050), start=True
    )
    dispatcher_address = dispatcher.target.split(""://"")[1]
    worker = tf.data.experimental.service.WorkerServer(
        tf.data.experimental.service.WorkerConfig(dispatcher_address=dispatcher_address), start=True
    )
    print(""Dispatcher target is "", dispatcher.target)
    return dispatcher, worker, dispatcher.target


def apply_transformations(ds_train):
    ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)
    ds_train = ds_train.cache()
    ds_train = ds_train.shuffle(60000)
    ds_train = ds_train.batch(128)
    ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)
    return ds_train


(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()
x_train = x_train / np.float32(255)
y_train = y_train.astype(np.int64)
ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))


def normalize_img(image, label):
    """"""Normalizes images: `uint8` -> `float32`.""""""
    return tf.cast(image, tf.float32) / 255.0, label


ds_train = apply_transformations(ds_train)
# Create dataset however you were before using the tf.data service.
dataset = ds_train
if FLAGS.local:
    dispatcher, worker, service = local_service()
else:
    dispatcher_address = ""localhost""
    dispatcher_port = ""50050""
    service = ""grpc://{}:{}"".format(dispatcher_address, dispatcher_port)
if FLAGS.distribute:
    processing_mode = ""distributed_epoch""
else:
    processing_mode = ""parallel_epochs""

# This will register the dataset with the tf.data service cluster so that
# tf.data workers can run the dataset to produce elements. The dataset returned
# from applying `distribute` will fetch elements produced by tf.data workers.
dataset = dataset.apply(
    tf.data.experimental.service.distribute(processing_mode=processing_mode, service=service)
)

for (x1, y1), (x2, y2) in zip(dataset, ds_train):
    np.allclose(x1, x2)
    np.allclose(y1, y2)

print(""verified mnist dataset locally vs over service"")

# script to run 
python -m pip install --upgrade pip
python -m pip install tensorflow==2.18.0
python -m pip install 'protobuf<4'
screen -d -m python start_dataservice.py
python3 test_dataset_service.py --local=False
```

### Relevant log output

```shell
2025-01-14 21:56:19.778399: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1736891779.795141    9168 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1736891779.800177    9168 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-14 21:56:19.815971: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
I0000 00:00:1736891783.518634    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 889 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:10:1c.0, compute capability: 8.0
I0000 00:00:1736891783.520395    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 37945 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:10:1d.0, compute capability: 8.0
I0000 00:00:1736891783.522012    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 37945 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:20:1c.0, compute capability: 8.0
I0000 00:00:1736891783.523626    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 37945 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:20:1d.0, compute capability: 8.0
I0000 00:00:1736891783.525222    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 37945 MB memory:  -> device: 4, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:90:1c.0, compute capability: 8.0
I0000 00:00:1736891783.526807    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 37945 MB memory:  -> device: 5, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:90:1d.0, compute capability: 8.0
I0000 00:00:1736891783.528377    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 37945 MB memory:  -> device: 6, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:a0:1c.0, compute capability: 8.0
I0000 00:00:1736891783.529933    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 37945 MB memory:  -> device: 7, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:a0:1d.0, compute capability: 8.0
/test/bin/testDataservice: line 5:  9168 Segmentation fault      (core dumped) python ${BIN_DIR}/test_dataset_service.py --local=False
```",1
Yolov8-seg.pt segmentation model is deployed on Android after training,"**System information**
- Android Device information (use `adb shell getprop ro.build.fingerprint`
  if possible):
”“”
vivo/PD2020/PD2020:10/QP1A.190711.020/compiler10141555:user/release-keys
“”“
- TensorFlow Lite in Play Services SDK version (found in `build.gradle`):
”“”
    implementation 'org.tensorflow:tensorflow-lite-task-vision:0.4.0'
    implementation 'org.tensorflow:tensorflow-lite-gpu-delegate-plugin:0.4.0'
    implementation 'org.tensorflow:tensorflow-lite-gpu:2.9.0'
“”“
- Google Play Services version
  (`Settings` > `Apps` > `Google Play Services` > `App details`):

**Standalone code to reproduce the issue**
”“Here is the full code of the program”“
""""""
public class MainActivity extends AppCompatActivity {
    private String MODEL = ""best_float32_metadata.tflite"";

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);

        Bitmap bitmap = BitmapFactory.decodeResource(getResources(), R.drawable.a3);
        bitmap = imageScale(bitmap, 640, 640);
        TensorImage tensorImage = TensorImage.fromBitmap(bitmap);

        Log.e(""HENG"", String.valueOf(tensorImage.getBuffer()));
        Log.e(""HENG"", String.valueOf(tensorImage.getTensorBuffer()));
        Log.e(""HENG"", String.valueOf(tensorImage.getDataType()));
        Log.e(""HENG"", String.valueOf(tensorImage.getColorSpaceType()));

        ImageSegmenter.ImageSegmenterOptions options = ImageSegmenter.ImageSegmenterOptions.builder()
                .setBaseOptions(BaseOptions.builder().build())
                .setOutputType(OutputType.CONFIDENCE_MASK)
                .build();

        ImageSegmenter imageSegmenter = null;

        try {
            imageSegmenter = ImageSegmenter.createFromFileAndOptions(this, MODEL, options);
        } catch (IOException e) {
            throw new RuntimeException(e);
        }
        List<Segmentation> results = imageSegmenter.segment(tensorImage);
        Log.e(""HENG"", ""HELLO: ""+results.toString());
    }

    // 图像缩放方法
    public static Bitmap imageScale(Bitmap bitmap, int new_w, int new_h) {
        Bitmap scaledBitmap = Bitmap.createScaledBitmap(bitmap, new_w, new_h, true);
        return scaledBitmap;
    }
}
""""""

**Any other info / logs**
Please allow me to repeat my question. Thank you,
First, (I may have solved the first problem, but I am not sure) I trained my data set with yolov8-seg.pt to get a model. I converted it to tflite format, copied the 32-bit model generated by best_float32.tflite into asssets in Android, and then modified the path of model to run the following original code. I got two error messages: ""1 Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images.2、java.lang.IllegalStateException: Error getting native address of native library: task_vision_jni”, After I searched, I found“ https://stackoverflow.com/questions/66727627/failed-to-initialize-detector-input-tensor-has-type-ktflitefloat32-ml-kit ”I got a copy of the code in this link. After I tried to run it, I put the model I got into assets again. After running, it was not the above error message (the error message is the second problem).
Second, (from the follow-up to the first question), I also got two errors after running the modified model ""best_float32_metadata.tflite"". The first one is ""java.lang.illegalargumentexception: error occurred when initializing imagesegment: image segmentation models are expected to have only 1 output, found 2"". It says that the model actually returns two outputs, which is a very important question. The second one seems to be the same as the first one, ""java.lang.runtimeexception: unable to start action""activity componentinfo{com.example.yoloseg_android/com.example.yoloseg_android.mainactivity}: java.lang.illegalstateexception: error getting native address of native library: task_vision_jni "", I don't understand this.
These are my two problems. I think the second problem should be solved.

<!-- Failed to upload ""YoloSegAndroid.zip"" -->

Note: I can use the deeplabv3.tflite model officially provided by tensorflow to get the output smoothly",1
GPU Profiling: MemoryProfile do not contain memory events when profile remote worker.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

nightly

### Custom code

No

### OS platform and distribution

Ubuntu 22.04

### Python version

Python 3.12

### CUDA/cuDNN version

CUDA 12.4

### GPU model and memory

A100 80GB

### Current behavior?

Start a simple any collective training with Tensorflow cluster config. And then use RPC client capture_profile in Tensorboard or tf.profiler.experimental.client.trace. 
No any memory profile events or OP profiler, but only trace view.

### Standalone code to reproduce the issue

**tf_allreduce.py**
```python
import tensorflow as tf
from tensorflow.python.ops.collective_ops import all_reduce, all_reduce_v2
from tensorflow.python.eager import context
from tensorflow.core.protobuf import config_pb2
from tensorflow.python.distribute import cluster_resolver as cluster_resolver_lib

cluster_resolver = cluster_resolver_lib.TFConfigClusterResolver()
cluster = cluster_resolver.cluster_spec()
task_type = cluster_resolver.task_type
task_id = cluster_resolver.task_id

experimental_config = config_pb2.ConfigProto.Experimental(
    share_cluster_devices_in_session=False,
    share_session_state_in_clusterspec_propagation=False
)
config = config_pb2.ConfigProto(experimental=experimental_config)
config.experimental.collective_group_leader = '/job:worker/replica:0/task:0'
server = tf.distribute.Server(cluster,
                              job_name=task_type,
                              task_index=task_id,
                              protocol=""grpc"", # ""grpc+verbs""
                              config=config)
run_options = config_pb2.RunOptions()

with tf.compat.v1.Session(target=server.target, config=config) as sess:
    tensor = tf.Variable(tf.ones([2, 2]), dtype=tf.float32)
    init = tf.compat.v1.global_variables_initializer()
    sess.run(init)
    sess.run(tf.print([""tensor:"",tensor]))

    reduced_tensor = all_reduce(tensor, group_size=2, group_key=4321, instance_key=1234, merge_op='Add', final_op='Id', communication_hint='auto')
    run_options.experimental.collective_graph_key = 6
    while True:
        sess.run(tf.print([""reduced_tensor:"",reduced_tensor]), options=run_options)
```

Run script to start server.
```bash
CUDA_VISIBLE_DEVICES=0 TF_CONFIG='{""cluster"":{""worker"":[""localhost:2223"",""localhost:2224""]},""task"":{""type"":""worker"",""index"":0}}' python tf_allreduce.py&
CUDA_VISIBLE_DEVICES=1 TF_CONFIG='{""cluster"":{""worker"":[""localhost:2223"",""localhost:2224""]},""task"":{""type"":""worker"",""index"":1}}' python tf_allreduce.py&
```

 use capture_profile in Tensorboard or tf.profiler.experimental.client.trace.
```python
tf.profiler.experimental.client.trace(
  'grpc://localhost:2223,grpc://localhost:2224',
   '/tmp/my_tb_dir',
   2000,
)
```

Try to convert xplane.pb to memory_profile, nothing show.
```python
from tensorflow.python.profiler.internal import _pywrap_profiler as profiler_wrapper
json = profiler_wrapper.xspace_to_tools_data([""xxx.xplane""], ""memory_profile"")
```

**Relevant log output**
```
{""memoryProfilePerAllocator"":{},""numHosts"":1,""memoryIds"":[]}
```

Relative issue: #48146 ",1
keras model.save does not respect `include_optimizer=False`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.19.0-dev20250105

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Saving a model using keras with `include_optizer = False` results in a model being saved with optimizer

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1x5NJs9nFxmExhuy8_f_fOehHmIOmk-CZ?usp=sharing
```


### Relevant log output

_No response_",1
How can I use local CUDA instead of hermetic CUDA to build? ,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18

### Custom code

No

### OS platform and distribution

Ubuntu 24.04

### Mobile device

_No response_

### Python version

3.12

### Bazel version

6.5.0

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Configures about hermetic CUDA are confusing. It seems to bring extra dependencies into the output binary. I want to build a tf-lib depends on local CUDA libs.

### Standalone code to reproduce the issue

```shell
-
```


### Relevant log output

_No response_",1
"KeyError: ""There is no item named 'PetImages\\Cat\\0.jpg' in the archive"" When Running TensorFlow Locally(CPU) on Anaconda in VS Code.","### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.1

### Custom code

Yes

### OS platform and distribution

window11

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am a beginner and encountering an issue while trying to run TensorFlow locally using Anaconda in VS Code. The same code runs smoothly on Google Colab, but when executed locally, it fails during the dataset download process with the following error.

What I've Tried
Re-downloading TensorFlow and TensorFlow Datasets to ensure they are up to date.
Manually Unzipping the Dataset and verifying if 'PetImages/Cat/0.jpg' exists in the archive.
Re-adjusting Python, TensorFlow, and TensorFlow Datasets Versions to match those in Colab by using Python 3.10.11 instead of Python 3.10.12.
Recreating the Virtual Environment in Anaconda to ensure a clean setup.
Downloading the Cats vs Dogs Dataset from Different Sources, but the issue persists.
Asking ChatGPT for assistance, but the issue remains unresolved.

Additional Information
On Google Colab, the same code runs without any issues, and the dataset downloads successfully.
In VS Code, the error consistently occurs during the dataset download process, indicating that 'PetImages/Cat/0.jpg' is missing from the archive.
Network Stability: I have a stable internet connection, and downloads complete without interruption, but the error persists.

Questions
Why does the KeyError occur in VS Code but not in Google Colab?
Could this be related to the way the dataset is being downloaded or unzipped locally?
Are there any compatibility issues between the Python/TensorFlow versions and the dataset?
Request for Help
I would greatly appreciate any guidance or suggestions on how to resolve this issue. Thank you in advance for your assistance!


### Standalone code to reproduce the issue

```shell
import tensorflow_datasets as tfds
import tensorflow as tf
import numpy as np



CatsVsDogs_OrgData, info=tfds.load(name='cats_vs_dogs', with_info=True,
                  split=tfds.Split.TRAIN)
```


### Relevant log output

```shell
PS C:\Users\jbb86\桌面\圖樣辨識> & C:/Users/jbb86/桌面/圖樣辨識/.venv/Scripts/python.exe c:/Users/jbb86/桌面/圖樣辨識/.venv/CatsVsDogs.py
2025-01-03 22:38:53.599253: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-01-03 22:38:54.247816: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\Users\jbb86\tensorflow_datasets\cats_vs_dogs\4.0.1...
Dl Size...: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 824887076/824887076 [00:00<00:00, 803489819418.28 MiB/s]
Dl Completed...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 974.06 url/s]
Generating splits...:   0%|                                   2025-01-03 22:38:55.958212: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
  File ""c:\Users\jbb86\桌面\圖樣辨識\.venv\CatsVsDogs.py"", line 7, in <module>
    CatsVsDogs_OrgData, info=tfds.load(name='cats_vs_dogs', with_info=True,
  File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\site-packages\tensorflow_datasets\core\logging\__init__.py"", line 176, in __call__
    return function(*args, **kwargs)
  File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\site-packages\tensorflow_datasets\core\load.py"", line 661, in load
    _download_and_prepare_builder(dbuilder, download, download_and_prepare_kwargs)
  File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\site-packages\tensorflow_datasets\core\load.py"", line 517, in _download_and_prepare_builder
    dbuilder.download_and_prepare(**download_and_prepare_kwargs)
  File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\site-packages\tensorflow_datasets\core\logging\__init__.py"", line 176, in __call__
    return function(*args, **kwargs)
  File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\site-packages\tensorflow_datasets\core\dataset_builder.py"", line 756, in download_and_prepare
    self._download_and_prepare(
  File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\site-packages\tensorflow_datasets\core\dataset_builder.py"", line 1752, in _download_and_prepare
    split_infos = self._generate_splits(dl_manager, download_config)
  File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\site-packages\tensorflow_datasets\core\dataset_builder.py"", line 1727, in _generate_splits
    future = split_builder.submit_split_generation(
  File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\site-packages\tensorflow_datasets\core\split_builder.py"", line 436, in submit_split_generation
    return self._build_from_generator(**build_kwargs)
  File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\site-packages\tensorflow_datasets\core\split_builder.py"", line 496, in _build_from_generator
    for key, example in utils.tqdm(
  File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\site-packages\tqdm\std.py"", line 1181, in __iter__
    for obj in iterable:
  File ""C:\Users\jbb86\桌面\圖樣辨識\.venv\lib\site-packages\tensorflow_datasets\image_classification\cats_vs_dogs.py"", line 117, in _generate_examples
    new_fobj = zipfile.ZipFile(buffer).open(fname)
  File ""C:\Users\jbb86\AppData\Local\Programs\Python\Python310\lib\zipfile.py"", line 1516, in open
    zinfo = self.getinfo(name)
  File ""C:\Users\jbb86\AppData\Local\Programs\Python\Python310\lib\zipfile.py"", line 1443, in getinfo
    raise KeyError(
KeyError: ""There is no item named 'PetImages\\\\Cat\\\\0.jpg' in the archive""
```
",1
Tensorflow not supported on Windows + ARM CPUs,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18

### Custom code

No

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I can't import tensorflow


### Standalone code to reproduce the issue

```shell
I can't import tensorflow. Installation is successful. I uninstalled and reinstalled
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
File ~\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:70
     69 try:
---> 70   from tensorflow.python._pywrap_tensorflow_internal import *
     71 # This try catch logic is because there is no bazel equivalent for py_extension.
     72 # Externally in opensource we must enable exceptions to load the shared object
     73 # by exposing the PyInit symbols with pybind. This error will only be
     74 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.
     75 
     76 # This logic is used in other internal projects using py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
Cell In[9], line 1
----> 1 import tensorflow as tf

File ~\anaconda3\Lib\site-packages\tensorflow\__init__.py:40
     37 _os.environ.setdefault(""ENABLE_RUNTIME_UPTIME_TELEMETRY"", ""1"")
     39 # Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596
---> 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import
     41 from tensorflow.python.tools import module_util as _module_util
     42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader

File ~\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py:85
     83     sys.setdlopenflags(_default_dlopen_flags)
     84 except ImportError:
---> 85   raise ImportError(
     86       f'{traceback.format_exc()}'
     87       f'\n\nFailed to load the native TensorFlow runtime.\n'
     88       f'See https://www.tensorflow.org/install/errors '
     89       f'for some common causes and solutions.\n'
     90       f'If you need help, create an issue '
     91       f'at https://github.com/tensorflow/tensorflow/issues '
     92       f'and include the entire stack trace above this error message.')

ImportError: Traceback (most recent call last):
  File ""C:\Users\dhima\anaconda3\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 70, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```
",1
No GPU support for tf.image.resize(method='bicubic') ,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

tf.image.resize() does not support GPU execution with 'bicubic' interpolation method. 

*I can help add this feature* 

### Standalone code to reproduce the issue

```shell
print(tf.config.list_physical_devices())
tf.debugging.set_log_device_placement(True)

# runs on GPU
img = tf.ones([4,100,100,1])
img_resized = tf.image.resize(img, [50,50])

# runs on CPU (no GPU implementation available!)
img2 = tf.ones([4,100,100,1])
img2_resized = tf.image.resize(img2, [50,50], method='bicubic')
```


### Relevant log output

_No response_",1
tensorflow-opt-cuda: error running on Linux via GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.18.0

### Custom code

No

### OS platform and distribution

manjaro

### Mobile device

_No response_

### Python version

3.12.7

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.4

### GPU model and memory

RTX 4090 - 24GB

### Current behavior?

Hello all,
thank you for tensorflow!
I have installed:

`sudo pacman -S python-tensorflow-opt-cuda`

Actually I cannot run a tensorflow programm in IDE that runs well in windows - without GPU usage only on CPU.

I have this error on Manjaro - with GPU:

`Process finished with exit code 134 (interrupted by signal 6:SIGABRT)`

Python PyTorch on GPU just runs fine, though!

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn import datasets


digits = datasets.load_digits()
#
#
# # 2. Skalieren der Merkmale auf den Bereich [0, 1]
scaler = MinMaxScaler()
digits_scaled = scaler.fit_transform(digits.data)
#

# # Creating the encoder
enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
#
result = enc.fit_transform(digits.target.reshape(-1, 1))

ratio = 0.2
X_train, X_test, y_train, y_test = train_test_split(digits_scaled, result, test_size=ratio, random_state=42)

model1 = tf.keras.models.Sequential()
model1.add(tf.keras.layers.Input(X_train.shape[1:])) #Process finished with exit code 134 (interrupted by signal 6:SIGABRT)

# model1.add(tf.keras.layers.Dense(128, input_dim=64, activation=""relu"")) # hidden1
# model1.add(tf.keras.layers.Dense(64, activation=""relu"")) # hidden2
# model1.add(tf.keras.layers.Dense(10, activation='softmax')) # outputlayer
# model1.summary()
# model1.compile(loss=""categorical_crossentropy"", optimizer=tf.keras.optimizers.SGD(learning_rate=0.01), # Adam()
#                metrics=(['accuracy']))
```


### Relevant log output

```shell
   ~/PycharmProjects/alfatraining_projekt_4/week1    main  python digits_uebung.py                                                                                                                 ✔   
2024-12-20 07:53:19.219496: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1734677599.229682    9678 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1734677599.232640    9678 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
onehot labels:
 [[1. 0. 0. ... 0. 0. 0.]
 [0. 1. 0. ... 0. 0. 0.]
 [0. 0. 1. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 1. 0.]
 [0. 0. 0. ... 0. 0. 1.]
 [0. 0. 0. ... 0. 1. 0.]]
/usr/include/c++/14.1.1/bits/stl_vector.h:1130: constexpr std::vector<_Tp, _Alloc>::reference std::vector<_Tp, _Alloc>::operator[](size_type) [with _Tp = pybind11::object; _Alloc = std::allocator<pybind11::object>; reference = pybind11::object&; size_type = long unsigned int]: Assertion '__n < this->size()' failed.
zsh: IOT instruction (core dumped)  python digits_uebung.py
```
",1
How to run TFLite benchmark with QNN delegate in Android,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.15.0

### Custom code

No

### OS platform and distribution

macOS 15.2

### Mobile device

One Plus 7 Pro, Android 11

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have built/installed/run TFLite benchmark following this [instruction](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark#on-android) for Android, and used TensorFlow 2.15.0 according to [issue#66015](https://github.com/tensorflow/tensorflow/issues/66015). I test the benchmark via the following commands and the output result seems correct.
```shell
adb push /Users/handleychen/Github/tensorflow/tensorflow/bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model /data/local/tmp
adb shell chmod +x /data/local/tmp/benchmark_model
adb shell ""mkdir /data/local/tmp/models""
adb push /Users/handleychen/Github/tensorflow/models/*.tflite /data/local/tmp/models
adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224.tflite --num_threads=4 --enable_op_profiling=true
adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224.tflite --use_gpu=true --enable_op_profiling=true
adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224.tflite --use_nnapi=true --enable_op_profiling=true
``` 
[benchmark result.txt](https://github.com/user-attachments/files/18197819/benchmark.result.txt)

Now I want to run the benchmark with QNN delegate. I [setup the on device environment](https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/TfLite-Delegate_setup.html#on-device-environment-setup) and [run a QNN delegate using an external delegate](https://docs.qualcomm.com/bundle/publicresource/topics/80-70015-54/sample-applications.html#run-a-qnn-delegate-using-an-external-delegate). The [model](https://storage.googleapis.com/download.tensorflow.org/models/tflite/task_library/image_classification/android_java/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite) being tested comes from tflite example [image_classification](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification).  I tested the benchmark using the following commands, but the result was a failure.
```shell
adb shell ""mkdir /data/local/tmp/qnn_delegate""
adb push /Users/handleychen/Github/quic/SDK/qairt/2.26.0.240828/lib/aarch64-android/* /data/local/tmp/qnn_delegate
adb shell
cd /data/local/tmp
export LD_LIBRARY_PATH=/data/local/tmp/qnn_delegate
export ADSP_LIBRARY_PATH=""/data/local/tmp/qnn_delegate""
./benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite --external_delegate_path=/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so --external_delegate_options='backend_type:gpu;'
./benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite --external_delegate_path=/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so --external_delegate_options='backend_type:htp;'
# I also tried setting htp_precision:1, but the result was the same.
./benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite --external_delegate_path=/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so --external_delegate_options='backend_type:htp;htp_precision:1'
``` 
```shell
# for gpu delegate
……
INFO: Though EXTERNAL delegate is explicitly applied, the model graph will not be executed by the delegate.
……

# for npu delegate
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [/data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite]
INFO: External delegate path: [/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so]
INFO: External delegate options: [backend_type:htp;htp_precision:1]
INFO: Loaded model /data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: EXTERNAL delegate created.
ERROR: [QNN Delegate] Failed to create device_handle for Backend ID 6, error=1008
ERROR: Restored original execution plan after delegate application failure.
ERROR: Failed to apply EXTERNAL delegate.
ERROR: Benchmarking failed.
``` 
The full output is attached. [benchmarkQNN result.txt](https://github.com/user-attachments/files/18206356/benchmarkQNN.result.txt)

I have also tested it on Android phones equipped with Snapdragon 855 and Gen 3 chips, and the results were the same.

Could anyone tell me how to deal with this?

### Standalone code to reproduce the issue

```shell
as described above
```


### Relevant log output

_No response_",1
Aborted (core dumped) in `LearnedUnigramCandidateSampler`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific inputs,`tf.raw_ops.LearnedUnigramCandidateSampler` triggers crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

true_classes = tf.constant([], dtype=tf.int64)
num_true = 3590707793247644003
num_sampled = 126
unique = False
range_max = 186785497093039093
seed = 8997
seed2 = 0

tf.raw_ops.LearnedUnigramCandidateSampler(
    true_classes=true_classes,
    num_true=num_true,
    num_sampled=num_sampled,
    unique=unique,
    range_max=range_max,
    seed=seed,
    seed2=seed2,
    name=None
)
```


### Relevant log output

```shell
2024-12-17 11:36:29.305345: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at candidate_sampler_ops.cc:37 : INVALID_ARGUMENT: Attr num_true has value 3590707793247644003 out of range for an int32
2024-12-17 11:36:29.305378: F external/local_tsl/tsl/lib/random/weighted_picker.cc:28] Check failed: N >= 0 (0 vs. -2090015755)
Aborted (core dumped)
```
",2
[XLA] `tf.keras.layers.LSTM` behaves differently on GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

nightly

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When executing LSTM on **XLA**, it fails.
However, when executing it without XLA, it passes.
The above failure is on GPU.
If I use CPU as backend, with or without XLA both pass the check.

### Standalone code to reproduce the issue

```python
import os
import tensorflow
import tensorflow as tf
tf.random.set_seed(42)
class RecurrentModel(tf.keras.Model):

    def __init__(self):
        super(RecurrentModel, self).__init__()
        self.lstm = tf.keras.layers.LSTM(units=64, return_sequences=True)

    @tf.function(jit_compile=True)
    def call(self, x):
        return self.lstm(x)


model = RecurrentModel()


input_shape = (10, 20, 1)
x = tf.random.normal(shape=input_shape)

inputs = [x]

output = model(*inputs)
print(output)
```


### Relevant log output

```shell
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-4-0938fdccd1fa> in <cell line: 24>()
     22 inputs = [x]
     23 
---> 24 output = model(*inputs)
     25 print(output)

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51   try:
     52     ctx.ensure_initialized()
---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                         inputs, attrs, num_outputs)
     55   except core._NotOkStatusException as e:

InvalidArgumentError: Exception encountered when calling RecurrentModel.call().

Detected unsupported operations when trying to compile graph __inference_call_877[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263] on XLA_GPU_JIT: CudnnRNNV3 (No registered 'CudnnRNNV3' OpKernel for XLA_GPU_JIT devices compatible with node {{node lstm_3_1/CudnnRNNV3}}){{node lstm_3_1/CudnnRNNV3}}
The op is created at: 
File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code
File ""/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py"", line 37, in <module>
File ""/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 992, in launch_instance
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 619, in start
File ""/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 195, in start
File ""/usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
File ""/usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once
File ""/usr/lib/python3.10/asyncio/events.py"", line 80, in _run
File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 685, in <lambda>
File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 738, in _run_callback
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 825, in inner
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 361, in process_one
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 261, in dispatch_shell
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 539, in execute_request
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 302, in do_execute
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 539, in run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 2975, in run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3030, in _run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code
File ""<ipython-input-4-0938fdccd1fa>"", line 24, in <cell line: 24>
File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 826, in __call__
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 1376, in _maybe_build
File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/core.py"", line 212, in compute_output_spec
File ""<ipython-input-1-0938fdccd1fa>"", line 13, in call
File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 901, in __call__
File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""/usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py"", line 46, in __call__
File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/lstm.py"", line 570, in call
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py"", line 406, in call
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/lstm.py"", line 537, in inner_loop
File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/rnn.py"", line 841, in lstm
File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/rnn.py"", line 933, in _cudnn_lstm
	tf2xla conversion failed while converting __inference_call_877[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions. [Op:__inference_call_877]

Arguments received by RecurrentModel.call():
  • x=tf.Tensor(shape=(10, 20, 1), dtype=float32)
```
",1
`tensorflow.experimental.numpy.kron` not working with multidimensional Arrays,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu

### Mobile device

_No response_

### Python version

3.10.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

This is a new issue in replacement for https://github.com/tensorflow/tensorflow/issues/59761 as suggested by @tilakrayal

I tested the function against numpy and it throws an error when the `ndim` of the input tensors is greater than 2.
I run the code on the latest TensorFlow version on PyPI and the nightly version, and I get the same failures.

Also, I am not getting as much debug information only this error

`UnimplementedError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Broadcast between [2,1,5,1,2,1,5,1] and [1,1,1,2,1,3,1,4] is not supported yet. [Op:Mul] name: `

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
try:
    a = tf.constant(np.arange(100).reshape(2, 5, 2, 5))
    b = tf.constant(np.arange(24).reshape(2, 3, 4))
    print(a.ndim) # 4
    print(b.ndim) # 3

    y = tf.experimental.numpy.kron(a, b)

    print(y.shape)
except:
    print(""Can't use tf.experimental.numpy.kron on multi-dimensional arrays"")

x = np.arange(100).reshape(2, 5, 2, 5)
y = np.arange(24).reshape(2, 3, 4)

print(x.ndim) # 4
print(y.ndim) # 3

z = np.kron(x, y)

print(z.shape) # (2, 10, 6, 20)
```


### Relevant log output

```shell
UnimplementedError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Broadcast between [2,1,5,1,2,1,5,1] and [1,1,1,2,1,3,1,4] is not supported yet. [Op:Mul] name:
```
",1
error message is inconsistent with documentation in `tf.raw_ops.MaxPoolGradWithArgmax`,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

According to [Doc tf.raw_ops.MaxPoolGradWithArgmax](https://www.tensorflow.org/api_docs/python/tf/raw_ops/MaxPoolGradWithArgmax), the argument `argmax` can be `int32` or `int64`. However, after actual testing, the parameter `argmax` can only support tensor input of data type `int64`.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input_tensor = tf.constant(1, shape=[1, 2, 2, 1], dtype=tf.float32)
grad_tensor = tf.constant(1, shape=[1, 1, 1, 1], dtype=tf.float32)
argmax_indices = tf.constant(1, shape=[1, 1, 1, 1], dtype=tf.int32)
ksize = [1, 2, 2, 1]
strides = [1, 1, 1, 1]
padding = ""VALID""

output_grad = tf.raw_ops.MaxPoolGradWithArgmax(
    input=input_tensor,
    grad=grad_tensor,
    argmax=argmax_indices,
    ksize=ksize,
    strides=strides,
    padding=padding,
    include_batch_in_index=False
)
```


### Relevant log output

```shell
NotFoundError: Could not find device for node: {{node MaxPoolGradWithArgmax}} = MaxPoolGradWithArgmax[T=DT_FLOAT, Targmax=DT_INT32, include_batch_in_index=false, ksize=[1, 2, 2, 1], padding=""VALID"", strides=[1, 1, 1, 1]]
All kernels registered for op MaxPoolGradWithArgmax:
  device='CPU'; T in [DT_DOUBLE]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_HALF]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_INT32]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_INT8]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_INT16]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_UINT32]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_INT64]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_UINT64]; Targmax in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]; Targmax in [DT_INT64]
  device='GPU'; T in [DT_FLOAT]; Targmax in [DT_INT64]
  device='GPU'; T in [DT_BFLOAT16]; Targmax in [DT_INT64]
  device='GPU'; T in [DT_HALF]; Targmax in [DT_INT64]
 [Op:MaxPoolGradWithArgmax] name:
```
",1
Division by zero error at random places if GPU is used,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

Linux, RedHatEnterprise 8.6

### Mobile device

_No response_

### Python version

N/A

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.3

### GPU model and memory

Quadro RTX 6000

### Current behavior?

(I do not use Python and found no nightly build for C API)
I have a simple program that builds graphs through the c_api and executes sessions. It works perfectly as long as it is run only on CPU. If GPU is involved then at random places the program generates a Div0 error. The very same program run twice within one minute on the same hardware, etc. behave differently.
The error is somewhere deep in TF/Cuda because in the error dump I see 15 program jumps outside my code (what I can still debug).
I tried it on the different partition of a HPC, but both behave the same way. Card is Quadro RTX 6000, driver 565.57.01 Cuda 12.7 as seen in nvidia-smi, but 12.3 is available as libraries.
I tried with many different settings, etc., but cannot identify any rootcause. I am not even sure if the bug is in the TF binary or in one of the Cuda libraries (or elsewhere).

### Standalone code to reproduce the issue

```shell
I use a Pascal program, called examples, available here: https://github.com/zsoltszakaly/tensorflowforpascal.
It is compiled on the HPC with fpc -MObjFPC -Sh -Fl../tensorflow/lib examples.pas.
The tensorflow/lib directory has
Jan  1  2000  libtensorflow_framework.so -> libtensorflow_framework.so.2
Dec 10 11:04  libtensorflow_framework.so.2 -> libtensorflow_framework.so.2.18.0
Jan  1  2000  libtensorflow_framework.so.2.18.0
Jan  1  2000  libtensorflow.so -> libtensorflow.so.2
Dec 10 11:00  libtensorflow.so.2 -> libtensorflow.so.2.18.0
Jan  1  2000  libtensorflow.so.2.18.0
```


### Relevant log output

```shell
It generates every time a session is run, correctly this:
I0000 00:00:1733834127.549506 1676833 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38484 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:41:00.0, compute capability: 8.0

But in the last one (again, it is random when!):
An unhandled exception occurred at $00007F223F261EA1:
EDivByZero: Division by zero
  $00007F223F261EA1
  $00007F223F1E2E72
  $00007F223F1E1D5D
  $00007F223F1E4183
  $00007F223F1E4AD2
  $00007F223F1E6DAE
  $00007F223F1D842B
  $00007F223F1D4241
  $00007F223EA97604
  $00007F223EA9608F
  $00007F223EA921E6
  $00007F223EA9067A
  $00007F223EA90231
  $00007F222E718923
  $00007F222E72561B
  $00000000004626AA
  $0000000000462931
```
",1
ValueError: as_list() is not defined on an unknown TensorShape.,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.6 LTS

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The created dataset's <class 'tensorflow.python.data.ops.prefetch_op._PrefetchDataset'> shape is unknown.
which when used for model.evaluate results in error:

```
self.model.evaluate(self.data_loader.get_valid_loader(batch_size), verbose=1)
  File ""/home/perfuser/shailesh/9_dec/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/perfuser/shailesh/9_dec/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
    ^^^^^^^^^^^^^^^
ValueError: as_list() is not defined on an unknown TensorShape.
```
Apologies for sharing partial code.
My question is: how can I set the shape of  tensorflow.python.data.ops.prefetch_op._PrefetchDataset dataset, or what is the solution to resolve the issue ""as_list() is not defined on an unknown TensorShape""?

The same dataset with an unknown shape worked with TensorFlow 2.13.

### Standalone code to reproduce the issue

```shell
ds_val = ds_val.map(lambda x: tf.py_function(self.read_nifti_file,
                                                     [x, False], [tf.float32, tf.float32]),
                            num_parallel_calls=tf.data.experimental.AUTOTUNE)
self.model.evaluate(self.data_loader.get_valid_loader(batch_size), verbose=1)
```
```


### Relevant log output

```shell
ds_val 3 <_PrefetchDataset element_spec=(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None), TensorSpec(shape=<unknown>, dtype=tf.float32, name=None))>

type(ds_val) <class 'tensorflow.python.data.ops.prefetch_op._PrefetchDataset'>
type(ds) <class 'tuple'>
shape(ds) (2, 64, 64, 64, 1)
type(ds) <class 'tuple'>
shape(ds) (2, 64, 64, 64, 1)
type(ds) <class 'tuple'>
shape(ds) (2, 64, 64, 64, 1)
type(ds) <class 'tuple'>
shape(ds) (2, 64, 64, 64, 1)
type(ds) <class 'tuple'>
shape(ds) (2, 64, 64, 64, 1)
type(ds) <class 'tuple'>
shape(ds) (2, 64, 64, 64, 1)
type(ds) <class 'tuple'>
shape(ds) (2, 64, 64, 64, 1)
type(ds) <class 'tuple'>
shape(ds) (2, 64, 64, 64, 1)
type(ds) <class 'tuple'>
shape(ds) (2, 64, 64, 64, 1)
```
",1
Update Python in Docker images to 3.11.x and ditch `3.11.0rc1`,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.18

### Custom code

No

### OS platform and distribution

Docker

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The Python version of the docker images is outdated and should be updated

### Standalone code to reproduce the issue

```shell
docker run -it tensorflow/tensorflow python
```


### Relevant log output

```shell
Python 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>>
```
",1
"The warning ""The structure of `inputs` doesn't match the expected structure"" when training a functional model","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v2.13.1-0-gf841394b1b7 2.13.1 (Nightly: v1.12.1-119104-gf8fd6f53fa3 2.19.0-dev20241204)

### Custom code

Yes

### OS platform and distribution

Windows 11 23H2 22631.4460

### Mobile device

Windows 11 23H2 22631.4460

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the model is functional, not Sequential, the warning has occured:

```
Epoch 1/5
<path-to-python>\lib\site-packages\keras\src\models\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor']. Received: the structure of inputs=*
  warnings.warn(
```

Yes, the warning message has interrupted on parenthesis. When I've run the same code in Nightly, the warning message is:

```
Epoch 1/5
<path-to-python>\lib\site-packages\keras\src\models\functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.
Expected: ['keras_tensor']
Received: inputs=Tensor(shape=(None, 10))
  warnings.warn(msg)
```

After the warning, the training continues normally, but because of this warning, I can't be sure that the model works as I expect.

I've traced the source and found that in `Lib\site-packages\keras\src\tree\optree_impl.py` on line 95 comparasion of expected and actual structure failed. Now I place the traced variables here:

```
>>> a
<tf.Tensor 'data:0' shape=(None, 10) dtype=float64>
>>> b
[<KerasTensor shape=(None, 10), dtype=float32, sparse=False, name=keras_tensor>]
>>> a_structure
PyTreeSpec(*, NoneIsLeaf)
>>> b_structure
PyTreeSpec([*], NoneIsLeaf)
```

The data passed to the `fit` function fully corresponds to the [documentation](https://keras.io/api/models/model_training_apis/#fit-method). The warning appears independently of whether I use numpy array or PyDataset as dataset of `fit` function.



### Standalone code to reproduce the issue

```shell
from keras.models import Model
from keras.layers import Dense, Input, Flatten, Concatenate
from keras import utils
import numpy as np
import tensorflow as tf

class SamplesSet(utils.PyDataset):
    
    def __init__(self, batch_size, **kwargs):
        super().__init__(**kwargs)
        self.batch_size = batch_size
        
    def __len__(self):
        return 1
    
    def __getitem__(self, idx):
        x1 = np.random.uniform(size=10*self.batch_size).reshape((self.batch_size, 10))
        y = np.arange(self.batch_size)
        return x1, y
    
train = SamplesSet(100)
x1_train = np.random.uniform(size=10*100).reshape((100, 10))
y_train = np.arange(100)

input1 = Input(shape=(10,))
l1 = Dense(1)(input1)
d2 = Dense(1, activation='sigmoid')(l1)
model = Model(inputs=[input1], outputs=[d2])
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

history = model.fit(x1_train, y_train, epochs=5, verbose=1)
# In the all cases below warning occures too
# history = Model.fit(train, epochs=5, verbose=1) 
# ret = model.predict(np.arange(10)[np.newaxis,:])
# ret = model.predict(tf.constant([[0,1,2,3,4,5,6,7,8,9]]))
```


### Relevant log output

_No response_",1
TensorFlow source code compilation error,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.18.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 24.04.1 LTS

### Mobile device

_No response_

### Python version

3.12.1

### Bazel version

7.3.1

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

My CPU supports AVX512F, I want to build it from source to support my CPU instruction set

### Standalone code to reproduce the issue

```shell
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such target '//tensorflow/tools/pip_package:build_pip_package': target 'build_pip_package' not declared in package 'tensorflow/tools/pip_package' defined by /usr/workerdir/workerspace/tensorflow/tensorflow/tools/pip_package/BUILD (did you mean 'build_pip_package.py'? Tip: use `query ""//tensorflow/tools/pip_package:*""` to see all the targets in that package)
WARNING: Target pattern parsing failed.
```


### Relevant log output

```shell
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such target '//tensorflow/tools/pip_package:build_pip_package': target 'build_pip_package' not declared in package 'tensorflow/tools/pip_package' defined by /usr/workerdir/workerspace/tensorflow/tensorflow/tools/pip_package/BUILD (did you mean 'build_pip_package.py'? Tip: use `query ""//tensorflow/tools/pip_package:*""` to see all the targets in that package)
WARNING: Target pattern parsing failed.
```
",1
[XLA] TF XLA outputs abnormal value when compiling `Embedding`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

nightly

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

For `Embedding` operator, when I set the `input_dim=1` (which means it indexed from 0 to 0), the output always returns **0 without XLA**.

After compilation, the outputs are usually some random tensors.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tf.random.set_seed(42)
x = tf.constant([1])


# uncompiled model
class Model(tf.keras.Model):

    def __init__(self):
        super(Model, self).__init__()
        self.embedding = tf.keras.layers.Embedding(1, 1)

    def call(self, x):
        output = self.embedding(x)
        return output


m = Model()

output1 = m(x)


# compiled model
class Model(tf.keras.Model):

    def __init__(self):
        super(Model, self).__init__()
        self.embedding = tf.keras.layers.Embedding(1, 1)

    @tf.function(jit_compile=True)
    def call(self, x):
        output = self.embedding(x)
        return output


m = Model()
output2 = m(x)

print(output1)
print(output2)
```


### Relevant log output

```shell
tf.Tensor([[0.]], shape=(1, 1), dtype=float32)
tf.Tensor([[-0.00567592]], shape=(1, 1), dtype=float32)
```
",1
Potential Remote Code Execution (RCE) Vulnerability in Custom Layers Handling,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

 2.18.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

Kali Linux 2024.1

### Python version

Python 3.11.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA: 11.2 

### GPU model and memory

NVIDIA GTX 1080

### Current behavior?

Currently, TensorFlow (primarily through Keras) allows users to create custom layers that can contain malicious code. When a model containing these custom layers is loaded or run, the malicious code in the layer can be executed without any restrictions or filtering. This opens up potential exploits because the executed code could potentially damage the system or steal sensitive data. For example, in this report, the saved model contains a MaliciousLayer layer that calls the os.system() system command to execute malicious code when the model is loaded, which could result in corruption or unauthorized data acquisition.



I hope TensorFlow can provide protection that limits or filters the execution of malicious code in registered custom layers. For example, by blocking malicious functions such as os.system() or prohibiting the use of code that can execute external commands when the model is loaded. In addition, the use of custom layers that contain unwanted external code should be protected by restrictions or sandboxing. This will ensure that the loaded model cannot inject malicious code into the user's system, thereby increasing the security of using TensorFlow models.

The main issue here is that TensorFlow (in this case Keras) allows users to define custom layers that can execute malicious code when the model is loaded and used. This is a potential security hole that can be exploited if malicious code is injected into the model.

The challenge is the ability to include custom layers that execute malicious code. TensorFlow/Keras does not filter or restrict the use of malicious code in registered custom layers, which allows exploits like this.

So, the vulnerability is in the implementation of custom layers in TensorFlow/Keras

Who can exploit the vulnerability?

An attacker or malicious individual who gains access to a TensorFlow model saved in the .keras format and has knowledge of how to create malicious custom layers can exploit this vulnerability. Typically, this would involve actors with an understanding of TensorFlow programming or machine learning who can inject harmful code into the model to be executed when the model is loaded.

What do they gain from it?

An attacker who successfully exploits this vulnerability can execute malicious code on the victim's system when the model is loaded. This allows them to steal sensitive personal data, such as login credentials, financial data, or other private information. Additionally, they can leverage remote code execution to gain control of the system or cause further dama

### Standalone code to reproduce the issue

```shell
1.Create a Python file, for example kontol13.py, containing the following code to create a model with a malicious layer (MaliciousLayer):


import tensorflow as tf
from tensorflow.keras.layers import Layer
import os

# Defines a custom layer containing malicious code
@tf.keras.utils.register_keras_serializable()
class MaliciousLayer(Layer):
    def __init__(self, **kwargs):
        super(MaliciousLayer, self).__init__(**kwargs)

    def call(self, inputs):
        # Code that is executed when the model runs
        os.system('echo ""Malicious Code Executed!""')
        return inputs

# Creating a model with dangerous layers
model = tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=(10,)),
    tf.keras.layers.Dense(10, activation='relu'),
    MaliciousLayer(),  # Layer berbahaya
    tf.keras.layers.Dense(1)
])

# Save the model in Keras format (.keras)
model.save('kontol13.keras')

print(""Malicious model created and saved as 'kontol13.keras'."")
```
2.Run the above script to create and save the model containing the malicious layers

I also saved the result in google colab:

https://colab.research.google.com/drive/1IwpwNGOeTPUgYu4Y4bAl7WufOXBvOQy9?authuser=1#scrollTo=EYbfvtj6O6wU&line=1&uniqifier=1
```


### Relevant log output

```shell
I run this code on kali linux this is the output

2024-12-04 17:45:53.327622: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() are called are written to STDERR
E0000 00:00:1733309153.640400 154 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1733309153.730398 154 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-12-04 17:45:54.592724: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/funsociety/.local/lib/python3.11/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.
 warnings.warn(
2024-12-04 17:46:03.552814: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)
Malicious Code Executed!
```
",1
Are checkpoints broken in >= 2.16?,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.16, 2.17

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The example given in https://www.tensorflow.org/guide/checkpoint does not seem to work as expected in 2.16 and 2.17, while working fine in 2.15. After restoring and restarting the training process, it starts training from the very beginning.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1n76Mu5BhdBJBSXc7cXYJMr0lMDER2JRa?usp=sharing
```


### Relevant log output

_No response_",1
How to load partial parameters from subclass model in TF2?,"Hi, all, 
    I have constructed a keras model with subclass mode in TensorFlow2.x, and trained it with a dataset named dataset1(which have 4000 classification, namely the vocabulary size is 4000), and save the checkpoint with tf.train.Checkpoint and tf.train.CheckpointManager. And now I want to finetune it with a new dataset named dataset2(which have 500 classification, the vocabulary size is 500), when directely restore the parameters from the saved checkpoint, it will fail because of the mismatch shape (ie, the embedding layer, the output layer of encoder and decoder etc).

   So, how to restore partial parameter from the checkpoint model, or ignore the shape mismatch parameters when restore from the checkpoint. Here is a simplified example, 

	# custom design layer
	class CustomLayer(tf.keras.layers.Layer):
	    def __init__(self, unit1, unit2, **kwargs):
	        super(CustomLayer, self).__init__(**kwargs)
	        self.dense1 = tf.keras.layers.Dense(unit1, activation='relu')
	        self.dense2 = tf.keras.layers.Dense(unit2, activation='softmax')
	 
	    def call(self, inputs):
	        x = self.dense1(inputs)
	        return self.dense2(x)

	# keras model
	class CustomModel(tf.keras.Model):
	    def __init__(self, unit1, unit2):
	        super(SourceModel, self).__init__()
	        self.layer = tf.keras.layers.Dense(128, activation='relu')
	        self.custom_layer = CustomLayer(64, 10)
	 
	    def call(self, inputs):
	        x = self.layer(inputs)
	        x = self.custom_layer(x)
	        return x

	# original model
        old_model = CustomModel(64, 10)
        optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.9, beta_2=0.98, epsilon=1e-9)
        ckpt = tf.train.Checkpoint(step=tf.Variable(0), model=old_model, optimizer=optimizer)
        for sample in dataset1:
                training and save checkpoint ......

        # new model
        new_model = CustomModel(64, 5)
        **how to restore partial parameters from the checkpoint saved before and start a new train in the dastaset2?**

",0
Clarify the `constant_op.constant(2)` statement,"It would be helpful to clarify the `constant_op.constant(2)` statement by explaining the corresponding import statement.

https://github.com/tensorflow/tensorflow/blob/5bc9d26649cca274750ad3625bd93422617eed4b/tensorflow/python/ops/summary_ops_v2.py#L1062-L1066",0
band width calculation support for ddr5,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

master

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The bandwidth is calculated by the following code
// 8 is the number of bits per byte. 2 is accounted for
// double data rate (DDR).
  device.set_bandwidth(properties.memoryBusWidth / 8 *
                       properties.memoryClockRate * 2ULL);


https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/clusters/utils.cc#L132

Similar code also exists in XLA：

if (mem_clock_khz && mem_bus_width_bits) {
      // Times 2 because HBM is DDR memory; it gets two data bits per each
      // data lane.
      auto memory_bandwidth =
          uint64_t{2} * (*mem_clock_khz) * 1000 * (*mem_bus_width_bits) / 8;
      device_plane->AddStatValue(
          *device_plane->GetOrCreateStatMetadata(
              GetStatTypeStr(StatType::kDevCapMemoryBandwidth)),
          memory_bandwidth);
    }

https://github.com/openxla/xla/blob/main/xla/backends/profiler/gpu/cupti_collector.cc#L433
https://github.com/openxla/xla/blob/main/xla/backends/profiler/gpu//rocm_collector.cc#L531

but for DDR5 should times 4, Is there any way to distinguish it and fix it?

### Standalone code to reproduce the issue

```shell
// 8 is the number of bits per byte. 2 is accounted for
// double data rate (DDR).
device.set_bandwidth(properties.memoryBusWidth / 8 *
                       properties.memoryClockRate * 2ULL);

to

// 8 is the number of bits per byte. 2 is accounted for
// double data rate (DDR).
device.set_bandwidth(properties.memoryBusWidth / 8 *
                       properties.memoryClockRate * 4ULL);


for ddr5
```


### Relevant log output

_No response_",1
XLA recompilation every time when shape changed on GPU,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf2.18

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have seen a lot of dynamic dim configurations in the OpenXLA repo. I am using TF GPU-XLA. How should I configure it so that recompilation is not triggered during GPU dynamic dim?

### Standalone code to reproduce the issue

```shell
@tf.function(input_signature=[tf.TensorSpec(shape=[2, None], dtype=tf.float32),
                              tf.TensorSpec(shape=[None, 2], dtype=tf.float32),
                              tf.TensorSpec(shape=[], dtype=tf.float32),],
             jit_compile=True)
def foo(x, y, z):
    exp = tf.add(x, z)
    abs = tf.sin(exp)
    add = tf.add(abs, tf.transpose(y, ))
    mm = tf.matmul(add, y)
    return mm

x = tf.ones([2, 3]).gpu()
y = tf.ones([3, 2]).gpu()
z = tf.ones([]).gpu()

out = foo(x, y, z) # Compile

x = tf.ones([2, 4]).gpu()
y = tf.ones([4, 2]).gpu()
z = tf.ones([]).gpu()

out = foo(x, y, z)  # Compile

x = tf.ones([2, 5]).gpu()
y = tf.ones([5, 2]).gpu()
z = tf.ones([]).gpu()

out = foo(x, y, z)  # Compile
```


### Relevant log output

_No response_",1
"Unspecific error message ""None values not supported"" when no labels are provided in dataset","### Issue type

Documentation Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

If I write some tf/keras code that fails to include a label in the dataset, the resulting error message is:  `ValueError: None values not supported.`

The backtrace leads me through `optree/ops.py`.

It's not clear from the message what the `None` values are referring to, nor does the backtrace give me any useful information (as far as I can tell) about the source of the error. I posted this issue here: https://stackoverflow.com/q/79236884/1613983

If the error message could instead make reference to a missing labels set, null `y_true`, etc. that would make it much easier to figure out what's going on.

Somewhat ironically, I ended up solving it after asking GPT o1 about the problem, and it got the answer in one go.

### Standalone code to reproduce the issue

```shell
from tensorflow.keras import layers, Model, Input
from tensorflow.keras.models import Sequential
import pandas as pd
import numpy as np
import tensorflow as tf

input_data = pd.DataFrame(np.random.rand(1000, 10))

data = tf.data.Dataset.zip({'input_values' : tf.data.Dataset.from_tensor_slices(input_data.values)})

batch_size = 100
train_split = 0.8

train_rows = int(train_split * input_data.shape[0])
train_dataset = data.take(train_rows)
validation_dataset = data.skip(train_rows)

train_data_batched = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)
validation_data_batched = validation_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)

num_outputs = 10

input_layer = Input(shape=(num_outputs,), name=f'input_values')
output = layers.Dense(num_outputs, activation='sigmoid', name='output')(input_layer)

# Define the model
model = Model(
    inputs=[input_layer],
    outputs=output,
)

max_epochs = 10

def loss(y_true, y_pred):
    return 2.0

model.compile(
    loss=loss,
    optimizer='adam',
)

history = model.fit(
    train_data_batched,
    epochs=max_epochs,
    validation_data=validation_data_batched
)
```


### Relevant log output

```shell
File /tmp/virtualenvs/python3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--> 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

File /tmp/virtualenvs/python3.11/lib/python3.11/site-packages/optree/ops.py:752, in tree_map(func, tree, is_leaf, none_is_leaf, namespace, *rests)
    750 leaves, treespec = _C.flatten(tree, is_leaf, none_is_leaf, namespace)
    751 flat_args = [leaves] + [treespec.flatten_up_to(r) for r in rests]
--> 752 return treespec.unflatten(map(func, *flat_args))

ValueError: None values not supported.
```
",1
MixedPrecision + XLA: Seen floating point types of different precisions,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18.0

### Custom code

No

### OS platform and distribution

Google Colab

### Mobile device

_No response_

### Python version

Google Colab default

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When using bilinear interpolation + XLA + mixed_float16 policy issue raises during compilation.
Without bilinear interpolation or without XLA or without mixed_float16 there is no issue.

In Google Colab i have this issue only on CPU with TF 2.17 and both CPU & GPU with TF 2.18.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1joSiScbM7Stc9bn1C4R_4sFkDzakrTsJ?usp=sharing
```


### Relevant log output

```shell
---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
<ipython-input-4-9cc273be2d5a> in <cell line: 28>()
     26 model.compile(loss='mse', optimizer='adam', run_eagerly=False, jit_compile=True)
     27 
---> 28 model.fit(dataset)

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51   try:
     52     ctx.ensure_initialized()
---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                         inputs, attrs, num_outputs)
     55   except core._NotOkStatusException as e:

InternalError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main

  File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code

  File ""/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py"", line 37, in <module>

  File ""/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 992, in launch_instance

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 619, in start

  File ""/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 195, in start

  File ""/usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever

  File ""/usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once

  File ""/usr/lib/python3.10/asyncio/events.py"", line 80, in _run

  File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 685, in <lambda>

  File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 738, in _run_callback

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 825, in inner

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 377, in dispatch_queue

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 250, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 748, in __init__

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 361, in process_one

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 261, in dispatch_shell

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 539, in execute_request

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 302, in do_execute

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 539, in run_cell

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 2975, in run_cell

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3030, in _run_cell

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code

  File ""<ipython-input-4-9cc273be2d5a>"", line 28, in <cell line: 28>

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 368, in fit

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 216, in function

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 129, in multi_step_on_iterator

during context [Unknown]: Seen floating point types of different precisions in %multiply.43589 = f32[2,8,8,1280]{3,2,1,0} multiply(f32[2,8,8,1280]{3,2,1,0} %add.43539, f16[2,8,8,1280]{3,2,1,0} %multiply.43588), metadata={op_type=""Mul"" op_name=""mul_9"" source_file=""/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py"" source_line=1196}, but mixed precision is disallowed.
	 [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_124474]
```
",1
"Very serious! Using this method will definitely result in memory leaks, I hope you can provide support","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

ubuntu 2.2 or mac m1

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have tried various methods, but the memory is definitely leaking, it seems that the release of memory cannot keep up. Through the logs, it can be found that there is periodic memory recycling, but with the increase of time, there is still a clear upward trend

### Standalone code to reproduce the issue

```shell
import gc

import keras
import numpy as np
import psutil
from keras.optimizers import Adam
from keras.layers import Dense, Dropout, Input, LSTM
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import time
import json


num_samples = 6
num_features = 3
num_classes = 4
epochs = 50
batch_size = 2
identifier = ""test_model""
num_iterations = 500  

def build_model(X, num_classes):
    model = Sequential()
    model.add(Input(shape=(X.shape[1], X.shape[2])))
    model.add(LSTM(16, return_sequences=True))
    model.add(LSTM(16))
    model.add(Dropout(0.4))
    model.add(Dense(8, activation='tanh'))
    model.add(Dense(num_classes, activation='softmax'))

    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy'])
    return model


data_X = np.random.rand(num_samples, num_features)
data_Y = np.random.randint(0, num_classes, size=(num_samples, 1))  


data_Y = np.eye(num_classes)[data_Y.flatten()]  
print(type(data_X))

scaler = MinMaxScaler()
data_X_scaled = scaler.fit_transform(data_X)


train_X, test_X, train_Y, test_Y = train_test_split(data_X_scaled, data_Y, train_size=0.6, random_state=42)


train_X = np.expand_dims(train_X, axis=1)
test_X = np.expand_dims(test_X, axis=1)

for iteration in range(num_iterations):
   
    tf.keras.backend.clear_session()
    tf.compat.v1.reset_default_graph()
    model = build_model(train_X, num_classes)
 
    model_name = f""model_{iteration}""
    early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=0, restore_best_weights=True)
    print(f""Iteration {iteration + 1}/{num_iterations}"")
    process = psutil.Process()
    mem_info = process.memory_info()
    print(f""start Current memory usage: {mem_info.rss / (1024 * 1024):.2f} MB"")  # RSS - Resident Set Size
    try:
        history = model.fit(train_X, train_Y, epochs=epochs, batch_size=batch_size, shuffle=True,
                            validation_data=(test_X, test_Y), verbose=0)
        print(f""Training model: {model.name}"")
        del model
        tf.keras.backend.clear_session()
        gc.collect()
    except Exception as e:
        print(""err:"", e)
    finally:
        process = psutil.Process()
        mem_info = process.memory_info()
        print(f""end Current memory usage: {mem_info.rss / (1024 * 1024):.2f} MB"")  # RSS - Resident Set Size

print(""end！"")
```


### Relevant log output

```shell
Iteration 1/500
start Current memory usage: 450.69 MB
Training model: sequential
end Current memory usage: 524.41 MB
Iteration 2/500
start Current memory usage: 524.52 MB
Training model: sequential
end Current memory usage: 564.97 MB
Iteration 3/500
start Current memory usage: 564.98 MB
Training model: sequential
end Current memory usage: 598.00 MB
Iteration 4/500
start Current memory usage: 598.03 MB
Training model: sequential
end Current memory usage: 624.69 MB
Iteration 5/500
start Current memory usage: 624.69 MB
Training model: sequential
end Current memory usage: 653.89 MB
Iteration 6/500
start Current memory usage: 653.91 MB
Training model: sequential
end Current memory usage: 679.45 MB
Iteration 7/500
start Current memory usage: 679.45 MB
Training model: sequential
end Current memory usage: 701.59 MB
Iteration 8/500
start Current memory usage: 701.59 MB
Training model: sequential
end Current memory usage: 726.83 MB
Iteration 9/500
start Current memory usage: 726.84 MB
Training model: sequential
end Current memory usage: 749.56 MB
Iteration 10/500
start Current memory usage: 749.56 MB
Training model: sequential
end Current memory usage: 782.56 MB
Iteration 11/500
start Current memory usage: 782.56 MB
Training model: sequential
end Current memory usage: 805.92 MB
Iteration 12/500
start Current memory usage: 805.92 MB
Training model: sequential
end Current memory usage: 833.17 MB
Iteration 13/500
start Current memory usage: 833.17 MB
Training model: sequential
end Current memory usage: 852.84 MB
Iteration 14/500
start Current memory usage: 852.84 MB
Training model: sequential
end Current memory usage: 875.05 MB
Iteration 15/500
start Current memory usage: 875.06 MB
Training model: sequential
end Current memory usage: 901.56 MB
Iteration 16/500
start Current memory usage: 901.56 MB
Training model: sequential
end Current memory usage: 930.62 MB
Iteration 17/500
start Current memory usage: 705.70 MB
Training model: sequential
end Current memory usage: 762.64 MB
Iteration 18/500
start Current memory usage: 762.70 MB
Training model: sequential
end Current memory usage: 798.06 MB
Iteration 19/500
start Current memory usage: 798.17 MB
Training model: sequential
end Current memory usage: 824.98 MB
Iteration 20/500
start Current memory usage: 824.98 MB
Training model: sequential
end Current memory usage: 850.34 MB
Iteration 21/500
start Current memory usage: 850.42 MB
Training model: sequential
end Current memory usage: 876.81 MB
Iteration 22/500
start Current memory usage: 876.81 MB
Training model: sequential
end Current memory usage: 904.02 MB
Iteration 23/500
start Current memory usage: 904.08 MB
Training model: sequential
end Current memory usage: 929.70 MB
Iteration 24/500
start Current memory usage: 929.73 MB
Training model: sequential
end Current memory usage: 952.33 MB
Iteration 25/500
start Current memory usage: 952.34 MB
Training model: sequential
end Current memory usage: 952.28 MB
Iteration 26/500
start Current memory usage: 952.47 MB
Training model: sequential
end Current memory usage: 980.39 MB
Iteration 27/500
start Current memory usage: 978.78 MB
Training model: sequential
end Current memory usage: 999.02 MB
Iteration 28/500
start Current memory usage: 999.05 MB
Training model: sequential
end Current memory usage: 1023.50 MB
Iteration 29/500
start Current memory usage: 1023.53 MB
Training model: sequential
end Current memory usage: 1047.80 MB
Iteration 30/500
start Current memory usage: 1047.83 MB
Training model: sequential
end Current memory usage: 1068.88 MB
Iteration 31/500
start Current memory usage: 1068.94 MB
Training model: sequential
end Current memory usage: 1095.78 MB
Iteration 32/500
start Current memory usage: 1095.78 MB
Training model: sequential
end Current memory usage: 1119.03 MB
Iteration 33/500
start Current memory usage: 1119.03 MB
Training model: sequential
end Current memory usage: 1039.41 MB
Iteration 34/500
start Current memory usage: 1022.78 MB
Training model: sequential
end Current memory usage: 1040.88 MB
Iteration 35/500
start Current memory usage: 1040.70 MB
Training model: sequential
end Current memory usage: 1054.58 MB
Iteration 36/500
start Current memory usage: 1054.58 MB
Training model: sequential
end Current memory usage: 1076.16 MB
Iteration 37/500
start Current memory usage: 1076.19 MB
Training model: sequential
end Current memory usage: 1097.02 MB
Iteration 38/500
start Current memory usage: 1097.03 MB
Training model: sequential
end Current memory usage: 1113.70 MB
Iteration 39/500
start Current memory usage: 1114.12 MB
Training model: sequential
end Current memory usage: 1140.30 MB
Iteration 40/500
start Current memory usage: 1140.33 MB
Training model: sequential
end Current memory usage: 1163.81 MB
Iteration 41/500
start Current memory usage: 1163.86 MB
Training model: sequential
end Current memory usage: 1195.83 MB
Iteration 42/500
start Current memory usage: 1195.83 MB
Training model: sequential
end Current memory usage: 1221.53 MB
Iteration 43/500
start Current memory usage: 1221.55 MB
Training model: sequential
end Current memory usage: 1231.09 MB
Iteration 44/500
start Current memory usage: 1231.14 MB
Training model: sequential
end Current memory usage: 1245.78 MB
Iteration 45/500
start Current memory usage: 1199.55 MB
Training model: sequential
end Current memory usage: 1221.59 MB
Iteration 46/500
start Current memory usage: 1221.59 MB
Training model: sequential
end Current memory usage: 1249.11 MB
Iteration 47/500
start Current memory usage: 1249.22 MB
Training model: sequential
end Current memory usage: 1275.50 MB
Iteration 48/500
start Current memory usage: 1259.83 MB
Training model: sequential
end Current memory usage: 1290.91 MB
Iteration 49/500
start Current memory usage: 1285.67 MB
Training model: sequential
end Current memory usage: 1296.75 MB
Iteration 50/500
start Current memory usage: 1296.75 MB
Training model: sequential
end Current memory usage: 1306.59 MB
Iteration 51/500
start Current memory usage: 1306.59 MB
Training model: sequential
end Current memory usage: 1287.53 MB
Iteration 52/500
start Current memory usage: 1287.53 MB
Training model: sequential
end Current memory usage: 1297.23 MB
Iteration 53/500
start Current memory usage: 1297.25 MB
Training model: sequential
end Current memory usage: 1285.45 MB
Iteration 54/500
start Current memory usage: 1285.45 MB
Training model: sequential
end Current memory usage: 1290.36 MB
Iteration 55/500
start Current memory usage: 1282.14 MB
Training model: sequential
end Current memory usage: 1302.14 MB
Iteration 56/500
start Current memory usage: 1302.14 MB
Training model: sequential
end Current memory usage: 1287.70 MB
Iteration 57/500
start Current memory usage: 1287.75 MB
Training model: sequential
end Current memory usage: 1282.77 MB
Iteration 58/500
start Current memory usage: 1271.38 MB
Training model: sequential
end Current memory usage: 1232.14 MB
Iteration 59/500
start Current memory usage: 1212.70 MB
Training model: sequential
end Current memory usage: 1201.16 MB
Iteration 60/500
start Current memory usage: 1200.53 MB
Training model: sequential
end Current memory usage: 1169.45 MB
Iteration 61/500
start Current memory usage: 1169.45 MB
Training model: sequential
end Current memory usage: 1209.73 MB
Iteration 62/500
start Current memory usage: 1207.19 MB
Training model: sequential
end Current memory usage: 1226.28 MB
Iteration 63/500
start Current memory usage: 1226.28 MB
Training model: sequential
end Current memory usage: 1231.45 MB
Iteration 64/500
start Current memory usage: 1210.11 MB
Training model: sequential
end Current memory usage: 1176.00 MB
Iteration 65/500
start Current memory usage: 1173.97 MB
Training model: sequential
end Current memory usage: 1201.42 MB
Iteration 66/500
start Current memory usage: 1201.42 MB
Training model: sequential
end Current memory usage: 1223.94 MB
Iteration 67/500
start Current memory usage: 1222.50 MB
Training model: sequential
end Current memory usage: 1229.80 MB
Iteration 68/500
start Current memory usage: 1227.14 MB
Training model: sequential
end Current memory usage: 1219.02 MB
Iteration 69/500
start Current memory usage: 1210.48 MB
Training model: sequential
end Current memory usage: 1247.17 MB
Iteration 70/500
start Current memory usage: 1245.94 MB
Training model: sequential
end Current memory usage: 1259.84 MB
Iteration 71/500
start Current memory usage: 1259.86 MB
Training model: sequential
end Current memory usage: 1286.39 MB
Iteration 72/500
start Current memory usage: 1286.53 MB
Training model: sequential
end Current memory usage: 1316.52 MB
Iteration 73/500
start Current memory usage: 1311.53 MB
Training model: sequential
end Current memory usage: 1338.72 MB
Iteration 74/500
start Current memory usage: 1338.75 MB
Training model: sequential
end Current memory usage: 1348.45 MB
Iteration 75/500
start Current memory usage: 1338.30 MB
Training model: sequential
end Current memory usage: 1354.97 MB
Iteration 76/500
start Current memory usage: 1353.83 MB
Training model: sequential
end Current memory usage: 1385.67 MB
Iteration 77/500
start Current memory usage: 1385.69 MB
Training model: sequential
end Current memory usage: 1408.83 MB
Iteration 78/500
start Current memory usage: 1408.88 MB
Training model: sequential
end Current memory usage: 1430.91 MB
Iteration 79/500
start Current memory usage: 1430.94 MB
Training model: sequential
end Current memory usage: 1443.62 MB
Iteration 80/500
start Current memory usage: 1428.00 MB
Training model: sequential
end Current memory usage: 1436.50 MB
Iteration 81/500
start Current memory usage: 1436.64 MB
Training model: sequential
end Current memory usage: 1454.66 MB
Iteration 82/500
start Current memory usage: 1440.91 MB
Training model: sequential
end Current memory usage: 1461.81 MB
Iteration 83/500
start Current memory usage: 1460.47 MB
Training model: sequential
end Current memory usage: 1481.19 MB
Iteration 84/500
start Current memory usage: 1481.19 MB
Training model: sequential
end Current memory usage: 1477.84 MB
Iteration 85/500
start Current memory usage: 1477.84 MB
Training model: sequential
end Current memory usage: 1493.55 MB
Iteration 86/500
start Current memory usage: 1493.58 MB
Training model: sequential
end Current memory usage: 1509.50 MB
Iteration 87/500
start Current memory usage: 1509.50 MB
Training model: sequential
end Current memory usage: 1543.94 MB
Iteration 88/500
start Current memory usage: 1542.83 MB
Training model: sequential
end Current memory usage: 1516.17 MB
Iteration 89/500
start Current memory usage: 1516.20 MB
Training model: sequential
end Current memory usage: 1470.17 MB
Iteration 90/500
start Current memory usage: 1470.22 MB
Training model: sequential
end Current memory usage: 1443.72 MB
Iteration 91/500
start Current memory usage: 1444.36 MB
Training model: sequential
end Current memory usage: 1486.23 MB
Iteration 92/500
start Current memory usage: 1476.41 MB
Training model: sequential
end Current memory usage: 1524.97 MB
Iteration 93/500
start Current memory usage: 1524.97 MB
Training model: sequential
end Current memory usage: 1534.94 MB
Iteration 94/500
start Current memory usage: 1551.98 MB
Training model: sequential
end Current memory usage: 1853.48 MB
Iteration 95/500
start Current memory usage: 1853.48 MB
Training model: sequential
end Current memory usage: 1790.12 MB
Iteration 96/500
start Current memory usage: 1792.27 MB
Training model: sequential
end Current memory usage: 1883.20 MB
Iteration 97/500
start Current memory usage: 1879.05 MB
Training model: sequential
end Current memory usage: 1759.69 MB
Iteration 98/500
start Current memory usage: 1669.66 MB
Training model: sequential
end Current memory usage: 1596.77 MB
Iteration 99/500
start Current memory usage: 1597.12 MB
Training model: sequential
end Current memory usage: 1568.83 MB
Iteration 100/500
start Current memory usage: 1532.98 MB
Training model: sequential
end Current memory usage: 1516.75 MB
Iteration 101/500
start Current memory usage: 1465.98 MB
Training model: sequential
end Current memory usage: 1486.66 MB
Iteration 102/500
start Current memory usage: 1483.34 MB
Training model: sequential
end Current memory usage: 1523.19 MB
Iteration 103/500
start Current memory usage: 1523.14 MB
Training model: sequential
end Current memory usage: 1532.77 MB
Iteration 104/500
start Current memory usage: 1531.14 MB
Training model: sequential
end Current memory usage: 1561.78 MB
Iteration 105/500
start Current memory usage: 1555.67 MB
Training model: sequential
end Current memory usage: 1586.70 MB
Iteration 106/500
start Current memory usage: 1586.75 MB
Training model: sequential
end Current memory usage: 1608.41 MB
Iteration 107/500
start Current memory usage: 1603.81 MB
Training model: sequential
end Current memory usage: 1629.00 MB
Iteration 108/500
start Current memory usage: 1629.05 MB
Training model: sequential
end Current memory usage: 1609.25 MB
Iteration 109/500
start Current memory usage: 1609.31 MB
Training model: sequential
end Current memory usage: 1630.09 MB
Iteration 110/500
start Current memory usage: 1629.20 MB
Training model: sequential
end Current memory usage: 1638.66 MB
Iteration 111/500
start Current memory usage: 1620.30 MB
Training model: sequential
end Current memory usage: 1642.81 MB
Iteration 112/500
start Current memory usage: 1642.94 MB
Training model: sequential
end Current memory usage: 1659.45 MB
Iteration 113/500
start Current memory usage: 1655.17 MB
Training model: sequential
end Current memory usage: 1687.80 MB
Iteration 114/500
start Current memory usage: 1673.33 MB
Training model: sequential
end Current memory usage: 1705.94 MB
Iteration 115/500
start Current memory usage: 1699.95 MB
Training model: sequential
end Current memory usage: 1708.22 MB
Iteration 116/500
start Current memory usage: 1707.88 MB
Training model: sequential
end Current memory usage: 1648.23 MB
Iteration 117/500
start Current memory usage: 1634.03 MB
Training model: sequential
end Current memory usage: 1670.97 MB
Iteration 118/500
start Current memory usage: 1671.97 MB
Training model: sequential
end Current memory usage: 1649.69 MB
Iteration 119/500
start Current memory usage: 1645.14 MB
Training model: sequential
end Current memory usage: 1698.64 MB
Iteration 120/500
start Current memory usage: 1699.69 MB
Training model: sequential
end Current memory usage: 1737.67 MB
Iteration 121/500
start Current memory usage: 1737.67 MB
Training model: sequential
end Current memory usage: 1738.05 MB
Iteration 122/500
start Current memory usage: 1721.47 MB
Training model: sequential
end Current memory usage: 1730.64 MB
Iteration 123/500
start Current memory usage: 1729.53 MB
Training model: sequential
end Current memory usage: 1766.12 MB
Iteration 124/500
start Current memory usage: 1761.22 MB
Training model: sequential
end Current memory usage: 1796.58 MB
Iteration 125/500
start Current memory usage: 1796.73 MB
Training model: sequential
end Current memory usage: 1709.02 MB
Iteration 126/500
start Current memory usage: 1721.67 MB
Training model: sequential
end Current memory usage: 1771.50 MB
Iteration 127/500
start Current memory usage: 1771.50 MB
Training model: sequential
end Current memory usage: 1777.38 MB
Iteration 128/500
start Current memory usage: 1757.58 MB
Training model: sequential
end Current memory usage: 1806.50 MB
Iteration 129/500
start Current memory usage: 1758.81 MB
Training model: sequential
end Current memory usage: 1812.45 MB
Iteration 130/500
start Current memory usage: 1812.86 MB
Training model: sequential
end Current memory usage: 1811.14 MB
Iteration 131/500
start Current memory usage: 1799.61 MB
Training model: sequential
end Current memory usage: 1835.33 MB
Iteration 132/500
start Current memory usage: 1716.38 MB
Training model: sequential
end Current memory usage: 1759.75 MB
Iteration 133/500
start Current memory usage: 1752.44 MB
Training model: sequential
end Current memory usage: 1818.41 MB
Iteration 134/500
start Current memory usage: 1811.42 MB
Training model: sequential
end Current memory usage: 1853.58 MB
Iteration 135/500
start Current memory usage: 1853.70 MB
Training model: sequential
end Current memory usage: 1858.50 MB
Iteration 136/500
start Current memory usage: 1858.56 MB
Training model: sequential
end Current memory usage: 1874.84 MB
Iteration 137/500
start Current memory usage: 1862.92 MB
Training model: sequential
end Current memory usage: 1768.23 MB
Iteration 138/500
start Current memory usage: 1762.73 MB
Training model: sequential
end Current memory usage: 1843.39 MB
Iteration 139/500
start Current memory usage: 1843.52 MB
Training model: sequential
end Current memory usage: 1885.88 MB
Iteration 140/500
start Current memory usage: 1885.95 MB
Training model: sequential
end Current memory usage: 1924.86 MB
Iteration 141/500
start Current memory usage: 1925.05 MB
Training model: sequential
end Current memory usage: 1946.80 MB
Iteration 142/500
start Current memory usage: 1946.69 MB
Training model: sequential
end Current memory usage: 1977.53 MB
Iteration 143/500
start Current memory usage: 1974.27 MB
Training model: sequential
end Current memory usage: 1995.17 MB
Iteration 144/500
start Current memory usage: 1992.41 MB
Training model: sequential
end Current memory usage: 1984.45 MB
Iteration 145/500
start Current memory usage: 1963.42 MB
Training model: sequential
end Current memory usage: 1947.31 MB
Iteration 146/500
start Current memory usage: 1944.47 MB
Training model: sequential
end Current memory usage: 1996.00 MB
Iteration 147/500
start Current memory usage: 1996.08 MB
Training model: sequential
end Current memory usage: 2008.41 MB
Iteration 148/500
start Current memory usage: 1999.69 MB
Training model: sequential
end Current memory usage: 1951.30 MB
Iteration 149/500
start Current memory usage: 1942.98 MB
Training model: sequential
end Current memory usage: 1992.28 MB
Iteration 150/500
start Current memory usage: 1982.86 MB
Training model: sequential
end Current memory usage: 2008.83 MB
Iteration 151/500
start Current memory usage: 2008.83 MB
Training model: sequential
end Current memory usage: 1946.42 MB
Iteration 152/500
start Current memory usage: 1946.92 MB
Training model: sequential
end Current memory usage: 1992.48 MB
Iteration 153/500
start Current memory usage: 1979.52 MB
Training model: sequential
end Current memory usage: 2035.66 MB
Iteration 154/500
start Current memory usage: 2023.91 MB
Training model: sequential
end Current memory usage: 2030.31 MB
Iteration 155/500
start Current memory usage: 1974.39 MB
Training model: sequential
end Current memory usage: 2029.30 MB
Iteration 156/500
start Current memory usage: 1997.42 MB
Training model: sequential
end Current memory usage: 2000.31 MB
Iteration 157/500
start Current memory usage: 1964.38 MB
Training model: sequential
end Current memory usage: 1979.45 MB
Iteration 158/500
start Current memory usage: 1973.12 MB
Training model: sequential
```
",1
"When using `tf.math.log1p` and NumPy's `np.log1p` with the same complex input, the outputs are inconsistent.","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

TensorFlow's `tf.math.log1p` produces inconsistent results with NumPy's `np.log1p` for complex inputs containing `inf`, such as `[inf+0.j, 0+inf.j, inf+inf.j]`. TensorFlow outputs `[inf+0.j, nan+nanj, nan+nanj]`, while NumPy returns `[inf+0.j, inf+1.57079633j, inf+0.78539816j]`.

### Standalone code to reproduce the issue

```shell
python
import tensorflow as tf
import numpy as np

test_input = tf.constant([complex(float('inf'), 0), complex(0, float('inf')), complex(float('inf'), float('inf'))], dtype=tf.complex128)

# TensorFlow computation
with tf.device('/CPU:0'):
    cpu_out = tf.math.log1p(test_input)

# NumPy computation
numpy_out = np.log1p(test_input.numpy())

print(f""CPU Output: {cpu_out}"")
print(f""NumPy Output: {numpy_out}"")
```


### Relevant log output

```shell
CPU Output: [inf +0.j nan+nanj nan+nanj]
NumPy Output: [inf+0.j         inf+1.57079633j inf+0.78539816j]
```
",1
Heap-buffer-overflow in `SparseMatrixSparseCholesky`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When batch_index is scalar, `SparseMatrixSparseCholesky` triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

indices = tf.constant([[0, 0], [1, 1], [2, 1], [2, 2], [3, 3]], dtype=tf.int64)
values = tf.constant([1.0, 2.0, 1.0, 3.0, 4.0], tf.float32)
dense_shape = tf.constant([4, 4], dtype=tf.int64)
input = tf.raw_ops.SparseTensorToCSRSparseMatrix(
    indices=indices, values=values, dense_shape=dense_shape, name=None
)
permutation = tf.constant([4,1,1,1], dtype=tf.int32)

tf.raw_ops.SparseMatrixSparseCholesky(
  input=input, permutation=permutation, type=tf.float32
)
```


### Relevant log output

```shell
=================================================================
==3331846==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x60700049d1d0 at pc 0x7faa202d6d87 bp 0x7ffd09bf8fe0 sp 0x7ffd09bf8fd0
WRITE of size 4 at 0x60700049d1d0 thread T0
    #0 0x7faa202d6d86 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*)::{lambda(long, long)#1}::operator()(long, long) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e2a2d86)
    #1 0x7faa4e5b373a in std::_Function_handler<void (long, long), tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&)::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long&&, long&&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41f373a)
    #2 0x7faa4e5b84e9 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41f84e9)
    #3 0x7faa4e5c1d41 in tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x4201d41)
    #4 0x7faa4ca98d71 in tensorflow::Shard(int, tsl::thread::ThreadPool*, long, long, std::function<void (long, long)>) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x26d8d71)
    #5 0x7faa202e1ab6 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e2adab6)
    #6 0x7faa4c09c14a in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1cdc14a)
    #7 0x7faa4bc22fe6 in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1862fe6)
    #8 0x7faa4baff306 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x173f306)
    #9 0x7faa4bb5cf74 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x179cf74)
    #10 0x7faa4bb6a8b1 in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x17aa8b1)
    #11 0x7faa13c8c2eb in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c582eb)
    #12 0x7faa13b21c2c in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*> > const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31aedc2c)
    #13 0x7faa13b2466a in tensorflow::ExecuteNode::Run() (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31af066a)
    #14 0x7faa13c5e939 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c2a939)
    #15 0x7faa13b11de4 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31addde4)
    #16 0x7faa13b15dd4 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31ae1dd4)
    #17 0x7faa13b1fd26 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31aebd26)
    #18 0x7faa02b08c33 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x20ad4c33)
    #19 0x7faa13c53e5e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c1fe5e)
    #20 0x7fa9f268645b in TFE_Execute (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x1065245b)
    #21 0x7faa4a034274 in TFE_Py_FastPathExecute_C(_object*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../_pywrap_tensorflow_internal.so+0x3c2274)
    #22 0x7fa9ddff2ccb in pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/_pywrap_tfe.so+0xb7ccb)
    #23 0x7fa9de109899 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/_pywrap_tfe.so+0x1ce899)
    #24 0x51ad66  (/usr/bin/python3.11+0x51ad66)
    #25 0x4e75db in _PyObject_MakeTpCall (/usr/bin/python3.11+0x4e75db)
    #26 0x4fb151 in _PyEval_EvalFrameDefault (/usr/bin/python3.11+0x4fb151)
    #27 0x531822 in _PyFunction_Vectorcall (/usr/bin/python3.11+0x531822)
    #28 0x541194 in PyObject_Call (/usr/bin/python3.11+0x541194)
    #29 0x4fefe0 in _PyEval_EvalFrameDefault (/usr/bin/python3.11+0x4fefe0)
    #30 0x62e1b3  (/usr/bin/python3.11+0x62e1b3)
    #31 0x4f3a66 in PyEval_EvalCode (/usr/bin/python3.11+0x4f3a66)
    #32 0x647c36  (/usr/bin/python3.11+0x647c36)
    #33 0x64534f  (/usr/bin/python3.11+0x64534f)
    #34 0x650d14  (/usr/bin/python3.11+0x650d14)
    #35 0x650a63 in _PyRun_SimpleFileObject (/usr/bin/python3.11+0x650a63)
    #36 0x650832 in _PyRun_AnyFileObject (/usr/bin/python3.11+0x650832)
    #37 0x64f786 in Py_RunMain (/usr/bin/python3.11+0x64f786)
    #38 0x61ee0c in Py_BytesMain (/usr/bin/python3.11+0x61ee0c)
    #39 0x7faaf80d1d8f  (/lib/x86_64-linux-gnu/libc.so.6+0x29d8f)
    #40 0x7faaf80d1e3f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x29e3f)
    #41 0x61ec94 in _start (/usr/bin/python3.11+0x61ec94)

0x60700049d1d0 is located 0 bytes to the right of 80-byte region [0x60700049d180,0x60700049d1d0)
allocated by thread T0 here:
    #0 0x7faaf84bf887 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cpp:145
    #1 0x7faa202d6803 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*)::{lambda(long, long)#1}::operator()(long, long) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e2a2803)
    #2 0x7faa4e5b373a in std::_Function_handler<void (long, long), tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&)::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long&&, long&&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41f373a)
    #3 0x7faa4e5b84e9 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41f84e9)
    #4 0x7faa4e5c1d41 in tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x4201d41)
    #5 0x7faa4ca98d71 in tensorflow::Shard(int, tsl::thread::ThreadPool*, long, long, std::function<void (long, long)>) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x26d8d71)
    #6 0x7faa202e1ab6 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e2adab6)
    #7 0x7faa4c09c14a in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1cdc14a)
    #8 0x7faa4bc22fe6 in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1862fe6)
    #9 0x7faa4baff306 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x173f306)
    #10 0x7faa4bb5cf74 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x179cf74)
    #11 0x7faa4bb6a8b1 in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x17aa8b1)
    #12 0x7faa13c8c2eb in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c582eb)
    #13 0x7faa13b21c2c in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*> > const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31aedc2c)
    #14 0x7faa13b2466a in tensorflow::ExecuteNode::Run() (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31af066a)
    #15 0x7faa13c5e939 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c2a939)
    #16 0x7faa13b11de4 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31addde4)
    #17 0x7faa13b15dd4 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31ae1dd4)
    #18 0x7faa13b1fd26 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31aebd26)
    #19 0x7faa02b08c33 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x20ad4c33)
    #20 0x7faa13c53e5e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c1fe5e)
    #21 0x7fa9f268645b in TFE_Execute (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x1065245b)
    #22 0x7faa4a034274 in TFE_Py_FastPathExecute_C(_object*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../_pywrap_tensorflow_internal.so+0x3c2274)
    #23 0x7fa9ddff2ccb in pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/_pywrap_tfe.so+0xb7ccb)
    #24 0x7fa9de109899 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/_pywrap_tfe.so+0x1ce899)
    #25 0x51ad66  (/usr/bin/python3.11+0x51ad66)

SUMMARY: AddressSanitizer: heap-buffer-overflow (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e2a2d86) in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*)::{lambda(long, long)#1}::operator()(long, long) const
Shadow bytes around the buggy address:
  0x0c0e8008b9e0: 00 00 00 00 00 00 fa fa fa fa fd fd fd fd fd fd
  0x0c0e8008b9f0: fd fd fd fd fa fa fa fa 00 00 00 00 00 00 00 00
  0x0c0e8008ba00: 00 fa fa fa fa fa 00 00 00 00 00 00 00 00 04 fa
  0x0c0e8008ba10: fa fa fa fa 00 00 00 00 00 00 00 00 04 fa fa fa
  0x0c0e8008ba20: fa fa 00 00 00 00 00 00 00 00 04 fa fa fa fa fa
=>0x0c0e8008ba30: 00 00 00 00 00 00 00 00 00 00[fa]fa fa fa fa fa
  0x0c0e8008ba40: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c0e8008ba50: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c0e8008ba60: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c0e8008ba70: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c0e8008ba80: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07
  Heap left redzone:       fa
  Freed heap region:       fd
  Stack left redzone:      f1
  Stack mid redzone:       f2
  Stack right redzone:     f3
  Stack after return:      f5
  Stack use after scope:   f8
  Global redzone:          f9
  Global init order:       f6
  Poisoned by user:        f7
  Container overflow:      fc
  Array cookie:            ac
  Intra object redzone:    bb
  ASan internal:           fe
  Left alloca redzone:     ca
  Right alloca redzone:    cb
  Shadow gap:              cc
==3331846==ABORTING
```
",2
The Doc of tfl.reverse_v2 need to be updated for supporting axes,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 22.04
- TensorFlow installed from (source or binary): source 
- TensorFlow version (or github SHA if from source): the latest version

**Standalone code to reproduce the issue** 
The [doc of tfl.reverse_v2](https://www.tensorflow.org/mlir/tfl_ops#tflreverse_v2_tflreversev2op) only supports axis, but [the kernel implementation](https://source.chromium.org/chromium/chromium/src/+/main:third_party/tflite/src/tensorflow/lite/kernels/reverse.cc;l=85) can support axes, the implementation seems not be consistent with the doc.

",0
Aborted (core dumped) in `RaggedBincount`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the value of size is close to the maximum value for the dtype, an integer overflow error will occur in following multiplication calculations.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

splits = tf.constant([0, 3, 5, 9], dtype=tf.int64)
values = tf.constant(1, shape=[3,3], dtype=tf.int64)
size = tf.constant(6522107765268123892, dtype=tf.int64)
weights = tf.constant(1, shape=[3,3], dtype=tf.float32)
counts = tf.raw_ops.RaggedBincount(splits=splits, values=values, size=size, weights=weights)
```


### Relevant log output

```shell
Status: INVALID_ARGUMENT: Encountered overflow when multiplying 3 with 6522107765268123892, result: -1
Aborted (core dumped)
```
",1
This method creates a model with a 100% memory leak loop using model. fit(),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

ubuntu 2.2 or mac m1

### Mobile device

ubuntu 2.2 or mac m1

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have tried various methods, but the memory is definitely leaking, it seems that the release of memory cannot keep up. Through the logs, it can be found that there is periodic memory recycling, but with the increase of time, there is still a clear upward trend

### Standalone code to reproduce the issue

```shell
import gc

import keras
import numpy as np
import psutil
from keras.optimizers import Adam
from keras.layers import Dense, Dropout, Input, LSTM
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import time
import json


num_samples = 6
num_features = 3
num_classes = 4
epochs = 50
batch_size = 2
identifier = ""test_model""
num_iterations = 500  

def build_model(X, num_classes):
    model = Sequential()
    model.add(Input(shape=(X.shape[1], X.shape[2])))
    model.add(LSTM(16, return_sequences=True))
    model.add(LSTM(16))
    model.add(Dropout(0.4))
    model.add(Dense(8, activation='tanh'))
    model.add(Dense(num_classes, activation='softmax'))

    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy'])
    return model


data_X = np.random.rand(num_samples, num_features)
data_Y = np.random.randint(0, num_classes, size=(num_samples, 1))  


data_Y = np.eye(num_classes)[data_Y.flatten()]  
print(type(data_X))

scaler = MinMaxScaler()
data_X_scaled = scaler.fit_transform(data_X)


train_X, test_X, train_Y, test_Y = train_test_split(data_X_scaled, data_Y, train_size=0.6, random_state=42)


train_X = np.expand_dims(train_X, axis=1)
test_X = np.expand_dims(test_X, axis=1)

best_loss = np.inf
best_model_data = None
for iteration in range(num_iterations):
   
    tf.keras.backend.clear_session()
    tf.compat.v1.reset_default_graph()
    model = build_model(train_X, num_classes)
 
    model_name = f""model_{iteration}""
    early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=0, restore_best_weights=True)
    print(f""Iteration {iteration + 1}/{num_iterations}"")
    process = psutil.Process()
    mem_info = process.memory_info()
    print(f""start Current memory usage: {mem_info.rss / (1024 * 1024):.2f} MB"")  # RSS - Resident Set Size
    try:
        history = model.fit(train_X, train_Y, epochs=epochs, batch_size=batch_size, shuffle=True,
                            validation_data=(test_X, test_Y), verbose=0)
        current_loss = history.history['loss'][-1]
        print(f""Training model: {model.name}"")
    
        del model
        tf.keras.backend.clear_session()
        gc.collect()
    except Exception as e:
        print(""err:"", e)
    finally:
        process = psutil.Process()
        mem_info = process.memory_info()
        print(f""end Current memory usage: {mem_info.rss / (1024 * 1024):.2f} MB"")  # RSS - Resident Set Size

print(""end！"")


if best_model_data:
    model_json = best_model_data[""model_architecture""]
    model_weights = json.loads(best_model_data[""model_weights""], object_hook=lambda d: np.array(d))
    model = tf.keras.models.model_from_json(model_json)
    model.set_weights(model_weights)
    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy'])
    print(""ok"")
else:
    print(""not found"")
```


### Relevant log output

```shell
Iteration 1/500
start Current memory usage: 450.69 MB
Training model: sequential
end Current memory usage: 524.41 MB
Iteration 2/500
start Current memory usage: 524.52 MB
Training model: sequential
end Current memory usage: 564.97 MB
Iteration 3/500
start Current memory usage: 564.98 MB
Training model: sequential
end Current memory usage: 598.00 MB
Iteration 4/500
start Current memory usage: 598.03 MB
Training model: sequential
end Current memory usage: 624.69 MB
Iteration 5/500
start Current memory usage: 624.69 MB
Training model: sequential
end Current memory usage: 653.89 MB
Iteration 6/500
start Current memory usage: 653.91 MB
Training model: sequential
end Current memory usage: 679.45 MB
Iteration 7/500
start Current memory usage: 679.45 MB
Training model: sequential
end Current memory usage: 701.59 MB
Iteration 8/500
start Current memory usage: 701.59 MB
Training model: sequential
end Current memory usage: 726.83 MB
Iteration 9/500
start Current memory usage: 726.84 MB
Training model: sequential
end Current memory usage: 749.56 MB
Iteration 10/500
start Current memory usage: 749.56 MB
Training model: sequential
end Current memory usage: 782.56 MB
Iteration 11/500
start Current memory usage: 782.56 MB
Training model: sequential
end Current memory usage: 805.92 MB
Iteration 12/500
start Current memory usage: 805.92 MB
Training model: sequential
end Current memory usage: 833.17 MB
Iteration 13/500
start Current memory usage: 833.17 MB
Training model: sequential
end Current memory usage: 852.84 MB
Iteration 14/500
start Current memory usage: 852.84 MB
Training model: sequential
end Current memory usage: 875.05 MB
Iteration 15/500
start Current memory usage: 875.06 MB
Training model: sequential
end Current memory usage: 901.56 MB
Iteration 16/500
start Current memory usage: 901.56 MB
Training model: sequential
end Current memory usage: 930.62 MB
Iteration 17/500
start Current memory usage: 705.70 MB
Training model: sequential
end Current memory usage: 762.64 MB
Iteration 18/500
start Current memory usage: 762.70 MB
Training model: sequential
end Current memory usage: 798.06 MB
Iteration 19/500
start Current memory usage: 798.17 MB
Training model: sequential
end Current memory usage: 824.98 MB
Iteration 20/500
start Current memory usage: 824.98 MB
Training model: sequential
end Current memory usage: 850.34 MB
Iteration 21/500
start Current memory usage: 850.42 MB
Training model: sequential
end Current memory usage: 876.81 MB
Iteration 22/500
start Current memory usage: 876.81 MB
Training model: sequential
end Current memory usage: 904.02 MB
Iteration 23/500
start Current memory usage: 904.08 MB
Training model: sequential
end Current memory usage: 929.70 MB
Iteration 24/500
start Current memory usage: 929.73 MB
Training model: sequential
end Current memory usage: 952.33 MB
Iteration 25/500
start Current memory usage: 952.34 MB
Training model: sequential
end Current memory usage: 952.28 MB
Iteration 26/500
start Current memory usage: 952.47 MB
Training model: sequential
end Current memory usage: 980.39 MB
Iteration 27/500
start Current memory usage: 978.78 MB
Training model: sequential
end Current memory usage: 999.02 MB
Iteration 28/500
start Current memory usage: 999.05 MB
Training model: sequential
end Current memory usage: 1023.50 MB
Iteration 29/500
start Current memory usage: 1023.53 MB
Training model: sequential
end Current memory usage: 1047.80 MB
Iteration 30/500
start Current memory usage: 1047.83 MB
Training model: sequential
end Current memory usage: 1068.88 MB
Iteration 31/500
start Current memory usage: 1068.94 MB
Training model: sequential
end Current memory usage: 1095.78 MB
Iteration 32/500
start Current memory usage: 1095.78 MB
Training model: sequential
end Current memory usage: 1119.03 MB
Iteration 33/500
start Current memory usage: 1119.03 MB
Training model: sequential
end Current memory usage: 1039.41 MB
Iteration 34/500
start Current memory usage: 1022.78 MB
Training model: sequential
end Current memory usage: 1040.88 MB
Iteration 35/500
start Current memory usage: 1040.70 MB
Training model: sequential
end Current memory usage: 1054.58 MB
Iteration 36/500
start Current memory usage: 1054.58 MB
Training model: sequential
end Current memory usage: 1076.16 MB
Iteration 37/500
start Current memory usage: 1076.19 MB
Training model: sequential
end Current memory usage: 1097.02 MB
Iteration 38/500
start Current memory usage: 1097.03 MB
Training model: sequential
end Current memory usage: 1113.70 MB
Iteration 39/500
start Current memory usage: 1114.12 MB
Training model: sequential
end Current memory usage: 1140.30 MB
Iteration 40/500
start Current memory usage: 1140.33 MB
Training model: sequential
end Current memory usage: 1163.81 MB
Iteration 41/500
start Current memory usage: 1163.86 MB
Training model: sequential
end Current memory usage: 1195.83 MB
Iteration 42/500
start Current memory usage: 1195.83 MB
Training model: sequential
end Current memory usage: 1221.53 MB
Iteration 43/500
start Current memory usage: 1221.55 MB
Training model: sequential
end Current memory usage: 1231.09 MB
Iteration 44/500
start Current memory usage: 1231.14 MB
Training model: sequential
end Current memory usage: 1245.78 MB
Iteration 45/500
start Current memory usage: 1199.55 MB
Training model: sequential
end Current memory usage: 1221.59 MB
Iteration 46/500
start Current memory usage: 1221.59 MB
Training model: sequential
end Current memory usage: 1249.11 MB
Iteration 47/500
start Current memory usage: 1249.22 MB
Training model: sequential
end Current memory usage: 1275.50 MB
Iteration 48/500
start Current memory usage: 1259.83 MB
Training model: sequential
end Current memory usage: 1290.91 MB
Iteration 49/500
start Current memory usage: 1285.67 MB
Training model: sequential
end Current memory usage: 1296.75 MB
Iteration 50/500
start Current memory usage: 1296.75 MB
Training model: sequential
end Current memory usage: 1306.59 MB
Iteration 51/500
start Current memory usage: 1306.59 MB
Training model: sequential
end Current memory usage: 1287.53 MB
Iteration 52/500
start Current memory usage: 1287.53 MB
Training model: sequential
end Current memory usage: 1297.23 MB
Iteration 53/500
start Current memory usage: 1297.25 MB
Training model: sequential
end Current memory usage: 1285.45 MB
Iteration 54/500
start Current memory usage: 1285.45 MB
Training model: sequential
end Current memory usage: 1290.36 MB
Iteration 55/500
start Current memory usage: 1282.14 MB
Training model: sequential
end Current memory usage: 1302.14 MB
Iteration 56/500
start Current memory usage: 1302.14 MB
Training model: sequential
end Current memory usage: 1287.70 MB
Iteration 57/500
start Current memory usage: 1287.75 MB
Training model: sequential
end Current memory usage: 1282.77 MB
Iteration 58/500
start Current memory usage: 1271.38 MB
Training model: sequential
end Current memory usage: 1232.14 MB
Iteration 59/500
start Current memory usage: 1212.70 MB
Training model: sequential
end Current memory usage: 1201.16 MB
Iteration 60/500
start Current memory usage: 1200.53 MB
Training model: sequential
end Current memory usage: 1169.45 MB
Iteration 61/500
start Current memory usage: 1169.45 MB
Training model: sequential
end Current memory usage: 1209.73 MB
Iteration 62/500
start Current memory usage: 1207.19 MB
Training model: sequential
end Current memory usage: 1226.28 MB
Iteration 63/500
start Current memory usage: 1226.28 MB
Training model: sequential
end Current memory usage: 1231.45 MB
Iteration 64/500
start Current memory usage: 1210.11 MB
Training model: sequential
end Current memory usage: 1176.00 MB
Iteration 65/500
start Current memory usage: 1173.97 MB
Training model: sequential
end Current memory usage: 1201.42 MB
Iteration 66/500
start Current memory usage: 1201.42 MB
Training model: sequential
end Current memory usage: 1223.94 MB
Iteration 67/500
start Current memory usage: 1222.50 MB
Training model: sequential
end Current memory usage: 1229.80 MB
Iteration 68/500
start Current memory usage: 1227.14 MB
Training model: sequential
end Current memory usage: 1219.02 MB
Iteration 69/500
start Current memory usage: 1210.48 MB
Training model: sequential
end Current memory usage: 1247.17 MB
Iteration 70/500
start Current memory usage: 1245.94 MB
Training model: sequential
end Current memory usage: 1259.84 MB
Iteration 71/500
start Current memory usage: 1259.86 MB
Training model: sequential
end Current memory usage: 1286.39 MB
Iteration 72/500
start Current memory usage: 1286.53 MB
Training model: sequential
end Current memory usage: 1316.52 MB
Iteration 73/500
start Current memory usage: 1311.53 MB
Training model: sequential
end Current memory usage: 1338.72 MB
Iteration 74/500
start Current memory usage: 1338.75 MB
Training model: sequential
end Current memory usage: 1348.45 MB
Iteration 75/500
start Current memory usage: 1338.30 MB
Training model: sequential
end Current memory usage: 1354.97 MB
Iteration 76/500
start Current memory usage: 1353.83 MB
Training model: sequential
end Current memory usage: 1385.67 MB
Iteration 77/500
start Current memory usage: 1385.69 MB
Training model: sequential
end Current memory usage: 1408.83 MB
Iteration 78/500
start Current memory usage: 1408.88 MB
Training model: sequential
end Current memory usage: 1430.91 MB
Iteration 79/500
start Current memory usage: 1430.94 MB
Training model: sequential
end Current memory usage: 1443.62 MB
Iteration 80/500
start Current memory usage: 1428.00 MB
Training model: sequential
end Current memory usage: 1436.50 MB
Iteration 81/500
start Current memory usage: 1436.64 MB
Training model: sequential
end Current memory usage: 1454.66 MB
Iteration 82/500
start Current memory usage: 1440.91 MB
Training model: sequential
end Current memory usage: 1461.81 MB
Iteration 83/500
start Current memory usage: 1460.47 MB
Training model: sequential
end Current memory usage: 1481.19 MB
Iteration 84/500
start Current memory usage: 1481.19 MB
Training model: sequential
end Current memory usage: 1477.84 MB
Iteration 85/500
start Current memory usage: 1477.84 MB
Training model: sequential
end Current memory usage: 1493.55 MB
Iteration 86/500
start Current memory usage: 1493.58 MB
Training model: sequential
end Current memory usage: 1509.50 MB
Iteration 87/500
start Current memory usage: 1509.50 MB
Training model: sequential
end Current memory usage: 1543.94 MB
Iteration 88/500
start Current memory usage: 1542.83 MB
Training model: sequential
end Current memory usage: 1516.17 MB
Iteration 89/500
start Current memory usage: 1516.20 MB
Training model: sequential
end Current memory usage: 1470.17 MB
Iteration 90/500
start Current memory usage: 1470.22 MB
Training model: sequential
end Current memory usage: 1443.72 MB
Iteration 91/500
start Current memory usage: 1444.36 MB
Training model: sequential
end Current memory usage: 1486.23 MB
Iteration 92/500
start Current memory usage: 1476.41 MB
Training model: sequential
end Current memory usage: 1524.97 MB
Iteration 93/500
start Current memory usage: 1524.97 MB
Training model: sequential
end Current memory usage: 1534.94 MB
Iteration 94/500
start Current memory usage: 1551.98 MB
Training model: sequential
end Current memory usage: 1853.48 MB
Iteration 95/500
start Current memory usage: 1853.48 MB
Training model: sequential
end Current memory usage: 1790.12 MB
Iteration 96/500
start Current memory usage: 1792.27 MB
Training model: sequential
end Current memory usage: 1883.20 MB
Iteration 97/500
start Current memory usage: 1879.05 MB
Training model: sequential
end Current memory usage: 1759.69 MB
Iteration 98/500
start Current memory usage: 1669.66 MB
Training model: sequential
end Current memory usage: 1596.77 MB
Iteration 99/500
start Current memory usage: 1597.12 MB
Training model: sequential
end Current memory usage: 1568.83 MB
Iteration 100/500
start Current memory usage: 1532.98 MB
Training model: sequential
end Current memory usage: 1516.75 MB
Iteration 101/500
start Current memory usage: 1465.98 MB
Training model: sequential
end Current memory usage: 1486.66 MB
Iteration 102/500
start Current memory usage: 1483.34 MB
Training model: sequential
end Current memory usage: 1523.19 MB
Iteration 103/500
start Current memory usage: 1523.14 MB
Training model: sequential
end Current memory usage: 1532.77 MB
Iteration 104/500
start Current memory usage: 1531.14 MB
Training model: sequential
end Current memory usage: 1561.78 MB
Iteration 105/500
start Current memory usage: 1555.67 MB
Training model: sequential
end Current memory usage: 1586.70 MB
Iteration 106/500
start Current memory usage: 1586.75 MB
Training model: sequential
end Current memory usage: 1608.41 MB
Iteration 107/500
start Current memory usage: 1603.81 MB
Training model: sequential
end Current memory usage: 1629.00 MB
Iteration 108/500
start Current memory usage: 1629.05 MB
Training model: sequential
end Current memory usage: 1609.25 MB
Iteration 109/500
start Current memory usage: 1609.31 MB
Training model: sequential
end Current memory usage: 1630.09 MB
Iteration 110/500
start Current memory usage: 1629.20 MB
Training model: sequential
end Current memory usage: 1638.66 MB
Iteration 111/500
start Current memory usage: 1620.30 MB
Training model: sequential
end Current memory usage: 1642.81 MB
Iteration 112/500
start Current memory usage: 1642.94 MB
Training model: sequential
end Current memory usage: 1659.45 MB
Iteration 113/500
start Current memory usage: 1655.17 MB
Training model: sequential
end Current memory usage: 1687.80 MB
Iteration 114/500
start Current memory usage: 1673.33 MB
Training model: sequential
end Current memory usage: 1705.94 MB
Iteration 115/500
start Current memory usage: 1699.95 MB
Training model: sequential
end Current memory usage: 1708.22 MB
Iteration 116/500
start Current memory usage: 1707.88 MB
Training model: sequential
end Current memory usage: 1648.23 MB
Iteration 117/500
start Current memory usage: 1634.03 MB
Training model: sequential
end Current memory usage: 1670.97 MB
Iteration 118/500
start Current memory usage: 1671.97 MB
Training model: sequential
end Current memory usage: 1649.69 MB
Iteration 119/500
start Current memory usage: 1645.14 MB
Training model: sequential
end Current memory usage: 1698.64 MB
Iteration 120/500
start Current memory usage: 1699.69 MB
Training model: sequential
end Current memory usage: 1737.67 MB
Iteration 121/500
start Current memory usage: 1737.67 MB
Training model: sequential
end Current memory usage: 1738.05 MB
Iteration 122/500
start Current memory usage: 1721.47 MB
Training model: sequential
end Current memory usage: 1730.64 MB
Iteration 123/500
start Current memory usage: 1729.53 MB
Training model: sequential
end Current memory usage: 1766.12 MB
Iteration 124/500
start Current memory usage: 1761.22 MB
Training model: sequential
end Current memory usage: 1796.58 MB
Iteration 125/500
start Current memory usage: 1796.73 MB
Training model: sequential
end Current memory usage: 1709.02 MB
Iteration 126/500
start Current memory usage: 1721.67 MB
Training model: sequential
end Current memory usage: 1771.50 MB
Iteration 127/500
start Current memory usage: 1771.50 MB
Training model: sequential
end Current memory usage: 1777.38 MB
Iteration 128/500
start Current memory usage: 1757.58 MB
Training model: sequential
end Current memory usage: 1806.50 MB
Iteration 129/500
start Current memory usage: 1758.81 MB
Training model: sequential
end Current memory usage: 1812.45 MB
Iteration 130/500
start Current memory usage: 1812.86 MB
Training model: sequential
end Current memory usage: 1811.14 MB
Iteration 131/500
start Current memory usage: 1799.61 MB
Training model: sequential
end Current memory usage: 1835.33 MB
Iteration 132/500
start Current memory usage: 1716.38 MB
Training model: sequential
end Current memory usage: 1759.75 MB
Iteration 133/500
start Current memory usage: 1752.44 MB
Training model: sequential
end Current memory usage: 1818.41 MB
Iteration 134/500
start Current memory usage: 1811.42 MB
Training model: sequential
end Current memory usage: 1853.58 MB
Iteration 135/500
start Current memory usage: 1853.70 MB
Training model: sequential
end Current memory usage: 1858.50 MB
Iteration 136/500
start Current memory usage: 1858.56 MB
Training model: sequential
end Current memory usage: 1874.84 MB
Iteration 137/500
start Current memory usage: 1862.92 MB
Training model: sequential
end Current memory usage: 1768.23 MB
Iteration 138/500
start Current memory usage: 1762.73 MB
Training model: sequential
end Current memory usage: 1843.39 MB
Iteration 139/500
start Current memory usage: 1843.52 MB
Training model: sequential
end Current memory usage: 1885.88 MB
Iteration 140/500
start Current memory usage: 1885.95 MB
Training model: sequential
end Current memory usage: 1924.86 MB
Iteration 141/500
start Current memory usage: 1925.05 MB
Training model: sequential
end Current memory usage: 1946.80 MB
Iteration 142/500
start Current memory usage: 1946.69 MB
Training model: sequential
end Current memory usage: 1977.53 MB
Iteration 143/500
start Current memory usage: 1974.27 MB
Training model: sequential
end Current memory usage: 1995.17 MB
Iteration 144/500
start Current memory usage: 1992.41 MB
Training model: sequential
end Current memory usage: 1984.45 MB
Iteration 145/500
start Current memory usage: 1963.42 MB
Training model: sequential
end Current memory usage: 1947.31 MB
Iteration 146/500
start Current memory usage: 1944.47 MB
Training model: sequential
end Current memory usage: 1996.00 MB
Iteration 147/500
start Current memory usage: 1996.08 MB
Training model: sequential
end Current memory usage: 2008.41 MB
Iteration 148/500
start Current memory usage: 1999.69 MB
Training model: sequential
end Current memory usage: 1951.30 MB
Iteration 149/500
start Current memory usage: 1942.98 MB
Training model: sequential
end Current memory usage: 1992.28 MB
Iteration 150/500
start Current memory usage: 1982.86 MB
Training model: sequential
end Current memory usage: 2008.83 MB
Iteration 151/500
start Current memory usage: 2008.83 MB
Training model: sequential
end Current memory usage: 1946.42 MB
Iteration 152/500
start Current memory usage: 1946.92 MB
Training model: sequential
end Current memory usage: 1992.48 MB
Iteration 153/500
start Current memory usage: 1979.52 MB
Training model: sequential
end Current memory usage: 2035.66 MB
Iteration 154/500
start Current memory usage: 2023.91 MB
Training model: sequential
end Current memory usage: 2030.31 MB
Iteration 155/500
start Current memory usage: 1974.39 MB
Training model: sequential
end Current memory usage: 2029.30 MB
Iteration 156/500
start Current memory usage: 1997.42 MB
Training model: sequential
end Current memory usage: 2000.31 MB
Iteration 157/500
start Current memory usage: 1964.38 MB
Training model: sequential
end Current memory usage: 1979.45 MB
Iteration 158/500
start Current memory usage: 1973.12 MB
Training model: sequential
```
",1
"Cannot send mail to github-admin@tensorflow.org, the contact address shown on front page of github.com/tensorflow","### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

irrelevant

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The front page of https://github.com/tensorflow lists github-admin@tensorflow.org as a contact address:

<p align=""center"">
<img width=""80%"" alt=""image"" src=""https://github.com/user-attachments/assets/a80e6161-1cde-4d10-b9a2-774f80294d15"">
</p>

However, attempting to send mail to that address results in an error message being mailed back:

```
We're writing to let you know that the group you tried to 
contact (github-admin) may not exist, or you may not have 
permission to post messages to the group. A few more 
details on why you weren't able to post:

 * You might have spelled or formatted the group name incorrectly.
 * The owner of the group may have removed this group.
 * You may need to join the group before receiving permission to post.
 * This group may not be open to posting.

If you have questions related to this or any other Google Group,
visit the Help Center at 
https://support.google.com/a/tensorflow.org/bin/topic.py?topic=25838.

Thanks,

[tensorflow.org](http://tensorflow.org/) admins
```


### Standalone code to reproduce the issue

```shell
Send an email message to github-admin@tensorflow.org
```


### Relevant log output

```shell
We're writing to let you know that the group you tried to 
contact (github-admin) may not exist, or you may not have 
permission to post messages to the group. A few more 
details on why you weren't able to post:

 * You might have spelled or formatted the group name incorrectly.
 * The owner of the group may have removed this group.
 * You may need to join the group before receiving permission to post.
 * This group may not be open to posting.

If you have questions related to this or any other Google Group,
visit the Help Center at 
https://support.google.com/a/tensorflow.org/bin/topic.py?topic=25838.

Thanks,

[tensorflow.org](http://tensorflow.org/) admins
```
",1
Aborted (core dumped) in `tf.raw_ops.SparseConcat`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific inputs, `SparseConcat` triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

indices1 = tf.constant(2, shape=[3,3], dtype=tf.int64)
values1 = tf.constant(""aaaabaaacaaadaaaeaaafaaagaaahaaaiaaajaaakaaalaaamaaanaaaoaaapaaaqaaaraaasaaataaauaaavaaawaaaxaaayaaazaabbaabcaabdaabeaabfaabgaabhaabiaabjaabkaablaabmaabnaaboaabpaabqaabraabsaabtaabuaabvaabwaabxaabyaabzaacbaaccaacdaaceaacfaacgaachaaciaacjaackaaclaacmaacnaacoaacpaacqaacraacsaactaacuaacvaacwaacxaacyaac"",
    shape=[3], dtype=tf.string)
shapes1 = tf.constant([5, 2, 2147483647], dtype=tf.int64)

indices2 = tf.constant(-2, shape=[4,3], dtype=tf.int64)
values2 = tf.constant("" "", shape=[4], dtype=tf.string)
shapes2 = tf.constant([5,1879048192,536870912], dtype=tf.int64)

concat_dim = 1
tf.raw_ops.SparseConcat(
    indices=[indices1, indices2], values=[values1, values2], shapes=[shapes1, shapes2], concat_dim=concat_dim, name=None
)
```


### Relevant log output

```shell
2024-11-24 06:36:10.994508: F tensorflow/core/framework/tensor_shape.cc:607] Non-OK-status: RecomputeNumElements() status: INVALID_ARGUMENT: Shape [5,1879048194,2147483647] results in overflow when computing number of elements
Aborted (core dumped)
```
",2
Issue created for Rollback of PR #80574: Update graph_info.h to avoid warning: multi-line comment [-Wcomment],"Merged PR #80574 is rolled back in f377b15fc677b8ebaaeab3c5156ecf145c7f0631.
    Please follow up with the reviewer and close this issue once its resolved.",0
Not GPU detected using tensorflow/tensorflow:latest-gpu docker image,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tensorflow/tensorflow:latest-gpu

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

sudo docker run --gpus all -it --rm tensorflow/tensorflow:latest-gpu    python -c ""import tensorflow as tf; print('Num GPUs Available:', len(tf.config.list_physical_devices('GPU')))""
2024-11-22 08:20:55.695121: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1732263655.707089       1 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1732263655.710704       1 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-22 08:20:55.722395: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-11-22 08:20:57.119693: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW
2024-11-22 08:20:57.119716: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:137] retrieving CUDA diagnostic information for host: 81f8d81af78d
2024-11-22 08:20:57.119720: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:144] hostname: 81f8d81af78d
2024-11-22 08:20:57.119789: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:168] libcuda reported version is: 545.23.6
2024-11-22 08:20:57.119804: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:172] kernel reported version is: 470.256.2
2024-11-22 08:20:57.119808: E external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:262] kernel version 470.256.2 does not match DSO version 545.23.6 -- cannot find working devices in this configuration
Num GPUs Available: 0

### Standalone code to reproduce the issue

```shell
sudo docker run --gpus all -it --rm tensorflow/tensorflow:latest-gpu    python -c ""import tensorflow as tf; print('Num GPUs Available:', len(tf.config.list_physical_devices('GPU')))""

I've also followed this recommendation. https://stackoverflow.com/questions/79127647/tensorflow-docker-not-using-gpu/79214187#79214187
```


### Relevant log output

```shell
root@35b972e97b30:/# nvidia-smi
Fri Nov 22 08:44:56 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.256.02   Driver Version: 470.256.02   CUDA Version: 12.3     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |
| N/A   38C    P0    22W /  N/A |     10MiB /  5946MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+
root@35b972e97b30:/# nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2023 NVIDIA Corporation
Built on Wed_Nov_22_10:17:15_PST_2023
Cuda compilation tools, release 12.3, V12.3.107
Build cuda_12.3.r12.3/compiler.33567101_0
```
",1
Floating point exception (core dumped) in `tf.raw_ops.Reshape`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific inputs,  `tf.raw_ops.Reshape` triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tensor = tf.constant(-3.5e+35, shape=[5], dtype=tf.float32)
shape = tf.constant([0, 1879048192, 100000000, 1610612736, -1], dtype=tf.int32)

tf.raw_ops.Reshape(tensor=tensor, shape=shape)
```


### Relevant log output

```shell
Floating point exception (core dumped)
```
",2
The documentation in `data_performance.ipynb` uses `py_function()` artificially,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

TF 2.18

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In the [""Better performance with the tf.data API"" guide](https://www.tensorflow.org/guide/data_performance), `tf.py_function()` is used several times in the mapping function. There are various performance boosts demonstrated by the guide with said mapping functions. However, there does not seem to be any practical reason why `tf.py_function()` is used inside the mapping functions. In fact, if you remove those, the behavior is the same; in other words, there doesn't seem to be a need for them at all.

Curiously, if you remove them from the examples and then perform the time measurements, the speedup goes away. For example, consider the following mapping function from [the guide](https://github.com/tensorflow/docs/blob/bbc0b9c70fc0bd4411793d1b0bcc56ef1dbc2405/site/en/guide/data_performance.ipynb#L447-L450):

```python
def mapped_function(s):
    # Do some hard pre-processing
    tf.py_function(lambda: time.sleep(0.03), [], ())
    return s
```

In the sequential case, it is shown that the following mapping code results in a time measurement of 0.49222556600034295:

```python
benchmark(
    ArtificialDataset()
    .map(mapped_function)
)
```

And, the optimized (parallel) version as follows results in a time measurement of 0.36392719900049997:

```python
benchmark(
    ArtificialDataset()
    .map(
        mapped_function,
        num_parallel_calls=tf.data.AUTOTUNE
    )
)
```

But, if I remove the `tf.py_function()` from the mapping function, I get comparable measurements from both examples, namely, 0.22448736599994845 and 0.2266392660001202:

```python
def mapped_function(s):
    # Do some hard pre-processing
    lambda: time.sleep(0.03), [], ()
    return s
```

In fact, that measurement is even better, which makes me believe that this example is contrived to show a performance benefit by using `num_parallel_calls` when in fact TF is already optimizing the code without it. Frivolously wrapping the function in `tf.py_function()` is most likely  causing TensorFlow *not* to optimize the function. Thus, is `num_parallel_calls` even needed to achieve better performance?

### Standalone code to reproduce the issue

Mentioned in the above description.

### Relevant log output

N/A.",1
Does tfl.quantize support QI4 data type,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 22.04
- TensorFlow installed from (source or binary): source 
- TensorFlow version (or github SHA if from source): the latest version

**Standalone code to reproduce the issue** 
The [doc of tfl.quantize](https://www.tensorflow.org/mlir/tfl_ops#tflquantize_tflquantizeop) supports QI4 data types, but [the kernel implementation](https://source.chromium.org/chromium/chromium/src/+/main:third_party/tflite/src/tensorflow/lite/kernels/quantize.cc;l=121) doesn't support QI4 data type, the implementation seems not be consistent with the doc.

",0
Aborted (core dumped) in `tf.raw_ops.MatrixSolve`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the shape of the matrix argument is empty and the gpu is available, tf.raw_ops.MatrixSolve triggers a crash.
It can be reproduced on tf-nightly when the gpu is available.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.raw_ops.MatrixSolve(matrix=tf.random.uniform([], dtype=tf.dtypes.double, maxval=1000000000), rhs=tf.random.uniform([1, 2], dtype=tf.dtypes.double, maxval=1000000000), adjoint=True)
```


### Relevant log output

```shell
2024-11-20 14:51:08.714846: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-11-20 14:51:08.775383: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-11-20 14:51:08.852267: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-11-20 14:51:08.876168: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-20 14:51:08.931206: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-11-20 14:51:16.385650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 535 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-11-20 14:51:16.387914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 71 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-11-20 14:51:16.542383: F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)
```
",2
Aborted (core dumped) in `tf.raw_ops.MatrixInverse`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the shape of the input argument is empty and the gpu is available, tf.raw_ops.MatrixInverse triggers a crash.
It can be reproduced on tf-nightly when the gpu is available.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.raw_ops.MatrixInverse(input=tf.cast(tf.random.uniform([], dtype=tf.dtypes.float32, maxval=60000), dtype=tf.complex128),adjoint=True)
```


### Relevant log output

```shell
2024-11-20 10:46:15.940818: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-11-20 10:46:16.001155: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-11-20 10:46:16.076386: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-11-20 10:46:16.100080: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-20 10:46:16.154057: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-11-20 10:46:23.889652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21903 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-11-20 10:46:23.891964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 71 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-11-20 10:46:24.756574: F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)
```
",2
tf.gather and workarouds are very slow on TPU,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.2 LTS

### Mobile device

TPU VM

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hi all,

I am trying to solve a performance bug that occurs during pretraining/fine-tuning a DeBERTa model on TPUs.

In a nutshell, the `tf.gather` implemention is very slow on TPUs (but very fast on GPUs).

I am now looking for a way, that boosts up the `tf.one_hot` + `tf.einsum` trick, which would have massive impact in pretraining DeBERTa models to widespread it's usage.

There are some issues that have also reported this issue:

* https://github.com/huggingface/transformers/issues/18239
* https://github.com/keras-team/keras-hub/issues/606

But with no solution yet. Any help is highly appreciated!

### Standalone code to reproduce the issue

Here's a code snippet with an example:

```python
def take_along_axis_v2(x, indices):
    # Only a valid port of np.take_along_axis when the gather axis is -1

    # TPU + gathers and reshapes don't go along well -- see https://github.com/huggingface/transformers/issues/18239
    if isinstance(tf.distribute.get_strategy(), tf.distribute.TPUStrategy):
        # [B, S, P] -> [B, S, P, D]
        one_hot_indices = tf.one_hot(indices, depth=x.shape[-1], dtype=x.dtype)

        # if we ignore the first two dims, this is equivalent to multiplying a matrix (one hot) by a vector (x)
        # grossly abusing notation: [B, S, P, D] . [B, S, D] = [B, S, P]
        gathered = tf.einsum(""ijkl,ijl->ijk"", one_hot_indices, x)

    # GPUs, on the other hand, prefer gathers instead of large one-hot+matmuls
    else:
        gathered = tf.gather(x, indices, batch_dims=2)

    return gathered
```

Taken from https://github.com/WissamAntoun/CamemBERTa/blob/1a1fb4a658729dfac2bb93842d88261132803ec3/modeling_tf_deberta_v2.py#L734-L750



### Relevant log output

_No response_",1
model.fit fails when the number of rows exceeds Int32.MaxValue,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.19.0-dev20241117

### Custom code

Yes

### OS platform and distribution

MacOS 15.1.0

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I would expect model.fit to handle training on extremely large NumPy arrays without limitations.

### Standalone code to reproduce the issue

```shell
import numpy as np
from keras import Sequential
from keras.layers import Dense

n = 2_147_483_648
x = np.zeros(n).astype(np.float32)
y = x

model = Sequential([
    Dense(64, activation=""relu"", input_shape=(1,)),
    Dense(1, activation=""sigmoid"")
])
model.compile(optimizer=""adam"", loss=""binary_crossentropy"")
model.fit(x=x,y=y, epochs=1, batch_size=1024, verbose=1)
```


### Relevant log output

```shell
ValueError: Invalid value in tensor used for shape: -2147483648
```
",1
tf.range still miss some dtypes support,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v2.18.0-rc2-4-g6550e4bd802 2.18.0

### Custom code

Yes

### OS platform and distribution

Google Colab

### Mobile device

No

### Python version

Google Colab default

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Same issue as in https://github.com/tensorflow/tensorflow/issues/72365 but now with unsigned dtypes

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tf.range(10, delta=1, dtype='uint8')
```


### Relevant log output

```shell
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-1-7b6ccd0e0a16> in <cell line: 3>()
      1 import tensorflow as tf
      2 
----> 3 tf.range(10, delta=1, dtype='uint8')

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   6000 def raise_from_not_ok_status(e, name) -> NoReturn:
   6001   e.message += ("" name: "" + str(name if name is not None else """"))
-> 6002   raise core._status_to_exception(e) from None  # pylint: disable=protected-access
   6003 
   6004 

InvalidArgumentError: Value for attr 'Tidx' of uint8 is not in the list of allowed values: bfloat16, half, float, double, int8, int16, int32, int64, uint16, uint32
	; NodeDef: {{node Range}}; Op<name=Range; signature=start:Tidx, limit:Tidx, delta:Tidx -> output:Tidx; attr=Tidx:type,default=DT_INT32,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT16, DT_UINT32]> [Op:Range] name:
```
",1
Bug: `Compiling upb/upb.c failed` due to `Clang` Version Mismatch in Tensorflow's Docker Build Image,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.18

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS

### Mobile device

_No response_

### Python version

3.9

### Bazel version

6.5.0

### GCC/compiler version

11.4.0

### CUDA/cuDNN version

CUDA 12.5.1 / cuDNN 9.3.0

### GPU model and memory

NVIDIA RTX 3090 24GB DDR6

### Current behavior?

I expected the `tensorflow-gpu` wheel to compile successfully. However I received the error detailed below. I believe it's because the required `Clang` version (`18.1.8`) installed in the docker image is incorrect. [According to the documentation](https://www.tensorflow.org/install/source#gpu), the required version of `Clang` is `17.0.6` for `tf2.18`. I pulled [`tensorflow/build:2.18-python3.9`](https://hub.docker.com/layers/tensorflow/build/2.18-python3.9/images/sha256-843f48fe24727cdef4d76ae2724edc8385b2b2b44e3e4da0752e84b9ca142a81?context=explore) from DockerHub.

Do I need to manually roll back to `Clang 17.0.6` within the container, or should I be pulling a different image?
```bash
ERROR: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/upb/BUILD:57:11: Compiling upb/upb.c failed: (Exit 1): clang failed: error executing command (from target @upb//:upb) /usr/lib/llvm-18/bin/clang -MD -MF bazel-out/k8-opt/bin/external/upb/_objs/upb/upb.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/upb/_objs/upb/upb.pic.o' '-DBAZEL_CURRENT_REPOSITORY=""upb""' -iquote ... (remaining 44 arguments skipped)
external/upb/upb/upb.c:192:10: error: defining a type within 'offsetof' is a Clang extension [-Werror,-Wgnu-offsetof-extensions]
  192 |   n &= ~(upb_alignof(upb_arena) - 1);
      |          ^~~~~~~~~~~~~~~~~~~~~~
external/upb/upb/upb.c:183:37: note: expanded from macro 'upb_alignof'
  183 | #define upb_alignof(type) offsetof (struct { char c; type member; }, member)
      |                                     ^~~~~~
/usr/lib/llvm-18/lib/clang/18/include/__stddef_offsetof.h:16:43: note: expanded from macro 'offsetof'
   16 | #define offsetof(t, d) __builtin_offsetof(t, d)
      |                                           ^
1 error generated.
Target //tensorflow/tools/pip_package:wheel failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 524.930s, Critical Path: 5.67s
INFO: 1437 processes: 820 internal, 617 local.
FAILED: Build did NOT complete successfully
```

### Standalone code to reproduce the issue

```shell
# Pull the docker image to build tf.
docker pull tensorflow/build:2.18-python3.9

# Pull the tensorflow repo.
git pull https://github.com/tensorflow/tensorflow.git

# Change to the tensorflow directory.
cd tensorflow

# Checkout and switch to the r2.18 branch.
git fetch origin && git checkout -b r2.18 origin/r2.18

# Run a container to build tensorflow.
docker run \
        -it \
	--name tf-build \
	-h tf-build \
	-u root \
	-e HOST_PERMS=""$(id -u):$(id -g)"" \
	--runtime=nvidia \
	--gpus=all \
	--rm \
	--shm-size=2g \
	--ulimit memlock=-1 \
	--ulimit stack=67108864 \
	-v $PWD:/mnt \
	-w /mnt \
	tensorflow/build:2.18-python3.9 \
	bash

# From within the container, configure the build.
./configure

# Compile the wheel.
bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --config=cuda --config=cuda_wheel --config=opt
```


### Relevant log output

```shell
tf-docker /mnt > ./configure
You have bazel 6.5.0 installed.
Please specify the location of python. [Default is /usr/bin/python3]:


Found possible Python library paths:
  /usr/lib/python3/dist-packages
  /usr/local/lib/python3.9/dist-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the hermetic CUDA version you want to use or leave empty to use the default version. 12.5.1


Please specify the hermetic cuDNN version you want to use or leave empty to use the default version. 9.3.0


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 8.6


Please specify the local CUDA path you want to use or leave empty to use the default version.


Please specify the local CUDNN path you want to use or leave empty to use the default version.


Please specify the local NCCL path you want to use or leave empty to use the default version.


Do you want to use clang as CUDA compiler? [Y/n]: y
Clang will be used as CUDA compiler.

Please specify clang path that to be used as host compiler. [Default is /usr/lib/llvm-18/bin/clang]:


You have Clang 18.1.8 installed.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: -march=native


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v1          	# Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=nogcp       	# Disable GCP support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished
tf-docker /mnt > bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --config=cuda --config=cuda_wheel --config=opt
Extracting Bazel installation...
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Reading 'startup' options from /mnt/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=173
INFO: Reading rc options for 'build' from /mnt/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /etc/bazel.bazelrc:
  'build' options: --action_env=DOCKER_CACHEBUSTER=1726964088092166976 --host_action_env=DOCKER_HOST_CACHEBUSTER=1726964088140283903
INFO: Reading rc options for 'build' from /mnt/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /mnt/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/lib/llvm-18/bin/clang --config=cuda_clang
INFO: Found applicable config definition build:short_logs in file /mnt/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /mnt/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda_clang in file /mnt/.bazelrc: --config=cuda --@local_config_cuda//:cuda_compiler=clang --copt=-Qunused-arguments --repo_env=HERMETIC_CUDA_COMPUTE_CAPABILITIES=sm_60,sm_70,sm_80,sm_89,compute_90 --host_linkopt=-fuse-ld=lld --host_linkopt=-lm --linkopt=-fuse-ld=lld --linkopt=-lm
INFO: Found applicable config definition build:cuda in file /mnt/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true
INFO: Found applicable config definition build:cuda in file /mnt/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_VERSION=12.5.1 --repo_env HERMETIC_CUDNN_VERSION=9.3.0 --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=8.6
INFO: Found applicable config definition build:cuda_clang in file /mnt/.bazelrc: --config=cuda --@local_config_cuda//:cuda_compiler=clang --copt=-Qunused-arguments --repo_env=HERMETIC_CUDA_COMPUTE_CAPABILITIES=sm_60,sm_70,sm_80,sm_89,compute_90 --host_linkopt=-fuse-ld=lld --host_linkopt=-lm --linkopt=-fuse-ld=lld --linkopt=-lm
INFO: Found applicable config definition build:cuda in file /mnt/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true
INFO: Found applicable config definition build:cuda in file /mnt/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_VERSION=12.5.1 --repo_env HERMETIC_CUDNN_VERSION=9.3.0 --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=8.6
INFO: Found applicable config definition build:cuda in file /mnt/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda --repo_env=HERMETIC_CUDA_VERSION=12.5.1 --repo_env=HERMETIC_CUDNN_VERSION=9.3.0 --@local_config_cuda//cuda:include_cuda_libs=true
INFO: Found applicable config definition build:cuda in file /mnt/.tf_configure.bazelrc: --repo_env HERMETIC_CUDA_VERSION=12.5.1 --repo_env HERMETIC_CUDNN_VERSION=9.3.0 --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=8.6
INFO: Found applicable config definition build:cuda_wheel in file /mnt/.bazelrc: --@local_config_cuda//cuda:include_cuda_libs=false
INFO: Found applicable config definition build:opt in file /mnt/.tf_configure.bazelrc: --copt=-march=native --host_copt=-march=native
INFO: Found applicable config definition build:linux in file /mnt/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /mnt/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_xla/third_party/py/python_repo.bzl:96:14:
HERMETIC_PYTHON_VERSION variable was not set correctly, using default version.
Python 3.9 will be used.
To select Python version, either set HERMETIC_PYTHON_VERSION env variable in
your shell:
  export HERMETIC_PYTHON_VERSION=3.12
OR pass it as an argument to bazel command directly or inside your .bazelrc
file:
  --repo_env=HERMETIC_PYTHON_VERSION=3.12
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_xla/third_party/py/python_repo.bzl:107:10: Using hermetic Python 3.9
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_cupti/linux-x86_64/cuda_cupti-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_nvtx/linux-x86_64/cuda_nvtx-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcusolver/linux-x86_64/libcusolver-linux-x86_64-11.6.3.83-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libnvjitlink/linux-x86_64/libnvjitlink-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcurand/linux-x86_64/libcurand-linux-x86_64-10.3.6.82-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_cudart/linux-x86_64/cuda_cudart-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cudnn/redist/cudnn/linux-x86_64/cudnn-linux-x86_64-9.3.0.75_cuda12-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcusparse/linux-x86_64/libcusparse-linux-x86_64-12.5.1.3-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_nvml_dev/linux-x86_64/cuda_nvml_dev-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcublas/linux-x86_64/libcublas-linux-x86_64-12.5.3.2-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/libcufft/linux-x86_64/libcufft-linux-x86_64-11.2.3.61-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_nvcc/linux-x86_64/cuda_nvcc-linux-x86_64-12.5.82-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_cccl/linux-x86_64/cuda_cccl-linux-x86_64-12.5.39-archive.tar.xz
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/nccl/hermetic/nccl_redist_init_repository.bzl:73:10: Downloading and extracting https://files.pythonhosted.org/packages/df/99/12cd266d6233f47d00daf3a72739872bdc10267d0383508b0b9c84a18bb6/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl
DEBUG: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_redist_init_repositories.bzl:269:10: Downloading and extracting https://developer.download.nvidia.com/compute/cuda/redist/cuda_nvprune/linux-x86_64/cuda_nvprune-linux-x86_64-12.5.82-archive.tar.xz
INFO: Analyzed target //tensorflow/tools/pip_package:wheel (759 packages loaded, 56246 targets configured).
INFO: Found 1 target...
ERROR: /root/.cache/bazel/_bazel_root/39de0dbcfb68c8735bd088c62fa061a4/external/upb/BUILD:57:11: Compiling upb/upb.c failed: (Exit 1): clang failed: error executing command (from target @upb//:upb) /usr/lib/llvm-18/bin/clang -MD -MF bazel-out/k8-opt/bin/external/upb/_objs/upb/upb.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/upb/_objs/upb/upb.pic.o' '-DBAZEL_CURRENT_REPOSITORY=""upb""' -iquote ... (remaining 44 arguments skipped)
external/upb/upb/upb.c:192:10: error: defining a type within 'offsetof' is a Clang extension [-Werror,-Wgnu-offsetof-extensions]
  192 |   n &= ~(upb_alignof(upb_arena) - 1);
      |          ^~~~~~~~~~~~~~~~~~~~~~
external/upb/upb/upb.c:183:37: note: expanded from macro 'upb_alignof'
  183 | #define upb_alignof(type) offsetof (struct { char c; type member; }, member)
      |                                     ^~~~~~
/usr/lib/llvm-18/lib/clang/18/include/__stddef_offsetof.h:16:43: note: expanded from macro 'offsetof'
   16 | #define offsetof(t, d) __builtin_offsetof(t, d)
      |                                           ^
1 error generated.
Target //tensorflow/tools/pip_package:wheel failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 524.930s, Critical Path: 5.67s
INFO: 1437 processes: 820 internal, 617 local.
FAILED: Build did NOT complete successfully
```
",1
tf.cast to int8 produce wrong number,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.15 - 2.17

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

input is a long list include random numbers.
tf.cast to int8 output is  different from numpy cast.

The strange thing is if you truncate the long input list to a short list then things works fine.

The code is straight forward, you can reproduce it in the colab

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/18dYjvY6JQk79hVq8JsjcWEwIG5KBuF1v?usp=sharing
```


### Relevant log output

```shell
NumPy Casted values (int8): 
 [   0    0    0    0    1   -1    0    3   -3    2   -2    0   50   21
 -125   62   25   31  127   -9  117   61  123   47  -20   28   52   36
  -43  -45  -84  118   37  -17   -6  -79    1   75  -45  -60 -103  -63
   85 -112   76   96  -56   86  -32 -108 -105 -121   -2  121   86  -54
   91  -55   36 -119   -3  -36   95  127 -105  -60   37   -9 -106    7
  -31  105   13 -103 -123   79   17  -48 -108  -56  -87 -128   35  -94
  -45  118  -91   86  -63  -43   77    1 -127  -16  -71  -73  -76  -15
  -11]
TensorFlow Casted values (int8): 
 [   0 -128    0    0    1   -1    0    3   -3    2   -2    0  127  127
  127  127  127  127  127  127  127  127  127  127  127  127  127  127
  127  127  127  127  127  127  127  127  127  127  127  127  127  127
  127  127  127  127  127  127  127  127  127  127  127  127  127  127
  127  127  127  127  127  127  127  127  127  127  127  127  127  127
  127  127  127  127  127  127  127  127  127  127  127  127  127  127
  127  127  127  127  127  127  127  127  127  127  127  127  -76  -15
  -11]
```
```
",1
Unable to register CUDA plug-ins runnung docker image latest-gpu-jypyter,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.18.0

### Custom code

No

### OS platform and distribution

docker desktop 4.35.1 , ubuntu 24.04.1 LTS, WSL 2.3.24.0

### Mobile device

None

### Python version

Python 3.11.0rc1 (provided by tensorflow docker image)

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.7

### GPU model and memory

NVIDIA GeForce RTX 4060 Ti 16G

### Current behavior?

I Strictly followed the instructions provided in:
https://www.tensorflow.org/install/docker 
https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html
Got correct results running a sample workload (as suggested in nvidia contaner toolkit installation manual)

![image](https://github.com/user-attachments/assets/6f8c74e6-e23d-4395-8ffc-314f69af5bc7)

Downloaded tensorflow/tensorflow latest-gpu-jupyter image and ran the container.
Opend a new jupyter notebook (http://127.0.0.1:8888/tree?token=...)
Importing tensorflow I wanted to check the GPU support.
Got error messages and empy available gpu list.
`2024-11-06 10:31:50.143673: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1730889110.283427      23 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1730889110.322596      23 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-06 10:31:50.712357: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.`



### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.__version__

print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
```


### Relevant log output

```shell
2024-11-06 10:31:50.143673: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1730889110.283427      23 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1730889110.322596      23 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-06 10:31:50.712357: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
'2.18.0'

Num GPUs Available:  0
2024-11-06 10:31:54.748888: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (34)
```
",1
Cannot Convert 51 to a shape - Movenet pose classification tutorial,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

No

### OS platform and distribution

Win11, colab notebook

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

the notebook pose_classification.ipynb tutorial works fine until the following cell: 

`# Define the model
inputs = tf.keras.Input(shape=(51))
embedding = landmarks_to_embedding(inputs)

layer = keras.layers.Dense(128, activation=tf.nn.relu6)(embedding)
layer = keras.layers.Dropout(0.5)(layer)
layer = keras.layers.Dense(64, activation=tf.nn.relu6)(layer)
layer = keras.layers.Dropout(0.5)(layer)
outputs = keras.layers.Dense(len(class_names), activation=""softmax"")(layer)

model = keras.Model(inputs, outputs)
model.summary()`

I've run this both locally and within colab and get the same results. Any advice or insight would be helpful. 

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#scrollTo=1Pte6b1bgWKv
```


### Relevant log output

```shell
ValueError                                Traceback (most recent call last)

<ipython-input-28-f905a19927b3> in <cell line: 2>()
      1 # Define the model
----> 2 inputs = tf.keras.Input(shape=(51))
      3 embedding = landmarks_to_embedding(inputs)
      4 
      5 layer = keras.layers.Dense(128, activation=tf.nn.relu6)(embedding)

2 frames

/usr/local/lib/python3.10/dist-packages/keras/src/backend/common/variables.py in standardize_shape(shape)
    528             raise ValueError(""Undefined shapes are not supported."")
    529         if not hasattr(shape, ""__iter__""):
--> 530             raise ValueError(f""Cannot convert '{shape}' to a shape."")
    531         if config.backend() == ""tensorflow"":
    532             if isinstance(shape, tf.TensorShape):

ValueError: Cannot convert '51' to a shape.
```
",1
Current LiteRT Android dependencies in documentation look broken,"I think after the recent TensorflowLite rename to LiteRT some pages in documentation where renamed incorrectly and are currently very confusing.
For a particular example, see this: https://ai.google.dev/edge/litert/android/gpu
- The docs say to add `com.google.ai.edge.litert:litert-gpu` and `com.google.ai.edge.litert:litert-gpu-api` with versions `2.X.Y`, which do not exist (current latest version is `1.0.1`), to a toml version catalog. 
- In the next paragraph it also switches into using gradle files instead of toml to declare other dependencies, which I found somewhat confusing.
- Later, in [standalone setup](https://ai.google.dev/edge/litert/android/gpu#add_project_dependencies_2), it says to include `com.google.ai.edge.litert:litert-gpu-delegate-plugin` dependency, which does not exist and also follows with a code snipped supposedly showing how to include it, but it shows other dependencies.
- [Other places](https://ai.google.dev/edge/litert/android/metadata/codegen#acceleration) too include dependencies with incorrect versions, like `com.google.ai.edge.litert:litert-gpu:2.3.0`

I just happend to start working with LiteRT for Android right now and found it very difficult to distinguish which parts of documentation are outdated and which aren't.",0
Create a trainable tensorflow or LiteRT (with signatures) graph from a frozen tensorflow model,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.17

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am working with a frozen TensorFlow model (saved as a .pb file) and am exploring ways to make it trainable again, in its original TensorFlow format and eventually a LiteRT model with [signatures to train](https://ai.google.dev/edge/litert/models/ondevice_training).

The goal is to restore training capabilities (such as fine-tuning or continued training) from a model that has already been frozen. I would appreciate guidance on how to approach this scenario.

### Key Points:
#### Frozen TensorFlow Model:
I have a TensorFlow model that was previously trained and saved in its frozen state (as a .pb file). This model no longer contains trainable variables, as they have been converted into constants. However, I would like to continue training this model on new data or fine-tune it for a different task.

**Question 1**: What is the recommended approach to restore a frozen TensorFlow model to a state where it can be trained or fine-tuned again? Specifically, how do I extract the original model architecture and variables from the frozen graph to make it trainable once more?

**Question 2**: If the frozen model has layers that are not needed for retraining, how can I unfreeze and selectively fine-tune only some parts of the model while keeping others frozen?

#### LiteRT Model:
I am also exploring the possibility of taking a LiteRT model that was previously trained & converted to LiteRT, and then restoring its training capability. 

**Question 3**: Is it possible to convert a TensorFlow Lite model back into TensorFlow (with the appropriate weights) and add appropriate training signatures and then convert it back to a trainable LiteRT model? If so, what are the best practices, and any potential challenges involved in such a conversion?

**Question 4**: If not, and I want to fine-tune a model that has been converted to LiteRT, would it be better to work with the original TensorFlow model and re-convert it to LiteRT after training, rather than trying to restore training from the LiteRT version?

#### Expected Outcome:
Any guidance on the best approach to make a frozen model trainable again (whether in TensorFlow or TensorFlow Lite) would be greatly appreciated. Specifically, I’m seeking clarity on:

- How to restore training capability from a frozen .pb file in TensorFlow.
- How to manage layer freezing or unfreezing during fine-tuning.
- Whether a TensorFlow Lite model can be retrained, or if it's more efficient to work with the TensorFlow version instead.

### Standalone code to reproduce the issue

```shell
_
```


### Relevant log output

_No response_",1
WSL instruction outdated,"Page URL: https://www.tensorflow.org/install/pip#windows-wsl2

At this point in time Ubuntu 24.04 requires (or, recommends) using a virtual environment to install Python packages. As a result, the following command will no longer work in WSL:

```
python3 -m pip install tensorflow[and-cuda]
```

Below is a summary of commands needed to set up a virtual environment and install Tensorflow with the CUDA libraries.

```
sudo apt install python3-venv

python3 -m venv tf

source ~/tf/bin/activate

pip install tensorflow[and-cuda]
```",0
TFLite Interpreter `experimental_preserve_all_tensors` yields different output,"### 1. System information

- OS Platform and Distribution: Pop!_OS 22.04 LTS
- TensorFlow installation: pip package
- TensorFlow library: 2.17.0

### 2. Code



```
import tensorflow as tf
import numpy as np

from ai_edge_litert.interpreter import Interpreter

IMAGE_SHAPE = (10, 10, 1)
IMAGE_SIZE = IMAGE_SHAPE[0]*IMAGE_SHAPE[1]*IMAGE_SHAPE[2]

tf.random.set_seed(0)
np.random.seed(0)
tf.keras.utils.set_random_seed(0)

inp_layer = tf.keras.layers.InputLayer(shape=IMAGE_SHAPE, batch_size=1)
conv_layer = tf.keras.layers.Conv2D(4, (3, 3), strides=(2, 2), padding=""same"", use_bias=True, activation=""relu"")

model = tf.keras.models.Sequential([
  inp_layer,
  conv_layer,
])

w = conv_layer.get_weights()
k = w[0]
b = w[1]

k[:, :, :, 0] = 0
k[:, :, :, 2] = 0
b = np.zeros(shape=w[1].shape)

print(""kernel:\n"", k)
print(""bias:\n"", b)

w[0] = k
w[1] = b
conv_layer.set_weights(w)

def representative_dataset():
  for _ in range(10):
    data = np.random.rand(1, *IMAGE_SHAPE)
    yield [data.astype(np.float32)]

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8

tflite_quant_model = converter.convert()

#filename = ""example.tflite""
#with open(filename, 'wb') as f:
#  f.write(tflite_quant_model)

#interp = Interpreter(filename)
#exper = Interpreter(filename, experimental_preserve_all_tensors=True)
interp = Interpreter(model_content=tflite_quant_model)
exper = Interpreter(model_content=tflite_quant_model, experimental_preserve_all_tensors=True)


inp=np.zeros(IMAGE_SIZE, dtype=np.int8)
arr = [62, 63, 64, 72, 73, 74, 82, 83, 84]
for i in arr:
  inp[i] = i
t = tf.constant(inp, shape=IMAGE_SHAPE, dtype=tf.int8)

print(""input:\n"", t, ""\n"")

def alloc_and_run(interpreter):
  interpreter.allocate_tensors()

  input_details = interpreter.get_input_details()
  output_details = interpreter.get_output_details()

  interpreter.set_tensor(input_details[0]['index'], [t])
  interpreter.invoke()

  return interpreter.get_tensor(output_details[0]['index'])

out_interp = alloc_and_run(interp)
out_exper = alloc_and_run(exper)

print(""out_interp:\n"", out_interp, ""\n"")
print(""out_exper:\n"", out_exper, ""\n"")

assert(tf.reduce_all(tf.equal(out_interp, out_exper)))
```

### 3. Failure after conversion
Conversion is successful, but interpreter and experimental interpreter yield a different result:
```
out_interp:
 [[[[-128  -19 -128  -68]
   [-128  -19 -128  -68]
   [-128  -19 -128  -68]
   [-128  -19 -128  -68]
   [-128  -32 -128 -128]]

  [[-128  -19 -128  -68]
   [-128  -19 -128  -68]
   [-128  -19 -128  -68]
   [-128  -19 -128  -68]
   [-128  -32 -128 -128]]

  [[-128  -23 -128  -47]
   [-128   28 -128  -61]
   [-128   15 -128  -78]
   [-128  -19 -128  -68]
   [-128  -32 -128 -128]]

  [[-128  -12 -128   -5]
   [-128   57 -128  -34]
   [-128   10 -128  -98]
   [-128  -19 -128  -68]
   [-128  -32 -128 -128]]

  [[-128 -123 -128  -75]
   [-128 -128 -128  -66]
   [-128 -128 -128 -120]
   [-128 -117 -128  -83]
   [-128 -128 -128 -128]]]] 

out_exper:
 [[[[-128  -19 -128  -68]
   [-128  -19 -128  -68]
   [-128  -19 -128  -68]
   [-128  -19 -128  -68]
   [-128  -32 -128 -128]]

  [[-128  -19 -128  -68]
   [-128  -19 -128  -68]
   [-128  -19 -128  -68]
   [-128  -19 -128  -68]
   [-128  -32 -128 -128]]

  [[-128  -23 -128  -47]
   [-128   28 -128  -61]
   [-128   15 -128  -78]
   [-128  -19 -128  -68]
   [-128  -32 -128 -128]]

  [[-128  -12 -128   -5]
   [-128   57 -128  -33]
   [-128   10 -128  -98]
   [-128  -19 -128  -68]
   [-128  -32 -128 -128]]

  [[-128 -123 -128  -75]
   [-128 -128 -128  -66]
   [-128 -128 -128 -120]
   [-128 -117 -128  -83]
   [-128 -128 -128 -128]]]] 

Traceback (most recent call last):
  File ""/home/sri/riptools/scratch5/example.py"", line 79, in <module>
    assert(tf.reduce_all(tf.equal(out_interp, out_exper)))
AssertionError
```
The difference here is:
`out_interp[0][3][1][3] = -33`
`out_exper[0][3][1][3] = -34`
This ""off-by-one"" may occur with other sized input tensors and larger non-zero inputs/weights as well.",1
Unable to install TensorFlow: No matching distribution found for TensorFlow!,"When trying to install TensorFlow via pip, I encounter an error stating that no matching distribution can be found. The command I used and the error message are as follows:

```
C:\Users\Enes> pip install tensorflow
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow
```
Operating System: Windows 10
Python Version: (Python 3.13.0)

I am currently using Python 3.13. Could this be related to compatibility with this specific Python version?

Could you provide guidance on how to resolve this issue, or suggest any compatible alternatives?",1
`tf.math.floormod` not throwing `Integer division by zero` error on GPU for tensor of int64 dtype,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.3

### GPU model and memory

_No response_

### Current behavior?

Running `tf.math.floormod` with a tensor containing zeroes in the denominator tensor on GPU does not throw a division by zero error when the tensor has a dtype of `int64`.

[colab](https://colab.research.google.com/drive/1e053_hcmu0sHQcMPop-_WZR6ya1bw6dy?usp=sharing)

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
A = tf.constant([[2,2],[2,2]], dtype=tf.int64)
B = tf.constant([[0,0],[0,0]], dtype=tf.int64)

with tf.device(""/gpu:0""):
    output_gpu = tf.math.floormod(A, B) # No error
    print(f""\nGPU: {output_gpu}\n"") # GPU: [[2 2] [2 2]]

with tf.device(""/cpu:0""):
    output_cpu = tf.math.floormod(A, B) 
    # InvalidArgumentError: Integer division by zero
```


### Relevant log output

```shell
GPU: [[2 2]
 [2 2]]

---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-2-ba17e88e7095> in <cell line: 9>()
      8 
      9 with tf.device(""/cpu:0""):
---> 10     output_cpu = tf.math.floormod(A, B)
     11     # InvalidArgumentError: Integer division by zero

2 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   5981 def raise_from_not_ok_status(e, name) -> NoReturn:
   5982   e.message += ("" name: "" + str(name if name is not None else """"))
-> 5983   raise core._status_to_exception(e) from None  # pylint: disable=protected-access
   5984 
   5985 

InvalidArgumentError: {{function_node __wrapped__FloorMod_device_/job:localhost/replica:0/task:0/device:CPU:0}} Integer division by zero [Op:FloorMod] name:
```
",1
`tf.linalg.lstsq` producing outputs with large inconsistencies between CPU and GPU with `float32` tensors,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.3

### GPU model and memory

_No response_

### Current behavior?

Passing tensors of `float32` to 'tf.linalg.lstsq' is producing very different output from CPU and GPU.

### Standalone code to reproduce the issue
[colab](https://colab.research.google.com/drive/1Fyh3HQs39NhGlOLEdzpqPKVkznBe6Fe9?usp=sharing)
```shell
import tensorflow as tf
import numpy as np

A = tf.constant([[[[0.37454012, 0.9507143, 0.7319939, 0.5986585, 0.15601864], [0.15599452, 0.05808361, 0.8661761, 0.601115, 0.7080726,], [0.02058449, 0.96990985, 0.83244264,
                0.21233912, 0.18182497], [0.1834045, 0.30424225, 0.52475643, 0.43194503, 0.29122913], [0.6118529, 0.13949387, 0.29214466, 0.36636186, 0.45606998]]]], dtype=tf.float32)

B = tf.constant([[[[0.59241456, 0.04645041], [0.94888556, 0.965632,], [0.684233, 0.4401525,], [
                0.9093204, 0.25877997], [0.54671025, 0.18485446]]]], dtype=tf.float32)

with tf.device(""/gpu:0""):
    output_gpu = tf.linalg.lstsq(A, B)
    
with tf.device( ""/cpu:0""):
    output_cpu = tf.linalg.lstsq(A, B)

np.testing.assert_allclose(output_cpu, output_gpu.cpu(), atol=100) # AssertionError
```


### Relevant log output

```shell
AssertionError: 
Not equal to tolerance rtol=1e-07, atol=100

Mismatched elements: 2 / 10 (20%)
Max absolute difference: 112.95715
Max relative difference: 0.2963447
 x: array([[[[-170.69518 ,   28.379017],
         [ 164.85085 ,  -28.04222 ],
         [-273.4264  ,   46.952198],...
 y: array([[[[-241.07059 ,   40.25023 ],
         [ 232.80626 ,  -39.505226],
         [-386.38354 ,   66.00629 ],...
```
",1
Thread ID in TensorBoard Profiler Trace Viewer Could Be Negative,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17

### Custom code

No

### OS platform and distribution

Linux Mint 21.2

### Mobile device

_No response_

### Python version

3.11

### Bazel version

6.5.0

### GCC/compiler version

clang 14.0.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

While using TensorFlow's profiler, I found that some thread IDs displayed in the Trace Viewer are negative. As shown in the image below, thread names such as `tf_Compute/-1030865605`, `tf_data_private_threadpool/-102013...`, etc., have negative thread IDs.

![image](https://github.com/user-attachments/assets/71d32241-d617-4e52-bcbf-d3140ebef440)

My log file is also attached.

[20241030-161104.zip](https://github.com/user-attachments/files/17587809/20241030-161104.zip)

### Standalone code to reproduce the issue

```shell
(Just the profiler demo code) https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras
```


### Relevant log output

_No response_",1
TensorFlow Stable Delegate Python API,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.16

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

TensorFlow Python API currently doesn't support running stable delegates. Are there any workarounds? If not, are there any plans to support this in the future?

### Standalone code to reproduce the issue

```shell
self._model = tf.lite.Interpreter(
    model_path=stryolov5.tflite,
    experimental_delegates=[delegate.so]
)
```


### Relevant log output

_No response_",1
Significant Discrepancy in `tf.linalg.triangular_solve` Results Between CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.18.0-rc2-4-g6550e4bd802 2.18.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.4 LTS x86_64

### Mobile device

_No response_

### Python version

3.10.0

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When using `tf.linalg.triangular_solve` with large matrices or specific triangular matrix conditions (e.g., `upper=True`, `transpose=True`, `unitriangular=True`), the GPU results significantly differ from the CPU results. 

The discrepancy includes extremely large Mean Absolute Error (MAE) values and infinite Mean Squared Error (MSE) values of the results, indicating a possible issue in the GPU implementation of the function.

### Standalone code to reproduce the issue

[colab](https://colab.research.google.com/drive/1zXcOAkxHV6snaMyWGMeKIukf5IVBd0Bh?usp=sharing)
[Safe Tensors](https://drive.google.com/file/d/1n1JyugXNAS9M2wmnk9u9vCVuHz0uE3R6/view?usp=sharing)

```python
import tensorflow as tf
import numpy as np
from safetensors.torch import load_file

def set_seed(seed_value=42):
    """"""Sets the random seed for reproducibility.""""""
    np.random.seed(seed_value)
    tf.random.set_seed(seed_value)

def tensorflow_version(input, cpu=True):
    set_seed()
    if cpu:
        device_string = ""/cpu:0""
    else:
        device_string = ""/gpu:0""

    with tf.device(device_string):
        b_tensor = tf.constant(input[""b""])
        A_tensor = tf.constant(input[""A""])

        upper = input.get(""upper"", True)
        transpose = input.get(""transpose"", False)
        unitriangular = input.get(""unitriangular"", False)

        solution = tf.linalg.triangular_solve(
            A_tensor, b_tensor, lower=not upper, adjoint=transpose
        )

        return {""triangular_solve_solution"": solution.numpy()}

def load_safe_tensor(file_path):
    safe_tensor_data = load_file(file_path)
    A_tensor = safe_tensor_data[""A""]
    b_tensor = safe_tensor_data[""b""]

    return {
        ""A"": tf.convert_to_tensor(A_tensor.numpy()),
        ""b"": tf.convert_to_tensor(b_tensor.numpy()),
    }

def calculate_differences(cpu_result, gpu_result):
    diff = np.abs(cpu_result - gpu_result)
    mae = np.mean(diff)
    mse = np.mean(diff**2)
    rmse = np.sqrt(mse)
    max_diff = np.max(diff)
    mean_relative_diff = np.mean(diff / (np.abs(cpu_result) + 1e-10))

    return {
        ""Mean Absolute Error"": mae,
        ""Mean Squared Error"": mse,
        ""Root Mean Squared Error"": rmse,
        ""Maximum Absolute Difference"": max_diff,
        ""Mean Relative Difference"": mean_relative_diff,
    }

def main():
    file_path = ""tensorflow_triangular_solve_3.safetensors""
    input_data = load_safe_tensor(file_path)
    input_data[""upper""] = True
    input_data[""transpose""] = True
    input_data[""unitriangular""] = True

    result_cpu = tensorflow_version(input_data, cpu=True)
    print(""CPU Result:"")
    print(result_cpu)

    if tf.config.list_physical_devices(""GPU""):
        result_gpu = tensorflow_version(input_data, cpu=False)
        print(""GPU Result:"")
        print(result_gpu)

        if result_gpu:
            cpu_solution = result_cpu[""triangular_solve_solution""]
            gpu_solution = result_gpu[""triangular_solve_solution""]
            differences = calculate_differences(cpu_solution, gpu_solution)
            for key, value in differences.items():
                print(f""{key}: {value}"")
    else:
        print(""GPU not available."")

if __name__ == ""__main__"":
    main()
```

#### **Issue Summary:**
- The `tf.linalg.triangular_solve` function produces vastly different results on the GPU compared to the CPU. The differences include an enormous Mean Absolute Error (MAE) and an infinite Mean Squared Error (MSE), indicating a severe discrepancy between the CPU and GPU results.
- The issue appears to be related to the use of specific arguments like `upper=True`, `transpose=True`, and `unitriangular=True`, suggesting that there might be a numerical precision or stability issue in the GPU implementation.
- It is unclear why these large discrepancies occur, but they point to a critical inconsistency that could impact numerical reliability for users depending on TensorFlow’s triangular solve functionality.


### Relevant log output

```shell
Mean Absolute Error: 1.566790629150662e+21
Mean Squared Error: inf
Root Mean Squared Error: inf
Maximum Absolute Difference: 1.4265324671452624e+26
```
",1
[TF-2.18] Protoc-related Segmentation Fault on GH200 when Building from Source ,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

TF 2.18

### Custom code

No

### OS platform and distribution

Rocky Linux 9.3 

### Mobile device

_No response_

### Python version

3.9

### Bazel version

6.5.0

### GCC/compiler version

17.0.6 (LLVM)

### CUDA/cuDNN version

CUDA-12.4.1/cuDNN-8.9.6

### GPU model and memory

Grace Hopper GH200 

### Current behavior?

Since there is no pip package for GH200, and Amazon only provides CPU wheel for `aarch64`, we are building `tf-2.18` from src. 

```
$ git branch 
master 
* r2.18
```
Both LLVM and Bazel versions are according to recommendation
```
$ clang --version 
clang version 17.0.6
Target: aarch64-unknown-linux-gnu
Thread model: posix
``` 
``` 
$ bazel --version 
bazel 6.5.0- (@non-git)
```

Bazel build failed with protoc-related segmentation fault. 

### Standalone code to reproduce the issue

```shell
./configure 
Please specify the location of python. [Default is /scratch/optpar01/.conda/tf2_src/bin/python3]: 

Found possible Python library paths:
  /scratch/optpar01/.conda/tf2_src/lib/python3.9/site-packages
Please input the desired Python library path to use.  Default is [/scratch/optpar01/.conda/tf2_src/lib/python3.9/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: 
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the hermetic CUDA version you want to use or leave empty to use the default version. 12.4.1

Please specify the hermetic cuDNN version you want to use or leave empty to use the default version. 8.9.6

Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 7.0,8.0,9.0

Please specify the local CUDA path you want to use or leave empty to use the default version. 

Please specify the local CUDNN path you want to use or leave empty to use the default version. 

Please specify the local NCCL path you want to use or leave empty to use the default version. 


Do you want to use clang as CUDA compiler? [Y/n]: 
Clang will be used as CUDA compiler.

Please specify clang path that to be used as host compiler. [Default is /scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-13.3.0/llvm-17.0.6-52ysnwfyyhc6tchulc5wlen7h6yc3fju/bin/clang]: 

You have Clang 17.0.6 installed.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: --copt=-Wno-error=unused-command-line-argument

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
Not configuring the WORKSPACE for Android builds.
```


### Relevant log output

```shell
$ bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --config=cuda --config=cuda_wheel --verbose_failures
ERROR: /scratch/optpar01/.cache/_bazel_optpar01/1b4d76cef27e0cbebd5791dfb99ed3a8/external/local_tsl/tsl/profiler/protobuf/BUILD:36:17: Action external/local_tsl/tsl/profiler/protobuf/profiler_service.grpc.pb.h failed: (Segmentation fault): protoc failed: error executing command (from target @local_tsl//tsl/profiler/protobuf:_profiler_service_cc_grpc_proto_grpc_codegen) 
  (cd /scratch/optpar01/.cache/_bazel_optpar01/1b4d76cef27e0cbebd5791dfb99ed3a8/execroot/org_tensorflow && \
  exec env - \
    CLANG_CUDA_COMPILER_PATH=/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-13.3.0/llvm-17.0.6-52ysnwfyyhc6tchulc5wlen7h6yc3fju/bin/clang-17 \
    LD_LIBRARY_PATH=/apps/ARM_node/ARM_applications/python/3.12.4/lib:/apps/ARM_node/ARM_applications/python/3.12.4/lib/python3.12:/usr/lib64:/usr/lib64/slurm:/usr/local/lib \
    PATH=/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-13.3.0/bazel-6.5.0-es6l5xjprjwauy22iuwlxoj4uztdcxsu/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-13.3.0/zip-3.0-pxcrfxbv4w274tczkoz7t4a54rv3r5de/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/clang-17.0.6/openjdk-11.0.23_9-s6cgcclfuzjttl5cdh3evsp3rrmuuvo6/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-13.3.0/bazel-6.5.0-es6l5xjprjwauy22iuwlxoj4uztdcxsu/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-13.3.0/zip-3.0-pxcrfxbv4w274tczkoz7t4a54rv3r5de/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-11.4.1/bzip2-1.0.8-jvd2b77w6emrwhj2g7mquhr7oteeb2uk/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/clang-17.0.6/openjdk-11.0.23_9-s6cgcclfuzjttl5cdh3evsp3rrmuuvo6/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-13.3.0/llvm-17.0.6-52ysnwfyyhc6tchulc5wlen7h6yc3fju/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-13.3.0/unzip-6.0-fpkt734eot7dakwydys5mgo5yomxwac3/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-11.4.1/curl-8.7.1-ca7uqkpgoorf7yzvajfyab2dfty7eitr/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-13.3.0/llvm-17.0.6-52ysnwfyyhc6tchulc5wlen7h6yc3fju/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-13.3.0/swig-4.1.1-gpwxntk2voqulme22h55ngp5ztgzyg6x/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-11.4.1/pcre2-10.43-bzcntkq3ul5umao7fcke5w4vrlyssbk7/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-13.3.0/lua-5.3.6-lec5qldurljnfjg4jo2fvsekfughzwru/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-13.3.0/unzip-6.0-fpkt734eot7dakwydys5mgo5yomxwac3/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-11.4.1/readline-8.2-47up4u4jjwf4io45zeioeyrh3wwwg4tg/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-11.4.1/curl-8.7.1-ca7uqkpgoorf7yzvajfyab2dfty7eitr/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-11.4.1/openssl-3.3.1-7uxchckf5qrcojfgntn77hvtoxjjkrq6/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-11.4.1/nghttp2-1.62.0-x5y5gbq2qybl4iiqvjv7bflc2nlx35zu/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-11.4.1/hwloc-2.9.3-onlxonz6ibms2b6fxhk7wqcdpo5l5myn/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-11.4.1/ncurses-6.5-fvbf5zfdj2bcwylse6cv6mztrwadhgyv/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-13.3.0/binutils-2.43.1-t3kxibwizkzbb7e3boxm3qkpngjuzzns/bin:/scratch/optpar01/spack/opt/spack/linux-rocky9-neoverse_v2/gcc-11.4.1/zstd-1.5.6-miqeogpdx7z7lcovzh3bsfwr2tzihxw2/bin:/scratch/optpar01/.conda/tf2_src/bin:/apps/ARM_node/ARM_applications/Miniconda/24.5.0/condabin:/apps/ARM_node/ARM_applications/python/3.12.4/bin:/home01/optpar01/.cargo/bin:/scratch/optpar01/spack/bin:/home01/optpar01/apps/build/gv/3.7.4/bin:/apps/applications/htop/3.0.5:/apps/applications/nvtop/1.1.0/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home01/optpar01/bin \
    PYTHON_BIN_PATH=/scratch/optpar01/.conda/tf2_src/bin/python3 \
    PYTHON_LIB_PATH=/scratch/optpar01/.conda/tf2_src/lib/python3.9/site-packages \
    SPACK_LOADED_HASHES=es6l5xjprjwauy22iuwlxoj4uztdcxsu:52ysnwfyyhc6tchulc5wlen7h6yc3fju \
    SPACK_PYTHON=/usr/bin/python3 \
    SPACK_ROOT=/scratch/optpar01/spack \
    TF2_BEHAVIOR=1 \
  bazel-out/aarch64-opt-exec-50AE0418/bin/external/com_google_protobuf/protoc '--plugin=protoc-gen-PLUGIN=bazel-out/aarch64-opt-exec-50AE0418/bin/external/com_github_grpc_grpc/src/compiler/grpc_cpp_plugin' '--PLUGIN_out=services_namespace=grpc,generate_mock_code=true:bazel-out/aarch64-opt/bin/external/local_tsl' '--proto_path=external/local_tsl' '--proto_path=external/local_tsl' '--proto_path=bazel-out/aarch64-opt/bin/external/com_google_protobuf/_virtual_imports/any_proto' '--proto_path=bazel-out/aarch64-opt/bin/external/com_google_protobuf/_virtual_imports/api_proto' '--proto_path=bazel-out/aarch64-opt/bin/external/com_google_protobuf/_virtual_imports/source_context_proto' '--proto_path=bazel-out/aarch64-opt/bin/external/com_google_protobuf/_virtual_imports/type_proto' '--proto_path=bazel-out/aarch64-opt/bin/external/com_google_protobuf/_virtual_imports/compiler_plugin_proto' '--proto_path=bazel-out/aarch64-opt/bin/external/com_google_protobuf/_virtual_imports/descriptor_proto' '--proto_path=bazel-out/aarch64-opt/bin/external/com_google_protobuf/_virtual_imports/duration_proto' '--proto_path=bazel-out/aarch64-opt/bin/external/com_google_protobuf/_virtual_imports/empty_proto' '--proto_path=bazel-out/aarch64-opt/bin/external/com_google_protobuf/_virtual_imports/field_mask_proto' '--proto_path=bazel-out/aarch64-opt/bin/external/com_google_protobuf/_virtual_imports/struct_proto' '--proto_path=bazel-out/aarch64-opt/bin/external/com_google_protobuf/_virtual_imports/timestamp_proto' '--proto_path=bazel-out/aarch64-opt/bin/external/com_google_protobuf/_virtual_imports/wrappers_proto' '--proto_path=external/local_tsl' '--proto_path=bazel-out/aarch64-opt/bin/external/local_tsl/external/local_tsl' external/local_tsl/tsl/profiler/protobuf/profiler_service.proto)
```
```
",1
`lookup_ops.StaticVocabularyTable` and `lookup_ops.StaticVocabularyTableV1`  can cause a crash,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly  2.19.0-dev20241025

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `segmentation fault issue` in TensorFlow when I used API `lookup_ops.StaticVocabularyTable` or `lookup_ops.StaticVocabularyTableV1` . I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build).
Please find the [gist](https://colab.research.google.com/drive/1NGnFQqFl6Jy_Xt7wBL3ynmSVs9AiFw_l?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
import os
from tensorflow.python.ops import lookup_ops

def _createVocabFile(basename, values=('brain', 'salad', 'surgery')):
    vocabulary_file = os.path.join(""/tmp"", basename)
    with open(vocabulary_file, 'w') as f:
        f.write('\n'.join(values) + '\n')
    return vocabulary_file

vocab_file = _createVocabFile('feat_to_id_1.txt')
vocab_size = 9223372036854775807
oov_buckets = 1
table = lookup_ops.StaticVocabularyTable(lookup_ops.TextFileIdTableInitializer(vocab_file, vocab_size=vocab_size), oov_buckets, experimental_is_anonymous=True)
#lookup_ops.StaticVocabularyTableV1(lookup_ops.TextFileIdTableInitializer(vocab_file, vocab_size=vocab_size), oov_buckets, experimental_is_anonymous=True)
```


### Relevant log output

```shell
Segmentation fault (core dumped)
```
",2
"`gen_list_ops.tensor_list_concat_v2` aborts with ""Check failed: size >= 0 (0 vs. -1)""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly  2.19.0-dev20241025

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

Linux Ubuntu 20.04.3 LTS

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build)

Please find the [[gist](https://colab.research.google.com/drive/1Y-ZQk0KgIGfzCMfLVdemJ2UqXLeQWKUd?usp=sharing)](https://colab.research.google.com/drive/1Y-ZQk0KgIGfzCMfLVdemJ2UqXLeQWKUd?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import gen_list_ops
from tensorflow.python.ops import list_ops

l = list_ops.tensor_list_reserve(element_dtype=dtypes.float32, element_shape=None, num_elements=3)
t = gen_list_ops.tensor_list_concat_v2(l, element_dtype=dtypes.float32, element_shape=list_ops._build_element_shape((None, 3)), leading_dims=[-1, 3, 5])
```


### Relevant log output

```shell
F tensorflow/core/framework/tensor_shape.cc:587] Check failed: size >= 0 (0 vs. -1)
Aborted (core dumped)
```
",2
"`data_flow_ops.Barrier` aborts with ""Check failed: i >= 0 (0 vs. -100)"" ","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly  2.19.0-dev20241025

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build)

Please find the [gist](https://colab.research.google.com/drive/1OOOIvZ7brRDjRqshv36CA_55BlPbgI4e?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import data_flow_ops
from tensorflow.python.eager import context
import tensorflow as tf

with context.graph_mode():
    sess = tf.compat.v1.Session()
    with sess.as_default():
        b = data_flow_ops.Barrier((dtypes.float32, dtypes.float32), shapes=((), ()), name='B')
        keys = [b'a', b'b', b'c', b'd']
        values_0 = [10.0, 20.0, 30.0, 40.0]
        values_1 = [100.0, 200.0, 300.0, 400.0]
        insert_1_1_op = b.insert_many(-100, keys[0:2], values_1[0:2]) 
        insert_1_1_op.run()
```


### Relevant log output

```shell
F tensorflow/core/kernels/barrier_ops.cc:286] Check failed: i >= 0 (0 vs. -100)
Aborted (core dumped)
```
",2
Regression: TF 2.18 crashes with cudaSetDevice failing due to GPU being busy,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tensorflow[and-cuda] 2.18

### Custom code

No

### OS platform and distribution

Rocky 9.1

### Mobile device

_No response_

### Python version

3.12.7

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

get_build_info gives cuda 12.5.1, cudnn version 9, Driver version 525.78.01, system CUDA version 12.0

### GPU model and memory

RTX 3090

### Current behavior?

Tensorflow[and-cuda] 2.17 installed cuda_version 12.3 and cudnn 8, and the code below works with the normal error spew about being unable to register cuda factories. With 2.18, I get a crash about the gpu is busy.

This is on a desktop workstation with a window manager, so TF cannot have the entire GPU, but I'm not sure if that's the cause of the problem.

These are pretty normal conda environments, with `pip install ""tensorflow[and-cuda]""` and then 

```
export XLA_FLAGS=""--xla_gpu_cuda_data_dir=/path/to/my/conda/environment""


NVIDIA_DIR=$(dirname $(dirname $(python -c ""import nvidia.cudnn; print(nvidia.cudnn.__file__)"")))

pathAccum=""""

for dir in $NVIDIA_DIR/*; do
    if [ -d ""$dir/lib"" ]; then
        pathAccum=""$dir/lib:$pathAccum""
    fi
done

export LD_LIBRARY_PATH=""${pathAccum}${LD_LIBRARY_PATH}""
```

inside the activate.d for the conda environment. 
I have tried with and without this activate script. It is necessary for 2.17, and I get the same error when I remove it for 2.18. I'm not sure if this is due to ""hermetic cuda"", since it's not at all clear what that means to an end user.

Interestingly, `tf.config.list_physical_devices(""GPU"")` correctly identifies the GPU, it returns `PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')`. 

### Standalone code to reproduce the issue

```shell
#!/usr/bin/env python3
# TensorFlow and tf.keras
from tensorflow import keras

model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(10)
])
```


### Relevant log output

```shell
TF 2.17 (no bug):

2024-10-25 14:07:40.179745: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-10-25 14:07:40.187066: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-10-25 14:07:40.194965: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-10-25 14:07:40.197313: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-10-25 14:07:40.203777: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-10-25 14:07:40.624911: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/scratch/miniconda/install/envs/bpreveal-teak/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1729883261.043484 2360745 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1729883261.057001 2360745 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1729883261.057122 2360745 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1729883261.058905 2360745 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1729883261.059226 2360745 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1729883261.059269 2360745 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1729883261.521319 2360745 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1729883261.521431 2360745 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1729883261.521503 2360745 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-10-25 14:07:41.521550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18871 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6



TF 2.18 (with bug):

2024-10-25 14:02:53.202498: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-10-25 14:02:53.209011: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1729882973.216967 2360012 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1729882973.219326 2360012 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-10-25 14:02:53.227106: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/scratch/miniconda/install/envs/bpreveal-testing/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
Traceback (most recent call last):
  File ""/home/cm2363/scratch/./tfTest.py"", line 5, in <module>
    model = keras.Sequential([
            ^^^^^^^^^^^^^^^^^^
  File ""/scratch/miniconda/install/envs/bpreveal-testing/lib/python3.12/site-packages/keras/src/models/sequential.py"", line 76, in __init__
    self._maybe_rebuild()
  File ""/scratch/miniconda/install/envs/bpreveal-testing/lib/python3.12/site-packages/keras/src/models/sequential.py"", line 141, in _maybe_rebuild
    self.build(input_shape)
  File ""/scratch/miniconda/install/envs/bpreveal-testing/lib/python3.12/site-packages/keras/src/layers/layer.py"", line 226, in build_wrapper
    original_build_method(*args, **kwargs)
  File ""/scratch/miniconda/install/envs/bpreveal-testing/lib/python3.12/site-packages/keras/src/models/sequential.py"", line 187, in build
    x = layer(x)
        ^^^^^^^^
  File ""/scratch/miniconda/install/envs/bpreveal-testing/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/scratch/miniconda/install/envs/bpreveal-testing/lib/python3.12/site-packages/keras/src/backend/tensorflow/core.py"", line 125, in convert_to_tensor
    return tf.convert_to_tensor(x, dtype=dtype)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.InternalError: cudaSetDevice() on GPU:0 failed. Status: CUDA-capable device(s) is/are busy or unavailable
```
",2
It doesn't support on python3.13,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

macos sequoia arm

### Mobile device

_No response_

### Python version

3.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

It doesn't work when I want to install via terminal with the installation code. 

### Standalone code to reproduce the issue

```shell
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow
```


### Relevant log output

_No response_",1
Does TFLite dequantize opertor support constant buffer input,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 24.04
- TensorFlow installed from (source or binary): source 
- TensorFlow version (or github SHA if from source): the latest version

**Standalone code to reproduce the issue** 
<img width=""724"" alt=""image"" src=""https://github.com/user-attachments/assets/849a0952-90ce-42c4-b2dc-9a9e7333fb0b"">
The [tflite model for user input](https://github.com/fujunwei/tensorflow/blob/tflite_label_image/tensorflow/lite/examples/python/dequantize_input.tflite) can work as expected

<img width=""713"" alt=""image"" src=""https://github.com/user-attachments/assets/43cafc84-0ea6-4a06-84b4-4772c6cea8e8"">

The [tflite model with constant buffer](https://github.com/fujunwei/tensorflow/blob/tflite_label_image/tensorflow/lite/examples/python/dequantize_constant.tflite) can't compute, so does [dequantize](https://www.tensorflow.org/mlir/tfl_ops#tfldequantize_tfldequantizeop) support constant input?

**Any other info / logs**

You can also modify [dequantize_tester in the Repo](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/dequantize_tester.cc#L110) to reproduce this issue like below code:

1, Create a constant buffer
```
  const auto buffer_data = builder.CreateVector( reinterpret_cast<const uint8_t*>(constant_buffer.data()),
                                                                              constant_buffer.size());
  const auto buffer_index = base::checked_cast<uint32_t>(buffers.size());
  buffers.emplace_back(::tflite::CreateBuffer(builder, buffer_data));
```

2, Use the `buffer_index ` when creating tensor:
```
const std::array<flatbuffers::Offset<::tflite::Tensor>, 2> tensors{{
      ::tflite::CreateTensor(
          builder,
          builder.CreateVector<int32_t>(Shape().data(), Shape().size()),
          Unsigned() ? ::tflite::TensorType_UINT8 : ::tflite::TensorType_INT8,
          /*buffer=*/buffer_index , /*name=*/0,
          ::tflite::CreateQuantizationParameters(
              builder, /*min=*/0, /*max=*/0,
              builder.CreateVector<float>({InputScale()}),
              builder.CreateVector<int64_t>({InputZeroPoint()}))),
      ::tflite::CreateTensor(
          builder,
          builder.CreateVector<int32_t>(Shape().data(), Shape().size()),
          ::tflite::TensorType_FLOAT32,
          /*buffer=*/0, /*name=*/builder.CreateString(""dequantizeLinearOutput"")),
  }};

```",0
How to map ScatterElements with TFLite operator,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 24.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): the latest version

**Standalone code to reproduce the issue** 
[WebNN scatterElements](https://source.chromium.org/chromium/chromium/src/+/main:services/webnn/public/mojom/webnn_graph.mojom;l=1028?q=webnn_graph.&ss=chromium%2Fchromium%2Fsrc)  operation first copies the values of `input` tensor to `output` tensor, and then overwrites the values of `output` tensor to values specified by `updates` tensor at specific index positions specified by `indices` tensor along `axis` dimension.

For example: Scatter elements along axis 0
```
 input = [[0.0, 0.0, 0.0],
          [0.0, 0.0, 0.0],
          [0.0, 0.0, 0.0]]
 indices = [[1, 0, 2],
            [0, 2, 1]]
 updates = [[1.0, 1.1, 1.2],
            [2.0, 2.1, 2.2]]
 output = [[2.0, 1.1, 0.0]
           [1.0, 0.0, 2.2]
           [0.0, 2.1, 1.2]]
```

there is no TFLite operator to map with scatterElements, can the [BuiltinOperator_STABLEHLO_SCATTER](https://source.chromium.org/chromium/chromium/src/+/main:third_party/tflite/src/tensorflow/compiler/mlir/lite/schema/schema_generated.h;l=1210;bpv=0;bpt=1)  implement the `scatterElements` operation?

**Any other info / logs**

WebNN scatterElements is similar with [ONNX ScatterElements](https://onnx.ai/onnx/operators/onnx__ScatterElements.html).
",0
How to map GatherElements with TFLite operator,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 24.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): the latest version

**Standalone code to reproduce the issue** 
[WebNN GatherElements](https://source.chromium.org/chromium/chromium/src/+/main:services/webnn/public/mojom/webnn_graph.mojom;l=606?q=webnn_graph.&ss=chromium%2Fchromium%2Fsrc)  operation gather elements from the axis dimension of the input tensor indexed by the indices tensor following the equation below:

```
 output[dIndex0, ..., dIndexN] =  input[dIndex0, ..., indices[dIndex0, ..., dIndexN], ..., dIndexN]
                                                        ^ This is dAxis, indicated by `axis` parameter.
```

For example:
```
an input =  [[ 0,  1,  2],
             [10, 11, 12],
             [20, 21, 22]] with shape (3, 3),
an indices = [[1, 0],
             [2, 1],
             [0, 2]] with shape (3, 2),
and axis = 1,
the output should be [[ 1,  0],
                       [12, 11],
                       [20, 22]] with shape (3, 2).
```

WebNN has supported gather and gatherND like below table:

| WebNN operation | TFLite operator| Status|
| ------------ | ---------------| ---------------|
| gather | [tfl.gather](https://www.tensorflow.org/mlir/tfl_ops#tflgather_tflgatherop) | Done |
| gatherND| [tfl.gather_nd](https://www.tensorflow.org/mlir/tfl_ops#tflgather_nd_tflgatherndop) | Done |
| gatherElements| | |

but there is no TFLite operator to map with gatherElements, can the [BuiltinOperator_STABLEHLO_GATHER](https://source.chromium.org/chromium/chromium/src/+/main:third_party/tflite/src/tensorflow/compiler/mlir/lite/schema/schema_generated.h;l=1221?q=BuiltinOperator_STABLEHLO_GATHER&ss=chromium%2Fchromium%2Fsrc)  implement the `gatherElements` operation?

**Any other info / logs**

WebNN gatherElements is similar with [ONNX GatherElements](https://onnx.ai/onnx/operators/onnx__GatherElements.html).
",0
No kernels registered for op `Conv2DBackpropInputV2`,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The existence of operator `Conv2DBackpropInputV2` is described in the official website document.https://www.tensorflow.org/api_docs/python/tf/raw_ops/Conv2DBackpropInputV2.
However, during my actual execution, the following error message appears:

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input_sizes = tf.constant(1, shape=[4], dtype=tf.int32)
filter_tensor = tf.constant(2, shape=[4], dtype=tf.float32)
out_backprop = tf.constant(5, shape=[1, 5, 5, 2], dtype=tf.float32)

strides = [1, 1, 1, 1]
padding = ""SAME""

tf.raw_ops.Conv2DBackpropInputV2(
    input=input_sizes,
    filter=filter_tensor,
    out_backprop=out_backprop,
    strides=strides,
    padding=padding
)
```


### Relevant log output

```shell
2024-10-21 12:30:18.492624: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Traceback (most recent call last):
  File ""/mnt/tests/Conv2DBackpropInput.py"", line 10, in <module>
    tf.raw_ops.Conv2DBackpropInputV2(
  File ""/mnt/origin/venv/tensorflow-nightly/lib/python3.11/site-packages/tensorflow/python/util/tf_export.py"", line 377, in wrapper
    return f(**kwargs)
           ^^^^^^^^^^^
  File ""/mnt/origin/venv/tensorflow-nightly/lib/python3.11/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 2030, in conv2d_backprop_input_v2
    _ops.raise_from_not_ok_status(e, name)
  File ""/mnt/origin/venv/tensorflow-nightly/lib/python3.11/site-packages/tensorflow/python/framework/ops.py"", line 5983, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node Conv2DBackpropInputV2}} = Conv2DBackpropInputV2[T=DT_INT32, data_format=""NHWC"", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true]
All kernels registered for op Conv2DBackpropInputV2:
  <no registered kernels>
 [Op:Conv2DBackpropInputV2] name:
```
",1
Overflow and Check fail in `tf.raw_ops.Conv2DBackpropInput`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Overflow : `input_sizes` is a one-dimensional tensor and contains maximum values
Check fail：`input_sizes`'s shape is [2] and The dimension of `filter` is less than 2

### Standalone code to reproduce the issue

```shell
Overflow:

import tensorflow as tf

input_sizes = tf.constant([1, 5, 5, 999999999999], shape=[4], dtype=tf.int32)
filter_tensor = tf.constant(3, shape=[3, 3, 3, 2], dtype=tf.float32)
out_backprop = tf.constant(5, shape=[1, 5, 5, 2], dtype=tf.float32)

strides = [1, 1, 1, 1]
padding = ""SAME""

tf.raw_ops.Conv2DBackpropInput(
    input_sizes=input_sizes,
    filter=filter_tensor,
    out_backprop=out_backprop,
    strides=strides,
    padding=padding
)

Check fail:
```python
import tensorflow as tf

input_sizes = tf.constant(1, shape=[2], dtype=tf.int32)
filter_tensor = tf.constant(2, shape=[1], dtype=tf.float32)
out_backprop = tf.constant(5, shape=[1, 5, 5, 2], dtype=tf.float32)

strides = [1, 1, 1, 1]
padding = ""SAME""

tf.raw_ops.Conv2DBackpropInput(
    input_sizes=input_sizes,
    filter=filter_tensor,
    out_backprop=out_backprop,
    strides=strides,
    padding=padding
)
```
```


### Relevant log output

```shell
Overflow:

2024-10-21 11:40:49.417611: F tensorflow/core/kernels/mkl/mkl_conv_grad_input_ops.cc:578] Non-OK-status: tensor::MakeShape(input_tensor, &input_tf_shape)
Status: INVALID_ARGUMENT: Dimension -727379969 must be >= 0
Aborted (core dumped)
```

Check fail:
```
2024-10-21 11:16:29.937265: F tensorflow/core/framework/tensor_shape.cc:357] Check failed: d < dims() (2 vs. 1)
Aborted (core dumped)
```
```
",1
Overflow in `tf.raw_ops.Fill`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Overflow in `tf.raw_ops.Fill` when there are too large values in `dims`.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1GDBN4lheNUIW704hsXVt3lW55UJue97S?usp=sharing
```


### Relevant log output

```shell
Kill
```
",1
tflite-support build is failing for elinux_aarch6,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tflite-support 0.4.4

### Custom code

No

### OS platform and distribution

Ubuntu 22.04 Arm

### Mobile device

NXP i.mx8 plus

### Python version

3.10.12

### Bazel version

5.1.1

### GCC/compiler version

11.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

pip install tflite-support on the NXP i.mx8 plus board gets a very old version of the package (0.1.0)

I need the 0.4.4 wheel built for arm64, to make use of TextEmbedder.

I expect the bazel build to succeed on the Ubuntu 22.04 arm64 based machine that I have (AWS T4G EC2 instance) from where I plan to take the wheel and deploy to the NXP device. 

The build is failing.

### Standalone code to reproduce the issue

```shell
1. Obtain the source code from https://github.com/tensorflow/tflite-support/archive/refs/tags/v0.4.4.tar.gz

2. Install bazel using https://github.com/bazelbuild/bazelisk/releases/download/v1.22.0/bazelisk-linux-arm64

3. Modify tensorflow_lite_support/tools/pip_package/rpi/build_arm_pip_package.sh to remove build for elinux_armhf, as I need only the elinux_aarch64 to be built.

4. Run tensorflow_lite_support/tools/pip_package/rpi/build_arm_pip_package.sh

It results in the errors show in in the log output. Same issue occurs when building from nightly as well as 0.4.3.
```


### Relevant log output

```shell
ERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/com_google_absl/absl/types/BUILD.bazel:154:11: Compiling absl/types/bad_optional_access.cc failed: (Exit 2): aarch64-none-linux-gnu-gcc failed: error executing command
  (cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/execroot/org_tensorflow_lite_support && \
  exec env - \
    PATH=/home/ubuntu/.cache/bazelisk/downloads/sha256/a590a28608772e779efc0c29bb678cd2a150deb27a9f8c557cc1d2b131a779ef/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/ubuntu/bazelisk \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
  /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/bin/aarch64-none-linux-gnu-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/lib/gcc/aarch64-none-linux-gnu/11.3.1/include -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/lib/gcc/aarch64-none-linux-gnu/11.3.1/include-fixed -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/aarch64-none-linux-gnu/include/c++/11.3.1/ -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/aarch64-none-linux-gnu/libc/usr/include/ -isystem /usr/include/python3.5 -isystem /usr/include/ -MD -MF bazel-out/aarch64-opt/bin/external/com_google_absl/absl/types/_objs/bad_optional_access/bad_optional_access.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/external/com_google_absl/absl/types/_objs/bad_optional_access/bad_optional_access.pic.o' -fPIC -iquote external/com_google_absl -iquote bazel-out/aarch64-opt/bin/external/com_google_absl -w '-std=c++17' -Wall -Wextra -Wcast-qual -Wconversion-null -Wformat-security -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wundef -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwrite-strings -DNOMINMAX -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -no-canonical-prefixes -fno-canonical-system-headers -c external/com_google_absl/absl/types/bad_optional_access.cc -o bazel-out/aarch64-opt/bin/external/com_google_absl/absl/types/_objs/bad_optional_access/bad_optional_access.pic.o)
# Configuration: 2e794a98601ad29846b443b77992a53d92a5a762ca0ee677f9a8aca3a1760abb
# Execution platform: @local_execution_config_platform//:platform
/home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/bin/aarch64-none-linux-gnu-gcc: 1: Syntax error: Unterminated quoted string
ERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/com_google_absl/absl/strings/BUILD.bazel:30:11: Compiling absl/strings/escaping.cc failed: (Exit 2): aarch64-none-linux-gnu-gcc failed: error executing command
  (cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/execroot/org_tensorflow_lite_support && \
  exec env - \
    PATH=/home/ubuntu/.cache/bazelisk/downloads/sha256/a590a28608772e779efc0c29bb678cd2a150deb27a9f8c557cc1d2b131a779ef/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/ubuntu/bazelisk \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
  /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/bin/aarch64-none-linux-gnu-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/lib/gcc/aarch64-none-linux-gnu/11.3.1/include -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/lib/gcc/aarch64-none-linux-gnu/11.3.1/include-fixed -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/aarch64-none-linux-gnu/include/c++/11.3.1/ -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/aarch64-none-linux-gnu/libc/usr/include/ -isystem /usr/include/python3.5 -isystem /usr/include/ -MD -MF bazel-out/aarch64-opt/bin/external/com_google_absl/absl/strings/_objs/strings/escaping.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/external/com_google_absl/absl/strings/_objs/strings/escaping.pic.o' -fPIC -iquote external/com_google_absl -iquote bazel-out/aarch64-opt/bin/external/com_google_absl -w '-std=c++17' -Wall -Wextra -Wcast-qual -Wconversion-null -Wformat-security -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wundef -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwrite-strings -DNOMINMAX -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -no-canonical-prefixes -fno-canonical-system-headers -c external/com_google_absl/absl/strings/escaping.cc -o bazel-out/aarch64-opt/bin/external/com_google_absl/absl/strings/_objs/strings/escaping.pic.o)
# Configuration: 2e794a98601ad29846b443b77992a53d92a5a762ca0ee677f9a8aca3a1760abb
# Execution platform: @local_execution_config_platform//:platform
/home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/bin/aarch64-none-linux-gnu-gcc: 1: Syntax error: Unterminated quoted string
ERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/com_google_absl/absl/strings/BUILD.bazel:30:11: Compiling absl/strings/internal/memutil.cc failed: (Exit 2): aarch64-none-linux-gnu-gcc failed: error executing command
  (cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/execroot/org_tensorflow_lite_support && \
  exec env - \
    PATH=/home/ubuntu/.cache/bazelisk/downloads/sha256/a590a28608772e779efc0c29bb678cd2a150deb27a9f8c557cc1d2b131a779ef/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/ubuntu/bazelisk \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
  /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/bin/aarch64-none-linux-gnu-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/lib/gcc/aarch64-none-linux-gnu/11.3.1/include -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/lib/gcc/aarch64-none-linux-gnu/11.3.1/include-fixed -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/aarch64-none-linux-gnu/include/c++/11.3.1/ -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/aarch64-none-linux-gnu/libc/usr/include/ -isystem /usr/include/python3.5 -isystem /usr/include/ -MD -MF bazel-out/aarch64-opt/bin/external/com_google_absl/absl/strings/_objs/strings/memutil.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/external/com_google_absl/absl/strings/_objs/strings/memutil.pic.o' -fPIC -iquote external/com_google_absl -iquote bazel-out/aarch64-opt/bin/external/com_google_absl -w '-std=c++17' -Wall -Wextra -Wcast-qual -Wconversion-null -Wformat-security -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wundef -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwrite-strings -DNOMINMAX -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -no-canonical-prefixes -fno-canonical-system-headers -c external/com_google_absl/absl/strings/internal/memutil.cc -o bazel-out/aarch64-opt/bin/external/com_google_absl/absl/strings/_objs/strings/memutil.pic.o)
# Configuration: 2e794a98601ad29846b443b77992a53d92a5a762ca0ee677f9a8aca3a1760abb
# Execution platform: @local_execution_config_platform//:platform
/home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/bin/aarch64-none-linux-gnu-gcc: 1: Syntax error: Unterminated quoted string
ERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/com_google_absl/absl/strings/BUILD.bazel:30:11: Compiling absl/strings/string_view.cc failed: (Exit 2): aarch64-none-linux-gnu-gcc failed: error executing command
  (cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/execroot/org_tensorflow_lite_support && \
  exec env - \
    PATH=/home/ubuntu/.cache/bazelisk/downloads/sha256/a590a28608772e779efc0c29bb678cd2a150deb27a9f8c557cc1d2b131a779ef/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/ubuntu/bazelisk \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
  /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/bin/aarch64-none-linux-gnu-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/lib/gcc/aarch64-none-linux-gnu/11.3.1/include -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/lib/gcc/aarch64-none-linux-gnu/11.3.1/include-fixed -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/aarch64-none-linux-gnu/include/c++/11.3.1/ -isystem /home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/aarch64-none-linux-gnu/libc/usr/include/ -isystem /usr/include/python3.5 -isystem /usr/include/ -MD -MF bazel-out/aarch64-opt/bin/external/com_google_absl/absl/strings/_objs/strings/string_view.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/external/com_google_absl/absl/strings/_objs/strings/string_view.pic.o' -fPIC -iquote external/com_google_absl -iquote bazel-out/aarch64-opt/bin/external/com_google_absl -w '-std=c++17' -Wall -Wextra -Wcast-qual -Wconversion-null -Wformat-security -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wundef -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwrite-strings -DNOMINMAX -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -no-canonical-prefixes -fno-canonical-system-headers -c external/com_google_absl/absl/strings/string_view.cc -o bazel-out/aarch64-opt/bin/external/com_google_absl/absl/strings/_objs/strings/string_view.pic.o)
# Configuration: 2e794a98601ad29846b443b77992a53d92a5a762ca0ee677f9a8aca3a1760abb
# Execution platform: @local_execution_config_platform//:platform
/home/ubuntu/.cache/bazel/_bazel_ubuntu/54ed875be5e8bbb87133512fb093e2b6/external/aarch64_linux_toolchain/bin/aarch64-none-linux-gnu-gcc: 1: Syntax error: Unterminated quoted string
Target //tensorflow_lite_support/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 31.453s, Critical Path: 0.47s
INFO: 33 processes: 30 internal, 3 local.
FAILED: Build did NOT complete successfully
```
",1
Multi-threaded execution throws an exception (using GPU).,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.19.0-dev20241018

### Custom code

Yes

### OS platform and distribution

Ubuntu 24.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Multi-threaded execution throws an exception (using GPU).

### Standalone code to reproduce the issue

```shell
import concurrent

import numpy as np
import tensorflow as tf
print(tf.__version__)

executor = concurrent.futures.ThreadPoolExecutor()


def sum(x, axis):
    return tf.reduce_sum(x, axis=axis)


futures = []

for i in range(1000):
    futures.clear()
    for _ in range(4):
        x = tf.convert_to_tensor(np.random.rand(100, 100))
        futures.append(executor.submit(sum, x, 1))
        x = tf.convert_to_tensor(np.random.rand(100))
        futures.append(executor.submit(sum, x, 0))
    concurrent.futures.wait(
        futures, return_when=concurrent.futures.ALL_COMPLETED
    )
    [future.result() for future in futures]
```


### Relevant log output

```shell
W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at reduction_ops_common.h:147 : INVALID_ARGUMENT: Invalid reduction dimension (1 for input with 1 dimension(s)
I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: Invalid reduction dimension (1 for input with 1 dimension(s)

```
```
",1
[Incorrect Result] `tf.math.reciprocal` returns `NaN` on `inf` input on Linux.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.17.0

### Custom code

No

### OS platform and distribution

AlmaLinux 9.4

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.math.reciprocal` returns `NaN` on Linux when input is `inf` or `-inf` and has dtype=complex128, shape >= 2.
The output is expected to be 0, since:
1. This behavior is not consistent with dtype=float64, where the output will be 0.
2. When input tensor contains only one value, the output will be 0.
3. The same code snippet will return different result on macOS, where the output is also 0.

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

input = tf.constant(np.inf, dtype=tf.float64)
out = tf.math.reciprocal(input)
# tf.Tensor(0.0, shape=(), dtype=float64)
print(out)

input = tf.constant(np.inf, dtype=tf.complex128)
out = tf.math.reciprocal(input)
# tf.Tensor(0j, shape=(), dtype=complex128)
print(out)

input = tf.constant([np.inf, np.inf], dtype=tf.complex128)
out = tf.math.reciprocal(input)
# On Linux: tf.Tensor([nan+nanj nan+nanj], shape=(2,), dtype=complex128)
# On macOS: tf.Tensor([0.+0.j 0.+0.j], shape=(2,), dtype=complex128)
print(out)
```


### Relevant log output

```shell
AttributeError: module 'ml_dtypes' has no attribute 'float8_e3m4'
tf.Tensor(0.0, shape=(), dtype=float64)
tf.Tensor(0j, shape=(), dtype=complex128)
tf.Tensor([nan+nanj nan+nanj], shape=(2,), dtype=complex128)
```
",1
Duplicate Logs After tf.saved_model.save with Custom Logging Configurations,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

Fedora Linux 40

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When using tf.saved_model.save, log messages are duplicated in my application. This happens because TensorFlow adds its own handlers, which interfere with my custom logging setup.

TensorFlow should respect existing logging configurations and avoid adding unnecessary handlers. Some documentation to help align TensorFlow’s logging with user-defined settings would be great.

Setting logger.propagate = False works but requires extra setup.

My suggestion is to check for existing log handlers before adding new ones.

Thanks for looking into this.

### Standalone code to reproduce the issue

Here is the minimal code to reproduce the issue. You can check the code and results in this [colab notebook](https://colab.research.google.com/drive/1IB3IM81FCX9LJ0Ae8uW9p5OCvvJ986fm?usp=sharing) as well.

```python

import logging
import logging.config
import sys
from typing import Tuple
import tensorflow as tf
import numpy as np


def create_keras_sequential_model() -> tf.keras.Model:
    return tf.keras.Sequential(
        [
            tf.keras.layers.Input(shape=(10,)),
            tf.keras.layers.Dense(64, activation=""relu""),
            tf.keras.layers.Dense(1),
        ]
    )


def generate_random_data(num_samples: int = 1000) -> Tuple[np.ndarray, np.ndarray]:
    x = np.random.random((num_samples, 10))
    y = np.random.random((num_samples, 1))
    return x, y


def compile_model(model: tf.keras.Model) -> None:
    model.compile(optimizer=""sgd"", loss=""mean_absolute_error"", metrics=[""mae""])


def fit_model(
    model: tf.keras.Model, x: np.ndarray, y: np.ndarray, epochs: int = 10
) -> None:
    model.fit(x, y, epochs=epochs, verbose=0)


def create_compile_return_model() -> None:
    model = create_keras_sequential_model()
    compile_model(model)
    x, y = generate_random_data()
    fit_model(model, x, y)
    return model


def get_custom_formatter_logger() -> logging.Logger:
    log_format_with_time = ""%(asctime)s - %(levelname)s %(message)s""
    logHandler = logging.StreamHandler(sys.stdout)
    formatter = logging.Formatter(log_format_with_time)
    logHandler.setFormatter(formatter)
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.DEBUG)
    logger.addHandler(logHandler)
    # logger.propagate = False #If you add this, then logs are not duplicated
    return logger


if __name__ == ""__main__"":

    logger = get_custom_formatter_logger()
    logger.info(""TF version: %s"", tf.__version__)
    logger.info(""Started"")
    keras_model = create_compile_return_model()
    logger.info(""Model created, compiled and trained. Now saving the model!"")
    tf.saved_model.save(keras_model, ""my_fake_model"")
    logger.info(""Model saved!"")
    logger.info(""Logs are now duplicated!"")
    logger.info(""Finished"")

```


### Relevant log output

```
2024-10-18 13:23:10,159 - INFO TF version: 2.17.0
2024-10-18 13:23:10,160 - INFO Started
2024-10-18 13:23:11,400 - INFO Model created, compiled and trained. Now saving the model!
2024-10-18 13:23:11,832 - INFO Model saved!
INFO:__main__:Model saved!
2024-10-18 13:23:11,832 - INFO Logs are now duplicated!
INFO:__main__:Logs are now duplicated!
2024-10-18 13:23:11,832 - INFO Finished
INFO:__main__:Finished
```
",1
"tf.linalg.expm fails to support half/float16 data type, which is inconsistent with doc","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

According to the documentation: https://www.tensorflow.org/api_docs/python/tf/linalg/expm
tf.linalg.expm is expected to accept float16 input, but it fails on float16 when actually running the following code.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
input = tf.constant(np.random.randn(1,1), dtype='float16')
out = tf.linalg.expm(input)
```
```


### Relevant log output

```shell
tensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node MatrixSolve}} = MatrixSolve[T=DT_HALF, adjoint=false]
All kernels registered for op MatrixSolve:
  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_HALF]
  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_HALF]
  device='GPU'; T in [DT_COMPLEX128]
  device='GPU'; T in [DT_COMPLEX64]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
 [Op:MatrixSolve] name:
```
",1
DLPack with Int32 tensor on the GPU: inconsistent eager mode / graph mode / XLA,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v1.12.1-117097-gecf05620570 2.19.0-dev20241016

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hello,

I realize that `int32` is a special dtype in TensorFlow for historical reasons. It seems that the handling of GPU int32-typed tensors has evolved over time.

Currently, the `device` field of a tensor created with:
```py
with tf.device('gpu'):
    x = tf.constant([0,1,2], tf.int32)
```
*does* indicate it's a GPU tensor: `/job:localhost/replica:0/task:0/device:GPU:0`.

However, when exporting and re-importing it via DLPack, it comes back as a CPU tensor.
There even seems to be a unit test validating this:
https://github.com/tensorflow/tensorflow/blob/d3de971a7348ecaefdbb920e580c37ebde10d780/tensorflow/python/dlpack/dlpack_test.py#L75-L78


However, @jhoydis found that this is *not* consistent between modes. In particular, if the tensor goes through an XLA-compiled function, it will correctly live on the GPU even after a round-trip through DLPack. (See reproducer below).

Would it please be possible to revisit this behavior, so that **exporting an int32 GPU tensor via DLPack does result in a GPU DLPack capsule in all modes, not just XLA?**

### Standalone code to reproduce the issue

```shell
import tensorflow as tf


def f_eager(x):
    return x
f_graph = tf.function()(f_eager)
f_xla = tf.function(jit_compile=True)(f_eager)


with tf.device('gpu'):
    x = tf.constant([0,1,2], tf.int32)
    print(""Original tensor:"", x.device)

    dlcapsule = tf.experimental.dlpack.to_dlpack(x)
    x_ = tf.experimental.dlpack.from_dlpack(dlcapsule)
    print(""Default:"", x_.device)

    dlcapsule = tf.experimental.dlpack.to_dlpack(f_eager(x))
    x_ = tf.experimental.dlpack.from_dlpack(dlcapsule)
    print(""Eager:"", x_.device)

    dlcapsule = tf.experimental.dlpack.to_dlpack(f_graph(x))
    x_ = tf.experimental.dlpack.from_dlpack(dlcapsule)
    print(""Graph:"", x_.device)

    dlcapsule = tf.experimental.dlpack.to_dlpack(f_xla(x))
    x_ = tf.experimental.dlpack.from_dlpack(dlcapsule)
    print(""XLA:"", x_.device)
```


### Relevant log output

```shell
Original tensor: /job:localhost/replica:0/task:0/device:GPU:0
Default: /job:localhost/replica:0/task:0/device:CPU:0
Eager: /job:localhost/replica:0/task:0/device:CPU:0
Graph: /job:localhost/replica:0/task:0/device:CPU:0
XLA: /job:localhost/replica:0/task:0/device:GPU:0
```
",1
Cannot pass $LOCAL_CUDNN_PATH as /usr,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0.rc0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

6.5.0

### GCC/compiler version

12

### CUDA/cuDNN version

8.9.0

### GPU model and memory

GTX 4090

### Current behavior?

If I pass `LOCAL_CUDNN_PATH`  as /usr, just like the docker image nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04, then tensorflow bazel would create a symlink: external/cudnn/include  as /usr/include, 
then, tensorflow would pass `-isystem external/cuda_cudnn/include`
after that, `#include_next` always skip the current directory external/cuda_cudnn/include, which is actually /usr/include, then bazel would report the error: 
```
/usr/local/include/c++/12/cstdlib:75:15: fatal error: stdlib.h: No such file or directory
 #include_next <stdlib.h>
               ^~~~~~~~~~
compilation terminated.
```
If I do not pass `LOCAL_CUDNN_PATH` and let tensorflow redownload a cudnn library, things would be very smooth.

### Standalone code to reproduce the issue

```shell
git submodule sync
git submodule update --init --recursive

export _GLIBCXX_USE_CXX11_ABI=1
. /work/conda_init.sh \
    && conda activate py3 \
    && HERMETIC_CUDA_VERSION=12.1.0 HERMETIC_CUDNN_VERSION=8.9.4.25 HERMETIC_CUDA_COMPUTE_CAPABILITIES=9.0  TF_NVCC_CLANG=1 TF_NEED_TENSORRT=1 LOCAL_CUDANN_PATH=/usr  LOCAL_CUDA_PATH=$CUDA_HOME  TF_NEED_CUDA=1  CLANG_CUDA_COMPILER_PATH=/llvm_release_17_with_nvptx/bin/clang ./configure
. /work/conda_init.sh \
    && conda activate py3 \
    && export LOCAL_CUDA_PATH=$CUDA_HOME \
    && bazel build --config=opt --copt=""-Iexternal/cuda_cudnn/include""   --copt=""-Wno-error=unused-command-line-argument""  //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --config=cuda --config=cuda_wheel  --config=dbg --copt=""-Wno-int-conversion""   --copt=""-Wno-error=extra-semi"" --copt=""-Wno-gnu-include-next""  --copt=""-Wno-error=c23-extensions""  --copt=""-Wno-error=overlength-strings""  --copt=""--gcc-install-dir=/usr/lib/gcc/x86_64-linux-gnu/12"" --verbose_failures  --subcommands
```
```


### Relevant log output

_No response_",2
tf.math.special.bessel_* has inconsistent result with scipy,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Based on the documentation, special function such as bessel_y0 should have consistent result with scipy. However, when receiving `-inf`, it has inconsistent results with scipy. Please check the reproducible for details.

### Standalone code to reproduce the issue

```shell
import scipy
import numpy as np
import tensorflow as tf
x = tf.constant(-np.inf, dtype='float64')
print(""TF:"", tf.math.special.bessel_y1(x))
print(""Scipy: "", scipy.special.y1(x))
print(""TF:"", tf.math.special.bessel_y0(x))
print(""Scipy: "", scipy.special.y0(x))
print(""TF:"", tf.math.special.bessel_k0(x))
print(""Scipy: "", scipy.special.k0(x))
print(""TF:"", tf.math.special.bessel_k1(x))
print(""Scipy: "", scipy.special.k1(x))
```


### Relevant log output

```shell
TF: tf.Tensor(-inf, shape=(), dtype=float64)
Scipy:  nan
TF: tf.Tensor(-inf, shape=(), dtype=float64)
Scipy:  nan
TF: tf.Tensor(inf, shape=(), dtype=float64)
Scipy:  nan
TF: tf.Tensor(inf, shape=(), dtype=float64)
Scipy:  nan
```
",1
"tf.math.is_strictly_increasing's behavior is not clear on a (2,2) matrix","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When receiving this input:
```
x = tf.constant([[1,2],[2,3]])
```
`tf.math.is_strictly_increasing` outputs `False` instead of `True`.
Also, for a tensor with shape (1,3,3):
```
x = tf.constant([[[-0.3188535,  -1.6029806,  -1.5352179],
  [-0.5704009,  -0.2167283,   0.2548743 ],
  [-0.14944994,  2.0107825,  -0.09678416]]])
```
It's output is still `False` instead of `True` even when x's first dimension only has one element.
Based on the description ""Elements of x are compared in row-major order."", it seems that elements in x are compared along row (i.e., the first dimension).
Therefore, to my understanding, if the first dimension contains only one element (such as 1x3x3 shape tensor), the output should be True. If the input is [[1,2],[3,4]], the output should also be `True` since the value in the first dimension is increasing (from `[1,2]` to `[3,4]`)


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
x = tf.constant([[1,2],[2,3]])
print(tf.math.is_strictly_increasing(x))  # False
x = tf.constant([[[-0.3188535,  -1.6029806,  -1.5352179],
  [-0.5704009,  -0.2167283,   0.2548743 ],
  [-0.14944994,  2.0107825,  -0.09678416]]])
print(tf.math.is_strictly_increasing(x))  # False
```
```


### Relevant log output

_No response_",1
argsort incorrectly handles very small floating-point numbers and -0.0 compared to other libraries (PyTorch and JAX),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

windows

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When using TensorFlow's argsort function on an array containing small floating-point numbers and both 0.0 and -0.0, the sort order is incorrect compared to other deep learning libraries such as PyTorch and JAX. TensorFlow incorrectly places 1.401298464324817e-45 (a very small positive number) before 0.0 and -0.0.

Expected behavior is that both 0.0 and -0.0 should be treated as equivalent and placed before any positive number, including very small ones like 1.401298464324817e-45. However, TensorFlow does not follow this behavior, whereas PyTorch correctly handles this.
```
import numpy as np
import torch
import tensorflow as tf
import jax.numpy as jnp

def test_argsort():
    # Input data, hardcoded as float32
    input_data = np.array([
        -0.0, 1.401298464324817e-45, 1.100000023841858, -0.0,
        5.960464477539063e-08, -2.0000100135803223, 1000000.0,
        722801.375, 0.0, -1.100000023841858
    ], dtype=np.float32)

    # PyTorch argsort
    pytorch_result = torch.argsort(torch.tensor(input_data, dtype=torch.float32)).numpy()
    print(f""PyTorch argsort result: {pytorch_result}"")

    # TensorFlow argsort
    tensorflow_result = tf.argsort(input_data).numpy().astype(np.int32)
    print(f""TensorFlow argsort result: {tensorflow_result}"")

    # JAX argsort
    jax_result = jnp.argsort(input_data).astype(np.int32)
    print(f""JAX argsort result: {jax_result}"")

if __name__ == ""__main__"":
    test_argsort()

```

### Standalone code to reproduce the issue

```shell
PyTorch argsort result: [5 9 0 3 8 1 4 2 7 6]
TensorFlow argsort result: [5 9 0 1 3 8 4 2 7 6]
JAX argsort result: [5 9 0 1 3 8 4 2 7 6]
```
Expected Behavior:
TensorFlow's argsort should place 0.0 and -0.0 before any positive number, including very small values like 1.401298464324817e-45. PyTorch demonstrates the correct behavior by treating 0.0 and -0.0 as equal and placing them in the correct order relative to other values.

Standalone Code to Reproduce the Issue:
The above Python code demonstrates the issue. It uses the same input data for PyTorch, TensorFlow, and JAX to show the difference in behavior. TensorFlow and JAX produce incorrect results by misplacing the small positive value before 0.0, while PyTorch produces the correct order.

Relevant Log Output:
No error logs are generated, but the incorrect behavior is clearly shown in the sorting results.
```


### Relevant log output

_No response_",1
Aborted (core dumped) due to Overflow : `tf.raw_ops.IRFFT3D`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

If the value contained in fft_length is the maximum value, it will cause an abort

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input = tf.constant(0, shape=[2,0,0,0] ,dtype=tf.complex64)
fft_length = tf.constant(1879048192, shape=[3], dtype=tf.int32)

tf.raw_ops.IRFFT3D(input=input, fft_length=fft_length)
```


### Relevant log output

```shell
2024-10-13 13:04:53.308156: F tensorflow/core/framework/tensor_shape.cc:607] Non-OK-status: RecomputeNumElements()
Status: INVALID_ARGUMENT: Shape [2,1879048192,1879048192,1879048192] results in overflow when computing number of elements
Aborted (core dumped)
```
",1
Aborted (core dumped) due to Overflow : `tf.raw_ops.IRFFT2D`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Since the value in fft_length is a maximum value, it will cause abort

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input = tf.constant(0, shape=[1,4,10,0,0] ,dtype=tf.complex64)
fft_length = tf.constant(2147483647, shape=[2], dtype=tf.int32)

tf.raw_ops.IRFFT2D(input=input, fft_length=fft_length)
```


### Relevant log output

```shell
2024-10-13 12:59:11.295197: F tensorflow/core/framework/tensor_shape.cc:607] Non-OK-status: RecomputeNumElements()
Status: INVALID_ARGUMENT: Shape [1,4,10,2147483647,2147483647] results in overflow when computing number of elements
Aborted (core dumped)
```
",1
Gradients of tf.linalg.expm not supported with JIT compilation,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tested 2.17 and 2.10, both have the issue

### Custom code

Yes

### OS platform and distribution

Ubuntu

### Mobile device

_No response_

### Python version

tested 3.9 and 3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Gradients of `tf.linalg.expm` can not be computed with JIT compilation. 

This is an issue, because tf 2.17 seems to have activated jit compilation for compiled models per default whereas earlier versions did not, breaking existing code.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

A = tf.Variable([[.4, 1.5], [.6, .1]], dtype=tf.float32)

@tf.function(jit_compile=True) #set jit_compile=False to make it work
def f(A):
    with tf.GradientTape() as tape:
        B = tf.linalg.expm(A)
    return tape.gradient(B, A)

f(A)
```


### Relevant log output

```shell
2024-10-11 11:17:27.281304: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at xla_ops.cc:577 : INVALID_ARGUMENT: XLA compilation requires a fixed tensor list size. Set the max number of elements. This could also happen if you're using a TensorArray in a while loop that does not have its maximum_iteration set, you can fix this by setting maximum_iteration to a suitable value.

Stack trace for op definition: 
File ""<frozen runpy>"", line 198, in _run_module_as_main
File ""<frozen runpy>"", line 88, in _run_code
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel_launcher.py"", line 18, in <module>
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/traitlets/config/application.py"", line 1075, in launch_instance
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelapp.py"", line 739, in start
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/tornado/platform/asyncio.py"", line 205, in start
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/base_events.py"", line 641, in run_forever
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/base_events.py"", line 1986, in _run_once
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/events.py"", line 88, in _run
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py"", line 545, in dispatch_queue
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py"", line 534, in process_one
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py"", line 437, in dispatch_shell
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/ipkernel.py"", line 362, in execute_request
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py"",
```
",1
tf.custom_gradient for function with kwarg shows unexpected behavior,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.17.0

### Custom code

No

### OS platform and distribution

Ubuntu 22.04.5 LTS

### Mobile device

_No response_

### Python version

3.12.7

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.3/8

### GPU model and memory

_No response_

### Current behavior?

I have a function that takes two tensors as inputs, one as argument and one as keyword argument.
The function has a custom gradient.

When ``tape.gradient`` for both input tensors with respect to the output of the function is called, TensorFlow throws an error, saying that only one gradient is expect and not two.

When the function is called with both inputs as arguments (and not one of them as kwarg), no error is thrown.


### Standalone code to reproduce the issue

```shell
@tf.custom_gradient
def func(x, y=0):
    z = 2*x + y
    def grad(dz):
        dx = 2*dz
        dy = dz
        return dx, dy
    return z, grad
x = tf.constant(2.)
y = tf.constant(3.)
with tf.GradientTape() as tape:
    tape.watch([x, y])
    z = func(x, y=y) #func(x, y) does not generate the error
grads = tape.gradient(z, [x, y])
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[129], line 14
     12     tape.watch([x, y])
     13     z = func(x, y=y)
---> 14 grads = tape.gradient(z, [x, y])

File ~/.local/lib/python3.12/site-packages/tensorflow/python/eager/backprop.py:1066, in GradientTape.gradient(self, target, sources, output_gradients, unconnected_gradients)
   1060   output_gradients = (
   1061       composite_tensor_gradient.get_flat_tensors_for_gradients(
   1062           output_gradients))
   1063   output_gradients = [None if x is None else ops.convert_to_tensor(x)
   1064                       for x in output_gradients]
-> 1066 flat_grad = imperative_grad.imperative_grad(
   1067     self._tape,
   1068     flat_targets,
   1069     flat_sources,
   1070     output_gradients=output_gradients,
   1071     sources_raw=flat_sources_raw,
   1072     unconnected_gradients=unconnected_gradients)
   1074 if not self._persistent:
   1075   # Keep track of watched variables before setting tape to None
   1076   self._watched_variables = self._tape.watched_variables()

File ~/.local/lib/python3.12/site-packages/tensorflow/python/eager/imperative_grad.py:67, in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)
     63 except ValueError:
     64   raise ValueError(
     65       ""Unknown value for unconnected_gradients: %r"" % unconnected_gradients)
---> 67 return pywrap_tfe.TFE_Py_TapeGradient(
     68     tape._tape,  # pylint: disable=protected-access
     69     target,
     70     sources,
     71     output_gradients,
     72     sources_raw,
     73     compat.as_str(unconnected_gradients.value))

File ~/.local/lib/python3.12/site-packages/tensorflow/python/ops/custom_gradient.py:588, in _eager_mode_decorator.<locals>.actual_grad_fn(*result_grad_components)
    585 flat_grads = composite_tensor_gradient.get_flat_tensors_for_gradients(
    586     nest.flatten(input_grads))
    587 if len(flat_grads) != arg_count:
--> 588   raise ValueError(
    589       f""custom_gradient function expected to return {arg_count} ""
    590       f""gradients, but returned {len(flat_grads)} instead."")
    591 return flat_grads + variable_grads

ValueError: custom_gradient function expected to return 1 gradients, but returned 2 instead.
```
",1
Tensorflow Distributed AlltoAll,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

Tf 2.9

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hi,

I am trying to find a workaround, or ideally an implementation of the MPI [AlltoAll](https://www.mpich.org/static/docs/v3.4/www3/MPI_Alltoall.html) primitive.

As far as I can tell, Tensorflow has an AlltoAll op, but only for TPUs: https://www.tensorflow.org/api_docs/python/tf/raw_ops/AllToAll.

In a distributed setup with 4 devices, this would allow me to go from
```bash
PerReplica:{
  0: [0 1 2 3],
  1: [0 1 2 3],
  2: [0 1 2 3],
  3: [0 1 2 3]
}
``` 
to 
```
PerReplica:{
  0: [0 0 0 0],
  1: [1 1 1 1],
  2: [2 2 2 2],
  3: [3 3 3 3]
}
```
with a single communication call. I have managed to get this working by sharding with DTensor, but I'm trying to stay within the `tf.distribute` setting. Is there an easy workaround?





### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
# tf.debugging.set_log_device_placement(True)
tf.config.set_visible_devices([], device_type='GPU')
# Create sharding mesh
def configure_virtual_cpus(ncpu):
    phy_devices = tf.config.list_physical_devices('CPU')
    tf.config.set_logical_device_configuration(phy_devices[0], [
        tf.config.LogicalDeviceConfiguration(),
    ] * ncpu)

ndev = 4
configure_virtual_cpus(ndev)
devices = [f'CPU:{i}' for i in range(ndev)]
strategy = tf.distribute.MirroredStrategy(devices)

arr = np.arange(0,4)
arr_total = np.stack([arr]*4)


def value_fn(ctx):
    return arr_total[ctx.replica_id_in_sync_group]


arr_tf = strategy.experimental_distribute_values_from_function(value_fn)
print(arr_tf)
# How to achieve the AllToAll operation here?
```


### Relevant log output

```shell
PerReplica:{
  0: [0 1 2 3],
  1: [0 1 2 3],
  2: [0 1 2 3],
  3: [0 1 2 3]
}
```
",1
Backward compatibility issue: failure to load models saved in TensorFlow format (Keras 2) in TensorFlow 2.17,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.9.1 (model saved), 2.17.0 (model loaded)

### Custom code

Yes

### OS platform and distribution

(Official Docker Image) Ubuntu 22.04.4 LTS

### Mobile device

_No response_

### Python version

3.8.10 (model saved),  3.11.0rc1 (model loaded)

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

## Description

I have encountered a backward compatibility issue when loading models saved with Keras 2 in TensorFlow 2.9 into TensorFlow 2.17, which now uses Keras 3 API. This issue impacts various loading methods, and there does not appear to be a straightforward solution to resolve the errors.

## Steps to reproduce

1. **Train and export a model in TensorFlow 2.9 with Keras 2 API**:
   - A simple Keras sequential model is created and trained on random data.
   - The model is saved using both `tf.saved_model.save` and `tf.keras.models.save_model` with `tf` save format (which is unsupported in Keras 3).
   
2. **Attempt to load the models in TensorFlow 2.17 with Keras 3 API**:
   - The models are loaded using TensorFlow’s `tf.saved_model.load`, `keras.layers.TFSMLayer`, and `tf.keras.models.load_model`.

3. **Observe the errors**:
   - When loading using `tf.saved_model.load`, the error `'_UserObject' object has no attribute 'add_slot'` occurs.
   - When loading using `keras.layers.TFSMLayer`, the same `'_UserObject' object has no attribute 'add_slot'` error is triggered.
   - When loading using `tf.keras.models.load_model`, a different error appears: `File format not supported.` Because Keras 3 has dropped support for the default `tf` save format in version 2!

## Expected behavior

While I understand that issues related to loading legacy Keras models saved with the `tf` save format using Keras are out of scope for TensorFlow and should be addressed by the Keras team, the functionality surrounding TensorFlow's `tf.saved_model`, which uses the `SavedModel` bundle, is part of TensorFlow Core. Since this format is shared across different runtimes, it should remain backward compatible. Therefore, models saved in earlier versions of TensorFlow using the `SavedModel` format should load seamlessly in newer TensorFlow versions, without requiring users to rebuild their models or encountering compatibility errors.


### Standalone code to reproduce the issue

## Minimal example to reproduce the issue

A minimal code example to reproduce the issue is available in this repository: [Reproduce TF Model Compatibility Issue.](https://github.com/arianmaghsoudnia/reproduce-tf-model-compat-issue)

Please follow the steps in the README file.



### Relevant log output

_No response_",1
tf.nn.conv2d terminates process with invalid input shape instead of raising an exception,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS

### Mobile device

_No response_

### Python version

3.11.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

TensorFlow terminates the process when passing an invalid input shape to `tf.nn.conv2d`. Instead of raising a Python exception that can be caught with a try-except block.

I expected TensorFlow to raise a catchable Python exception indicating that the input tensor shape is invalid. This would allow the error to be handled in a try-except block, instead of terminating the process. The error message should clearly explain the shape mismatch issue.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# Define invalid input tensor and kernel
input_tensor = [[1.0, 2.0, 3.0]]
kernel = [[0.5, 0.5], [0.5, 0.5]]

try:
    # Create TensorFlow constants
    input_tf = tf.constant(input_tensor, dtype=tf.float32)
    kernel_tf = tf.constant(kernel, dtype=tf.float32)
    
    # Attempt to perform convolution, expecting an error
    output_tf = tf.nn.conv2d(
        tf.expand_dims(input_tf, axis=0), 
        tf.expand_dims(kernel_tf, axis=0), 
        strides=[1, 1, 1, 1], 
        padding='VALID'
    )
    
    print(""TensorFlow Output:"", output_tf.numpy())
except Exception as e:
    print(""TensorFlow Error:"", e)
```


### Relevant log output

```shell
2024-10-09 14:53:31.118589: F ./tensorflow/core/util/tensor_format.h:427] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 3, 0, C Aborted (core dumped)
```
",1
RuntimeError: failed to create XNNPACK runtimeNode number 2977 (TfLiteXNNPackDelegate) failed to prepare.,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Google Colab with Python 3.10.12
- TensorFlow installation (pip package or built from source):
pip package
- TensorFlow library (version, if pip package or github SHA, if built from source):
v2.17.0

### 2. Code

```
import tensorflow as tf

saved_model_dir = '/content/saved_model'

num_calibration_steps = 100

input = tf.cast(tf.random.normal((1, 640, 640, 3)), tf.float32)
dummy_input = tf.cast(tf.random.normal((1, 2)), tf.int64)

def representative_dataset_gen():
    for _ in range(num_calibration_steps):
        yield [dummy_input, input] #model has 2 input tensors

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset_gen
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS_INT8,
  tf.lite.OpsSet.SELECT_TF_OPS
]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8

tflite_quant_model = converter.convert()

# Save the quantized model to a local file
with open('quantized_model.tflite', 'wb') as f:
    f.write(tflite_quant_model)
```

### 3. Failure after conversion

After converting the model from ONNX using onnx2tf, I got saved_model, which I tried to convert to int8 quantized model using the code above. When trying to inference the model, after reading the model by the interpreter and calling the function `allocate_tensors()` 
```
interpreter = tf.lite.Interpreter(model_path=""/content/quantized_model.tflite"")
interpreter.allocate_tensors()
```
I get the following error:

```
RuntimeError                              Traceback (most recent call last)
[<ipython-input-7-b6b80a3bdf94>](https://localhost:8080/#) in <cell line: 6>()
      4 interpreter = tf.lite.Interpreter(model_path=""/content/quantized_model.tflite"")
      5 print(interpreter.get_input_details())
----> 6 interpreter.allocate_tensors()

[/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/interpreter.py](https://localhost:8080/#) in allocate_tensors(self)
    535   def allocate_tensors(self):
    536     self._ensure_safe()
--> 537     return self._interpreter.AllocateTensors()
    538 
    539   def _safe_to_run(self):

RuntimeError: failed to create XNNPACK runtimeNode number 2977 (TfLiteXNNPackDelegate) failed to prepare.
```

Could someone give me some advice, suggestions on how to solve this error? I couldn't even find that anyone has solved the same problem. 
The closest to this error is this [issue](https://github.com/tensorflow/tensorflow/issues/61395), but the workaround is to convert the ONNX model to Keras and for my complex model it is not possible to fix.
",1
fatal error: 'NEON_2_SSE.h' file not found - macOS x86_64 build tensorflowlite_c library,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

macOS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

AppleClang 15.0.0.15000309

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When trying to build the tensorflowlite_c lib according to [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md) guide, the build fails on macOS x86_64 plattform with the following error code:
```output
 In file included from /Users/runner/work/tflite-c-lib/tflite-c-lib/tensorflow/tensorflow/lite/delegates/xnnpack/quantization_util.cc:21:
In file included from /Users/runner/work/tflite-c-lib/tflite-c-lib/tensorflow/tensorflow/lite/kernels/internal/optimized/optimized_ops.h:32:
In file included from /Users/runner/work/tflite-c-lib/tflite-c-lib/tensorflow/tensorflow/lite/kernels/internal/common.h:35:
/Users/runner/work/tflite-c-lib/tflite-c-lib/tensorflow/tensorflow/lite/kernels/internal/optimized/neon_check.h:25:10: fatal error: 'NEON_2_SSE.h' file not found
#include ""NEON_2_SSE.h""  // IWYU pragma: export
         ^~~~~~~~~~~~~~
1 error generated.
```

### Standalone code to reproduce the issue

The error happens when I build using github workflows. [This](https://github.com/faressc/tflite-c-lib/actions/runs/11238016239/job/31241774171) is the action run.

### Relevant log output

```shell
In file included from /Users/runner/work/tflite-c-lib/tflite-c-lib/tensorflow/tensorflow/lite/delegates/xnnpack/quantization_util.cc:21:
In file included from /Users/runner/work/tflite-c-lib/tflite-c-lib/tensorflow/tensorflow/lite/kernels/internal/optimized/optimized_ops.h:32:
In file included from /Users/runner/work/tflite-c-lib/tflite-c-lib/tensorflow/tensorflow/lite/kernels/internal/common.h:35:
/Users/runner/work/tflite-c-lib/tflite-c-lib/tensorflow/tensorflow/lite/kernels/internal/optimized/neon_check.h:25:10: fatal error: 'NEON_2_SSE.h' file not found
#include ""NEON_2_SSE.h""  // IWYU pragma: export
         ^~~~~~~~~~~~~~
1 error generated.
```
",2
"tensorflow.python.ops.signal.dct_ops.dct aborts with ""Assertion failure no zero-sized FFTs""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly 2.19.0-dev20241007

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have confirmed that above code would crash on `tf-nightly 2.19.0-dev20241007` (nightly-build)
Please find the [gist](https://colab.research.google.com/drive/1oBjZoqp6WZn_VU-CTxZ3bspUc6v9D51s?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
import numpy as np
from tensorflow.python.eager import def_function
from tensorflow.python.framework import tensor_spec
from tensorflow.python.ops.signal import dct_ops

def test_with_dynamic_dimensions(dct_type, norm, shape, dtype):
    @def_function.function
    def func(signals):
        return dct_ops.dct(signals, n=norm, type=dct_type, norm=None)
    signals_spec = tensor_spec.TensorSpec([None] * len(shape), dtype)
    f = func.get_concrete_function(signals_spec)
    f(np.zeros([0], dtype=dtype))
test_with_dynamic_dimensions(3, None, [3], np.float32)
```


### Relevant log output

```shell
DUCC FFT c2r failed: 
bazel-out/k8-opt/bin/external/ducc/_virtual_includes/fft/ducc/src/ducc0/fft/fft1d_impl.h: 2948 (static Trpass<Tfs> ducc0::detail_fft::rfftpass<float>::make_pass(size_t, size_t, size_t, const Troots<Tfs> &, bool) [Tfs = float]):

Assertion failure
no zero-sized FFTs

Aborted (core dumped)
```
",2
tensorflow.python.ops.parsing_ops.parse_single_sequence_example can cause a crash,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly 2.19.0-dev20241007

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

Linux Ubuntu 20.04.3 LTS

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have confirmed that above code would crash on `tf-nightly 2.19.0-dev20241007` (nightly-build)

Please find the [gist](https://colab.research.google.com/drive/17PzKxkDEr3N8E9D9Kk_mT2A1LyPpoZZe?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
from tensorflow.core.example import example_pb2
from tensorflow.core.example import feature_pb2
from tensorflow.python.framework import dtypes
from tensorflow.python.framework import ops
from tensorflow.python.ops import parsing_ops
example = example_pb2.Example
feature = feature_pb2.Feature
features = lambda d: feature_pb2.Features(feature=d)
bytes_feature = lambda v: feature(bytes_list=feature_pb2.BytesList(value=v))
int64_feature = lambda v: feature(int64_list=feature_pb2.Int64List(value=v))
float_feature = lambda v: feature(float_list=feature_pb2.FloatList(value=v))
feature_list = lambda l: feature_pb2.FeatureList(feature=l)
feature_lists = lambda d: feature_pb2.FeatureLists(feature_list=d)
sequence_example = example_pb2.SequenceExample

def testSequenceExampleListWithWrongShapeFails():
    original = sequence_example(feature_lists=feature_lists({'a': feature_list([int64_feature([2, 3]), int64_feature([2, 3, 4])])}))
    serialized = original.SerializeToString()
    parsing_ops.parse_single_sequence_example(**
        ({
            'example_name': 'in1',
            'serialized': ops.convert_to_tensor(serialized),
            'sequence_features': {'a': parsing_ops.FixedLenSequenceFeature((0, 0), dtypes.int64)}
        }))
testSequenceExampleListWithWrongShapeFails()
```


### Relevant log output

```shell
Floating point exception (core dumped)
```
",2
`SimpleDynamicBuffer::AddString` is calling `memcpy` with null data,"I've noticed this hitting on our ubsan builds recently:

```
../../third_party/tflite/src/tensorflow/compiler/mlir/lite/utils/string_utils.cc:32:10: runtime error: null pointer passed as argument 1, which is declared to never be null
../../build/linux/debian_bullseye_amd64-sysroot/usr/include/string.h:44:28: note: nonnull attribute specified here
    #0 0x5a36de826450 in mlir::TFL::SimpleDynamicBuffer::AddString(char const*, unsigned long) third_party/tflite/src/tensorflow/compiler/mlir/lite/utils/string_utils.cc:32:3
    #1 0x5a36de825d3e in tflite::DynamicBuffer::AddString(char const*, unsigned long) third_party/tflite/src/tensorflow/lite/string_util.cc:37:28
    #2 0x5a36de82924d in PopulateTensor<std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char> > > third_party/tflite_support/src/tensorflow_lite_support/cc/task/core/task_utils.h:125:13
    #3 0x5a36de82924d in tflite::task::processor::UniversalSentenceEncoderPreprocessor::Preprocess(std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&) third_party/tflite_support/src/tensorflow_lite_support/cc/task/processor/universal_sentence_encoder_preprocessor.cc:58:3
    #4 0x5a36de81d3f7 in tflite::task::text::TextEmbedder::Preprocess(std::__Cr::vector<TfLiteTensor*, std::__Cr::allocator<TfLiteTensor*>> const&, std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&) third_party/tflite_support/src/tensorflow_lite_support/cc/task/text/text_embedder.cc:174:25
    #5 0x5a36de81cd8c in tflite::task::core::BaseTaskApi<tflite::task::processor::EmbeddingResult, std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&>::InferWithFallback(std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&) third_party/tflite_support/src/tensorflow_lite_support/cc/task/core/base_task_api.h:146:5
    #6 0x5a36de81cc40 in tflite::task::text::TextEmbedder::Embed(std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&) third_party/tflite_support/src/tensorflow_lite_support/cc/task/text/text_embedder.cc:169:10
    #7 0x5a36d2728c24 in ai_chat::TextEmbedder::EmbedText(std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&, tflite::task::processor::EmbeddingResult&) brave/components/ai_chat/core/browser/text_embedder.cc:271:49
    #8 0x5a36d2728073 in ai_chat::TextEmbedder::EmbedSegments() brave/components/ai_chat/core/browser/text_embedder.cc:287:19
    #9 0x5a36c8c67059 in ai_chat::TextEmbedderUnitTest::EmbedSegments(ai_chat::TextEmbedder*)::'lambda'()::operator()() const brave/components/ai_chat/core/browser/text_embedder_unittest.cc:67:58
    #10 0x5a36c6762969 in base::OnceCallback<void ()>::Run() && base/functional/callback.h:156:12
    #11 0x5a36d4977df2 in base::TaskAnnotator::RunTaskImpl(base::PendingTask&) base/task/common/task_annotator.cc:202:34
    #12 0x5a36d49dcfe9 in RunTask<(lambda at ../../base/task/thread_pool/task_tracker.cc:678:35)> base/task/common/task_annotator.h:90:5
    #13 0x5a36d49dcfe9 in base::internal::TaskTracker::RunTaskImpl(base::internal::Task&, base::TaskTraits const&, base::internal::TaskSource*, base::internal::SequenceToken const&) base/task/thread_pool/task_tracker.cc:677:19
    #14 0x5a36d49dd0f1 in base::internal::TaskTracker::RunSkipOnShutdown(base::internal::Task&, base::TaskTraits const&, base::internal::TaskSource*, base::internal::SequenceToken const&) base/task/thread_pool/task_tracker.cc:662:3
    #15 0x5a36d49dc1f5 in base::internal::TaskTracker::RunTask(base::internal::Task, base::internal::TaskSource*, base::TaskTraits const&) base/task/thread_pool/task_tracker.cc:520:5
    #16 0x5a36d4af81fb in base::test::TaskEnvironment::TestTaskTracker::RunTask(base::internal::Task, base::internal::TaskSource*, base::TaskTraits const&) base/test/task_environment.cc:1028:46
    #17 0x5a36d49db5a5 in base::internal::TaskTracker::RunAndPopNextTask(base::internal::RegisteredTaskSource) base/task/thread_pool/task_tracker.cc:415:5
    #18 0x5a36d4a0cabd in base::internal::WorkerThread::RunWorker() base/task/thread_pool/worker_thread.cc:493:36
    #19 0x5a36d4a0c100 in base::internal::WorkerThread::RunPooledWorker() base/task/thread_pool/worker_thread.cc:379:3
    #20 0x5a36d4a0bc86 in base::internal::WorkerThread::ThreadMain() base/task/thread_pool/worker_thread.cc:359:7
    #21 0x5a36d4a3e1ec in base::(anonymous namespace)::ThreadFunc(void*) base/threading/platform_thread_posix.cc:101:13
    #22 0x7695b109ca93 in start_thread nptl/pthread_create.c:447:8
    #23 0x7695b1129c3b in clone3 misc/../sysdeps/unix/sysv/linux/x86_64/clone3.S:78
```",1
Failed to build `tensorflow_cc` in Windows when linking,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-rc0

### Custom code

No

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.12.7

### Bazel version

6.5.0

### GCC/compiler version

Clang 18.1.7

### CUDA/cuDNN version

No

### GPU model and memory

_No response_

### Current behavior?

I built the C++ library in Windows with LLVM/Clang 18.1.7 by command:
```bash
bazel build --config=release_cpu_windows --config=win_clang //tensorflow:tensorflow_cc
```
All compilation works well, but linking at the final task failed. It seems a lot of symbols lost when linking, such as `Session`, `SavedModelBundleInterface`, et al. They were all basic functions or classes and should not be missed.

CUDA was excluded.

I have tried many LLVM/Clang versions from 17 to 19. Looks like it has nothing to do with the compiler version.

### Standalone code to reproduce the issue

```shell
bazel build --config=release_cpu_windows --config=win_clang //tensorflow:tensorflow_cc
```


### Relevant log output

```shell
D:/tensorflow/tensorflow/BUILD:1316:21: Linking tensorflow/tensorflow_cc.dll failed: (Exit 1): lld-link.exe failed: error executing command (from target //tensorflow:tensorflow_cc.dll) 
  cd /d D:/output_base/execroot/org_tensorflow
  SET CLANG_COMPILER_PATH=C:Program FilesLLVMbinclang.exe
    SET LIB=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.41.34120\ATLMFC\lib\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.41.34120\lib\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.26100.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\\lib\10.0.26100.0\\um\x64;C:\Program Files\LLVM\lib\clang\18\lib\windows
    SET PATH=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.41.34120\bin\HostX64\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\VCPackages;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files\Microsoft Visual Studio\2022\Community\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\FSharp\Tools;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\DiagnosticsHub\Collector;C:\Program Files (x86)\Windows Kits\10\bin\10.0.26100.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\Tools\;;C:\WINDOWS\system32;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\Linux\bin\ConnectionManagerExe;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\vcpkg
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Python312/python.exe
    SET PYTHON_LIB_PATH=C:/Python312/Lib/site-packages
    SET TEMP=E:\tmp
    SET TF2_BEHAVIOR=1
    SET TMP=E:\tmp
  C:\Program Files\LLVM\bin\lld-link.exe @bazel-out/x64_windows-opt/bin/tensorflow/tensorflow_cc.dll-2.params
# Configuration: ee29c35b2efb4ddb1fe39799ba1e7aae463cd78c5b2bb106c9b875ad299c989f
# Execution platform: //tensorflow/tools/toolchains/win:x64_windows-clang-cl
lld-link: warning: ignoring unknown argument '-lm'
lld-link: warning: ignoring unknown argument '-lpthread'
lld-link: warning: ignoring unknown argument '-lm'
lld-link: warning: ignoring unknown argument '-lpthread'
lld-link: warning: ignoring unknown argument '-lm'
lld-link: warning: duplicate symbol: TF_DataTypeSize
>>> defined at tf_datatype.lo.lib(tf_datatype.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_NewBuffer
>>> defined at tf_buffer.lib(tf_buffer.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_NewStatus
>>> defined at tf_status.lib(tf_status.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_GetCode
>>> defined at tf_status.lib(tf_status.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_Message
>>> defined at tf_status.lib(tf_status.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_DeleteStatus
>>> defined at tf_status.lib(tf_status.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_NewBufferFromString
>>> defined at tf_buffer.lib(tf_buffer.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_SetStatus
>>> defined at tf_status.lib(tf_status.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_DeleteTensor
>>> defined at tf_tensor.lib(tf_tensor.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_DeleteBuffer
>>> defined at tf_buffer.lib(tf_buffer.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_TensorData
>>> defined at tf_tensor.lib(tf_tensor.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_NewTensor
>>> defined at tf_tensor.lib(tf_tensor.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_SetPayload
>>> defined at tf_status.lib(tf_status.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_NumDims
>>> defined at tf_tensor.lib(tf_tensor.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_Dim
>>> defined at tf_tensor.lib(tf_tensor.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_AllocateTensor
>>> defined at tf_tensor.lib(tf_tensor.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_TensorBitcastFrom
>>> defined at tf_tensor.lib(tf_tensor.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_TensorElementCount
>>> defined at tf_tensor.lib(tf_tensor.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: warning: duplicate symbol: TF_ForEachPayload
>>> defined at tf_status.lib(tf_status.obj)
>>> defined at libtensorflow_framework.so.2.18.0

lld-link: error: <root>: undefined symbol: class tensorflow::Session * __cdecl tensorflow::NewSession(struct tensorflow::SessionOptions const &)
lld-link: error: <root>: undefined symbol: public: virtual __cdecl tensorflow::SavedModelBundleInterface::~SavedModelBundleInterface(void)
lld-link: error: <root>: undefined symbol: bool __cdecl tensorflow::MaybeSavedModelDirectory(class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>> const &)
lld-link: error: <root>: undefined symbol: int `private: static class lts_20230802::container_internal::btree<struct absl::lts_20230802::container_internal::set_params<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>, struct std::less<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>>, class std::allocator<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>>, 256, 0>>::btree_node<struct absl::lts_20230802::container_internal::set_params<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>, struct std::less<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>>, class std::allocator<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>>, 256, 0>> * __cdecl absl::lts_20230802::container_internal::btree<struct absl::lts_20230802::container_internal::set_params<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>, struct std::less<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>>, class std::allocator<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>>, 256, 0>>::EmptyNode(void)'::`2'::$TSS0
lld-link: error: <root>: undefined symbol: `public: class std::unique_ptr<class tensorflow::RunHandler, struct std::default_delete<class tensorflow::RunHandler>> __cdecl tensorflow::RunHandlerPool::Impl::Get(__int64, __int64, class tensorflow::RunOptions_Experimental_RunHandlerPoolOptions const &)'::`2'::`local static thread guard'{2}
lld-link: error: <root>: undefined symbol: class std::unordered_map<enum tensorflow::DataType, enum tensorflow::FullTypeId, struct tensorflow::DataTypeHasher, struct std::equal_to<enum tensorflow::DataType>, class std::allocator<struct std::pair<enum tensorflow::DataType const, enum tensorflow::FullTypeId>>> *tensorflow::DT_TO_FT
lld-link: error: <root>: undefined symbol: private: static class std::vector<class tsl::core::RefCountPtr<class tensorflow::Rendezvous>, class std::allocator<class tsl::core::RefCountPtr<class tensorflow::Rendezvous>>> &tensorflow::LocalRendezvous::aborted_rendezs_
lld-link: error: <root>: undefined symbol: private: static class tsl::mutex &tensorflow::LocalRendezvous::aborted_rendezs_mu_
lld-link: error: <root>: undefined symbol: class tsl::monitoring::Counter<2> *tensorflow::metrics::eager_client_error_counter
lld-link: error: <root>: undefined symbol: struct absl::lts_20230802::container_internal::btree<struct absl::lts_20230802::container_internal::set_params<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>, struct std::less<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>>, class std::allocator<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>>, 256, 0>>::EmptyNodeType *`private: static class absl::lts_20230802::container_internal::btree_node<struct absl::lts_20230802::container_internal::set_params<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>, struct std::less<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>>, class std::allocator<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>>, 256, 0>> * __cdecl absl::lts_20230802::container_internal::btree<struct absl::lts_20230802::container_internal::set_params<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>, struct std::less<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>>, class std::allocator<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>>, 256, 0>>::EmptyNode(void)'::`2'::empty_node
lld-link: error: <root>: undefined symbol: private: static class tsl::mutex tensorflow::tfdbg::DebugEventsWriter::factory_mu_
lld-link: error: <root>: undefined symbol: char const *tensorflow::kDisableJitKernelsEnvVar
lld-link: error: <root>: undefined symbol: public: static __int64 tensorflow::CollectiveExecutor::kInvalidId
lld-link: error: <root>: undefined symbol: char const *tensorflow::kJitKernelLabel
lld-link: error: <root>: undefined symbol: public: static class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>> const tensorflow::LogMemory::kLogMemoryLabel
lld-link: error: <root>: undefined symbol: class tsl::monitoring::Counter<5> *tensorflow::metrics::mlir_bridge_first_phase_counter
lld-link: error: <root>: undefined symbol: class tsl::monitoring::Counter<1> *tensorflow::metrics::mlir_second_phase_count
lld-link: error: <root>: undefined symbol: void * (__cdecl *nsync::nsync_malloc_ptr_)(unsigned __int64)
lld-link: error: <root>: undefined symbol: struct nsync::lock_type_s *nsync::nsync_reader_type_
lld-link: error: <root>: undefined symbol: struct nsync::lock_type_s *nsync::nsync_writer_type_
lld-link: error: too many errors emitted, stopping now (use /errorlimit:0 to see all errors)
```
",1
Can't compile Tensorflow 2.17 from source for cpu on fedora 40 : undefined reference,"
## I'm trying to compile Tensorflow 2.17 on a new fresh install of Fedora40 lxqt desktop (official spin).

#### what i've donne (all command as root):

 - Fresh Fedora install
 - dnf update
 - reboot
 - dnf install python3-devel g++ gcc cmake python3-pip git
   eigen3-devel
 - pip install -U --user pip
 - pip install -U  pip six numpy wheel setuptools mock
 - wget -O bazel
   https://github.com/bazelbuild/bazelisk/releases/download/v1.22.0/bazelisk-linux-amd64
 - mv bazel /bin/
 - chmod 555 /bin/bazel
 - git clone https://github.com/tensorflow/tensorflow.git
 - cd tensorflow
 - git checkout r2.17
 - export TF_PYTHON_VERSION=3.12
 - export LD_LIBRARY_PATH=/usr/local/lib
 - ./configure # <- answer default value but use GCC and say no to Cuda build, and
   used this optimisation flags: -march=native -mtune=native -O3
 - bazel build //tensorflow/tools/pip_package:wheel
   --repo_env=WHEEL_NAME=tensorflow_cpu --config=nonccl --config=opt --action_env=""LD_LIBRARY_PATH=${LD_LIBRARY_PATH}""

#### After a while a get this error:
```
ERROR: /home/brd/tensorflow/tensorflow/BUILD:1318:21: Linking tensorflow/libtensorflow_cc.so.2.17.1 failed: (Exit 1): gcc failed: error executing command (from target //tensorflow:libtensorflow_cc.so.2.17.1) /usr/bin/gcc @bazel-out/k8-opt/bin/tensorflow/libtensorflow_cc.so.2.17.1-2.params
/usr/bin/ld.gold: warning: bazel-out/k8-opt/bin/external/local_tsl/tsl/platform/cloud/_objs/gcs_file_system/gcs_file_system.pic.o: conflicting default version definition for _ZZZN3tsl17RamFileBlockCacheC4EmmmSt8functionIFN4absl12lts_202308026StatusERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEmmPcPmEEPNS_3EnvEENKUliPKcE0_clEiSK_E17vmodule_activated@@tensorflow
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/local_tsl/tsl/platform/cloud/_objs/gcs_file_system/gcs_file_system.pic.o: previous definition of _ZZZN3tsl17RamFileBlockCacheC4EmmmSt8functionIFN4absl12lts_202308026StatusERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEmmPcPmEEPNS_3EnvEENKUliPKcE0_clEiSK_E17vmodule_activated@@tensorflow here
bazel-out/k8-opt/bin/tensorflow/core/kernels/mkl/_objs/mkl_sparse_matrix_matmul_op/mkl_sparse_matrix_matmul_op.pic.o:mkl_sparse_matrix_matmul_op.cc:function tensorflow::register_kernel_0::{lambda(tensorflow::KernelDef const*)#1}::operator()(tensorflow::KernelDef const*) const::{lambda(tensorflow::OpKernelConstruction*)#1}::_FUN(tensorflow::OpKernelConstruction*):(.text._ZZNK10tensorflowL17register_kernel_0MUlPKNS_9KernelDefEE_clES3_ENUlPNS_20OpKernelConstructionEE_4_FUNES6_+0x18d): error: undefined reference to 'tensorflow::CSRMatMulOp<Eigen::ThreadPoolDevice, float>::CSRMatMulOp(tensorflow::OpKernelConstruction*)'
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:wheel failed to build
```
**how do i solve : undefined reference to 'tensorflow::CSRMatMulOp<Eigen::ThreadPoolDevice, float>::CSRMatMulOp(tensorflow::OpKernelConstruction\*)'?**

---
#### Version:
Fedora 40 lxqt desktop kernel 6.10.11-200.fc40.x86_64
(runing in virtualbox with: 8 cores and 23Gb of ram)

gcc (GCC) 14.2.1 20240912 (Red Hat 14.2.1-3)

g++ (GCC) 14.2.1 20240912 (Red Hat 14.2.1-3)

Python 3.12.6

GNU Make 4.4.1

cmake version 3.28.2

Bazelisk version: v1.22.0
Build label: 6.5.0
",1
NotImplementedError from tf.constant in trivial case,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Trying to make a tensor that has the same value for all items in the batch, see the following bare minimum code. 
I get `NotImplementedError: cannot convert a symbolic tf.Tensor (custom_model_5_1/strided_slice:0) to a numpy array.`
I am not trying to use numpy, this is an internal error.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import keras
import numpy as np

class CustomModel(keras.models.Model):
    def call(self, inputs):
        inputs_shape = tf.shape(inputs)
        return tf.constant(3.0, shape=(inputs_shape[0], 1), dtype=inputs.dtype)  # NotImplementedError
        #return 3.0 * tf.ones(shape=(inputs_shape[0], 1), dtype=inputs.dtype)  # OK

model = CustomModel()
model.compile(run_eagerly=False, loss=""mse"")  # OK if run_eagerly=True
model.fit(np.array([[0.0]]), np.array([[0.0]]))
```
```


### Relevant log output

```shell
{
	""name"": ""NotImplementedError"",
	""message"": ""Exception encountered when calling CustomModel.call().

Cannot convert a symbolic tf.Tensor (custom_model_5_1/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.

Arguments received by CustomModel.call():
  • inputs=tf.Tensor(shape=(None, 1), dtype=float32)"",
	""stack"": ""---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
Cell In[6], line 13
     11 model = CustomModel()
     12 model.compile(run_eagerly=False, loss=\""mse\"")  # OK if run_eagerly=True
---> 13 model.fit(np.array([[0.0]]), np.array([[0.0]]))

File /usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--> 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

Cell In[6], line 8, in CustomModel.call(self, inputs)
      6 def call(self, inputs):
      7     inputs_shape = tf.shape(inputs)
----> 8     return tf.constant(3.0, shape=(inputs_shape[0], 1), dtype=inputs.dtype)

File /usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3100, in prod(a, axis, dtype, out, keepdims, initial, where)
   2979 @array_function_dispatch(_prod_dispatcher)
   2980 def prod(a, axis=None, dtype=None, out=None, keepdims=np._NoValue,
   2981          initial=np._NoValue, where=np._NoValue):
   2982     \""\""\""
   2983     Return the product of array elements over a given axis.
   2984 
   (...)
   3098     10
   3099     \""\""\""
-> 3100     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,
   3101                           keepdims=keepdims, initial=initial, where=where)

File /usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:88, in _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs)
     85         else:
     86             return reduction(axis=axis, out=out, **passkwargs)
---> 88 return ufunc.reduce(obj, axis, dtype, out, **passkwargs)

NotImplementedError: Exception encountered when calling CustomModel.call().

Cannot convert a symbolic tf.Tensor (custom_model_5_1/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.

Arguments received by CustomModel.call():
  • inputs=tf.Tensor(shape=(None, 1), dtype=float32)""
}
```
",1
Request to bring back GPU compatibility checks for TFLite `model_analyzer`,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf_nightly == 2.19.0.dev20241003

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

While using the nightly version I discover that the GPU compatibility checks are deprecated for the [model_analyzer](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/analyzer_wrapper/model_analyzer.cc#L443) tool in TF lite, following [this PR](https://github.com/tensorflow/tensorflow/pull/74830).

Currently the code for checking GPU compatibility is deprecated, but the output still prints ""Your model is compatible with GPU delegate"" (because there are essentially no checks). IMO this is actually confusing. I would suggest to change the output print to ""Skipping GPU compatibility as it is deprecated"", or just deprecate the `gpu_compatibility` boolean flag.

The previous logic is handy enough for my use case to expose non-compatible operators beforehand, and apply the tunings required manually on the `.tflite` graph, so curious why it is deprecated and what are the plans moving forward. Thanks!

### Standalone code to reproduce the issue

```shell
Attached PR regarding deprecation: https://github.com/tensorflow/tensorflow/pull/74830
```


### Relevant log output

_No response_",1
Jit-compiling `tf.while_loop` inside `tf.vectorized_map` raises `InvalidArgumentError`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v2.17.0

### Custom code

Yes

### OS platform and distribution

Colab

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

MRE
-------
The following mock-up of `cumsum` attempts to JIT compile a `tf.vectorized_map`ped function containing a `tf.scan`:
```python
import tensorflow as tf

def cumsum(xs):
    return tf.scan(
        lambda a, x: a + x, elems=xs
    )

@tf.function(jit_compile=True)
def vec_cumsum(xs):
    return tf.vectorized_map(cumsum, elems=xs)

xs_batched = tf.reshape(tf.range(30), (3, 10))
vec_cumsum(xs_batched)
```

__Expected behaviour__: `vec_cumsum(xs_batched)` returns a batch of cumulative sums.

__Actual behaviour__: Even though all data structures are known statically at JIT compile time, an InvalidArgumentError is raised with ""No registered 'TensorListReserve'"".  The fault is clearly related to `tf.scan`'s use of `tf.while_loop`, as a (longer) example using naked `tf.while_loop(..., max_iterations=n)` will confirm.

In JAX, it is possible to jit-compile a `vmap`ped function containing a `lax.while_loop` indicating that this is possible in HLO.  It seems the `tf.function(jit_compile=True)` machinery may be mis-transpiling to HLO somehow.

May be related to #73367 also involving `tf.vectorized_map` and `tf.while_loop` (albeit with reversed scope)?

### Standalone code to reproduce the issue

```shell
Colab MRE: https://colab.research.google.com/drive/1bmq1t3PdtebCSlNd0t-iEFrXX7Q0qqZp?usp=sharing
```


### Relevant log output

```shell
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-4-26ea491cd046> in <cell line: 13>()
     11 
     12 # Fails with ""No registered 'TensorListReserve'""""
---> 13 vec_cumsum(xs_batched)

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51   try:
     52     ctx.ensure_initialized()
---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                         inputs, attrs, num_outputs)
     55   except core._NotOkStatusException as e:

InvalidArgumentError: Detected unsupported operations when trying to compile graph __inference_vec_cumsum_462[_XlaMustCompile=true,config_proto=13561319589895757934,executor_type=11160318154034397263] on XLA_CPU_JIT: TensorListReserve (No registered 'TensorListReserve' OpKernel for XLA_CPU_JIT devices compatible with node {{function_node __inference_while_fn_428}}{{node while_init/TensorArrayV2_4}}
	 (OpKernel was found, but attributes didn't match) Requested Attributes: element_dtype=DT_VARIANT, shape_type=DT_INT32){{function_node __inference_while_fn_428}}{{node while_init/TensorArrayV2_4}}
```
",1
Multithreading is not working with teansorflow,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tensorflow==2.15.0.post1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

python:3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am using bert model for classification and serving the model with gunicorn worker_class=gthreads, tf.config.threading.set_intra_op_parallelism_threads(1)
tf.config.threading.set_inter_op_parallelism_threads(1)

when I using above two line of code the code is working fine as expected and if increase the number to more than 1, the code getting blocked at the below line of code

# Make predictions
outputs = model_obj(inputs)

and also inorder to reduce the docker image size I am using 
RUN pip3 install torch==2.0.0+cpu -f https://download.pytorch.org/whl/torch_stable.html


before installing all the dependencies
Flask==2.2.5
g2p-en==2.1.0
gunicorn==21.2.0
jellyfish==1.0.3
kenlm==0.2.0
nltk==3.8.1
numpy==1.26.3
pandas==2.2.0
python-dotenv==1.0.1
requests==2.31.0
scikit-learn==1.4.0
semantic-router==0.0.17
semantic-router[fastembed]
sentence-transformers==2.3.1
tensorflow==2.15.0.post1
tensorflow-hub==0.16.0
theano==1.0.5
transformers==4.37.2
Werkzeug==2.2.2


please tell me why is my code is getting blocked if I use more than 1 thread.

### Standalone code to reproduce the issue

```shell
def intent_prediction(self, sentence, thresold_score):
        logger.info(f""Threshold Score: {thresold_score}"")
        try:
            model_obj = intent_object_dict[self.model]
        except KeyError:
            logger.error(""Model not found in intent_object_dict"")
            model_obj = self.load_model()

        if INTENT_MODEL == ""cohere"":
            score, intent = self.cohere_intent_prediction(sentence, model_obj)
        else:
            score, intent = self.Bert_intent_prediction(
                sentence, model_obj, thresold_score
            )
        return score, intent

def Bert_intent_prediction(self, sentence, model_obj, thresold_score):
        inputs = self.load_BERT_tokenizer(sentence)
        # Make predictions
        outputs = model_obj(inputs)
        # Get predicted class
        probabilities = tf.nn.softmax(outputs.logits, axis=1)
        predicted_class = tf.argmax(probabilities, axis=1).numpy()[0]
        matching_score = probabilities[0][predicted_class].numpy()
        try:
            intent_data = intent_label_dict[self.model]
        except Exception as e:
            logger.error(f""error while getting label: {e}"")
            intent_data = self.get_intent_labels()

        if matching_score >= thresold_score:
            logger.info(
                f""Matched Main Intent:\
    {intent_data[predicted_class]},\
    SCORE :{matching_score}""
            )
            logger.info(f""Matched Sentence: {intent_data[predicted_class]}"")
        else:
            logger.info(f""Intent not matched, score is {matching_score}"")

        intent = intent_data[predicted_class]

        return str(matching_score), intent
```


### Relevant log output

```shell
30-Sep-2024 15:50:42.761|INFO    |__init__|I want my sofa get cleaned|
    __init__.py:171|Enter into PUNC for intent...
30-Sep-2024 15:50:42.761|INFO    |phrase_sim|I want my sofa get cleaned|
    phrase_sim.py:72|Threshold Score: 0.7


after this the code is blocked
```
",1
Segmentation fault (core dumped) in `tf.data.experimental.SqlDataset`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the illegal input to tf.data.experimental.SqlDataset triggered when a crash, and will only come when iteration data.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

data_source_name = ""sqlite:///path/to/correct_database.db""

query = ""SELECT id, name FROM my_table""
output_types = (tf.int64, tf.string)
dataset = tf.data.experimental.SqlDataset(
    'sqlite', data_source_name, query, output_types)

for element in dataset:
    print(element)
```


### Relevant log output

```shell
2024-09-28 21:18:33.844482: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 21:18:33.907260: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 21:18:33.986019: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 21:18:34.009755: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 21:18:34.068897: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-09-28 21:18:38.768599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3114 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-09-28 21:18:38.769172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-09-28 21:18:39.132534: W tensorflow/core/kernels/data/experimental/sql_dataset_op.cc:209] Failed to connect to database: INVALID_ARGUMENT: Sqlite::Open(sqlite:///path/to/correct_database.db) failed: unable to open database file
Segmentation fault (core dumped)
```
",2
Aborted (core dumped) in `tf.linalg.det/slogdet/logdet/cholesky/inv`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

tf.linalg.det/slogdet/logdet/cholesky/inv triggered a crash when the input is empty. Note that this will only be triggered if the gpu is available.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

invalid_input = tf.zeros([])
tf.linalg.det(invalid_input)    # crash
tf.linalg.slogdet(invalid_input)  # crash
tf.linalg.cholesky(invalid_input)  # crash
tf.linalg.logdet(invalid_input)  # crash
tf.linalg.inv(invalid_input)  # crash
```


### Relevant log output

```shell
2024-09-28 21:11:10.188752: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 21:11:10.199880: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 21:11:10.213635: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 21:11:10.221654: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 21:11:10.279720: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-09-28 21:11:17.015480: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3114 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-09-28 21:11:17.015957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-09-28 21:11:17.154391: F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)
```
",2
Aborted (core dumped) in `tf.raw_ops.ResourceScatterNdop`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the type of resource_handle is inconsistent with that of updates,tf.raw_ops.ResourceScatterNdop triggers the crash. As follows:
tf.raw_ops.ResourceScatterNdUpdate
tf.raw_ops.ResourceScatterNdAdd
tf.raw_ops.ResourceScatterNdSub
tf.raw_ops.ResourceScatterNdMax
tf.raw_ops.ResourceScatterNdMin

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

resource_var = tf.Variable(initial_value=tf.zeros([2, 2], dtype=tf.int32), trainable=False)
resource_handle = resource_var.handle

indices = np.array([[2, 1], [1, 2]], dtype=np.int32)
updates = np.array([10, 20], dtype=np.float32)
tf.raw_ops.ResourceScatterNdUpdate(  # crash
    ref=resource_handle,
    indices=indices,
    updates=updates,
    use_locking=True
)

tf.raw_ops.ResourceScatterNdAdd(  # crash
    ref=resource_handle,
    indices=indices,
    updates=updates,
    use_locking=True
)
tf.raw_ops.ResourceScatterNdSub(  # crash
    ref=resource_handle,
    indices=indices,
    updates=updates,
    use_locking=True
)
tf.raw_ops.ResourceScatterNdMax(  # crash
    ref=resource_handle,
    indices=indices,
    updates=updates,
    use_locking=True
)
tf.raw_ops.ResourceScatterNdMin(  # crash
    ref=resource_handle,
    indices=indices,
    updates=updates,
    use_locking=True
)
```


### Relevant log output

```shell
2024-09-28 21:06:23.445185: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 21:06:23.508056: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 21:06:23.583640: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 21:06:23.607538: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 21:06:23.664877: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-09-28 21:06:31.527466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3114 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-09-28 21:06:31.527985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-09-28 21:06:31.782114: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (3 vs. 1) float expected, got int32
Aborted (core dumped)
```
",2
Aborted (core dumped) in `tf.io.encode_png`/`tf.compat.v1.image.encode_png`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The crash was triggered when an illegal image was passed to tf.io.encode_png/tf.compat.v1.image.encode_png

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

image = tf.cast(tf.tile([[[0, 0, 0, 1]], [[0, 0, 1, 0]]], [0, 0, 1]), tf.uint8)

encoded_image = tf.compat.v1.image.encode_png(image) # crash
tf.io.encode_png(image, compression=-1, name=None) #crash
```


### Relevant log output

```shell
2024-09-28 20:48:36.270008: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 20:48:36.332972: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 20:48:36.411391: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 20:48:36.428306: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 20:48:36.438336: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.2024-09-28 20:48:41.296886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3114 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.92024-09-28 20:48:41.297450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-09-28 20:48:41.475588: F tensorflow/core/lib/png/png_io.cc:350] 'image' Must be non NULL
Aborted (core dumped)
```
",2
Floating point exception (core dumped) in `tf.nn.depth_to_space`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific inputs, tf.nn.depth_to_space triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
try:
    # Create an empty tensor
    arg_0_tensor = tf.zeros([0, 2, 3, 12], dtype=tf.float32)
    # arg_0 = tf.identity(arg_0_tensor)
    arg_1 = 536870912
    out = tf.nn.depth_to_space(arg_0_tensor, arg_1)
except Exception as e:
    print(""Error:"", str(e))
```


### Relevant log output

```shell
2024-09-28 20:41:05.888017: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 20:41:05.950498: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 20:41:06.028236: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 20:41:06.052072: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 20:41:06.111011: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-09-28 20:41:11.970896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2704 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-09-28 20:41:11.973176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
Floating point exception (core dumped)
```
",2
Aborted (core dumped) in `tf.nn.max_pool/tf.nn.max_pool1d`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific inputs, tf.nn.max_pool triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

invalid_kernel_size = -1
invalid_operation = tf.nn.max_pool(
    tf.random.normal([1, 32, 32, 3]),
    ksize=[1, invalid_kernel_size, invalid_kernel_size, 1],
    strides=[1, 2, 2, 1],
    padding='SAME'
)
```

```
import tensorflow as tf
import sys

ksize = sys.maxsize + 100  # Set to a value larger than sys.maxsize
input_tensor = tf.random.normal(shape=(2, 10, 4))
result = tf.nn.max_pool1d(input=input_tensor, ksize=ksize, strides=1, padding='SAME')

```


### Relevant log output

```shell
2024-09-28 20:26:47.491907: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 20:26:47.554171: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 20:26:47.606570: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 20:26:47.610539: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 20:26:47.639739: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-09-28 20:26:54.579839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21471 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-09-28 20:26:54.582099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-09-28 20:26:55.563477: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
F0000 00:00:1727526415.563805  147227 cuda_dnn.cc:1107] Check failed: cudnnSetPoolingNdDescriptor( handle_.get(), (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum ? cudnn_max_pooling_mode : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING), propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd, shape.data(), padding.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0) 
*** Check failure stack trace: ***
Aborted (core dumped)
```
",2
Segmentation fault (core dumped) in `tf.profiler.experimental.Profile`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific inputs, tf.profiler.experimental.Profile triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

profiler_options = tf.profiler.experimental.ProfilerOptions(
    host_tracer_level=999,
    python_tracer_level=-1,
    device_tracer_level=10,
    delay_ms=None
)

with tf.profiler.experimental.Profile(None, options=profiler_options):
    a = tf.constant(1)
    b = tf.constant(2)
    c = a + b
    print(c.numpy())
```


### Relevant log output

```shell
2024-09-28 20:07:36.902909: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.2024-09-28 20:07:36.966049: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 20:07:36.998027: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 20:07:37.002984: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered2024-09-28 20:07:37.055864: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Segmentation fault (core dumped)
```
",2
Crash when calling TFSMLayer object during TF_lite conversion,"### 1. System information

- Linux Ubuntu 24.04
- TF 2.17.0 (pip) and Keras 3.5.0

### 2. Code

A TF model is first exported using Keras 3.5.0:

```model.export(model_name)```

Then when the following method is called below, the conversion crashes in TF 2.17.0 (log below). It works fine with TF 2.16.2.

```
def makeQuantizedTFmodel(A, dP):
    import tensorflow as tf    
    A2 = tf.cast(A, tf.float32)
    A = tf.data.Dataset.from_tensor_slices((A2)).batch(1)
    
    def representative_dataset_gen():
        for input_value in A.take(100):
            yield[input_value]
  
            import keras
            model = keras.layers.TFSMLayer(model_name), call_endpoint='serve')    
            converter = tf.lite.TFLiteConverter.from_keras_model(model)    

            converter.optimizations = [tf.lite.Optimize.DEFAULT]
            converter.representative_dataset = representative_dataset_gen
            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
            converter.inference_input_type = tf.uint8
            converter.inference_output_type = tf.uint8
            tflite_quant_model = converter.convert()

            with open(model_name+'.tflite', 'wb') as o:
                   o.write(tflite_quant_model)
```

The error with TF 2.17.0:

```
Traceback (most recent call last):
  File ""/home/nicola/test/DML/DataML.py"", line 1008, in <module>
    sys.exit(main())
             ^^^^^^
  File ""/home/nicola/test/DML/DataML.py"", line 160, in main
    train(sys.argv[2], None, None)
  File ""/home/nicola/test/DML/DataML.py"", line 398, in train
    makeQuantizedTFmodel(A, dP)
  File ""/home/nicola/test/DML/libDataML.py"", line 231, in makeQuantizedTFmodel
    tflite_quant_model = converter.convert()
                         ^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/lite.py"", line 1231, in wrapper
    return self._convert_and_export_metrics(convert_func, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/lite.py"", line 1183, in _convert_and_export_metrics
    result = convert_func(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/lite.py"", line 1749, in convert
    self._freeze_keras_model()
  File ""/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert_phase.py"", line 215, in wrapper
    raise error from None  # Re-throws the exception.
    ^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert_phase.py"", line 205, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/lite.py"", line 1690, in _freeze_keras_model
    input_signature = _model_input_signature(
                      ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/tflite_keras_util.py"", line 119, in model_input_signature
    input_specs = model._get_save_spec(  # pylint: disable=protected-access
                  ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'TFSMLayer' object has no attribute '_get_save_spec'. Did you mean: '_set_save_spec'?
```

",2
Failed to synchronize the stop event: CUDA_ERROR_UNKNOWN: unknown error,"Hello together,

I am trying to run the new-benchmark AI for my new platform:
https://pypi.org/project/new-ai-benchmark/

My System configuration is as follows:
*  TF Version: 2.17.0
*  Platform: Linux-6.8.0-45-generic-x86_64-with-glibc2.35
*  CPU: 13th Gen Intel(R) Core(TM) i9-13900E
*  CPU RAM: 62 GB
*  GPU/0: NVIDIA L4
*  GPU RAM: 20.3 GB
*  CUDA Version: 12.4
*  CUDA Build: V12.4.131

arun@arun:~$ nvidia-smi
Wed Sep 25 12:52:43 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.107.02             Driver Version: 550.107.02     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA L4                      On  |   00000000:01:00.0 Off |                    0 |
| N/A   55C    P8             13W /   72W |      14MiB /  23034MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A      1373      G   /usr/lib/xorg/Xorg                              4MiB |
+-----------------------------------------------------------------------------------------+


While executing the program, at any point of time while running, it fails to complete.
```

2024-09-25 10:02:40.968881: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1857] failed to synchronize the stop event: CUDA_ERROR_UNKNOWN: unknown error
E0000 00:00:1727251360.968902    4778 gpu_timer.cc:162] INTERNAL: Error destroying CUDA event: CUDA_ERROR_UNKNOWN: unknown error
E0000 00:00:1727251360.968910    4778 gpu_timer.cc:168] INTERNAL: Error destroying CUDA event: CUDA_ERROR_UNKNOWN: unknown error
2024-09-25 10:02:40.969770: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED
in external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc(6053): 'status'
[[{{node resnet_v2_152/block2/unit_8/bottleneck_v2/conv2/Conv2D}}]]
2024-09-25 10:02:40.969784: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED
in external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc(6053): 'status'
[[{{node resnet_v2_152/block2/unit_8/bottleneck_v2/conv2/Conv2D}}]]
[[output/_3]]
2024-09-25 10:02:40.969798: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 16128423615390660009
2024-09-25 10:02:40.992816: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleUnload(it.second)' failed with 'CUDA_ERROR_UNKNOWN'

2024-09-25 10:02:40.992867: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleUnload(it.second)' failed with 'CUDA_ERROR_UNKNOWN'

2024-09-25 10:02:40.992874: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleUnload(it.second)' failed with 'CUDA_ERROR_UNKNOWN'

2024-09-25 10:02:40.992880: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleUnload(it.second)' failed with 'CUDA_ERROR_UNKNOWN'

Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/tensorflow/python/client/session.py"", line 1401, in _do_call
    return fn(*args)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/client/session.py"", line 1384, in _run_fn
    return self._call_tf_sessionrun(options, feed_dict, fetch_list,
  File ""/usr/lib/python3/dist-packages/tensorflow/python/client/session.py"", line 1477, in _call_tf_sessionrun
    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,
tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.
```

In another instance, there is another reason for failure:

```
Failed to enqueue async memcpy from device to host: CUDA_ERROR_UNKNOWN: unknown error; host dst: 0x763a34bfcf48; GPU src: 0x76387c443400; size: 8=0x8
2024-09-25 11:53:30.387757: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1595] failed to free device memory at 0x76387c443400; result: CUDA_ERROR_UNKNOWN: unknown error
2024-09-25 11:53:30.387762: W tensorflow/core/kernels/gpu_utils.cc:88] Failed to check cudnn convolutions for out-of-bounds reads and writes with an error message: 'Failed to memcpy from device to host.'; skipping this check. This only means that we won't check cudnn for out-of-bounds reads and writes. This message will only be printed once.
2024-09-25 11:53:30.388810: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED
in external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc(6053): 'status'
[[{{node resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/Conv2D}}]]
2024-09-25 11:53:30.388848: F ./tensorflow/core/kernels/conv_2d_gpu.h:1028] Non-OK-status: GpuLaunchKernel( SwapDimension1And2InTensor3UsingTiles<T, kNumThreads, kTileSize, kTileSize, conjugate>, total_tiles_count, kNumThreads, 0, d.stream(), input, input_dims, output)
Status: INTERNAL: unknown error
```

Could you please help in resolving the issue? 

Thank you
Arun",1
Build Failure with ml_dtypes 0.4.0 on Power Architecture,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

master

### Custom code

No

### OS platform and distribution

linux/ppc64le

### Mobile device

_No response_

### Python version

3.9, 3.10, 3.11,3.12

### Bazel version

6.5.2

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When attempting to build TensorFlow on the Power architecture with ml-dtypes==0.4.0, the build fails due to an incompatibility with numpy 2.0.0rc1. On power, there is no wheel for ml_dtypes on pypi. Hence, Tensorflow build tries to build ml_dtypes from its source, which fails due to build time depencency on numpy 2.0.0rc1. https://github.com/jax-ml/ml_dtypes/blob/v0.4.0/pyproject.toml#L52.  Numpy 2.0.0rc1 is not available for any architetcure on pypi.
We had rasied this concern with ml_dtypes and they have fixed it in ml_dtypes 0.4.1 version.

To resolve this issue, we recommend updating the following files:

1. tensorflow/tools/pip_package/setup.py

Update the ml_dtypes version requirement from 0.4.0 to 0.4.1.
Current section: ml_dtypes >= 0.4.0, < 0.5.0

2. ci/official/requirements_updater/requirements.in

Update the ml_dtypes version requirement from 0.4.0 to 0.4.1.
Current section: ml_dtypes >= 0.4.0, < 0.5.0

After these changes, it should reflect in the requirements_lock.txt for different Python versions.

### Standalone code to reproduce the issue

```shell
Architecture: ppc64le (Power)
Python Version: 3.x
TensorFlow Version: master
ml_dtypes Version: 0.4.0
pip Version: 24.2

Steps to reproduce:
bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu

The error related to numpy==2.0.0rc1 will occur during the installation.
```


### Relevant log output

```shell
ERROR: /tensorflow/WORKSPACE:53:13: fetching whl_library rule //external:pypi_ml_dtypes: Traceback (most recent call last):
        File ""/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/rules_python/python/private/pypi/whl_library.bzl"", line 294, column 35, in _whl_library_impl
                repo_utils.execute_checked(
        File ""/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/rules_python/python/private/repo_utils.bzl"", line 182, column 29, in _execute_checked
                return _execute_internal(fail_on_error = True, *args, **kwargs)
        File ""/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/rules_python/python/private/repo_utils.bzl"", line 123, column 13, in _execute_internal
                fail((
Error in fail: repo.execute: whl_library.ResolveRequirement(pypi_ml_dtypes, ml-dtypes==0.4.0): end: failure:
  command: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/python_ppc64le-unknown-linux-gnu/bin/python3 -m python.private.pypi.whl_installer.wheel_installer --requirement ml-dtypes==0.4.0 --isolated --extra_pip_args ""{\""arg\"":[]}"" --pip_data_exclude ""{\""arg\"":[]}"" --environment ""{\""arg\"":{}}""
  return code: 1
  working dir: <default: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi_ml_dtypes>
  timeout: 600
  environment:
PYTHONPATH=""/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/rules_python:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__build:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__click:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__colorama:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__importlib_metadata:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__installer:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__more_itertools:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__packaging:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__pep517:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__pip:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__pip_tools:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__pyproject_hooks:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__setuptools:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__tomli:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__wheel:/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/pypi__zipp""
CPPFLAGS=""""
===== stdout start =====
Collecting ml-dtypes==0.4.0 (from -r /tmp/tmpae6v73x5 (line 1))
  Using cached ml_dtypes-0.4.0.tar.gz (692 kB)
  Installing build dependencies: started
  Installing build dependencies: finished with status 'error'
===== stdout end =====
===== stderr start =====
  error: subprocess-exited-with-error

  × pip subprocess to install build dependencies did not run successfully.
  │ exit code: 1
  ╰─> [3 lines of output]
      ERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11
      ERROR: Could not find a version that satisfies the requirement numpy==2.0.0rc1 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4, 1.25.0, 1.25.1, 1.25.2, 1.26.0, 1.26.1, 1.26.2, 1.26.3, 1.26.4, 2.0.0, 2.0.1, 2.0.2, 2.1.0rc1, 2.1.0, 2.1.1)
      ERROR: No matching distribution found for numpy==2.0.0rc1
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error
```
",1
Build Failure on AWS Graviton3 with Custom oneDNN (oneDNN-3.6-rc): Invalid Preprocessing Directives in dnnl_config.h,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf v2.17.0

### Custom code

No

### OS platform and distribution

Ubuntu 22.04.2 LTS

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

6.5.0

### GCC/compiler version

gcc version 11.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am unable to build TensorFlow with the latest oneDNN or custom oneDNN (oneDNN-3.6-rc) on AWS Graviton3 (aarch64) CPU. The build process fails with several compilation errors related to invalid preprocessing directives in the dnnl_config.h file.

I expected the build to complete successfully with the custom oneDNN settings, allowing TensorFlow to run efficiently on the AWS Graviton3 (aarch64) architecture.

### Standalone code to reproduce the issue

```shell
Clone the TensorFlow repository:

git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git checkout v2.17.0

Modify the relevant files as follows:

Update oneDNN version in tensorflow/workspace2.bzl.
Adjust mkldnn_acl.BUILD for versioning.

root@8c5bdc6a1bd7:/workdir/tensorflow# git diff
diff --git a/tensorflow/workspace2.bzl b/tensorflow/workspace2.bzl
index fd29dff05f3..7ed30157970 100644
--- a/tensorflow/workspace2.bzl
+++ b/tensorflow/workspace2.bzl
@@ -205,36 +205,24 @@ def _tf_repositories():
     tf_http_archive(
         name = ""onednn"",
         build_file = ""//third_party/mkl_dnn:mkldnn_v1.BUILD"",
-        sha256 = ""5131ac559a13daa6e2784d20ab24e4607e55aa6da973518086326a647d389425"",
-        strip_prefix = ""oneDNN-3.4.2"",
-        urls = tf_mirror_urls(""https://github.com/oneapi-src/oneDNN/archive/refs/tags/v3.4.2.tar.gz""),
+       sha256 = ""568428621a4912dd2159eaee97f646259c655acc271dc57bd75478daa9672ea5"",
+       strip_prefix = ""oneDNN-3.6-rc"",
+       urls = tf_mirror_urls(""https://github.com/oneapi-src/oneDNN/archive/refs/tags/v3.6-rc.tar.gz""),
     )
 
     tf_http_archive(
         name = ""mkl_dnn_acl_compatible"",
         build_file = ""//third_party/mkl_dnn:mkldnn_acl.BUILD"",
-        patch_file = [
-            ""//third_party/mkl_dnn:onednn_acl_threadcap.patch"",
-            ""//third_party/mkl_dnn:onednn_acl_reorder.patch"",
-            ""//third_party/mkl_dnn:onednn_acl_thread_local_scheduler.patch"",
-            ""//third_party/mkl_dnn:onednn_acl_fp32_bf16_reorder.patch"",
-            ""//third_party/mkl_dnn:onednn_acl_bf16_capability_detection_for_ubuntu20.04.patch"",
-            ""//third_party/mkl_dnn:onednn_acl_indirect_conv.patch"",
-        ],
-        sha256 = ""2f76b407ef8893cca71340f88cd800019a1f14f8ac1bbdbb89a84be1370b52e3"",
-        strip_prefix = ""oneDNN-3.2.1"",
-        urls = tf_mirror_urls(""https://github.com/oneapi-src/oneDNN/archive/refs/tags/v3.2.1.tar.gz""),
+       sha256 = ""568428621a4912dd2159eaee97f646259c655acc271dc57bd75478daa9672ea5"",
+        strip_prefix = ""oneDNN-3.6-rc"",
+        urls = tf_mirror_urls(""https://github.com/oneapi-src/oneDNN/archive/refs/tags/v3.6-rc.tar.gz""),
     )
 
     tf_http_archive(
         name = ""compute_library"",
-        patch_file = [
-            ""//third_party/compute_library:compute_library.patch"",
-            ""//third_party/compute_library:acl_thread_local_scheduler.patch"",
-        ],
-        sha256 = ""c4ca329a78da380163b2d86e91ba728349b6f0ee97d66e260a694ef37f0b0d93"",
-        strip_prefix = ""ComputeLibrary-23.05.1"",
-        urls = tf_mirror_urls(""https://github.com/ARM-software/ComputeLibrary/archive/v23.05.1.tar.gz""),
+        sha256 = ""e7e1b554129748c3aadf1a85de48d332afbef7c6c0c3c5be77a1cfb58311c57b"",
+        strip_prefix = ""ComputeLibrary-24.08.1"",
+        urls = tf_mirror_urls(""https://github.com/ARM-software/ComputeLibrary/archive/refs/tags/v24.08.1.tar.gz"")
     )
 
     tf_http_archive(
diff --git a/third_party/mkl_dnn/mkldnn_acl.BUILD b/third_party/mkl_dnn/mkldnn_acl.BUILD
index d67b62a98d2..083b3d7a627 100644
--- a/third_party/mkl_dnn/mkldnn_acl.BUILD
+++ b/third_party/mkl_dnn/mkldnn_acl.BUILD
@@ -128,8 +128,8 @@ expand_template(
     out = ""include/oneapi/dnnl/dnnl_version.h"",
     substitutions = {
         ""@DNNL_VERSION_MAJOR@"": ""3"",
-        ""@DNNL_VERSION_MINOR@"": ""2"",
-        ""@DNNL_VERSION_PATCH@"": ""1"",
+        ""@DNNL_VERSION_MINOR@"": ""6"",
+        ""@DNNL_VERSION_PATCH@"": ""0"",
         ""@DNNL_VERSION_HASH@"": ""N/A"",
     },
     template = ""include/oneapi/dnnl/dnnl_version.h.in"",
(END)


Attempt to build TensorFlow:

taskset -c 16-32 bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu --config=mkl_aarch64_threadpool --jobs=33 --local_cpu_resources=16 --verbose_failures -s
```


### Relevant log output

```shell
ERROR: /root/.cache/bazel/_bazel_root/58adfe0c0193ce259b2b32549c3d3a4f/external/mkl_dnn_acl_compatible/BUILD.bazel:138:11: Compiling src/common/batch_normalization.cpp failed: (Exit 1): gcc failed: error executing command (from target @mkl_dnn_acl_compatible//:mkl_dnn_acl) 
  (cd /root/.cache/bazel/_bazel_root/58adfe0c0193ce259b2b32549c3d3a4f/execroot/org_tensorflow && \
  exec env - \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF2_BEHAVIOR=1 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++14' -MD -MF bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/_objs/mkl_dnn_acl/batch_normalization.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/_objs/mkl_dnn_acl/batch_normalization.pic.o' -fPIC -DENABLE_NEON -DARM_COMPUTE_CPU_ENABLED -DARM_COMPUTE_ENABLE_NEON -DARM_COMPUTE_ENABLE_I8MM -DENABLE_FP32_KERNELS -DENABLE_QASYMM8_KERNELS -DENABLE_QASYMM8_SIGNED_KERNELS -DENABLE_QSYMM16_KERNELS -DENABLE_INTEGER_KERNELS -DENABLE_NHWC_KERNELS -DENABLE_NCHW_KERNELS -DARM_COMPUTE_GRAPH_ENABLED -DARM_COMPUTE_ENABLE_SVEF32MM -DARM_COMPUTE_ENABLE_FIXED_FORMAT_KERNELS -D_GLIBCXX_USE_NANOSLEEP -DARM_COMPUTE_OPENMP_SCHEDULER '-DDNNL_AARCH64_USE_ACL=1' '-DBAZEL_CURRENT_REPOSITORY=""mkl_dnn_acl_compatible""' -iquote external/mkl_dnn_acl_compatible -iquote bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible -iquote external/compute_library -iquote bazel-out/aarch64-opt/bin/external/compute_library -Ibazel-out/aarch64-opt/bin/external/compute_library/include/_virtual_includes/include -isystem external/mkl_dnn_acl_compatible/include -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/include -isystem external/mkl_dnn_acl_compatible/src -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src -isystem external/mkl_dnn_acl_compatible/src/common -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src/common -isystem external/mkl_dnn_acl_compatible/src/cpu -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src/cpu -isystem external/mkl_dnn_acl_compatible/src/cpu/aarch64/xbyak_aarch64/src -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src/cpu/aarch64/xbyak_aarch64/src -isystem external/mkl_dnn_acl_compatible/src/cpu/aarch64/xbyak_aarch64/xbyak_aarch64 -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src/cpu/aarch64/xbyak_aarch64/xbyak_aarch64 -isystem external/mkl_dnn_acl_compatible/src/cpu/gemm -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src/cpu/gemm -isystem external/compute_library/arm_compute/runtime -isystem bazel-out/aarch64-opt/bin/external/compute_library/arm_compute/runtime -isystem external/compute_library/src/core/NEON/kernels/arm_gemm -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/NEON/kernels/arm_gemm -isystem external/compute_library/src/core/NEON/kernels/assembly -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/NEON/kernels/assembly -isystem external/compute_library/src/core/NEON/kernels/convolution/common -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/NEON/kernels/convolution/common -isystem external/compute_library/src/core/NEON/kernels/convolution/winograd -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/NEON/kernels/convolution/winograd -isystem external/compute_library/src/core/cpu/kernels/assembly -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/cpu/kernels/assembly -isystem external/compute_library/src/cpu/kernels/assembly -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/cpu/kernels/assembly -isystem external/compute_library/src/core/NEON/kernels/arm_conv -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/NEON/kernels/arm_conv -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS '-std=c++17' -fopenmp-simd -fexceptions -UUSE_MKL -UUSE_CBLAS -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/mkl_dnn_acl_compatible/src/common/batch_normalization.cpp -o bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/_objs/mkl_dnn_acl/batch_normalization.pic.o)
# Configuration: 286713d3e237c869e8689debb2d6b060b16fc87de4d5e6ded144ba62ae251131
# Execution platform: @local_execution_config_platform//:platform
In file included from external/mkl_dnn_acl_compatible/include/oneapi/dnnl/dnnl_common_types.h:31,
                 from external/mkl_dnn_acl_compatible/include/oneapi/dnnl/dnnl_common.h:23,
                 from external/mkl_dnn_acl_compatible/include/oneapi/dnnl/dnnl.h:23,
                 from external/mkl_dnn_acl_compatible/src/common/batch_normalization.cpp:18:
bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/include/oneapi/dnnl/dnnl_config.h:112:2: error: invalid preprocessing directive #cmakedefine
  112 | #cmakedefine DNNL_GPU_VENDOR DNNL_VENDOR_${DNNL_GPU_VENDOR}
      |  ^~~~~~~~~~~
bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/include/oneapi/dnnl/dnnl_config.h:158:2: error: invalid preprocessing directive #cmakedefine
  158 | #cmakedefine DNNL_SYCL_GENERIC
      |  ^~~~~~~~~~~
bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/include/oneapi/dnnl/dnnl_config.h:181:2: error: invalid preprocessing directive #cmakedefine
  181 | #cmakedefine DNNL_DISABLE_GPU_REF_KERNELS
      |  ^~~~~~~~~~~
bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/include/oneapi/dnnl/dnnl_config.h:195:2: error: invalid preprocessing directive #cmakedefine01
  195 | #cmakedefine01 BUILD_GROUP_NORMALIZATION
      |  ^~~~~~~~~~~~~
bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/include/oneapi/dnnl/dnnl_config.h:206:2: error: invalid preprocessing directive #cmakedefine01
  206 | #cmakedefine01 BUILD_SDPA
      |  ^~~~~~~~~~~~~
bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/include/oneapi/dnnl/dnnl_config.h:224:2: error: invalid preprocessing directive #cmakedefine01
  224 | #cmakedefine01 BUILD_XE2
      |  ^~~~~~~~~~~~~
bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/include/oneapi/dnnl/dnnl_config.h:226:2: error: invalid preprocessing directive #cmakedefine01
  226 | #cmakedefine01 BUILD_GEMM_KERNELS_ALL
      |  ^~~~~~~~~~~~~
bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/include/oneapi/dnnl/dnnl_config.h:227:2: error: invalid preprocessing directive #cmakedefine01
  227 | #cmakedefine01 BUILD_GEMM_KERNELS_NONE
      |  ^~~~~~~~~~~~~
bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/include/oneapi/dnnl/dnnl_config.h:228:2: error: invalid preprocessing directive #cmakedefine01
  228 | #cmakedefine01 BUILD_GEMM_SSE41
      |  ^~~~~~~~~~~~~
bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/include/oneapi/dnnl/dnnl_config.h:229:2: error: invalid preprocessing directive #cmakedefine01
  229 | #cmakedefine01 BUILD_GEMM_AVX2
      |  ^~~~~~~~~~~~~
bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/include/oneapi/dnnl/dnnl_config.h:230:2: error: invalid preprocessing directive #cmakedefine01
  230 | #cmakedefine01 BUILD_GEMM_AVX512
      |  ^~~~~~~~~~~~~
SUBCOMMAND: # @boringssl//:crypto [action 'Compiling src/crypto/pem/pem_lib.c [for tool]', configuration: 6c76bd453e22b21125a2028c36fb69b9de59167ea2a1dca88d8da721e8db0553, execution platform: @local_execution_config_platform//:platform]
(cd /root/.cache/bazel/_bazel_root/58adfe0c0193ce259b2b32549c3d3a4f/execroot/org_tensorflow && \
  exec env - \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/aarch64-opt-exec-50AE0418/bin/external/boringssl/_objs/crypto/pem_lib.pic.d '-frandom-seed=bazel-out/aarch64-opt-exec-50AE0418/bin/external/boringssl/_objs/crypto/pem_lib.pic.o' -fPIC '-DBAZEL_CURRENT_REPOSITORY=""boringssl""' -iquote external/boringssl -iquote bazel-out/aarch64-opt-exec-50AE0418/bin/external/boringssl -isystem external/boringssl/src/include -isystem bazel-out/aarch64-opt-exec-50AE0418/bin/external/boringssl/src/include -g0 -w -DBORINGSSL_IMPLEMENTATION -Wa,--noexecstack -Wall -Werror '-Wformat=2' -Wsign-compare -Wmissing-field-initializers -Wwrite-strings -Wshadow -fno-common '-D_XOPEN_SOURCE=700' '-std=c11' -Wmissing-prototypes -Wold-style-definition -Wstrict-prototypes -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/boringssl/src/crypto/pem/pem_lib.c -o bazel-out/aarch64-opt-exec-50AE0418/bin/external/boringssl/_objs/crypto/pem_lib.pic.o)
# Configuration: 6c76bd453e22b21125a2028c36fb69b9de59167ea2a1dca88d8da721e8db0553
# Execution platform: @local_execution_config_platform//:platform
Target //tensorflow/tools/pip_package:wheel failed to build
INFO: Elapsed time: 165.542s, Critical Path: 28.45s
INFO: 5206 processes: 1083 internal, 4123 local.
FAILED: Build did NOT complete successfully
```
",1
tensorflow pjrt plugin,"I've developed a pjrt plugin using https://openxla.org/xla/pjrt_integration as a resource, which is working with jax. 

How do I load this plugin and run tensorflow models? Is there some kind of registration step, similar to jax? If I load the plugin into `import jax._src.xla_bridge` and call `tf.config.list_physical_devices()` I don't see my device, so I imagine it's something else. ",0
TensorFlow keeps creating threads when multi-GPU training （thread leak）,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.4

### GPU model and memory

Nvidia A800 

### Current behavior?

I was using this machine to train a GPT-2 example from (the data I used can also be found in this link, also here https://github.com/chinese-poetry/chinese-poetry.git) https://keras.io/examples/generative/gpt2_text_generation_with_kerasnlp/

Before we start, I would give a baseline amount of this machine's threads :
![10](https://github.com/user-attachments/assets/53e16381-e50f-4054-8c2a-b790fe2e077b)

When starting with the multi-GPU training of tensorflow by calling the tf.distribute.MirroredStrategy, the training process worked fine as usual. 

But with the time went by, I found the amount of threads increased with the training process going, here is the evidence of thread increasing when processing 3354th batch (I used cat /proc/""this programs' pid""/status to check the number of threads):
![3](https://github.com/user-attachments/assets/56bbfaf0-f59c-48f1-ae19-a98d563ad000)
![tf](https://github.com/user-attachments/assets/c546ea4b-855a-44e5-84c1-95d6d3f3aba4)
![2](https://github.com/user-attachments/assets/0d07b24d-e93e-451e-9e1e-51a26f7db60d)

Then, the evidence of thread increasing when processing 3791st batch (the amount of threads reached 22178):
![6](https://github.com/user-attachments/assets/508fed5c-8aec-4deb-8e68-74dbf6aa5613)
![4](https://github.com/user-attachments/assets/77507790-70fd-4abf-b72c-e9d831565879)
![5](https://github.com/user-attachments/assets/d8945e53-23d9-4fb7-b50d-37962b93a692)

When calculating  the 5054th batch, the training program got an error, and I captured a count of threads before the error (achieved around 31120 threads):
![8](https://github.com/user-attachments/assets/15b433cc-eefb-451f-a31c-2a286e17afec)
![8](https://github.com/user-attachments/assets/813835d2-3834-400a-b8fe-ec27f15d89ad)

I checked an similar issue in https://github.com/tensorflow/tensorflow/issues/62466, but I cannot find a solution, moreover, I have run other examples like diffusion model using this machine and the same tensorflow env with multi-GPU training, which worked fine and no any problems. So, could you please give me a help for this problem, very appreciated.





### Standalone code to reproduce the issue

```shell
Here is my code

import os

os.environ[""KERAS_BACKEND""] = ""tensorflow""  # or ""tensorflow"" or ""torch""

import keras_nlp
import keras
import tensorflow as tf
import time

keras.mixed_precision.set_global_policy(""mixed_float16"")

import os
import json
import datetime

train_ds = (
    tf.data.Dataset.from_tensor_slices(paragraphs)
    .batch(36)
    .cache()
    .prefetch(tf.data.AUTOTUNE)
)

strategy = tf.distribute.MirroredStrategy([""GPU:0"", ""GPU:1"", ""GPU:2"", ""GPU:3"", ""GPU:4"", ""GPU:5""])
print(""Number of devices: {}"".format(strategy.num_replicas_in_sync))

preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(
    ""gpt2_base_en"",
    sequence_length=128,
)

# Open a strategy scope.
with strategy.scope():
# To speed up training and generation, we use preprocessor of length 128
# instead of full length 1024.
    gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(
        ""gpt2_base_en"", preprocessor=preprocessor
    )
    num_epochs = 5

    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_path,
        save_weights_only=True,
        monitor=""accuracy"",
        # monitor=""i_loss"",
        mode=""min"",
        save_best_only=True,
        save_freq=""epoch""
    )
    learning_rate = 5e-4
    
    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    gpt2_lm.compile(
        optimizer=keras.optimizers.Adam(learning_rate),
        loss=loss,
        weighted_metrics=[""accuracy""],
    )

    gpt2_lm.fit(train_ds, epochs=num_epochs, callbacks=[
        checkpoint_callback,
        tensorboard_callback,
    ],
                )
```


### Relevant log output

```shell
2024-09-21 00:24:25.840848: W tensorflow/compiler/tf2xla/kernels/random_ops.cc:57] Warning: Using tf.random.uniform with XLA compilation will ignore seeds; consider using tf.random.stateless_uniform instead if reproducible behavior is desired. gpt2_causal_lm/gpt2_backbone/embeddings_dropout/dropout/random_uniform/RandomUniform
2024-09-21 00:24:25.846135: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2024-09-21 00:24:26.559075: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert
2024-09-21 00:24:26.572007: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert
2024-09-21 00:24:26.573183: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert
2024-09-21 00:24:26.581128: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert
2024-09-21 00:24:26.581197: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert
2024-09-21 00:24:26.588190: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert
12024-09-21 00:25:01.986918: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'
ptxas warning : Registers are spilled to local memory in function 'fusion_1225'
ptxas warning : Registers are spilled to local memory in function 'fusion_1111'
ptxas warning : Registers are spilled to local memory in function 'fusion_1220'
ptxas warning : Registers are spilled to local memory in function 'fusion_1124'
ptxas warning : Registers are spilled to local memory in function 'fusion_1175'
ptxas warning : Registers are spilled to local memory in function 'fusion_1128'
ptxas warning : Registers are spilled to local memory in function 'fusion_1006'
ptxas warning : Registers are spilled to local memory in function 'fusion_1015'

2024-09-21 00:25:02.215951: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2024-09-21 00:25:02.420703: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'
ptxas warning : Registers are spilled to local memory in function 'fusion_1225'
ptxas warning : Registers are spilled to local memory in function 'fusion_1111'
ptxas warning : Registers are spilled to local memory in function 'fusion_1220'
ptxas warning : Registers are spilled to local memory in function 'fusion_1124'
ptxas warning : Registers are spilled to local memory in function 'fusion_1175'
ptxas warning : Registers are spilled to local memory in function 'fusion_1128'
ptxas warning : Registers are spilled to local memory in function 'fusion_1006'
ptxas warning : Registers are spilled to local memory in function 'fusion_1015'

2024-09-21 00:25:02.542130: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'
ptxas warning : Registers are spilled to local memory in function 'fusion_1225'
ptxas warning : Registers are spilled to local memory in function 'fusion_1111'
ptxas warning : Registers are spilled to local memory in function 'fusion_1220'
ptxas warning : Registers are spilled to local memory in function 'fusion_1124'
ptxas warning : Registers are spilled to local memory in function 'fusion_1175'
ptxas warning : Registers are spilled to local memory in function 'fusion_1128'
ptxas warning : Registers are spilled to local memory in function 'fusion_1006'
ptxas warning : Registers are spilled to local memory in function 'fusion_1015'

2024-09-21 00:25:03.595774: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'
ptxas warning : Registers are spilled to local memory in function 'fusion_1225'
ptxas warning : Registers are spilled to local memory in function 'fusion_1111'
ptxas warning : Registers are spilled to local memory in function 'fusion_1220'
ptxas warning : Registers are spilled to local memory in function 'fusion_1124'
ptxas warning : Registers are spilled to local memory in function 'fusion_1175'
ptxas warning : Registers are spilled to local memory in function 'fusion_1128'
ptxas warning : Registers are spilled to local memory in function 'fusion_1006'
ptxas warning : Registers are spilled to local memory in function 'fusion_1015'

12024-09-21 00:25:04.071526: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'
ptxas warning : Registers are spilled to local memory in function 'fusion_1225'
ptxas warning : Registers are spilled to local memory in function 'fusion_1111'
ptxas warning : Registers are spilled to local memory in function 'fusion_1220'
ptxas warning : Registers are spilled to local memory in function 'fusion_1124'
ptxas warning : Registers are spilled to local memory in function 'fusion_1175'
ptxas warning : Registers are spilled to local memory in function 'fusion_1128'
ptxas warning : Registers are spilled to local memory in function 'fusion_1006'
ptxas warning : Registers are spilled to local memory in function 'fusion_1015'

2024-09-21 00:25:04.130504: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'
ptxas warning : Registers are spilled to local memory in function 'fusion_1225'
ptxas warning : Registers are spilled to local memory in function 'fusion_1111'
ptxas warning : Registers are spilled to local memory in function 'fusion_1220'
ptxas warning : Registers are spilled to local memory in function 'fusion_1124'
ptxas warning : Registers are spilled to local memory in function 'fusion_1175'
ptxas warning : Registers are spilled to local memory in function 'fusion_1128'
ptxas warning : Registers are spilled to local memory in function 'fusion_1006'
ptxas warning : Registers are spilled to local memory in function 'fusion_1015'

2024-09-21 00:25:05.333297: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
5054/8663 [================>.............] - ETA: 9:14 - loss: 11.8715 - accuracy: 2.2702terminate called after throwing an instance of 'std::system_error'
terminate called recursively
  what():  Resource temporarily unavailable
Aborted (core dumped)
```
",2
"`tf.slice` triggers XLA recompilation on each call despite static shape, while `xla.dynamic_slice` does not","### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.18.0-dev20240919

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When using `tf.function` with `jit_compile=True`, I've observed that `tf.slice` leads to XLA recompilation at each function call, even when the tensor shape are static. Explicitly using `tensorflow.compiler.tf2xla.python.xla.dynamic_slice` instead resolves this issue and prevents recompilation.

Expected behavior: `tf.slice` should not trigger XLA recompilation when shapes are static, similar to `xla.dynamic_slice`.

Actual behavior: `tf.slice` causes XLA recompilation on each call leading to a 100x slower performance.

Question: Shouldn't `tf.slice` be automatically converted to `xla.dynamic_slice` or an equivalent XLA operation to avoid unnecessary recompilation?

### Standalone code to reproduce the issue

```shell
import time

import tensorflow as tf
import tensorflow.compiler.tf2xla.python.xla as xla

@tf.function(jit_compile=True)
def func_xla_slice(n):
  range = tf.range(100)
  slice = xla.dynamic_slice(range, [n], [50])
  return slice

@tf.function(jit_compile=True)
def func_tf_slice(n):
  range = tf.range(100)
  slice = tf.slice(range, [n], [50])
  return slice

if __name__ == '__main__':
  print(tf.version.VERSION)

  # tracing calls
  func_xla_slice(tf.constant(0))
  func_tf_slice(tf.constant(0))

  start = time.time()
  for i in range(1, 50):
    func_xla_slice(tf.constant(i))
  print(f""XLA slice took {(time.time() - start) * 1000:.0f} ms"")

  start = time.time()
  for i in range(1, 50):
    func_tf_slice(tf.constant(i))
  print(f""TF slice took {(time.time() - start) * 1000:.0f} ms"")
```


### Relevant log output

```shell
XLA slice took 19 ms
TF slice took 2016 ms
```
",1
"tf.python.ops.array_ops.transpose aborts with ""Check failed: d >= 0 (0 vs. -1)""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly 2.18.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `aborted issue` in TensorFlow when I used API `array_ops.transpose`

### Standalone code to reproduce the issue

```shell
import numpy as np
from tensorflow.python.ops import array_ops

x =np.arange(0, 8).reshape([2, 4]).astype(np.float32)
y = np.array([-1, 0]).astype(np.int32)
array_ops.transpose(x, y,conjugate = False)
```


### Relevant log output

```shell
2024-09-19 16:16:30.137164: F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)
```
",1
Code error when feature name has multiple `_`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.17.0

### Custom code

Yes

### OS platform and distribution

5.15.149-99.162.amzn2.x86_64

### Mobile device

_No response_

### Python version

Python 3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Expect there shouldn't be errors just by changing feature name. 

### Standalone code to reproduce the issue
This is a code sample that will work normally

```
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Concatenate
from tensorflow.keras import Model

import pandas as pd
import numpy as np

df = pd.DataFrame()
numeric_feature_name = 'a' * 27
categorical_feature_name = 'b' * 11
df[numeric_feature_name] = range(1000)
df[categorical_feature_name] = 'a'
df['label'] = 1

numeric_feature_layer = tf.keras.Input(shape=(1,), name=numeric_feature_name, dtype='float32')
categorical_feature_layer = tf.keras.Input(shape=(1,), name=categorical_feature_name, dtype=""string"")
encoding_layer = get_category_encoding_layer(vocab=['a'])
encoded_categorical_feature = encoding_layer(categorical_feature_layer)

all_inputs = [numeric_feature_layer, categorical_feature_layer]
encoded_features = [numeric_feature_layer, encoded_categorical_feature]
concat_features = Concatenate()(encoded_features)
output = Dense(units=1, activation='sigmoid')(concat_features)
model = Model(inputs=all_inputs, outputs=output)

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

dataframe_x = df[[numeric_feature_name, categorical_feature_name]]
dataframe_y = df['label']
df2 = ((dict(dataframe_x), dataframe_y))
ds = tf.data.Dataset.from_tensor_slices(df2)
ds = ds.batch(32)
ds_train = ds

model.fit(
    ds_train,
    epochs=10,
    batch_size=300,
    verbose=1
)
```

However, if I change the feature name, the same code will throw error

```
df = pd.DataFrame()
## Just change the feature name here
numeric_feature_name = 'a_b_c_d_e_f_g' 
categorical_feature_name = 'a_b_c_d_e_f'
df[numeric_feature_name] = range(1000)
df[categorical_feature_name] = 'a'
df['label'] = 1

numeric_feature_layer = tf.keras.Input(shape=(1,), name=numeric_feature_name, dtype='float32')
categorical_feature_layer = tf.keras.Input(shape=(1,), name=categorical_feature_name, dtype=""string"")
encoding_layer = get_category_encoding_layer(vocab=['a'])
encoded_categorical_feature = encoding_layer(categorical_feature_layer)

all_inputs = [numeric_feature_layer, categorical_feature_layer]
encoded_features = [numeric_feature_layer, encoded_categorical_feature]
concat_features = Concatenate()(encoded_features)
output = Dense(units=1, activation='sigmoid')(concat_features)
model = Model(inputs=all_inputs, outputs=output)

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

dataframe_x = df[[numeric_feature_name, categorical_feature_name]]
dataframe_y = df['label']
df2 = ((dict(dataframe_x), dataframe_y))
ds = tf.data.Dataset.from_tensor_slices(df2)
ds = ds.batch(32)
ds_train = ds

model.fit(
    ds_train,
    epochs=10,
    batch_size=300,
    verbose=1
)
```

We have tested that this error is on 2.17.0 and if we are using 2.15 tensorflow, both codes will run smoothly.
```


### Relevant log output

```shell
Epoch 1/10
2024-09-18 05:28:05.240962: W tensorflow/core/framework/op_kernel.cc:1817] OP_REQUIRES failed at cast_op.cc:122 : UNIMPLEMENTED: Cast string to float is not supported
---------------------------------------------------------------------------
UnimplementedError                        Traceback (most recent call last)
Cell In[14], line 8
      5 ds = ds.batch(32)
      6 ds_train = ds
----> 8 model.fit(
      9     ds_train,
     10     epochs=10,
     11     batch_size=300,
     12     verbose=1
     13 )

File ~/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--> 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

File ~/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51 try:
     52   ctx.ensure_initialized()
---> 53   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                       inputs, attrs, num_outputs)
     55 except core._NotOkStatusException as e:
     56   if name is not None:

UnimplementedError: Graph execution error:

Detected at node functional_5_1/Cast defined at (most recent call last):
  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/runpy.py"", line 196, in _run_module_as_main

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/runpy.py"", line 86, in _run_code

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel_launcher.py"", line 18, in <module>

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/traitlets/config/application.py"", line 1075, in launch_instance

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py"", line 739, in start

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py"", line 205, in start

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/asyncio/events.py"", line 80, in _run

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 545, in dispatch_queue

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 534, in process_one

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 437, in dispatch_shell

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py"", line 362, in execute_request

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 778, in execute_request

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py"", line 449, in do_execute

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py"", line 549, in run_cell

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3075, in run_cell

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3130, in _run_cell

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py"", line 128, in _pseudo_sync_runner

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3334, in run_cell_async

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3517, in run_ast_nodes

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3577, in run_code

  File ""/tmp/ipykernel_37779/4021243845.py"", line 8, in <module>

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py"", line 320, in fit

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py"", line 121, in one_step_on_iterator

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py"", line 108, in one_step_on_data

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py"", line 51, in train_step

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/layers/layer.py"", line 901, in __call__

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/ops/operation.py"", line 46, in __call__

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/models/functional.py"", line 167, in call

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/models/functional.py"", line 258, in _standardize_inputs

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/models/functional.py"", line 218, in _convert_inputs_to_tensors

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/ops/core.py"", line 822, in convert_to_tensor

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/core.py"", line 132, in convert_to_tensor

Cast string to float is not supported
	 [[{{node functional_5_1/Cast}}]] [Op:__inference_one_step_on_iterator_6125]
```
",1
`resource_create_op` operation can cause TensorFlow to crash.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 A `Segmentation fault`  could be raised in TensorFlow when using `test_ops.resource_create_op` . The code is as follows:

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.framework import test_ops
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import array_ops_stack


sess = tf.compat.v1.Session()

@tf.function
def func():
    r1 = test_ops.stub_resource_handle_op(container='a', shared_name='b')
    r2 = test_ops.stub_resource_handle_op(container='a', shared_name='c')
    c = array_ops_stack.stack([r1, r2])
    s = array_ops.strided_slice(c, [1], [2 ** 32])
    with sess.as_default():
        test_ops.resource_create_op(s)

func()
```


### Relevant log output

```shell
> Segmentation fault (core dumped)

The above code is confirmed to crash on `tf-nightly 2.18.0.dev20240817` (nightly-build)
```
",2
Encountering a `Segmentation fault` when using `data_flow_ops.FIFOQueue` in TensorFlow,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 A `Segmentation fault`  could be raised in TensorFlow when using `data_flow_ops.FIFOQueue` . The following code is confirmed to crash on `tf-nightly 2.18.0.dev20240817` (nightly-build)

### Standalone code to reproduce the issue

```shell
from tensorflow.python.framework import dtypes as dtypes_lib
from tensorflow.python.ops import data_flow_ops
import tensorflow as tf

q = data_flow_ops.FIFOQueue(10, dtypes_lib.float32, ())
elems = [10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0]
tmp_var64 = elems[4:8]

sess = tf.compat.v1.Session()
with sess.as_default():
    q.dequeue_up_to([])
```


### Relevant log output

```shell
> Segmentation fault (core dumped)
```
",2
`tf_cond.cond` and `tf.function` could cause an aborted issue,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 An `aborted issue`  could be raised in TensorFlow when using `tf_cond.cond` and `tf.function`. The code is as follows:

### Standalone code to reproduce the issue

```shell
from tensorflow.python.framework import constant_op
from tensorflow.python.framework import ops
from tensorflow.python.ops import cond as tf_cond
from tensorflow.python.ops import control_flow_assert
from tensorflow.python.ops import control_flow_ops
from tensorflow.python.platform import test

import tensorflow as tf
sess = tf.compat.v1.Session()

@tf.function
def func1():
    with sess.as_default():
        with ops.device(test.gpu_device_name()):
            pred = constant_op.constant([True, False])

        def fn1():
            return control_flow_ops.no_op()

        def fn2():
            with ops.device('/cpu:0'):
                return control_flow_assert.Assert(False, ['Wrong!'])

        r = tf_cond.cond(pred, fn1, fn2)

func1()
```


### Relevant log output

```shell
> 2024-09-12 16:43:14.634438: F tensorflow/core/framework/tensor.cc:852] Check failed: 1 == NumElements() (1 vs. 2)Must have a one element tensor
> Aborted (core dumped)
```
",1
Using `fft_ops`  would cause an `aborted issue`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 An `aborted issue`  could be raised in TensorFlow when using `fft_ops`. The code is as follows:

### Standalone code to reproduce the issue

```shell
import numpy as np
from tensorflow.python.ops.signal import fft_ops
import tensorflow as tf
tf.compat.v1.disable_eager_execution()


def _tf_ifft(x, rank, fft_length=None, feed_dict=None):
    with tf.compat.v1.Session() as sess:
        return sess.run(_tf_ifft_for_rank(rank)(x, fft_length), feed_dict=feed_dict)

def _tf_ifft_for_rank(rank):
    if rank == 1:
        return fft_ops.irfft
    elif rank == 2:
        return fft_ops.irfft2d
    elif rank == 3:
        return fft_ops.irfft3d
    else:
        raise ValueError('invalid rank')


rank = 1
extra_dims = 0
np_rtype = np.float32
np_ctype = np.complex64
dims = rank + extra_dims
x = np.zeros((1,) * dims).astype(np_ctype)
tmp_var22 = _tf_ifft(x, rank).shape
```


### Relevant log output

```shell
DUCC FFT c2r failed:
bazel-out/k8-opt/bin/external/ducc/_virtual_includes/fft/ducc/src/ducc0/fft/fft1d_impl.h: 2948 (static Trpass<Tfs> ducc0::detail_fft::rfftpass<float>::make_pass(size_t, size_t, size_t, const Troots<Tfs> &, bool) [Tfs = float]):

Assertion failure
no zero-sized FFTs

Aborted (core dumped)
```
",1
Using `gen_random_index_shuffle_ops.random_index_shuffle` with `rounds=-2` can cause a crash,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 A `Segmentation fault`  could be raised in TensorFlow when I used API `gen_random_index_shuffle_ops.random_index_shuffle` with `rounds=-2`. The code is as follows:

### Standalone code to reproduce the issue

```shell
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import gen_random_index_shuffle_ops
from tensorflow.python.ops import math_ops

seed = (74, 117)
seed_dtype = dtypes.int32
max_index = 129
index_dtype = dtypes.int32
rounds = 4

seen = (max_index + 1) * [False]
seed = math_ops.cast([seed[0], seed[1], 42], seed_dtype)
for index in range(max_index + 1):
    new_index = gen_random_index_shuffle_ops.random_index_shuffle(math_ops.cast(index, index_dtype), seed, max_index=math_ops.cast(max_index, index_dtype), rounds=rounds)
    # rounds = -2 causes the segmentfault
    new_index = gen_random_index_shuffle_ops.random_index_shuffle(math_ops.cast(index, index_dtype), seed, max_index=math_ops.cast(max_index, index_dtype), rounds=-2)
```


### Relevant log output

```shell
> Segmentation fault (core dumped)
```
",2
Got one aborted issue when using `data_flow_ops.MapStagingArea` ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 An `aborted issue`  could be raised in TensorFlow when I used API `data_flow_ops.MapStagingArea`. The code is as follows:

### Standalone code to reproduce the issue

```shell
from tensorflow.python.framework import dtypes
from tensorflow.python.framework import ops
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import data_flow_ops
from tensorflow.python.ops import math_ops
from tensorflow.python.platform import test
import tensorflow as tf


with ops.Graph().as_default() as g:
    with ops.device('/cpu:0'):
        x = array_ops.placeholder(dtypes.float32)
        pi = array_ops.placeholder(dtypes.int64)
        gi = array_ops.placeholder(dtypes.int64)
        v = 2.0 * (array_ops.zeros([]) + x)
    with ops.device(test.gpu_device_name()):
        stager = data_flow_ops.MapStagingArea([dtypes.float32])
        stage = stager.put(pi, [v], [0])
        k, y = stager.get([])
        y = math_ops.reduce_max(math_ops.matmul(y, y))
g.finalize()
with tf.compat.v1.Session(graph=g) as sess:
    sess.run(stage, feed_dict={x: -1, pi: 0})
    for i in range(10):
        _, yval = sess.run([stage, y], feed_dict={x: i, pi: i + 1, gi: i})
```


### Relevant log output

```shell
2024-09-12 15:57:44.445189: F tensorflow/core/framework/tensor.cc:852] Check failed: 1 == NumElements() (1 vs. 0)Must have a one element tensor
Aborted (core dumped)
```
",1
An `aborted issue` could be raised in TensorFlow when I used API `math_ops.cast` and `array_ops.split`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 An `aborted issue`  could be raised in TensorFlow when I used API `math_ops.cast` and `array_ops.split` . 

### Standalone code to reproduce the issue

```shell
import numpy as np

from tensorflow.python.framework import dtypes
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import math_ops
import tensorflow as tf

tf.compat.v1.disable_eager_execution()
sess = tf.compat.v1.Session()
with sess.as_default():
    a = math_ops.cast([2], dtypes.int32)
    b = math_ops.cast([1], dtypes.int32)
    value = np.random.rand(11, 11)
    array_ops.split(value, [a, b])
```


### Relevant log output

```shell
2024-09-12 15:49:03.711972: F tensorflow/core/framework/tensor_shape.cc:45] Check failed: NDIMS == dims() (1 vs. 2)Asking for tensor of 1 dimensions from a tensor of 2 dimensions
Aborted (core dumped)
```
",1
How to pack TFRT into wheel? And use it in saved_model_cli.,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.16.1

### Current behavior?

I have been compiled whole TF successfully by commands. And there is a tfrt directory in ./bazel-bin/tensorflow/core.
But after I run build_pip_package, the tfrt package didn't show up at /tensorflow/core/ in wheel.

I want to use TFRT in serving and somewhere else.

### Standalone code to reproduce the issue

```shell
bazel build --config=release_cpu_linux --config=tf_public_cache --build_event_json_file=/tf/pkg/bep.json tensorflow/tools/pip_package:build_pip_package

./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tf/pkg --cpu
```


### Relevant log output

_No response_",0
Wheels have different metadata on different platforms,"Hi! Some resolvers in Python such as poetry and uv try to create lockfiles from the user's requirements that work an any platform. For example, you could create a universal lockfile on linux and use it to install the project on windows.

For this, both poetry and uv read the `METADATA` file of a single wheel on the index (in this case, pypi) and assume its metadata applies to all other platforms, too. For tensorflow, there is currently different metadata for windows and for linux/mac. For windows, the `requires-dist` excluding the cuda packages is:

```
Requires-Dist: tensorflow-macos ==2.15.1 ; platform_system == ""Darwin"" and platform_machine == ""arm64""
Requires-Dist: tensorflow-cpu-aws ==2.15.1 ; platform_system == ""Linux"" and (platform_machine == ""arm64"" or platform_machine == ""aarch64"")
Requires-Dist: tensorflow-intel ==2.15.1 ; platform_system == ""Windows""
```

While for linux and mac it is:

```
Requires-Dist: absl-py (>=1.0.0)
Requires-Dist: astunparse (>=1.6.0)
Requires-Dist: flatbuffers (>=23.5.26)
Requires-Dist: gast (!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1)
Requires-Dist: google-pasta (>=0.1.1)
Requires-Dist: h5py (>=2.9.0)
Requires-Dist: libclang (>=13.0.0)
Requires-Dist: ml-dtypes (~=0.3.1)
Requires-Dist: numpy (<2.0.0,>=1.23.5)
Requires-Dist: opt-einsum (>=2.3.2)
Requires-Dist: packaging
Requires-Dist: protobuf (!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3)
Requires-Dist: setuptools
Requires-Dist: six (>=1.12.0)
Requires-Dist: termcolor (>=1.1.0)
Requires-Dist: typing-extensions (>=3.6.6)
Requires-Dist: wrapt (<1.15,>=1.11.0)
Requires-Dist: tensorflow-io-gcs-filesystem (>=0.23.1)
Requires-Dist: grpcio (<2.0,>=1.24.3)
Requires-Dist: tensorboard (<2.16,>=2.15)
Requires-Dist: tensorflow-estimator (<2.16,>=2.15.0)
Requires-Dist: keras (<2.16,>=2.15.0)
```

That means depending on whether we read a windows wheel or a unix wheel, we get a different lockfile.

Would it be possible for tensorflow to write the same METADATA for all platforms and gate the platform specific entries with `platform_system` markers?

For uv, we've considered reading the METADATA files for all wheels, but this has major drawbacks: We have to make 17 network requests for pypi instead of 1 for each version we try, slowing resolution down. There is also no perfect mapping between environment markers (which usually tell us which dependencies to install on which platform) and wheel tags, so when METADATA can be different between wheels we'd also have to capture this in lockfiles.

I hope I explained good enough why identical METADATA files across all wheels of a version are important for us, I can add more details about how the resolver works if you have more questions.

This is similar to the problem discussed at https://github.com/tensorflow/tensorflow/issues/62346#issuecomment-1798633528.",1
check failed: !PyErr_Occurred() when constructing two uint64 tensors,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When running the following code, tensorflow will directly raise program abort with the error message: `./tensorflow/python/eager/pywrap_tensor_conversion.h:58] Check failed: !PyErr_Occurred()`

```
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['CUDA_VISIBLE_DEVICES'] = ''
import warnings
warnings.filterwarnings(""ignore"")
import tensorflow as tf

lower1 = -1
try:
    lower1 = tf.constant(lower1, dtype='uint64')
except:
    ...
lower2 = -2
lower2 = tf.constant(lower2, dtype='uint64')
```

It seems the problem occurs when TensorFlow tries to construct **two uint64 tensors**. Although it is invalid to convert negative int to unsigned, an exception is more proper as program abort will directly kill the process.

Indeed, only constructing one uint64 tensor will properly raises an OverFlow exception. 
This issue only occurs when repeatedly constructing two uint64 tensors.
Another weird thing is that, **if I change the value of `lower2` to either `-1` or `-3` instead of `-2`**, this issue does not occur.


### Standalone code to reproduce the issue

```shell
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['CUDA_VISIBLE_DEVICES'] = ''
import warnings
warnings.filterwarnings(""ignore"")
import tensorflow as tf

lower1 = -1
try:
    lower1 = tf.constant(lower1, dtype='uint64')
except:
    ...
lower2 = -2
lower2 = tf.constant(lower2, dtype='uint64')
```
```


### Relevant log output

```shell
F ./tensorflow/python/eager/pywrap_tensor_conversion.h:58] Check failed: !PyErr_Occurred() 
Aborted (core dumped)
```
",1
Gradients can't be computed for keras embeddings,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.15.1

### Custom code

Yes

### OS platform and distribution

Windows 11, Ubuntu 22.04LTS

### Mobile device

_No response_

### Python version

3.11.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

We have a problem in shap for quite some while now with embeddings (see [here](https://github.com/shap/shap/issues/3440)). Since we manipulate the graph to adjust the gradient calculation in order to produce shap values we need the layers to be backpropagatable. This does not seem the case for `tensorflow.keras.layers.Embedding` and we do not know a way around this.

(In a previous version there was the possibility to [manipulate](https://github.com/shap/shap/blob/master/shap/explainers/_deep/deep_tf.py#L412-L416) the [`_IsBackpropagatable` function](https://github.com/tensorflow/tensorflow/blob/v1.10.0/tensorflow/python/ops/gradients_impl.py#L293) but this is no longer possible)

In the example below one can see that the gradients just become `None` if the model contains an embedding layer. 
Is there a way around this, so that we can calculate gradients for embeddings again?

### Standalone code to reproduce the issue

```python
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Load the IMDb dataset
max_features = 10000  # Only consider the top 10,000 words
maxlen = 100  # Only consider the first 100 words of each movie review

(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)
X_train = pad_sequences(X_train, maxlen=maxlen)
X_test = pad_sequences(X_test, maxlen=maxlen)

# Build the model
model = models.Sequential()
embedding_layer = layers.Embedding(input_dim=max_features, output_dim=128, input_length=maxlen)
model.add(embedding_layer)
flat_layer = layers.Flatten()
model.add(flat_layer)
dense_layer = layers.Dense(1, activation='sigmoid')
model.add(dense_layer)

# Build the same model except for the embedding layer
new_model = models.Sequential()
new_model.add(flat_layer)
new_model.add(layers.Dense(1, activation=""sigmoid""))

# Forward pass and gradient extraction
@tf.function
def get_gradients_model(inputs):
    inputs = tf.cast(inputs, tf.float32)  # Convert inputs to float32
    with tf.GradientTape() as tape:
        tape.watch(inputs)  # Watch the input tensor to compute gradients w.r.t. it
        predictions = model(inputs)
    
    gradients = tape.gradient(predictions, inputs)
    
    return predictions, gradients

@tf.function
def get_gradients_new_model(inputs):
    inputs = tf.cast(inputs, tf.float32)  # Convert inputs to float32
    with tf.GradientTape() as tape:
        tape.watch(inputs)  # Watch the input tensor to compute gradients w.r.t. it
        predictions = new_model(inputs)
    
    gradients = tape.gradient(predictions, inputs)
    
    return predictions, gradients

# Example usage
sample_input = X_train[:1]  # Select a sample from the training set
sample_label = y_train[:1]  # Corresponding label

predictions, gradients = get_gradients_model(sample_input)
predictions2, gradients2 = get_gradients_new_model(sample_input)
print(""Gradients for model:"", gradients)
print(""Gradients for new_model:"", gradients2)
```


### Relevant log output

Gradients for model: None
Gradients for new_model: tf.Tensor(
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0.]], shape=(1, 100), dtype=float32)",1
Fail import tensorflow if rules_python is installed.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17.0

### Custom code

No

### OS platform and distribution

Ubuntu 24.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When I installed both TensorFlow and Mesop simultaneously, I encountered an error while importing TensorFlow. It seems that installing Mesop also installs `rules_python`, which causes TensorFlow to malfunction.

Specifically, at the following code:
https://github.com/tensorflow/tensorflow/blob/bc90265931a0ca90ee2e7c2ef988ad6634733961/tensorflow/python/platform/resource_loader.py#L117-L119

`r` becomes `None`, leading to an error.

According to `rules_python`:
https://github.com/bazelbuild/rules_python/blob/0.26.0/python/runfiles/runfiles.py#L40

It seems possible that `r = runfiles.Create()` can indeed return `None`.

While it seems the issue could be linked to Mesop installing rules_python, there might also be an underlying problem with TensorFlow. I would appreciate your help in investigating further.

### Standalone code to reproduce the issue

```shell
$ pip install ""tf-nightly==2.18.0.dev20240906"" ""mesop==0.12.3""
$ python -c ""import tensorflow""

# I run this in docker image of python:3.11.
```


### Relevant log output

```shell
2024-09-06 21:45:07.412511: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-09-06 21:45:07.415758: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-09-06 21:45:07.425482: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1725659107.441013      78 cuda_dnn.cc:8322] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1725659107.445948      78 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-06 21:45:07.463219: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/__init__.py"", line 53, in <module>
    from tensorflow._api.v2 import compat
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/_api/v2/compat/__init__.py"", line 8, in <module>
    from tensorflow._api.v2.compat import v1
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/_api/v2/compat/v1/__init__.py"", line 30, in <module>
    from tensorflow._api.v2.compat.v1 import compat
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py"", line 8, in <module>
    from tensorflow._api.v2.compat.v1.compat import v1
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py"", line 47, in <module>
    from tensorflow._api.v2.compat.v1 import lite
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/_api/v2/compat/v1/lite/__init__.py"", line 9, in <module>
    from tensorflow._api.v2.compat.v1.lite import experimental
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/_api/v2/compat/v1/lite/experimental/__init__.py"", line 8, in <module>
    from tensorflow._api.v2.compat.v1.lite.experimental import authoring
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/_api/v2/compat/v1/lite/experimental/authoring/__init__.py"", line 8, in <module>
    from tensorflow.lite.python.authoring.authoring import compatible # line: 263
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/lite/python/authoring/authoring.py"", line 42, in <module>
    from tensorflow.lite.python import convert
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/lite/python/convert.py"", line 151, in <module>
    _deprecated_conversion_binary = _resource_loader.get_path_to_datafile(
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/python/platform/resource_loader.py"", line 118, in get_path_to_datafile
    new_fpath = r.Rlocation(
                ^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'Rlocation'
```
",1
"Calibrator segfaults trying to log the ""while"" operation","### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 22.04
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.17.0

### 2. Code

[reproducer.zip](https://github.com/user-attachments/files/16894947/reproducer.zip)

### 3. Failure after conversion

Segmentation fault (signal 11) during conversion

### 5. (optional) Any other info / logs

The ""while"" operation does a check if an output tensor of the body subgraph is the same as the corresponding input tensor. If it's the case, it deallocates its own output tensor. The check is done at the prepare stage, so the affected tensor is already included in the ""loggable_outputs"" list by the calibrator. Then the calibrator tries to read the data from the deallocated tensor and segfaults. I've debugged it up to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/calibration/calibrator.cc#L267 and found that the `tensor.data.f == nullptr`.
The check in question was introduced between 2.13 and 2.14, so it might be considered a regression: https://github.com/tensorflow/tensorflow/commit/7d49fd431ee5cebbb76eda88bc17e48921e10c85
",0
`np.cumprod` on ndarray with type `tensorflow.python.framework.dtypes.bfloat16.as_numpy_dtype` can cause `segmentfault,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240828

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `segmentfault issue` in TensorFlow when I used API `np.cumprod` on ndarray with type `tensorflow.python.framework.dtypes.bfloat16.as_numpy_dtype` .  I have confirmed that the code would crash on `tf-nightly-2.18.0.dev20240817` and `tf-nightly-2.18.0.dev20240828` (nightly-build)

### Standalone code to reproduce the issue

```shell
import numpy as np
from tensorflow.python.framework import dtypes


def numpy_reverse(x, axis):
    length = len(x.shape)
    if axis < 0:
        axis = length + axis
    ix = [slice(None, None, -1) if i == axis else slice(None) for i in range(length)]
    return x[tuple(ix)]


axis = 0
x = np.zeros([595]).astype(dtypes.bfloat16.as_numpy_dtype) # crash
# x = np.zeros([595]).astype(float) # works well
length = len(x.shape)
x = numpy_reverse(x, axis=0)
ix_head = [slice(0, 1) if i == axis else slice(None) for i in range(length)]
ix_init = [slice(0, -1) if i == axis else slice(None) for i in range(length)]
init = np.ones_like(x[tuple(ix_head)])
np_out = np.concatenate([init, np.cumprod(x[tuple(ix_init)], axis)], axis=axis)
```


### Relevant log output

```shell
Segmentation fault (core dumped)
```
",2
tf.raw_ops.FakeQuantWithMinMaxVarsPerChannel raises a program abort,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When setting the `num_bits` in a large integer, this API raises the program abort.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
inputs = tf.constant(0.57681304)
min = tf.constant(2.1311088)
max = tf.constant(2.4402196)
num_bits = 10
narrow_range = False
tf.raw_ops.FakeQuantWithMinMaxVarsPerChannel(inputs=inputs,min=min,max=max,num_bits=num_bits,narrow_range=narrow_range)
```
```


### Relevant log output

```shell
F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)
```
",1
tensorflow.python.ops.gen_math_ops.sparse_bincount can cause a crash,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered a `Segmentation fault ` issue in TensorFlow when using the `gen_math_ops.sparse_bincount` API. I have confirmed that the code would crash on `tf-nightly 2.18.0.dev20240817` (nightly-build)

### Standalone code to reproduce the issue

```shell
from tensorflow.python.ops import gen_math_ops

values = [0, 1, 2, 2]
binary = False
indices = [[], [], [990, 2], [2, 349]]
dense_shape = []
gen_math_ops.sparse_bincount(indices=indices, values=values, dense_shape=dense_shape, size=3, weights=[],binary_output=binary)
```


### Relevant log output

```shell
Segmentation fault (core dumped)
```
",2
tensorflow.python.ops.state_ops.scatter_nd_update can cause a crash,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered a `Segmentation fault` in TensorFlow when I used API `state_ops.scatter_nd_update`  with empty indices.
I have confirmed that the code would crash on `tf-nightly 2.18.0.dev20240817` (nightly-build)

### Standalone code to reproduce the issue

```shell
import numpy as np

from tensorflow.python.framework import constant_op
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import resource_variable_ops
from tensorflow.python.ops import state_ops


def testSimpleResource():
    indices = constant_op.constant([], dtype=dtypes.int32)
    for dtype in (dtypes.int32, dtypes.bfloat16):
        updates = constant_op.constant([], dtype=dtype)
        ref = resource_variable_ops.ResourceVariable((0, 0, 0, 0, 0, 0, 0, 0), dtype=dtype)
        scatter = state_ops.scatter_nd_update(ref, indices, updates)

testSimpleResource()
```


### Relevant log output

```shell
> Segmentation fault (core dumped)
```
",2
 Aborted (core dumped): Check failed: d < dims() (1 vs. 1),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `aborted issue` in TensorFlow when I used API `array_ops.scatter_nd` . The code is as follows:

```python
import numpy as np
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import array_ops

GRADIENT_TESTS_DTYPES = (dtypes.bfloat16, dtypes.float16, dtypes.float32, dtypes.float64)

def scatter_nd(indices, updates, shape):
    return array_ops.scatter_nd(indices, updates, shape)

def testExtraIndicesDimensions():
    indices = array_ops.zeros((1, 1, 2), dtypes.int32)
    updates = array_ops.zeros([1], dtypes.int32)
    shape = np.array((2, 2))
    scatter = scatter_nd(indices, updates, shape)

testExtraIndicesDimensions()
```

> 2024-08-31 12:17:41.010855: F tensorflow/core/framework/tensor_shape.cc:357] Check failed: d < dims() (1 vs. 1)
> Aborted (core dumped)

### Standalone code to reproduce the issue

```shell
import numpy as np
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import array_ops

GRADIENT_TESTS_DTYPES = (dtypes.bfloat16, dtypes.float16, dtypes.float32, dtypes.float64)

def scatter_nd(indices, updates, shape):
    return array_ops.scatter_nd(indices, updates, shape)

def testExtraIndicesDimensions():
    indices = array_ops.zeros((1, 1, 2), dtypes.int32)
    updates = array_ops.zeros([1], dtypes.int32)
    shape = np.array((2, 2))
    scatter = scatter_nd(indices, updates, shape)

testExtraIndicesDimensions()
```


### Relevant log output

```shell
2024-08-31 12:17:41.010855: F tensorflow/core/framework/tensor_shape.cc:357] Check failed: d < dims() (1 vs. 1)
Aborted (core dumped)
```
I have confirmed that above code would crash on `tf-nightly 2.18.0.dev20240817` (nightly-build)",2
tf.math.floordiv produces incorrect result when the denominator is `-inf`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Based on the documentation https://www.tensorflow.org/api_docs/python/tf/math/floordiv, `tf.math.floordiv` should be equivalent to python's `//` operator. However, when the `x=1.4` and `y=-np.inf`, `tf.math.floordiv` outputs `-0.0` while `//` outputs `-1.0`.

I also checked Numpy and PyTorch's APIs, both output `-1.0`. 

It seems that the implementation of `tf.math.floordiv` is different from others, it would be nice if you can fix the implementation inconsistency, or make this inconsistency in the documentation.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import torch
import numpy as np
a = 1.4
b = -np.inf
print(""Numpy's result: "", np.floor_divide(a, b))
print(""Python's // result: "", a // b)
print(f""TF's result: {tf.math.floordiv(a, b)}"")
print(f""PyTorch's result: {torch.floor_divide(torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32))}"")
```
```


### Relevant log output

```shell
Numpy's result:  -1.0
Python's // result:  -1.0
TF's result: -0.0
PyTorch's result: -1.0
```
",1
[TFLite] Could log level control be added to the TFLite C API?,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

[`tflite::LoggerOptions::SetMinimumLogSeverity()`](https://github.com/tensorflow/tensorflow/blob/daa9e4e0d0a04626fb97cf9b11c1ae6be46c6517/tensorflow/lite/logger.h#L37) provides a method for controlling the log level in C++.
This allows more detailed logging than INFO or limits logging to ERROR or higher in prod builds.

However, we cannot use this feature from the C API. 
I think this enhancement for the C API is useful and good for future additions on other platforms, such as Swift.
Is it possible to add bindings for the `tflite::LoggerOptions` class to the C API ?

I am sorry if I missed this features of the C API.
I will be happy to help you, for example, try to create a PR.

### Standalone code to reproduce the issue

```shell
n/a
```


### Relevant log output

_No response_",1
tf.raw_ops.Round outputs zeros for any integer tensor,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Expects the same tensor as the input according to the specification https://www.tensorflow.org/api_docs/python/tf/raw_ops/Round

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
x = tf.constant([-2, -1, 1, 2, 3])
tf.raw_ops.Round(x=x)
```


### Relevant log output

```shell
>>> import tensorflow as tf
>>> x = tf.constant([-2, -1, 1, 2, 3], dtype=tf.int32)
>>> tf.raw_ops.Round(x=x)
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 0, 0, 0, 0])>
```
",1
"With the same input and parameter settings, there is a large difference in the output of Dense layer on GPU and CPU.","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.12.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

We found that when training the model, there is a large difference in the output of the DENSE layer on the cpu and gpu when using the same input tensor and parameter settings.
```
CPU output: [[3.4838054e+34]]
GPU output: [[3.4838057e+34]]
2.4758800785707605e+27
```
We have tried some similar inputs, but no such problems have occurred.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
import pickle
import h5py

tf.random.set_seed(42)
tf.config.experimental.enable_op_determinism()

def chebyshev_distance(A: np.ndarray, B: np.ndarray):
    if A is None or B is None:
        return 0.0
    if A.shape != B.shape:
        return 9999999
    else:
        return float(np.max(np.abs(A - B)))


tf.random.set_seed(42)

x_input = np.array([[37.63115]])
# x_input = np.array([[38.63115]])
# x_input = np.array([[3.763115]])
print(x_input)

dense_layer = tf.keras.layers.Dense(units=1, activation='exponential', use_bias=True, activity_regularizer=None, bias_constraint=None, bias_initializer='random_normal', kernel_initializer='he_normal', bias_regularizer=None, kernel_regularizer=None, kernel_constraint=None)

weights = [np.array([[2.112561]], dtype=np.float32), np.array([0.03791478], dtype=np.float32)]

dense_layer.build((1,))  
dense_layer.set_weights(weights)

with tf.device('/CPU:0'):
    x_cpu = tf.constant(x_input, dtype=tf.float32)
    output_cpu = dense_layer(x_cpu)
    print(""CPU output:"", output_cpu.numpy())


if tf.config.list_physical_devices('GPU'):
    with tf.device('/GPU:0'):
        x_gpu = tf.constant(x_input, dtype=tf.float32)
        output_gpu = dense_layer(x_gpu)
        print(""GPU output:"", output_gpu.numpy())
else:
    print(""GPU not available."")
    
output_diff = chebyshev_distance(output_cpu.numpy(), output_gpu.numpy())
print(output_diff)
```


### Relevant log output

_No response_",1
"Error while loading Tensorflow plugins - cuFFT, cuDNN, cuBLAS","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.17

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

```Tensorflow==2.16.1``` does not produce the following log output errors. I tried to read the source for ```tensorflow==2.17.0 and 2.16.1``` to at least try find out what might be the issue. The following is what I found out:

The executed piece of code for registering the plugin ```cuFFT``` is located at the file: ```tensorflow-2.16.1/third_party/xla/xla/stream_executor/cuda/cuda_fft.c``` (you can change the tensorflow version # appropriately) and reproduced below:

```c++
void initialize_cufft() {
  absl::Status status =
      PluginRegistry::Instance()->RegisterFactory<PluginRegistry::FftFactory>(
          cuda::kCudaPlatformId, ""cuFFT"",
          [](internal::StreamExecutorInterface *parent) -> fft::FftSupport * {
            gpu::GpuExecutor *cuda_executor =
                dynamic_cast<gpu::GpuExecutor *>(parent);
            if (cuda_executor == nullptr) {
              LOG(ERROR) << ""Attempting to initialize an instance of the cuFFT ""
                         << ""support library with a non-CUDA StreamExecutor"";
              return nullptr;
            }

            return new gpu::CUDAFft(cuda_executor);
          });
  if (!status.ok()) {
    LOG(ERROR) << ""Unable to register cuFFT factory: "" << status.message();
  }
}
```
This function should be responsible for creating the ```PluginRegistry``` object defined in the file: ```tensorflow-2.16.1/third_party/xla/xla/stream_executor/plugin_registry.h```. This object has a very important comment, reproduced below:

```
//The PluginRegistry is a singleton that maintains the set of registered
// ""support library"" plugins. Currently, there are four kinds of plugins:
// BLAS, DNN, and FFT. Each interface is defined in the corresponding
// gpu_{kind}.h header.

// Registers the specified factory with the specified platform.
 // Returns a non-successful status if the factory has already been registered
 // with that platform (but execution should be otherwise unaffected).
```
The class should be a ```Singleton```, and even if it has been registered once an attempt to register it again will fail but tensorflow should work as expected.

And below is the function responsible for the registration, from the file: ```tensorflow-2.16.1/third_party/xla/xla/stream_executor/plugin_registry.cc```:

```c++

template <typename FACTORY_TYPE>
absl::Status PluginRegistry::RegisterFactoryInternal(
    const std::string& plugin_name, FACTORY_TYPE factory,
    std::optional<FACTORY_TYPE>* factories) {
  absl::MutexLock lock{&GetPluginRegistryMutex()};

  if (factories->has_value()) {
    return absl::AlreadyExistsError(
        absl::StrFormat(""Attempting to register factory for plugin %s when ""
                        ""one has already been registered"",
                        plugin_name));
  }

  (*factories) = factory;
  return absl::OkStatus();
}
```
I am not entirely sure as to when and where the very first object of ```cuFFT PluginRegistery``` is created for tensorflow to display this error. I believe there has to be a point from running ```import tensorflow``` and calling the above function ```initialize_cufft``` where the ```PluginRegistry``` object is created and since it must be a ```Singleton```, hence the error. I hope someone can elaborate further on this, or provide better clarity.

### Standalone code to reproduce the issue

```shell
python -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""
```


### Relevant log output

```shell
2024-08-26 16:31:19.008920: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-26 16:31:19.027228: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-26 16:31:19.032798: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-26 16:31:19.047347: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-26 16:31:20.104940: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1724682680.723847    4894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1724682680.805189    4894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1724682680.805836    4894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-08-26 16:31:20.806043: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2432] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 5.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
```
",1
Using --config=cuda_clang ignores compute capabilities and TensorRT values set by the configuration script,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.0

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When running the configuration script, using Clang for CUDA (`--config=cuda_clang`) is an option. When this is enabled, however, the compute capability and TensorRT settings (which set `TF_CUDA_COMPUTE_CAPABILITIES` and `TF_NEED_TENSORRT` respectively) are ignored, as `--config=cuda_clang` [sets these unconditionally](https://github.com/tensorflow/tensorflow/blob/v2.17.0/.bazelrc#L229-L243).

### Standalone code to reproduce the issue

```shell
$ TF_NEED_CUDA=1 TF_CUDA_CLANG=1 TF_CUDA_COMPUTE_CAPABILITIES=sm_87 TF_NEED_TENSORRT=0 ./configure

# The following does not respect the given compute capability and TensorRT options
$ bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow

# As a workaround, the variables can be set manually in the build command
$ bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --repo_env=TF_CUDA_COMPUTE_CAPABILITIES=... --repo_env=TF_NEED_TENSORRT=...
```


### Relevant log output

_No response_",1
`tf.keras.Sequential` not a `tf.Module` since 2.16?,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.16

### Custom code

Yes

### OS platform and distribution

google colab

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I noticed that my custom module `.variables` attribute doesn't work properly in the newest versions of TF (since 2.16). That is, it doesn't contain the parameters of a `tf.keras.Sequential` model that is assigned to an attribute of my model in `__init__` (see MRE below).

Is this an intended change? What's the proper way to use a `tf.keras.Model` inside a `tf.Module` now?

### Standalone code to reproduce the issue

```python
import tensorflow as tf

print(f""{tf.__version__=}"")

class X(tf.Module):
    def __init__(self):
        super().__init__()
        self.xxx = tf.keras.Sequential([tf.keras.layers.Dense(4)])
    def __call__(self, X):
        tf.print(self, ""called"")
        return self.xxx(X)

x=X()
x(tf.random.normal(shape=(2, 3)))
print(f""{x.variables=}"")
print(f""{list(x.submodules)=}"")
print(f""{isinstance(x.xxx, tf.Module)=}"")
```

Output in tf 2.15.1:
```
tf.__version__='2.15.1'
<__main__.X object at 0x7cde2c935db0> called
x.variables=(<tf.Variable 'dense/kernel:0' shape=(3, 4) dtype=float32, numpy=
array([[-0.28077292,  0.2326845 ,  0.7107136 , -0.17309707],
       [ 0.5492227 , -0.37808335,  0.27558255, -0.5152782 ],
       [ 0.69788885, -0.5949336 ,  0.63280034,  0.08288908]],
      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>)
list(x.submodules)=[<keras.src.engine.sequential.Sequential object at 0x7cdd9f3fb460>, <keras.src.engine.input_layer.InputLayer object at 0x7cdd9eac41f0>, <keras.src.layers.core.dense.Dense object at 0x7cdda7323910>]
isinstance(x.xxx, tf.Module)=True
```

Output in tf 2.16.1:
```
tf.__version__='2.16.1'
<__main__.X object at 0x7e7297de1f00> called
x.variables=()
list(x.submodules)=[]
isinstance(x.xxx, tf.Module)=False
```

Output in tf 2.17.0:
```
tf.__version__='2.17.0'
<__main__.X object at 0x783c2818d690> called
x.variables=()
list(x.submodules)=[]
isinstance(x.xxx, tf.Module)=False
```



### Relevant log output

_No response_",1
Proposal for Enhancing Neural Network Training: Unified Gradient Direction for Faster Convergence and Improved Accuracy,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

ALL

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In the current implementation of neural network training within TensorFlow, gradient descent typically relies on the average of gradients during the optimization process. While this approach is effective, there may be potential to enhance training efficiency and accuracy by exploring alternative methods for determining the direction of gradient updates.

I have developed a method that identifies a unified direction rather than merely averaging the gradients, and preliminary experiments suggest that this approach leads to faster convergence and improved model accuracy. I believe this could be a valuable addition to TensorFlow's optimization techniques.

I would appreciate the TensorFlow Development Team's feedback on this idea and am happy to provide further details or collaborate on its implementation.

### Standalone code to reproduce the issue

```shell
https://github.com/TingjiaInFuture/UnionGradientDescent
```


### Relevant log output

_No response_",1
Einsum optimization setting guidelines,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.16

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Existing tensorflow supports [einops](https://www.tensorflow.org/api_docs/python/tf/einsum), which enable seamless tensor reduction ops. Parameter to this functionality (kwarg) is the `optimize` parameter, i.e.
```
optimize: Optimization strategy to use to find contraction path using opt_einsum. Must be 'greedy', 'optimal', 'branch-2', 'branch-all' or 'auto'. (optional, default: 'greedy').
```

Is it documented somewhere what the use cases that benefit from these options are in practice?  Further, are effects of these expected to this library perhaps? https://optimized-einsum.readthedocs.io/en/stable/

Simple benchmarking doesn't really reveal any (significant?) differences. Thanks!

### Standalone code to reproduce the issue

```python
A simple benchmark of form:

import tensorflow as tf
import numpy as np
import time

# Define three large matrices A, B, and C of size 1000x1000
A = tf.random.uniform((1000, 1000), dtype=tf.float32)
B = tf.random.uniform((1000, 1000), dtype=tf.float32)
C = tf.random.uniform((1000, 1000), dtype=tf.float32)
D = tf.random.uniform((1000, 1000), dtype=tf.float32)
E = tf.random.uniform((1000, 1000), dtype=tf.float32)

greedy = []
non_greedy = []
for _ in range(1000):
    # Perform matrix multiplication using tf.einsum with optimize='greedy'
    start_time_greedy = time.time()
    result_greedy = tf.einsum('ij,jk,kl,lm,mn->in', A, B, C, D, E, optimize='greedy')
    end_time_greedy = time.time()

    # Perform matrix multiplication using tf.einsum with optimize='optimal'
    start_time_branch_all = time.time()
    result_branch_all = tf.einsum('ij,jk,kl,lm,mn->in', A, B, C, D, E, optimize='optimal')
    end_time_branch_all = time.time()

    # Print the execution times
    greedy.append(end_time_greedy - start_time_greedy)
    non_greedy.append(end_time_branch_all - start_time_branch_all)

print(f""Greedy time: {np.mean(greedy)}"")
print(f""Optimal time: {np.mean(non_greedy)}"")
```

### Relevant log output

```
Greedy time: 0.067701251745224
Optimal time: 0.06806995582580566
```",1
tf.sparse.reduce_sum error in JIT,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.17.0-rc1-2-gad6d8cc177d

### Custom code

Yes

### OS platform and distribution

Ubuntu Mate 22.04

### Mobile device

_No response_

### Python version

Python 3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA 12.3

### GPU model and memory

_No response_

### Current behavior?

Error when using tf.sparse.reduce_sum and JIT compilation.

I have written a layer that passes all my unit tests except when I use in a model with predict or train. 
Would that make sense with the JIT compilation on? I am not fully certain how and when this works.

Anyway, I am not exactly sure why my layer code fails, but I think that this minimum reproducible example captures the issue.

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

batch_size = 4
input_shape = (3, 3)

indices = np.array([[0, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 1], [2, 0, 0], [2, 0, 1], [3, 0, 0], [3, 0, 1]])
inputs = tf.sparse.SparseTensor(dense_shape=(batch_size, *input_shape),
                                indices=indices,
                                values=[9, 1, 9, 1, 9, 1, 9, 1])


@tf.function(input_signature=[tf.SparseTensorSpec(
    shape=(4, 3, 3),
    dtype=tf.dtypes.int32)], jit_compile=True)
def get_batch_sum(inputs):
    # same problem with tf.sparse.reduce_max
    return tf.sparse.reduce_sum(tf.sparse.reduce_sum(inputs, axis=1, output_is_sparse=True), axis=1)


sum_out = get_batch_sum(inputs)
print(sum_out)
```


### Relevant log output

```shell
2024-08-20 16:31:01.690779: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at xla_ops.cc:577 : INVALID_ARGUMENT: Detected unsupported operations when trying to compile graph __inference_get_batch_sum_15[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263] on XLA_GPU_JIT: SparseReduceSumSparse (No registered 'SparseReduceSumSparse' OpKernel for XLA_GPU_JIT devices compatible with node {{node SparseReduceSumSparse}}){{node SparseReduceSumSparse}}
The op is created at: 
File "".config/JetBrains/PyCharmCE2023.3/scratches/scratch_30.py"", line 21, in <module>
File "".config/JetBrains/PyCharmCE2023.3/scratches/scratch_30.py"", line 18, in get_batch_sum
	tf2xla conversion failed while converting __inference_get_batch_sum_15[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.
Traceback (most recent call last):
(...)  

File "".config/JetBrains/PyCharmCE2023.3/scratches/scratch_30.py"", line 18, in get_batch_sum
	tf2xla conversion failed while converting __inference_get_batch_sum_15[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions. [Op:__inference_get_batch_sum_15]
```
",1
HLO for TopK oddly casts uint8 input to uint32 before passing to radix sort. ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

TF 2 (HEAD of internal repo)

### Custom code

Yes

### OS platform and distribution

Google-Internal Environment

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

V100 (also reproducible on other GPUs)

### Current behavior?

An Alphabet model invokes `tf.math.top_k` with a tensor of dtype uint8 and shape (1, 1,32768). 
![strange_hlo_text_with_uint32_radix_sort](https://github.com/user-attachments/assets/945af324-b00b-405b-8a68-06b7c33cb1e1)

For this call, XLA ends up calling radix sort. However, the radix sort is sub-optimal because TensorFlow casts the inputs to uint32 (instead of using original dtype uint8). Of course, radix sort is faster across smaller dtypes (with fewer bytes).

> @@(u32[1,32768]{1,0}, s32[1,32768]{1,0}, u8[271871]{0}) custom-call(u32[1,32768]{1,0}, s32[1,32768]{1,0}), custom_call_target=""__cub$DeviceRadixSort

I would expect HLO text more like this, where the uint8 inputs are passed directly:

> (u8[1,32768]{1,0}, s32[1,32768]{1,0}, u8[310527]{0}) custom-call(u8[1,32768]{1,0}, s32[1,32768]{1,0}), custom_call_target=""__cub$DeviceRadixSort""

We actually use TF indirectly via the jax2tf bridge, and I see this comment in the code hinting that uint8 is incompatible with `tf.math.top_k`:
https://github.com/google/jax/blob/0b87bf48f97ace10c7aee19c8f980788891a2df7/jax/experimental/jax2tf/jax2tf.py#L3167

However, recently, @dimitar-asenov (on XLA GPU) has made some changes to XLA sorting logic that provides support for radix-sorting uint8.

Could `tf.math.top_k` lower to HLO that avoids this up-cast to uint32 before radix sort? I believe this would halve the latency of radix sort for uint8.

### Standalone code to reproduce the issue

```shell
To repro, collect an XProf trace for the following snippet. See attached screenshot for sample trace.


import tensorflow as tf

data = tf.random.uniform(
          shape=(1, 32768),
          minval=0,
          maxval=256,
          dtype=tf.int32,
      )
data = tf.cast(data, tf.uint8)
_, box_indices = tf.math.top_k(data, k=1024)


Or feel free to just find and run my internal experimental `benchmark_tf_top_k_uint8` binary on a machine with a GPU.
```


### Relevant log output

_No response_",1
Cannot dlopen some GPU libraries [can't find cuda driver] in rhel9,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.17.0-rc1-2-gad6d8cc177d 2.17.0

### Custom code

Yes

### OS platform and distribution

Redhat enterprise 9.4 base image

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2

### GPU model and memory

Tesla T4, 15360MiB

### Current behavior?

What is your question?

Describe the bug

Error when I run GPU test, wondering if my docker linux kernel version is too low? The reason I am asking is that, my ubuntu22 is working fine.

WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1723846435.253301 203 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-08-16 22:13:55.253747: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
""
My Kernel version,

bash-5.1# uname -a
Linux mlops-test-failed 5.10.220-209.869.amzn2.x86_64 https://github.com/rapidsai/cudf/issues/1 SMP Wed Jul 17 15:10:20 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

My OS/Docker image distro,
""
bash-5.1# uname -m && cat /etc/*release
x86_64
NAME=""Red Hat Enterprise Linux""
VERSION=""9.4 (Plow)""
ID=""rhel""
ID_LIKE=""fedora""
VERSION_ID=""9.4""
PLATFORM_ID=""platform:el9""
PRETTY_NAME=""Red Hat Enterprise Linux 9.4 (Plow)""
ANSI_COLOR=""0;31""
LOGO=""fedora-logo-icon""
CPE_NAME=""cpe:/o:redhat:enterprise_linux:9::baseos""
HOME_URL=""https://www.redhat.com/""
DOCUMENTATION_URL=""https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9""
BUG_REPORT_URL=""https://bugzilla.redhat.com/""

REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 9""
REDHAT_BUGZILLA_PRODUCT_VERSION=9.4
REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux""
REDHAT_SUPPORT_PRODUCT_VERSION=""9.4""
Red Hat Enterprise Linux release 9.4 (Plow)
Red Hat Enterprise Linux release 9.4 (Plow)
""
Both my ubuntu 22 and rhel9 were showing the nvidia-smi ok like the following.

bash-5.1# nvidia-smi
Fri Aug 16 22:29:43 2024
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06 Driver Version: 535.183.06 CUDA Version: 12.2 |
|-----------------------------------------+----------------------+----------------------+
| GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |
| Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |
| | | MIG M. |
|=========================================+======================+======================|
| 0 Tesla T4 On | 00000000:00:1E.0 Off | 0 |
| N/A 33C P8 11W / 70W | 0MiB / 15360MiB | 0% Default |
| | | N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes: |
| GPU GI CI PID Type Process name GPU Memory |
| ID ID Usage |
|=======================================================================================|
| No running processes found |
+---------------------------------------------------------------------------------------+

I am checking the following url,
https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html

and it says the rhel9 kernel version has to be 5.14.0-427.

Your input is appreciated!

### Standalone code to reproduce the issue

```shell
def run_gpu_test():
    gpus = tf.config.list_logical_devices('GPU')
    print(""Num GPUs Available: "", len(gpus))

run_gpu_test()

In Ubuntu22, it prints gpu number 1 with all the gpu information and in rhel9 it does not.

This is a Pod we created in eks, and by exec to the pod, we pasted the debugging information in the above section. I did nvidia-smi and both ubuntu22 and rhel9 shows GPU fine. Ubuntu22 works fine but not rhel9. The node we created is a AWS G4 instance, so it has tensorflow 12.7 and cuda 12.2 and we installed nvidia-plugin. I think this should be very easy to reproduce not sure if the aws kernel version matters.
```


### Relevant log output

```shell
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1723846435.253301 203 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-08-16 22:13:55.253747: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
""
```
",1
TFLite Model used in official documentation doesn't compile on Edge TPU Compiler,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 11
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 
  - pip show tensorflow  = 2.17.0
  - tf.__version__ = 2.18.0-dev20240815

### 2. Code
Using the code from [Post-training integer quantization](https://www.tensorflow.org/lite/performance/post_training_integer_quant)  official tutorial to create and convert a TensorFlow model to TFlite:
 
``` python
import logging
logging.getLogger(""tensorflow"").setLevel(logging.DEBUG)

import tensorflow as tf
import numpy as np
print(""TensorFlow version: "", tf.__version__)

# Load MNIST dataset
mnist = tf.keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# Normalize the input image so that each pixel value is between 0 to 1.
train_images = train_images.astype(np.float32) / 255.0
test_images = test_images.astype(np.float32) / 255.0

# Define the model architecture
model = tf.keras.Sequential([
  tf.keras.layers.InputLayer(shape=(28, 28)),
  tf.keras.layers.Reshape(target_shape=(28, 28, 1)),
  tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),
  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(10)
])

# Train the digit classification model
model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(
                  from_logits=True),
              metrics=['accuracy'])
model.fit(train_images, train_labels, epochs=5, validation_data=(test_images, test_labels))

def representative_data_gen():
  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):
    yield [input_value]

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen
# Ensure that if any ops can't be quantized, the converter throws an error
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.target_spec.supported_types = [tf.int8] 
# Set the input and output tensors to uint8 (APIs added in r2.3)
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
converter.experimental_new_quantizer = True

tflite_model_quant = converter.convert()

import pathlib

tflite_models_dir = pathlib.Path(""mnist_tflite_models/"")
tflite_models_dir.mkdir(exist_ok=True, parents=True)
tflite_model_quant_file = tflite_models_dir/""mnist_model_quant.tflite""
tflite_model_quant_file.write_bytes(tflite_model_quant)
```

Which displays the following output:

```
2024-08-16 18:07:19.094940: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-08-16 18:07:20.845501: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
TensorFlow version:  2.18.0-dev20240815
2024-08-16 18:07:25.704749: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Epoch 1/5
1875/1875 ━━━━━━━━━━━━━━━━━━━━ 7s 3ms/step - accuracy: 0.8672 - loss: 0.4860 - val_accuracy: 0.9722 - val_loss: 0.0965
Epoch 2/5
1875/1875 ━━━━━━━━━━━━━━━━━━━━ 7s 4ms/step - accuracy: 0.9738 - loss: 0.0919 - val_accuracy: 0.9768 - val_loss: 0.0735
Epoch 3/5
1875/1875 ━━━━━━━━━━━━━━━━━━━━ 8s 4ms/step - accuracy: 0.9797 - loss: 0.0696 - val_accuracy: 0.9799 - val_loss: 0.0622
Epoch 4/5
1875/1875 ━━━━━━━━━━━━━━━━━━━━ 7s 3ms/step - accuracy: 0.9833 - loss: 0.0565 - val_accuracy: 0.9800 - val_loss: 0.0600
Epoch 5/5
1875/1875 ━━━━━━━━━━━━━━━━━━━━ 6s 3ms/step - accuracy: 0.9843 - loss: 0.0524 - val_accuracy: 0.9813 - val_loss: 0.0595
INFO:tensorflow:Assets written to: C:\Users\Me\AppData\Local\Temp\tmptd9h6442\assets
INFO:tensorflow:Assets written to: C:\Users\Me\AppData\Local\Temp\tmptd9h6442\assets
Saved artifact at 'C:\Users\Me\AppData\Local\Temp\tmptd9h6442'. The following endpoints are available:

* Endpoint 'serve'
  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor')
Output Type:
  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)
Captures:
  1897857613456: TensorSpec(shape=(), dtype=tf.resource, name=None)
  1897859106576: TensorSpec(shape=(), dtype=tf.resource, name=None)
  1897857613072: TensorSpec(shape=(), dtype=tf.resource, name=None)
  1897857612688: TensorSpec(shape=(), dtype=tf.resource, name=None)
C:\Users\Me\.pyenv\pyenv-win\versions\3.12.4\Lib\site-packages\tensorflow\lite\python\convert.py:983: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.
  warnings.warn(
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1723828081.771633   14420 tf_tfl_flatbuffer_helpers.cc:359] Ignored output_format.
W0000 00:00:1723828081.772124   14420 tf_tfl_flatbuffer_helpers.cc:362] Ignored drop_control_dependency.
2024-08-16 18:08:01.772979: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: C:\Users\Me\AppData\Local\Temp\tmptd9h6442
2024-08-16 18:08:01.774123: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }
2024-08-16 18:08:01.774323: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: C:\Users\Me\AppData\Local\Temp\tmptd9h6442
I0000 00:00:1723828081.779428   14420 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled
2024-08-16 18:08:01.780783: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.
2024-08-16 18:08:01.819711: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: C:\Users\Me\AppData\Local\Temp\tmptd9h6442
2024-08-16 18:08:01.830586: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 57614 microseconds.
2024-08-16 18:08:01.847290: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2024-08-16 18:08:02.073373: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence
fully_quantize: 0, inference_type: 6, input_inference_type: UINT8, output_inference_type: UINT8
```

### 3. Failure after conversion
When I then run the newly created TFLite file through the `edgetpu_compiler` (via Docker) it fails saying it still has dynamic-sized tensors:
```
docker run --rm -it -v .:/home/edgetpu edgetpu-compiler edgetpu_compiler mnist_model_quant.tflite
Edge TPU Compiler version 16.0.384591198
Started a compilation timeout timer of 180 seconds.
ERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.
Compilation failed: Model failed in Tflite interpreter. Please ensure model can be loaded/run in Tflite interpreter.
Compilation child process completed within timeout period.
Compilation failed!
```
Any idea how I can fully convert the model to static-sized tensors. I tried the [suggestion](https://github.com/tensorflow/tensorflow/issues/57905#issuecomment-1292720032) of using `converter._experimental_new_quantizer` but that didn't help.",2
tritonserver preload trt plugin got warning message and many core files : Failed to compile generated PTX with ptxas. Falling back to compilation by driver.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.16.2

### Custom code

No

### OS platform and distribution

linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

tritonserver preload trt plugin got warning message and many core  dump files
`
2024-08-16 10:09:14.975649: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
2024-08-16 10:09:16.033970: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
2024-08-16 10:09:16.701031: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
2024-08-16 10:09:17.498157: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
2024-08-16 10:09:18.328719: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
`

![image](https://github.com/user-attachments/assets/c9cf824b-f27e-456d-aa43-af501aae0694)

I have an ensmble model, the first part is the tf model, the second part is the trt model. I have a trt plugin, and I load it as LD_PRELOAD. It won't be a problem if I load the two models separately. But when I load them at the same time this warning comes up and produces a lot of coredump files. Why is that? I don't understand how the trt plugin will affect tf

### Standalone code to reproduce the issue

```shell
LD_PRELOAD=/app/lib/ops/libtrtplugin.so --model-repository=/opt/model-repo-copy --model-control-mode=explicit --load-model=first_model --load-model=second_model  --load-model=ensmble_model  --log-verbose=0 --http-port=xxx--grpc-port=xxx --metrics-port=xxx --backend-config=tensorflow,version=2 --backend-config=tensorrt,version-compatible=true --disable-auto-complete-config 
```


### Relevant log output

_No response_",1
The average should not be computed in L2Pool2d,"The [ONNX L2Pool2d](https://github.com/onnx/onnx/blob/main/docs/Operators.md#lppool), [DirectML L2 Pooling Desc](https://docs.microsoft.com/en-us/windows/win32/api/directml/ns-directml-dml_lp_pooling_operator_desc) and [CoreML's l2_pool2d](https://apple.github.io/coremltools/source/coremltools.converters.mil.mil.ops.defs.html#coremltools.converters.mil.mil.ops.defs.iOS15.pool.l2_pool) calculate the l2 pooling by the expression `Y = (X1^2 + X2^2 + ... + Xn^2) ^ (1/2)`,  but [TFLite L2_PooL2d kernel implementation](https://source.chromium.org/chromium/chromium/src/+/main:third_party/tflite/src/tensorflow/lite/kernels/internal/optimized/optimized_ops.h;l=3341?q=src%2Ftensorflow%2Flite%2Fkernels%2Finternal%2Foptimized%2Foptimized_ops.h) has the average with the count of sum elements  `Y=((X1^2 + X2^2 + ... + Xn^2)/n) ^ (1/2)`,  is it an issue of the kernel implementation?

BTW, [the kernel of l2_norm](https://source.chromium.org/chromium/chromium/src/+/main:third_party/tflite/src/tensorflow/lite/kernels/internal/optimized/optimized_ops.h;l=1424?q=L2Normalization&ss=chromium%2Fchromium%2Fsrc) also has no the average.

",0
Unable to train/take gradient of integer variable under any condition,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.16.1-19-g810f233968c

### Custom code

Yes

### OS platform and distribution

Debian 11

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.6

### GPU model and memory

_No response_

### Current behavior?

Models with integer variables cannot have gradients computed (or be trained because of this). They fail with `ValueError: No gradients provided for any variable.` Please note that I'm referring to building/training models, _not_ post-training model quantization.

For context, I'm working on a somewhat novel approach to solving a type of engineering problem that includes both discrete and continuous values. In some cases, variables must be one of a set of numeric values (e.g. `1`, `2`, `3000`, etc). This isn't something that can really be split out into separate models and trained independently, as both these variables are used throughout a complex, multiple-input multiple-output layer based model. An approximation cannot be used either, as even a highly accurate approximation can feasibly result in an incorrect result under certain conditions.

Intuitively, a derivative cannot be calculated on integer values because they're discrete, and differentiation requires a continuous function over the domain of differentiation. However, floats (and any limited-precision data type) also suffer from this - but differentiation is generally considered ""viable"" for floats. Under any argument made for floats, integers should also be considered differentiable. There are certainly some issues with this for small number, but they have much less of an impact for larger ones (just like floats very close to 0 vs ""single digit"" floats i.e. `1`, `2`, etc).

If maintainers/triagers/project folks don't want to go in this direction, users like myself should be able to implement this via a `tf.custom_gradient` function. However, even when implementing this function, gradients/training fail in the exact same manner in the exact same place. When using a custom gradient function with an integer variable, TF fails with the aforementioned error message without even calling the custom gradient function. IMO this is pretty clearly a bug. See below for specific code to reproduce this.

### Standalone code to reproduce the issue

This fails (no custom gradient function, int32 type):

```python
import keras
import tensorflow as tf
import numpy as np

variable_dtype = tf.int32
# variable_dtype = tf.float32   # Uncommenting this fixes the issue


class BugTestLayer(keras.layers.Layer):
    # Layer is just y = self.var * x
    def build(self, input_shape):
        self.var = self.add_variable(
            (1,), initializer=""zeros"", dtype=variable_dtype)

    def call(self, input):
        return input * self.var


input_layer = keras.Input((1,), dtype=tf.int32)
test_layer = BugTestLayer()

output = test_layer(input_layer)
model = keras.Model(inputs=[input_layer], outputs=[output])

values = np.array([[i] for i in range(1000)])

model.compile(
    loss=[keras.losses.MeanSquaredError()],
    optimizer=keras.optimizers.RMSprop(),
    metrics=[keras.metrics.MeanSquaredError()],
)

# This will always raise a `ValueError: No gradients provided for any variable.`
# when using an integer type
history = model.fit(values, values, batch_size=1, epochs=2)
```

This works (no custom gradient function, float32 type):
```python
import keras
import tensorflow as tf
import numpy as np

# variable_dtype = tf.int32
variable_dtype = tf.float32   # Uncommenting this fixes the issue


class BugTestLayer(keras.layers.Layer):
    # Layer is just y = self.var * x
    def build(self, input_shape):
        self.var = self.add_variable(
            (1,), initializer=""zeros"", dtype=variable_dtype)

    def call(self, input):
        return input * self.var


input_layer = keras.Input((1,), dtype=tf.int32)
test_layer = BugTestLayer()

output = test_layer(input_layer)
model = keras.Model(inputs=[input_layer], outputs=[output])

values = np.array([[i] for i in range(1000)])

model.compile(
    loss=[keras.losses.MeanSquaredError()],
    optimizer=keras.optimizers.RMSprop(),
    metrics=[keras.metrics.MeanSquaredError()],
)

# This will always raise a `ValueError: No gradients provided for any variable.`
# when using an integer type
history = model.fit(values, values, batch_size=1, epochs=2)
```

This fails (custom gradient function, int32 type):
```python
# %%

import keras
import tensorflow as tf
import numpy as np

# %%


class TestLayer(keras.layers.Layer):
    def build(self, input_shape):
        # dtype is the problem.
        # integer type results in ""No gradients provided for any variable"", while
        # floats work just fine.
        self.var = self.add_variable(
            (1,), initializer=""zeros"", dtype=tf.int32)

    @tf.custom_gradient
    def op(input, var):
        def grad(upstream, variables=None):
            return tf.squeeze(upstream * tf.cast(var, tf.float32), axis=[1]), \
                tf.squeeze(upstream * tf.cast(input, tf.float32), axis=[1])

        return tf.cast(input, tf.float32) * tf.cast(var, tf.float32), grad

    def call(self, input):
        return TestLayer.op(input, self.var)

# %%


input_layer = keras.Input((1,), name=""input_layer"", dtype=tf.int32)
test_layer = TestLayer()

output = test_layer(input_layer)
model = keras.Model(inputs=[input_layer], outputs=[output], name=""model"")

# %%

model.summary()
# Example evaluation (untrained)
model(tf.constant([[100]]))

keras.utils.plot_model(model, ""my_first_model.png"", show_dtype=True,
                       show_layer_names=True, show_shapes=True, show_trainable=True)


# %%

values = np.array([
    [i]
    for i in range(1000)
])

model.compile(
    loss=[keras.losses.MeanSquaredError()],
    optimizer=keras.optimizers.RMSprop(),
    metrics=[keras.metrics.MeanSquaredError()],
)

history = model.fit(values, values, batch_size=1, epochs=10)
```

This works (custom gradient function, float32 type):
```python
# %%

import keras
import tensorflow as tf
import numpy as np

# %%


class TestLayer(keras.layers.Layer):
    def build(self, input_shape):
        # dtype is the problem.
        # integer type results in ""No gradients provided for any variable"", while
        # floats work just fine.
        self.var = self.add_variable(
            (1,), initializer=""zeros"", dtype=tf.int32)

    @tf.custom_gradient
    def op(input, var):
        def grad(upstream, variables=None):
            return tf.squeeze(upstream * tf.cast(var, tf.float32), axis=[1]), \
                tf.squeeze(upstream * tf.cast(input, tf.float32), axis=[1])

        return tf.cast(input, tf.float32) * tf.cast(var, tf.float32), grad

    def call(self, input):
        return TestLayer.op(input, self.var)

# %%


input_layer = keras.Input((1,), name=""input_layer"", dtype=tf.float32)
test_layer = TestLayer()

output = test_layer(input_layer)
model = keras.Model(inputs=[input_layer], outputs=[output], name=""model"")

# %%

model.summary()
# Example evaluation (untrained)
model(tf.constant([[100]]))

keras.utils.plot_model(model, ""my_first_model.png"", show_dtype=True,
                       show_layer_names=True, show_shapes=True, show_trainable=True)


# %%

values = np.array([
    [i]
    for i in range(1000)
])

model.compile(
    loss=[keras.losses.MeanSquaredError()],
    optimizer=keras.optimizers.RMSprop(),
    metrics=[keras.metrics.MeanSquaredError()],
)

history = model.fit(values, values, batch_size=1, epochs=10)
```


### Relevant log output

Logs for the last failure:

```shell
Epoch 1/10
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
File /workspaces/power sim 2/test_docs.py:14
      3 values = np.array([
      4     [i]
      5     for i in range(1000)
      6 ])
      8 model.compile(
      9     loss=[keras.losses.MeanSquaredError()],
     10     optimizer=keras.optimizers.RMSprop(),
     11     metrics=[keras.metrics.MeanSquaredError()],
     12 )
---> 14 history = model.fit(values, values, batch_size=1, epochs=10)

File ~/.local/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--> 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

File ~/.local/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:662, in BaseOptimizer._filter_empty_gradients(self, grads, vars)
    659         missing_grad_vars.append(v.name)
    661 if not filtered_grads:
--> 662     raise ValueError(""No gradients provided for any variable."")
    663 if missing_grad_vars:
    664     warnings.warn(
    665         ""Gradients do not exist for variables ""
    666         f""{list(reversed(missing_grad_vars))} when minimizing the loss.""
    667         "" If using `model.compile()`, did you forget to provide a ""
    668         ""`loss` argument?""
    669     )

ValueError: No gradients provided for any variable.
```
",1
Segmentation fault (core dumped) in `tf.config.threading.set_inter_op_parallelism_threads`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Crash triggered when input boundary values into tf.config.threading.set_intra_op_parallelism_threads

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/12s6D2GuBFEWAdvFdCvjbs4nK888vLGvm?usp=sharing
```


### Relevant log output

```shell
2024-08-10 21:36:47.636016: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-08-10 21:36:47.703371: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-10 21:36:47.791020: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-10 21:36:47.817978: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-10 21:36:47.883418: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-10 21:36:56.611712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22279 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-08-10 21:36:56.617085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1717 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
Segmentation fault (core dumped)
```
",2
Aborted (core dumped) in `tf.config.threading.set_intra_op_parallelism_threads`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Crash triggered when input negative numbers into tf.config.threading.set_intra_op_parallelism_threads.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/17JV6ppGU1XtQg25PKa8itDhihAspgHIz?usp=sharing
```


### Relevant log output

```shell
2024-08-10 21:29:55.538868: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-08-10 21:29:55.869155: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-10 21:29:55.967845: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-10 21:29:56.002644: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-10 21:29:56.222202: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-10 21:30:05.961788: F external/local_tsl/tsl/platform/threadpool.cc:112] Check failed: num_threads >= 1 (1 vs. -1)
Aborted (core dumped)
```
",2
Issue with softmax warning appearing in Tensorflow 2.17.0,"### TensorFlow version

2.17.0

### OS platform and distribution

Google Colab

### Current behavior?

Hi, everyone.

I am practicing implementing a Transformer model that machine translates English into Korean by reading TensorFlow guides and books. However, I am having trouble because an unknown UserWarning appears during the final translation process. In that issue, I've never used softmax before, but warning me about using it. This problem appears after the model has finished training and when making inferences.

I searched to see if there were any cases similar to mine, but it seems that no solution was found in any of them. 



same issue: https://github.com/tensorflow/tensorflow/issues/67758



This problem did not exist in tensorflow 2.15 but appeared in 2.17.0. I can't even guess what could be causing it. For those of you who are curious about the full code, I am leaving a Colab link. You can easily reproduce it by running it with Ctrl + F9 in Google Colab. The execution time of the entire code is approximately 5 minutes ~ 5 minutes and 30 seconds on a T4 GPU. That issue is at the bottom.

Colab Link: https://colab.research.google.com/drive/1IMFWoJ1s5ReKU9LYENROpAsZ47D6cG8T?usp=sharing

The data I used is 'kor-eng.zip' located at ""https://www.manythings.org/anki/"".

I'm really sorry for not writing the comments in English.

### Standalone code to reproduce the issue

```shell
class Transformer(keras.Model):
    def __init__(self, *, num_layers, encoder_sequence_length, decoder_sequence_length, source_vocab_size, target_vocab_size, embed_dim,
                 dense_dim, num_heads, dropout_rate):
        super().__init__()
        self.encoder = Encoder(num_layers=num_layers, sequence_length=encoder_sequence_length, input_dim=source_vocab_size, embed_dim=embed_dim,
                              dense_dim=dense_dim, num_heads=num_heads, dropout_rate=dropout_rate)
        self.decoder = Decoder(num_layers=num_layers, sequence_length=decoder_sequence_length, input_dim=target_vocab_size, embed_dim=embed_dim,
                              dense_dim=dense_dim, num_heads=num_heads, dropout_rate=dropout_rate)
        self.final_layer = tf.keras.layers.Dense(units=target_vocab_size)

    def call(self, inputs):
        encoder_inputs, decoder_inputs = inputs
        encoder_pad_mask = tf.math.not_equal(encoder_inputs, 0)[:, tf.newaxis]
        decoder_pad_mask = tf.math.not_equal(decoder_inputs, 0)[:, tf.newaxis]

        decoder_sequence_length = tf.shape(decoder_inputs)[1]
        causal_mask = tf.linalg.band_part(tf.ones((decoder_sequence_length, decoder_sequence_length), tf.bool), -1, 0)

        encoder_inputs = self.encoder(inputs=encoder_inputs, encoder_pad_mask=encoder_pad_mask)  # Shape: (batch_size, encoder_sequence_length, embed_dim)
        decoder_inputs = self.decoder(inputs=decoder_inputs, encoder_outputs=encoder_inputs,
                                      encoder_pad_mask=encoder_pad_mask, decoder_pad_mask=decoder_pad_mask,
                                      causal_mask=causal_mask)  # Shape: (batch_size, decoder_sequence_length, embed_dim)

        logits = self.final_layer(decoder_inputs)  # Shape: (batch_size, decoder_sequence_length, target_vocab_size)

        try:
            # losses/metrics가 커지지 않게 keras_mask를 제거
            del logits._keras_mask
        except AttributeError:
            pass

        return logits


class Translator(tf.Module):
    # 데이터 전처리 함수
    @staticmethod
    def preprocess_text(text_: str, max_repeat: int=2) -> str:
        """"""텍스트 문자열 중 일부 규칙적인 부분을 전처리 하는 함수

        Args:
            text_: 텍스트 문자열 -> str
            max_repeat: 똑같은 문자열이 연속해서 반복할 수 있는 최대 횟수 -> int

        Returns:
            text_: 전처리 된 텍스트 문자열 -> str
        """"""

        text_ = text_.lower()
        text_ = re.sub(pattern=rf""[^\w\s{string.punctuation}]"", repl=r"""", string=text_)
        text_ = re.sub(pattern=r""\?+"", repl=r""?"", string=text_) # ?가 2번 이상 연속되면 ?로 수정
        text_ = re.sub(pattern=r""\!+"", repl=r""!"", string=text_) # !가 2번 이상 연속되면 !로 수정
        text_ = re.sub(pattern=r""(?P<char>\D)(?P=char){"" + str(max_repeat - 1) + r"",}"", repl=r""\g<char>"" * max_repeat, string=text_) # 숫자가 아닌 똑같은 문자열이 repeat번 이상 연속되면 repeat만큼으로 수정
        text_ = re.sub(pattern=r""\.{2,}"", repl=r""..."", string=text_) # ..을 ...으로 변경
        text_ = re.sub(pattern=r""\.\.\.(?P<s>\w)"", repl=r""... \g<s>"", string=text_) # 띄어쓰기 교정
        text_ = re.sub(pattern=r""\s+"", repl="" "", string=text_)

        # 영어 축약형을 풀기
        # 's 축약은 is / has 또는 아예 소유격으로 가능해서 제외
        # 'd 축약은 had / would / could 등으로 가능해서 제외
        # 'll 축약은 shall / will로 가능해서 제외
        text_ = re.sub(pattern=r""\bi'm\b"", repl=r""i am"", string=text_)
        text_ = re.sub(pattern=r""\b(?P<subj>you|we|they|there|who|when|where|what|how|why)'re\b"", repl=r""\g<subj> are"", string=text_)
        text_ = re.sub(pattern=r""\b(?P<verb>is|are|was|were|do|does|did|have|has|had|must|should|may|might|could|would|ought|dare|need)n't\b"", repl=r""\g<verb> not"", string=text_)
        text_ = re.sub(pattern=r""\bwon't\b"", repl=r""will not"", string=text_)
        text_ = re.sub(pattern=r""\bcan't\b"", repl=r""can not"", string=text_)
        text_ = re.sub(pattern=r""\bshan't\b"", repl=r""shall not"", string=text_)
        text_ = re.sub(pattern=r""\b(?P<subj>i|you|they|we|should|could|would|must|not)'ve\b"", repl=r""\g<subj> have"", string=text_)

        text_ = text_.strip()
        return text_

    # tensorflow의 바이트 문자열 데이터 전처리 함수
    @tf.function
    def tf_preprocess_text(text_: tf.string, max_repeat: int=2) -> tf.string:
        """"""텍스트 문자열 중 일부 규칙적인 부분을 전처리 하는 함수

        Args:
            text_: 바이트 문자열 -> tf.string
            max_repeat: 똑같은 문자열이 연속해서 반복할 수 있는 최대 횟수 -> int

        Returns:
            text_: 전처리 된 바이트 문자열 -> tf.string
        """"""

        text_ = tf.strings.lower(input=text_)
        text_ = tf.strings.regex_replace(input=text_, pattern=rf""[^\w\s{string.punctuation}]"", rewrite=r"""")
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\?+"", rewrite=r""?"") # ?가 2번 이상 연속되면 ?로 수정
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\!+"", rewrite=r""!"") # !가 2번 이상 연속되면 !로 수정

        # 숫자가 아닌 똑같은 문자열이 repeat번 이상 연속되면 repeat만큼으로 수정
        stacks = tf.TensorArray(dtype=tf.string, size=0, dynamic_size=True)
        for s in tf.strings.bytes_split(text_):
            if stacks.size() >= max_repeat:
                if tf.strings.regex_full_match(input=s, pattern=r""\D""):
                    back_s = stacks.gather(indices=tf.range(start=stacks.size() - max_repeat, limit=stacks.size(), delta=1))
                    if tf.math.reduce_all(back_s == s):
                        continue

            stacks = stacks.write(stacks.size(), s)

        text_ = tf.strings.reduce_join(inputs=stacks.stack())

        text_ = tf.strings.regex_replace(input=text_, pattern=r""\.{2,}"", rewrite=r""..."") # ..을 ...으로 변경
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\.\.\.(\w)"", rewrite=r""... \1"") # 띄어쓰기 교정
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\s+"", rewrite=r"" "")

        # 영어 축약형을 풀기
        # 's 축약은 is / has 또는 아예 소유격으로 가능해서 제외
        # 'd 축약은 had / would / could 등으로 가능해서 제외
        # 'll 축약은 shall / will로 가능해서 제외
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\bi'm\b"", rewrite=r""i am"")
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\b(you|we|they|there|who|when|where|what|how|why)'re\b"", rewrite=r""\1 are"")
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\b(is|are|was|were|do|does|did|have|has|had|must|should|may|might|could|would|ought|dare|need)n't\b"", rewrite=r""\1 not"")
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\bwon't\b"", rewrite=r""will not"")
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\bcan't\b"", rewrite=r""can not"")
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\bsha't\b"", rewrite=r""shall not"")
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\b(i|you|they|we|should|could|would|must|not)'ve\b"", rewrite=r""\1 have"")

        text_ = tf.strings.strip(input=text_)
        return text_

    def __init__(self, source_tokenizer, target_tokenizer, target_length, model):
        super().__init__()
        self.source_tokenizer = source_tokenizer
        self.target_tokenizer = target_tokenizer
        self.target_length = target_length
        self.model = model

    def __call__(self, sentence):
        sentence = Translator.tf_preprocess_text(text_=sentence)
        sentence_token = self.source_tokenizer.tokenize(sentence)[tf.newaxis]

        encoder_input = sentence_token

        starts = tf.constant(2, dtype=tf.int32)[tf.newaxis]
        ends = tf.constant(3, dtype=tf.int32)[tf.newaxis]
        decoder_token = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)
        decoder_token = decoder_token.write(0, starts)

        for i in tf.range(self.target_length):
            decoder_input = tf.transpose(decoder_token.stack())
            predictions = self.model([encoder_input, decoder_input], training=False)
            predictions = predictions[0, -1, :] # 마지막 토큰으로부터 예측
            predicted_id = tf.argmax(input=predictions, output_type=tf.int32)[tf.newaxis]

            if predicted_id == ends:
                break

            decoder_token = decoder_token.write(i + 1, predicted_id)

        decoder_token = tf.transpose(decoder_token.stack())[0]
        decoder_token = decoder_token[1:]

        return self.target_tokenizer.detokenize(decoder_token)
```


### Relevant log output

```shell
/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis 3 of a tensor of shape (1, 8, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?
  warnings.warn(
English: tom came here to learn french.
Translated Korean: 톰은 프랑스어를 배우러 여기에 왔어. 
Real Korean: <s> 톰은 프랑스어를 배우려고 여기에 왔어. </s>
```
",1
Memory leak in training,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.16.1 and 2.17

### Custom code

Yes

### OS platform and distribution

Debian 11

### Mobile device

_No response_

### Python version

Python 3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuda/12.0.0_gcc-10.4.0 and cudnn/8.9.7.29-12_gcc-10.4.0

### GPU model and memory

different GPU, among which Tesla V100-SXM2-32GB

### Current behavior?

I have a memory leak (GPU memory and RAM constantly increase) during my training. 

**This does not happen with Tensorflow 2.11.1**. 

[Here is the project](https://github.com/deep-finder/tirfm-deepfinder).
Unfortunately, this is not my code and I do not have time to create a minimal standalone code.

### Standalone code to reproduce the issue

```shell
def printMemoryUsage(self):
        gpus = tf.config.list_physical_devices('GPU')             
        for gpu in gpus:
            gpuNameRoot = gpu.name.split(':')[0] + ':'
            memory_info = tf.config.experimental.get_memory_info(gpu.name.replace(gpuNameRoot, ''))
            self.display(f'Memory info of GPU {gpu.name}: current: {memory_info[""current""]/1e9:.2f}, peak: {memory_info[""peak""]/1e9:.2f}')
            try:
                import psutil
                virtual_memory = psutil.virtual_memory()
                print(f'Memory info of CPU: total:{virtual_memory[0]/1e9:.2f}Gb, available: {virtual_memory[1]/1e9:.2f}Gb, percent: {virtual_memory[2]}%')
            except:
                pass

[...]
        # Training loop:
        for e in range(self.epochs):
            # TRAINING:
            start = time.time()
            list_loss_train = []
            list_acc_train = []
            for it in range(self.steps_per_epoch):
                if self.flag_direct_read:
                    batch_data, batch_target = self.generate_batch_direct_read(path_data, path_target, self.batch_size, objlist_train)
                else:
                    batch_data, batch_target, idx_list = self.generate_batch_from_array(data_list, target_list, self.batch_size, objlist_train)

                if self.sample_weights is not None:
                    sample_weight = self.sample_weights[idx_list]
                else:
                    sample_weight = None

                loss_train = self.net.train_on_batch(batch_data, batch_target,
                                                     class_weight=self.class_weight,
                                                     sample_weight=sample_weight)

                self.display('epoch %d/%d - it %d/%d - loss: %0.3f - acc: %0.3f' % (e + 1, self.epochs, it + 1, self.steps_per_epoch, loss_train[0], loss_train[1]))

                self.printMemoryUsage()

                list_loss_train.append(loss_train[0])
                list_acc_train.append(loss_train[1])
                del batch_data
                del batch_target
                if idx_list is not None:
                    del idx_list
                gc.collect()
```


### Relevant log output

```shell
With Tensorflow 2.11.1:


=============================================================
epoch 3/50 - it 1/100 - loss: 2.012 - acc: 0.976
Memory info of GPU /physical_device:GPU:0: current: 0.02, peak: 2.90
Memory info of CPU: total:201.37Gb, available: 191.38Gb, percent: 5.0%
epoch 3/50 - it 2/100 - loss: 2.008 - acc: 0.985
Memory info of GPU /physical_device:GPU:0: current: 0.02, peak: 2.90
Memory info of CPU: total:201.37Gb, available: 191.38Gb, percent: 5.0%
epoch 3/50 - it 3/100 - loss: 2.004 - acc: 0.992
Memory info of GPU /physical_device:GPU:0: current: 0.02, peak: 2.90
Memory info of CPU: total:201.37Gb, available: 191.38Gb, percent: 5.0%
epoch 3/50 - it 4/100 - loss: 2.006 - acc: 0.987
Memory info of GPU /physical_device:GPU:0: current: 0.02, peak: 2.90
Memory info of CPU: total:201.37Gb, available: 191.38Gb, percent: 5.0%
epoch 3/50 - it 5/100 - loss: 2.006 - acc: 0.989
```


With Tensorflow 2.16.1 and 2.17:

```
epoch 1/50 - it 6/100 - loss: 2.473 - acc: 0.305
Memory info of GPU /physical_device:GPU:0: current: 0.18, peak: 2.55
Memory info of CPU: total:201.37Gb, available: 190.78Gb, percent: 5.3%
epoch 1/50 - it 7/100 - loss: 2.470 - acc: 0.394
Memory info of GPU /physical_device:GPU:0: current: 0.21, peak: 2.58
Memory info of CPU: total:201.37Gb, available: 190.58Gb, percent: 5.4%
epoch 1/50 - it 8/100 - loss: 2.466 - acc: 0.463
Memory info of GPU /physical_device:GPU:0: current: 0.24, peak: 2.61
Memory info of CPU: total:201.37Gb, available: 190.38Gb, percent: 5.5%
epoch 1/50 - it 9/100 - loss: 2.461 - acc: 0.519
Memory info of GPU /physical_device:GPU:0: current: 0.27, peak: 2.64
Memory info of CPU: total:201.37Gb, available: 190.11Gb, percent: 5.6%
epoch 1/50 - it 10/100 - loss: 2.455 - acc: 0.564
Memory info of GPU /physical_device:GPU:0: current: 0.29, peak: 2.67
Memory info of CPU: total:201.37Gb, available: 189.94Gb, percent: 5.7%
epoch 1/50 - it 11/100 - loss: 2.448 - acc: 0.603
Memory info of GPU /physical_device:GPU:0: current: 0.32, peak: 2.70
Memory info of CPU: total:201.37Gb, available: 189.67Gb, percent: 5.8%
epoch 1/50 - it 12/100 - loss: 2.437 - acc: 0.634
Memory info of GPU /physical_device:GPU:0: current: 0.35, peak: 2.72
Memory info of CPU: total:201.37Gb, available: 189.48Gb, percent: 5.9%
epoch 1/50 - it 13/100 - loss: 2.425 - acc: 0.662
Memory info of GPU /physical_device:GPU:0: current: 0.38, peak: 2.75
Memory info of CPU: total:201.37Gb, available: 189.21Gb, percent: 6.0%
epoch 1/50 - it 14/100 - loss: 2.408 - acc: 0.685
Memory info of GPU /physical_device:GPU:0: current: 0.41, peak: 2.78
Memory info of CPU: total:201.37Gb, available: 189.03Gb, percent: 6.1%
epoch 1/50 - it 15/100 - loss: 2.389 - acc: 0.705
Memory info of GPU /physical_device:GPU:0: current: 0.44, peak: 2.81
Memory info of CPU: total:201.37Gb, available: 188.79Gb, percent: 6.2%
epoch 1/50 - it 16/100 - loss: 2.369 - acc: 0.723
Memory info of GPU /physical_device:GPU:0: current: 0.46, peak: 2.84
Memory info of CPU: total:201.37Gb, available: 188.54Gb, percent: 6.4%
epoch 1/50 - it 17/100 - loss: 2.350 - acc: 0.739
Memory info of GPU /physical_device:GPU:0: current: 0.49, peak: 2.87
Memory info of CPU: total:201.37Gb, available: 188.37Gb, percent: 6.5%
epoch 1/50 - it 18/100 - loss: 2.332 - acc: 0.753
Memory info of GPU /physical_device:GPU:0: current: 0.52, peak: 2.89
Memory info of CPU: total:201.37Gb, available: 188.10Gb, percent: 6.6%
epoch 1/50 - it 19/100 - loss: 2.315 - acc: 0.765
Memory info of GPU /physical_device:GPU:0: current: 0.55, peak: 2.92
Memory info of CPU: total:201.37Gb, available: 187.87Gb, percent: 6.7%
epoch 1/50 - it 20/100 - loss: 2.300 - acc: 0.776
Memory info of GPU /physical_device:GPU:0: current: 0.58, peak: 2.95
Memory info of CPU: total:201.37Gb, available: 187.64Gb, percent: 6.8%
epoch 1/50 - it 21/100 - loss: 2.286 - acc: 0.786
Memory info of GPU /physical_device:GPU:0: current: 0.61, peak: 2.98
Memory info of CPU: total:201.37Gb, available: 187.49Gb, percent: 6.9%
epoch 1/50 - it 22/100 - loss: 2.274 - acc: 0.795
Memory info of GPU /physical_device:GPU:0: current: 0.63, peak: 3.01
Memory info of CPU: total:201.37Gb, available: 187.25Gb, percent: 7.0%
epoch 1/50 - it 23/100 - loss: 2.262 - acc: 0.804
Memory info of GPU /physical_device:GPU:0: current: 0.66, peak: 3.04
Memory info of CPU: total:201.37Gb, available: 186.97Gb, percent: 7.1%
epoch 1/50 - it 24/100 - loss: 2.251 - acc: 0.811
Memory info of GPU /physical_device:GPU:0: current: 0.69, peak: 3.06
Memory info of CPU: total:201.37Gb, available: 186.81Gb, percent: 7.2%
epoch 1/50 - it 25/100 - loss: 2.241 - acc: 0.818
Memory info of GPU /physical_device:GPU:0: current: 0.72, peak: 3.09
Memory info of CPU: total:201.37Gb, available: 186.59Gb, percent: 7.3%
epoch 1/50 - it 26/100 - loss: 2.232 - acc: 0.825
Memory info of GPU /physical_device:GPU:0: current: 0.75, peak: 3.12
Memory info of CPU: total:201.37Gb, available: 186.36Gb, percent: 7.5%
epoch 1/50 - it 27/100 - loss: 2.224 - acc: 0.831
Memory info of GPU /physical_device:GPU:0: current: 0.78, peak: 3.15
Memory info of CPU: total:201.37Gb, available: 186.09Gb, percent: 7.6%
epoch 1/50 - it 28/100 - loss: 2.216 - acc: 0.836
Memory info of GPU /physical_device:GPU:0: current: 0.80, peak: 3.18
Memory info of CPU: total:201.37Gb, available: 185.90Gb, percent: 7.7%
epoch 1/50 - it 29/100 - loss: 2.209 - acc: 0.841
Memory info of GPU /physical_device:GPU:0: current: 0.83, peak: 3.21
Memory info of CPU: total:201.37Gb, available: 185.65Gb, percent: 7.8%
epoch 1/50 - it 30/100 - loss: 2.202 - acc: 0.846
Memory info of GPU /physical_device:GPU:0: current: 0.86, peak: 3.23
Memory info of CPU: total:201.37Gb, available: 185.46Gb, percent: 7.9%
epoch 1/50 - it 31/100 - loss: 2.196 - acc: 0.851
Memory info of GPU /physical_device:GPU:0: current: 0.89, peak: 3.26
Memory info of CPU: total:201.37Gb, available: 185.24Gb, percent: 8.0%
epoch 1/50 - it 32/100 - loss: 2.190 - acc: 0.855
Memory info of GPU /physical_device:GPU:0: current: 0.92, peak: 3.29
Memory info of CPU: total:201.37Gb, available: 184.99Gb, percent: 8.1%
epoch 1/50 - it 33/100 - loss: 2.185 - acc: 0.859
Memory info of GPU /physical_device:GPU:0: current: 0.95, peak: 3.32
Memory info of CPU: total:201.37Gb, available: 184.75Gb, percent: 8.3%
epoch 1/50 - it 34/100 - loss: 2.180 - acc: 0.863
Memory info of GPU /physical_device:GPU:0: current: 0.97, peak: 3.35
Memory info of CPU: total:201.37Gb, available: 184.51Gb, percent: 8.4%
epoch 1/50 - it 35/100 - loss: 2.174 - acc: 0.866
Memory info of GPU /physical_device:GPU:0: current: 1.00, peak: 3.38
Memory info of CPU: total:201.37Gb, available: 184.36Gb, percent: 8.4%
epoch 1/50 - it 36/100 - loss: 2.170 - acc: 0.870
Memory info of GPU /physical_device:GPU:0: current: 1.03, peak: 3.40
Memory info of CPU: total:201.37Gb, available: 184.06Gb, percent: 8.6%
epoch 1/50 - it 37/100 - loss: 2.165 - acc: 0.873
Memory info of GPU /physical_device:GPU:0: current: 1.06, peak: 3.43
Memory info of CPU: total:201.37Gb, available: 183.89Gb, percent: 8.7%
epoch 1/50 - it 38/100 - loss: 2.161 - acc: 0.876
Memory info of GPU /physical_device:GPU:0: current: 1.09, peak: 3.46
Memory info of CPU: total:201.37Gb, available: 183.66Gb, percent: 8.8%
epoch 1/50 - it 39/100 - loss: 2.157 - acc: 0.879
Memory info of GPU /physical_device:GPU:0: current: 1.11, peak: 3.49
Memory info of CPU: total:201.37Gb, available: 183.42Gb, percent: 8.9%
epoch 1/50 - it 40/100 - loss: 2.153 - acc: 0.881
Memory info of GPU /physical_device:GPU:0: current: 1.14, peak: 3.52
Memory info of CPU: total:201.37Gb, available: 183.26Gb, percent: 9.0%
epoch 1/50 - it 41/100 - loss: 2.150 - acc: 0.884
Memory info of GPU /physical_device:GPU:0: current: 1.17, peak: 3.54
Memory info of CPU: total:201.37Gb, available: 183.02Gb, percent: 9.1%
```
```
",1
Gradient computation for `vectorized_map` nested inside `while_loop`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v2.17.0

### Custom code

Yes

### OS platform and distribution

Linux Mint 21.3

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am trying to compute gradients for a `tf.vectorized_map`ped function nested within a call to `tf.while_loop` (and hence also `tf.map_fn`) as in [this (trivial!) Colab example](https://colab.research.google.com/drive/1IEsQAM_AU2H0bfiOfxdphk5dyi9kmS8g?usp=sharing).  The top level function can compute its return value in all three execution modes (eager, graph, XLA).  

I expected to also be able to compute the gradients of the function with respect to its inputs.  This works under eager and graph mode, but not XLA mode where an InvalidArgument exception occurs:

> InvalidArgumentError: Input 1 to node `gradient_tape/while/gradients/while/PartitionedCall_grad/PartitionedCall/gradients/pfor/Tile_grad/Reshape_1` with op Reshape must be a compile-time constant.

In the example, all shapes are well-specified (by hard coding in this trivial example), so it feels like some shape information is getting lost somewhere.  Is this a bug, or a ""feature"" for which there is a workaround, I wonder? 



### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1IEsQAM_AU2H0bfiOfxdphk5dyi9kmS8g?usp=sharing
```


### Relevant log output

```shell
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-1-151283a3b91b> in <cell line: 36>()
     34 print(""Eager mode:"", value_and_grads(0.1, 0.4))  # Eager mode runs
     35 print(""Graph mode:"", tf.function(lambda: value_and_grads(0.1, 0.4))()) # Graph mode runs
---> 36 print(""XLA mode:"", tf.function(lambda: value_and_grads(0.1, 0.4), jit_compile=True)()) # XLA fails

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51   try:
     52     ctx.ensure_initialized()
---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                         inputs, attrs, num_outputs)
     55   except core._NotOkStatusException as e:

InvalidArgumentError: Input 1 to node `gradient_tape/while/gradients/while/PartitionedCall_grad/PartitionedCall/gradients/pfor/Tile_grad/Reshape_1` with op Reshape must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.

Stack trace for op definition: 
File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code
File ""/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py"", line 37, in <module>
File ""/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 992, in launch_instance
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 619, in start
File ""/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 195, in start
File ""/usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
File ""/usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once
File ""/usr/lib/python3.10/asyncio/events.py"", line 80, in _run
File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 685, in <lambda>
File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 738, in _run_callback
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 825, in inner
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 361, in process_one
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 261, in dispatch_shell
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 539, in execute_request
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 302, in do_execute
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 539, in run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 2975, in run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3030, in _run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code
File ""<ipython-input-1-151283a3b91b>"", line 36, in <cell line: 36>
File ""<ipython-input-1-151283a3b91b>"", line 36, in 
File ""<ipython-input-1-151283a3b91b>"", line 36, in 
File ""<ipython-input-1-151283a3b91b>"", line 30, in value_and_grads

	 [[{{function_node __inference___backward_f_460_482}}{{node gradients/pfor/Tile_grad/Reshape_1}}]]
	tf2xla conversion failed while converting while_body_347_grad_431_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code
File ""/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py"", line 37, in <module>
File ""/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 992, in launch_instance
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 619, in start
File ""/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 195, in start
File ""/usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
File ""/usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once
File ""/usr/lib/python3.10/asyncio/events.py"", line 80, in _run
File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 685, in <lambda>
File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 738, in _run_callback
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 825, in inner
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 361, in process_one
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 261, in dispatch_shell
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 539, in execute_request
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 302, in do_execute
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 539, in run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 2975, in run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3030, in _run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code
File ""<ipython-input-1-151283a3b91b>"", line 36, in <cell line: 36>
File ""<ipython-input-1-151283a3b91b>"", line 36, in 
File ""<ipython-input-1-151283a3b91b>"", line 36, in 
File ""<ipython-input-1-151283a3b91b>"", line 30, in value_and_grads

	 [[gradient_tape/while/while_grad]]
	tf2xla conversion failed while converting __inference_<lambda>_538[_XlaMustCompile=true,config_proto=3175580994766145631,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions. [Op:__inference_<lambda>_538]
```
",1
[BUG] Optimizer crash on TPU: `AttributeError: 'NoneType' object has no attribute 'extended'`,"### Issue type: Bug

System info:

- Kaggle TPU VM v3-8
- Python 3.10
- TensorFlow 2.16.1

### Standalone code to reproduce the issue

```python
try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    tpu_strategy = tf.distribute.TPUStrategy(tpu)
    print(""TPU setup successful"")
except (ValueError, ImportError) as e:
    tpu_strategy = tf.distribute.get_strategy()

class BertSLPModel(tf.keras.Model):
    def __init__(self):
        super(BertSLPModel, self).__init__()
        self.bert = bert
        self.dropout = tf.keras.layers.Dropout(dropout_rate)
        self.classifier = tf.keras.layers.Dense(num_classes)

    def call(self, inputs):
        input_ids, attention_mask = inputs
        bert_output = self.bert(input_ids, attention_mask=attention_mask)
        pooled_output = bert_output.pooler_output
        dropout_output = self.dropout(pooled_output)
        logit = self.classifier(dropout_output)
        return logit

with tpu_strategy.scope():
    model = BertSLPModel()
    model.compile(
        optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-5),
        loss=tf.keras.losses.SparseCategoricalCrossentropy(),
        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],
    )
    model.fit(
        train_dataset,
        validation_data=test_dataset,
        epochs=epochs,
        batch_size=batch_size
    )
```


### Relevant log output

```shell
AttributeError: in user code:

    File ""/tf_keras/src/engine/training.py"", line 1398, in train_function  *
        return step_function(self, iterator)
    File ""/tf_keras/src/engine/training.py"", line 1370, in run_step  *
        outputs = model.train_step(data)
    File ""/tf_keras/src/engine/training.py"", line 1151, in train_step  *
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File ""/tf_keras/src/optimizers/optimizer.py"", line 621, in minimize  *
        self.apply_gradients(grads_and_vars)
    File ""/tf_keras/src/optimizers/optimizer.py"", line 1300, in apply_gradients  *
        return super().apply_gradients(grads_and_vars, name=name)
    File ""/tf_keras/src/optimizers/optimizer.py"", line 715, in apply_gradients  *
        self.build(trainable_variables)
    File ""/tf_keras/src/optimizers/rmsprop.py"", line 121, in build  *
        self._velocities.append(
    File ""/tf_keras/src/optimizers/optimizer.py"", line 1201, in add_variable_from_reference  *
        with strategy.extended.colocate_vars_with(model_variable):

    AttributeError: 'NoneType' object has no attribute 'extended'
```",2
Proposal: Conditionally substitute boringSSL with system OpenSSL,"Hello All,

As we know boringSSL doesn't support IBM Power architecture anymore, we are finding it difficult to build Tensorflow on Power with proper and updated boringSSL. Even if we just patch boringSSL to have Power support (by using older boringSSL or applying patch from previous commits that had Power support), it is difficult to maintain if boringSSL version gets updated with no equivalent changes for Power. 

Our Proposal: Use system installed OpenSSL instead of boringSSL based on a flag **only if** set. For e.g. USE_SYSTEM_OPENSSL

TF brings in boringSSL dependency directly and indirectly (through curl and grpc) during bazel build process. So, we have tried some changes which does following -
1. Use boringSSL and curl from the system (using TF_SYSTEM_LIBS)
2. Patched grpc to fallback to system openSSL include and library paths during its build. 
Note: grpc already provides similar mechanism when grpcio's wheel is being built through [setup.py](https://github.com/grpc/grpc/blob/master/setup.py#L301). But this mechanism doesn't work when we build grpc through bazel.

These changes have enabled us to build TF on Power, and our testing also didn't give any issues so far (some of the bazel tests and tensorflow/models). 

We want your opinion on this approach. Also, kindly let us know what all tests we should do to have better coverage of this replacement.

Thanks in advance. ",1
Seeking information on low-level TPU interaction and libtpu.so API,"I'm looking to build an automatic differentiation library for TPUs without using high-level front-ends like TensorFlow/JAX/PyTorch-XLA, but I'm finding information about lower-level TPU usage is practically non-existent.

Specifically, I'm interested in:
1. How to interact with TPUs at a lower level than what's typically exposed in TensorFlow
2. Information about the libtpu.so library and its API
3. Any resources or documentation on implementing custom TPU operations

Are there any insights or suggestions on how to approach this, particularly regarding TPU support? Any ideas or help would be greatly appreciated.

I understand that some of this information might be proprietary, but any guidance on what is possible or available would be very helpful.",0
TF strings do not work on the GPU as indices for  `tf.gather` / `tf.nn.embedding_lookup`,"### Issue type

Documentation Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.15

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

As the [`tf.gather`](https://www.tensorflow.org/api_docs/python/tf/gather) documentation suggests, the following code will indeed break on the CPU, but work on the GPU

![image](https://github.com/user-attachments/assets/4c3180fb-92ba-4d34-9518-8af5968f490e)

```py
indices= tf.constant([ 31., 117., 180., 255., 127.,  14.], dtype=tf.float32)

print(indices.shape)

labels = tf.constant([[1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1]], dtype=tf.int32)

print(labels.shape)

tf.nn.embedding_lookup(indices, labels)
```

But this does not to seem to be the case when indices are of type `tf.string` instead:

```py
indices= tf.constant([""a"", ""b"", ""c"", ""d"", ""e"", ""f""], dtype=tf.string)

print(indices.shape)

labels = tf.constant([[1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1]], dtype=tf.int32)

print(labels.shape)

tf.nn.embedding_lookup(indices, labels)
```

As the op indeed runs on the CPU (see log output)

A better approach is to indeed use a [`StaticHashTable`](https://www.tensorflow.org/api_docs/python/tf/lookup/StaticHashTable), and then lookup the attribute labels:

```py
text_values= tf.constant([""a"", ""b"", ""c"", ""d"", ""e"", ""f""], dtype=tf.string)
text_indices = tf.range(tf.size(text_values), dtype=tf.int32)

text_table = tf.lookup.StaticHashTable(
    initializer = tf.lookup.KeyValueTensorInitializer(
        keys=text_indices,
        values=text_values,
        key_dtype=tf.int32,
        value_dtype=tf.string
    ),
    default_value="""",
)

labels = tf.constant([[1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1]], dtype=tf.int32)

text_table.lookup(labels)
```

But I think this should be documented somewhere in `tf.gather` / `tf.nn.embedding_lookup`


Here is a notebook that demonstrates this behaviour:

https://colab.research.google.com/drive/15Ig6Sw39lXRkZ40TvZhs4Aq_v96t-caL?usp=sharing


### Standalone code to reproduce the issue

```shell
indices= tf.constant([""a"", ""b"", ""c"", ""d"", ""e"", ""f""], dtype=tf.string)

print(indices.shape)

labels = tf.constant([[1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1]], dtype=tf.int32)

print(labels.shape)

tf.nn.embedding_lookup(indices, labels)
```


### Relevant log output

```shell
(6,)
(1, 100)
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-9-e96bf33d6da0> in <cell line: 15>()
     13 print(labels.shape)
     14 
---> 15 tf.nn.embedding_lookup(indices, labels)

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   5881 def raise_from_not_ok_status(e, name) -> NoReturn:
   5882   e.message += ("" name: "" + str(name if name is not None else """"))
-> 5883   raise core._status_to_exception(e) from None  # pylint: disable=protected-access
   5884 
   5885 

InvalidArgumentError: {{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[0,1] = -1 is not in [0, 6) [Op:GatherV2] name:
```
",1
Sparse segment mean/sum gives random result on empty tensor,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.12.1

### Custom code

No

### OS platform and distribution

Linux GPU

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When inputs to `math_ops.sparse_segment_mean` or `math_ops.sparse_segment_sum` are empty. The gradient of the OP is not 0, but some random values from previous tensors. The gradient should be zero when inputs are empty

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.client import timeline
from tensorflow.python.ops import math_ops

tf.compat.v1.disable_eager_execution()


def construct_model():
  embed_dim = 128
  cache_size = 2**18 + 1
  with tf.device(""/GPU:0""):
    params = tf.compat.v1.Variable(
        shape=(cache_size, embed_dim),
        dtype=tf.float32,
        initial_value=tf.ones(shape=(cache_size, embed_dim), dtype=tf.float32))

  ids = tf.constant([], dtype=tf.int32)
  segment_ids = tf.constant([], dtype=tf.int32)

  embed_pooling = math_ops.sparse_segment_sum(
      params,
      ids,
      segment_ids,
      num_segments=8192,
      name=""sss"",
  )

  #print(embed_pooling)
  loss = tf.reduce_sum(embed_pooling)
  trainable_vars = tf.compat.v1.trainable_variables()
  grads = tf.gradients(loss, trainable_vars)
  grads_and_vars = [(g, v) for g, v in zip(grads, trainable_vars)]

  sanity_check_ops = [
      #tf.print(grads[0]),
      tf.debugging.check_numerics(grads[0], message=""""),
      tf.debugging.assert_equal(tf.reduce_sum(grads[0]), tf.constant(0.)),
      #CRITICAL step to reproduce
      tf.random.uniform((81920, 1280), 100, 101)
  ]
  adam_opt = tf.compat.v1.train.AdamOptimizer(
      learning_rate=0.001, beta1=0.8, beta2=0.88)
  with tf.control_dependencies(sanity_check_ops):

    step = adam_opt.apply_gradients(grads_and_vars)

  return step


tf.config.threading.set_inter_op_parallelism_threads(32)
tf.config.threading.set_intra_op_parallelism_threads(32)
run_op = construct_model()

profile_options = tf.compat.v1.RunOptions(
    trace_level=tf.compat.v1.RunOptions.FULL_TRACE)
run_metadata = tf.compat.v1.RunMetadata()

with tf.compat.v1.Session() as sess:
  sess.run(tf.compat.v1.global_variables_initializer())
  print(sess._config)

  for i in range(10):
    sess.run(run_op, run_metadata=run_metadata)
```


### Relevant log output

```shell
2024-07-30 14:39:23.458752: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-07-30 14:39:23.505787: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-07-30 14:39:25.698665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79078 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:e1:00.0, compute capability: 8.0
2024-07-30 14:39:25.701973: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
2024-07-30 14:39:27.165519: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: assertion failed: [Condition x == y did not hold element-wise:] [x (Sum_1:0) = ] [16775995] [y (Const_4:0) = ] [0]
	 [[{{function_node assert_equal_1_Assert_AssertGuard_false_41}}{{node Assert}}]]
device_count {
  key: ""CPU""
  value: 1
}
device_count {
  key: ""GPU""
  value: 1
}
intra_op_parallelism_threads: 32
inter_op_parallelism_threads: 32
gpu_options {
  visible_device_list: ""0""
  experimental {
  }
}
experimental {
}

Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn
    return self._call_tf_sessionrun(options, feed_dict, fetch_list,
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun
    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,
tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [Condition x == y did not hold element-wise:] [x (Sum_1:0) = ] [16775995] [y (Const_4:0) = ] [0]
	 [[{{function_node assert_equal_1_Assert_AssertGuard_false_41}}{{node Assert}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/tmp/t.py"", line 65, in <module>
    sess.run(run_op, run_metadata=run_metadata)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py"", line 968, in run
    result = self._run(None, fetches, feed_dict, options_ptr,
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run
    results = self._do_run(handle, final_targets, final_fetches,
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run
    return self._do_call(_run_fn, feeds, fetches, targets, options,
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call
    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter
tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:

assertion failed: [Condition x == y did not hold element-wise:] [x (Sum_1:0) = ] [16775995] [y (Const_4:0) = ] [0]
	 [[{{node Assert}}]]
```
",1
Does TensorFlow2.13.0 support RISC-V,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Linux openeuler-riscv-4-2 6.6.0

### Mobile device

_No response_

### Python version

3.11.6

### Bazel version

5.3.0

### GCC/compiler version

12.3.1

### CUDA/cuDNN version

no

### GPU model and memory

no

### Current behavior?

I recently tried to build TensorFlow2.13.0 with bazel5.3.0 on RISC-V, but I encountered the following error during the build process：
```
ERROR: /home/tf2130/.cache/bazel/_bazel_tf2130/4d8a15755e0d938e330a7b941554a2cb/external/mkl_dnn_v1/BUILD.bazel:146:11: Compiling src/cpu/x64/rnn/brgemm_cell_common_bwd.cpp failed: (Exit 1): gcc failed: error executing command /usr/lib64/ccache/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 67 arguments skipped)
external/mkl_dnn_v1/src/cpu/x64/rnn/brgemm_cell_common_bwd.cpp: In member function 'void dnnl::impl::cpu::x64::brgemm_diff_src_layer_iter_t<weights_t, scratch_t, gemm_acc_t>::execute() const':
external/mkl_dnn_v1/src/cpu/x64/rnn/brgemm_cell_common_bwd.cpp:102:37: error: 'const struct dnnl::impl::cpu::rnn_utils::diff_src_brgemm_conf_t' has no member named 'isa'
  102 |             && rnn_.diff_src_brgemm.isa == x64::avx512_core_bf16_amx_bf16) {
      |                                     ^~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 69717.150s, Critical Path: 1675.59s
INFO: 9452 processes: 1564 internal, 7888 local.
FAILED: Build did NOT complete successfully
```

### Standalone code to reproduce the issue

```shell
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git checkout tags/v2.13.0
bazel build //tensorflow/tools/pip_package:build_pip_package --local_ram_resoues=1024 --jobs=6
```


### Relevant log output

_No response_",1
[Question] - Extend some GPU op,"### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v2.17.0-rc1-2-gad6d8cc177d 2.17.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.6 LTS

### Mobile device

_No response_

### Python version

3.9.19

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

A30

### Current behavior?

I'm running tensorflow_model_server with the sample saved_model_half_plus_two_gpu model. One of the layers there is Mul

""Mul"" op: ""Mul"" input: ""a/read"" input: ""_arg_x_0_0/_7"" device: ""/job:localhost/replica:0/task:0/device:GPU:0"" attr { key: ""T"" value { type: DT_FLOAT } } attr { key: ""_XlaHasReferenceVars"" value { b: true } }

I would like to add some custom code before/after the code runs on the GPU (I have a docker with TensorFlow serving, it downloads the TensorFlow code and comiles it via bazel). I am able to add a print in the OpKernel::OpKernel code, but I would like to do so in the relevant GPU operator compute's function. That I am unable to find/do.

Tried to print/log to file in either of the following places but it did **NOT** work:
- tensorflow/core/util/gpu_kernel_helper.h               in GpuLaunchKernel function
- tensorflow/core/kernels/cwise_ops_common.h     in BinaryOp::Compute

Any idea what am I missing? Obviously my code adds catch as they compile and the code in OpKernel::OpKernel  does work.


### Standalone code to reproduce the issue

```shell
Not relevant...
```


### Relevant log output

_No response_",1
Inspect `tf.data.AUTOTUNE `values during runtime,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.16.1-0-g5bc9d26649c

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When loading data with a `tf.data.Dataset`, it is possible to apply transforms like `.prefetch(buffer_size)`, where the `buffer_size` can be optimized during runtime if set to `tf.data.AUTOTUNE`. When printing `tf.data.AUTOTUNE`, it returns `-1` (as described in the documentation on this feature). However, I want to know what value the optimized `buffer_size` takes during my training loop. This does not seem to be described in the current documentation.

### Standalone code to reproduce the issue

```shell
Any training loop using a `tf.data.Dataset` dataloader & optimized buffer sizes using `tf.data.AUTOTUNE`.
```


### Relevant log output

_No response_",1
tf.raw_ops.BlockLSTMV2: tensorflow/core/framework/tensor.cc:1075] Check failed: 0 <= start (0 vs. -9) Aborted (core dumped) ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0.dev20240717

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

Python version: 3.10.14 

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered a bug in TensorFlow when I used API `tf.raw_ops.BlockLSTMV2`  . The code is as follows:

```python
import tensorflow as tf

seq_len_max = tf.constant(5, shape=(), dtype=tf.int64)
x = tf.constant([[[8., 1.]],[[7., 6.]]], shape=(2, 1, 2), dtype=tf.float16)
cs_prev = tf.constant([[5., 3.]], shape=(1, 2), dtype=tf.float16)
h_prev = tf.constant([[3., 1.]], shape=(1, 2), dtype=tf.float16)
w = tf.constant([[  3.,  -1.,   7.,  -2.,  -8.,   2.,  -5.,   2.],
                 [ -3.,   2.,  -6.,  -4.,   0.,   0.,   2.,  -6.],
                 [-10.,   0.,   8.,   2.,  -1.,  -5.,   4.,  -7.],
                 [  1., -10.,  -6.,   2.,  -2., -10.,  -3.,   5.]], shape=(4, 8), dtype=tf.float16)

wci = tf.constant([ 7., -7.], shape=(2,), dtype=tf.float16)
wcf = tf.constant([-10., 7.], shape=(2,), dtype=tf.float16)
wco = tf.constant([-3., -5.], shape=(2,), dtype=tf.float16)
b = tf.constant([  1.,   6.,  -4.,  -2.,  -4.,   8., -10.,  -2.], shape=(8,), dtype=tf.float16)


tf.raw_ops.BlockLSTMV2(cell_clip=-10,use_peephole=True,seq_len_max=seq_len_max,x=x,cs_prev=cs_prev,h_prev=h_prev,w=w,wci=wci,wcf=wcf,wco=wco,b=b,)
```

The error message was as follows:

```shell
2024-07-23 20:47:52.963893: E tensorflow/core/util/util.cc:131] oneDNN supports DT_HALF only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.
2024-07-23 20:47:52.968139: F tensorflow/core/framework/tensor.cc:1075] Check failed: 0 <= start (0 vs. -9)
Aborted (core dumped)
```

The above code would crash on `tf-nightly 2.18.0.dev20240717` (nightly-build). To reproduce the issue, I provided that a [colab notebook](https://colab.research.google.com/drive/1UxYr3Fg7uKd_cZA-ekKrM9WsfJK-ox_Z?usp=sharing) to reproduce the error.

 

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

seq_len_max = tf.constant(5, shape=(), dtype=tf.int64)
x = tf.constant([[[8., 1.]],[[7., 6.]]], shape=(2, 1, 2), dtype=tf.float16)
cs_prev = tf.constant([[5., 3.]], shape=(1, 2), dtype=tf.float16)
h_prev = tf.constant([[3., 1.]], shape=(1, 2), dtype=tf.float16)
w = tf.constant([[  3.,  -1.,   7.,  -2.,  -8.,   2.,  -5.,   2.],
                 [ -3.,   2.,  -6.,  -4.,   0.,   0.,   2.,  -6.],
                 [-10.,   0.,   8.,   2.,  -1.,  -5.,   4.,  -7.],
                 [  1., -10.,  -6.,   2.,  -2., -10.,  -3.,   5.]], shape=(4, 8), dtype=tf.float16)

wci = tf.constant([ 7., -7.], shape=(2,), dtype=tf.float16)
wcf = tf.constant([-10., 7.], shape=(2,), dtype=tf.float16)
wco = tf.constant([-3., -5.], shape=(2,), dtype=tf.float16)
b = tf.constant([  1.,   6.,  -4.,  -2.,  -4.,   8., -10.,  -2.], shape=(8,), dtype=tf.float16)


tf.raw_ops.BlockLSTMV2(cell_clip=-10,use_peephole=True,seq_len_max=seq_len_max,x=x,cs_prev=cs_prev,h_prev=h_prev,w=w,wci=wci,wcf=wcf,wco=wco,b=b,)
```


### Relevant log output

```shell
2024-07-23 20:47:52.963893: E tensorflow/core/util/util.cc:131] oneDNN supports DT_HALF only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.
2024-07-23 20:47:52.968139: F tensorflow/core/framework/tensor.cc:1075] Check failed: 0 <= start (0 vs. -9)
Aborted (core dumped)
```
",2
tf.strided_slice new_axis_mask inconsistency,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly, 2.17.0, 2.16.2, 2.16.1

### Custom code

No

### OS platform and distribution

macOS, Linux

### Mobile device

_No response_

### Python version

3.10.12, 3.12.4

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I'm seeing that when `spec_size` (as described in the docs as the length of the `begin`, `end`, `strides` arrays) is less than the rank of the `input` tensor, the behavior of `strided_slice` differs when `new_axis_mask` for the bits between `len(begin)` and `tf.rank(input)` is specified.

Is this the expected behavior, and if so, is there anything else like this that differs when `spec_size != tf.rank(input)`? 

I'm noting that it's currently permitted for `spec_size > tf.rank(input)`, which yields the same result in this case as when `spec_size == tf.rank(input)`.

### Standalone code to reproduce the issue

```shell
Notebook:
https://colab.research.google.com/drive/1-LCENzCjorhzyDCqq5qfB4IFoxLbHjGI?usp=sharing
Code:

import numpy as np
import tensorflow as tf

@tf.function
def test(t, begin, end, mask):
    return tf.strided_slice(t, begin, end, new_axis_mask=mask)

t = tf.constant(np.arange(0,27).reshape((3,3,3)))
mask = 0b111

# Noting as comments the shape of the result

# shape = (1, 3, 3, 3)
print(test(t, [0], [3], mask))
# shape = (1, 1, 3, 3, 3)
print(test(t, [0, 0], [3, 3], mask))
# shape = (1, 1, 1, 3, 3, 3)
print(test(t, [0, 0, 0], [3, 3, 3], mask))
# shape = (1, 1, 1, 3, 3, 3)
print(test(t, [0, 0, 0, 0], [3, 3, 3, 3], mask))
```
```


### Relevant log output

_No response_",1
tf.raw_ops.MapUnstage: Check failed: 1 == NumElements() (1 vs. 120)Must have a one element tensor Aborted (core dumped),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0.dev20240717

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

Python version: 3.10.14 

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered a bug in TensorFlow when I used API `tf.raw_ops.MapUnstage`  with randomly generated tensors. The code is as follows:

```python
import tensorflow as tf

key = tf.random.uniform([15, 8], minval=-10, maxval=10, dtype=tf.int64)
indices = tf.random.uniform([1], minval=-10, maxval=10, dtype=tf.int32)
tf.raw_ops.MapUnstage(capacity=100,memory_limit=100,dtypes=[tf.float64],
                      container="""",shared_name="""",key=key,indices=indices)
```

The error message was as follows:

```shell
2024-07-22 22:05:52.420257: F tensorflow/core/framework/tensor.cc:852] Check failed: 1 == NumElements() (1 vs. 120)Must have a one element tensor
Aborted (core dumped)
```

I have confirmed that above code would crash on `tf-nightly 2.18.0.dev20240717` (nightly-build). Also, I provided that a [colab notebook](https://colab.research.google.com/drive/1PXsrbcckN5x_ooMjZr5Vds6KVKMve7ph?usp=sharing) to reproduce the error.



### Standalone code to reproduce the issue

```shell
import tensorflow as tf

key = tf.random.uniform([15, 8], minval=-10, maxval=10, dtype=tf.int64)
indices = tf.random.uniform([1], minval=-10, maxval=10, dtype=tf.int32)
tf.raw_ops.MapUnstage(capacity=100,memory_limit=100,dtypes=[tf.float64],
                      container="""",shared_name="""",key=key,indices=indices)
```


### Relevant log output

```shell
2024-07-22 22:05:52.420257: F tensorflow/core/framework/tensor.cc:852] Check failed: 1 == NumElements() (1 vs. 120)Must have a one element tensor
Aborted (core dumped)
```
",2
Wheel built for wrong python version (3.11 not 3.10),"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.17

### Custom code

No

### OS platform and distribution

Ubuntu 22.04.4 LTS

### Mobile device

_No response_

### Python version

3.10

### Bazel version

6.5.0

### GCC/compiler version

clang version 17.0.6 (++20231209124227+6009708b4367-1~exp1~20231209124336.77)

### CUDA/cuDNN version

12.3/8.9.7.29

### GPU model and memory

NVIDIA GeForce GTX 1070 8192MiB

### Current behavior?

```
pip install bazel-bin/tensorflow/tools/pip_package/wheel_house/tensorflow-2.17.0-cp311-cp311-linux_x86_64.whl 
Defaulting to user installation because normal site-packages is not writeable
ERROR: tensorflow-2.17.0-cp311-cp311-linux_x86_64.whl is not a supported wheel on this platform.
```
Python is 3.10 but the wheel generated is for 3.11 but that's not on the system:
```
python --version
Python 3.10.12
```
I cannot find a commandline option for bazel to change the minor version to 3.10. Is there any? Or how can I force bazel to generate a whell for 3.10?

### Standalone code to reproduce the issue

```shell
./configure
bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow --config=cuda --copt=-Wno-error=unused-command-line-argument
```


### Relevant log output

```shell
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Reading 'startup' options from /home/bernd/tensorflow/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=161
INFO: Reading rc options for 'build' from /home/bernd/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/bernd/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /home/bernd/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-12.3 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1 --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/lib/llvm-17/bin/clang --copt=-Wno-gnu-offsetof-extensions --config=cuda_clang
INFO: Found applicable config definition build:short_logs in file /home/bernd/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/bernd/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda_clang in file /home/bernd/tensorflow/.bazelrc: --config=cuda --action_env=TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang --repo_env=TF_CUDA_COMPUTE_CAPABILITIES=sm_60,sm_70,sm_80,sm_89,compute_90
INFO: Found applicable config definition build:cuda in file /home/bernd/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:cuda_clang in file /home/bernd/tensorflow/.bazelrc: --config=cuda --action_env=TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang --repo_env=TF_CUDA_COMPUTE_CAPABILITIES=sm_60,sm_70,sm_80,sm_89,compute_90
INFO: Found applicable config definition build:cuda in file /home/bernd/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:cuda in file /home/bernd/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:linux in file /home/bernd/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/bernd/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Analyzed target //tensorflow/tools/pip_package:wheel (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
Target //tensorflow/tools/pip_package:wheel up-to-date:
  bazel-bin/tensorflow/tools/pip_package/wheel_house
INFO: Elapsed time: 1.625s, Critical Path: 0.00s
INFO: 1 process: 1 internal.
INFO: Build completed successfully, 1 total action
```
",1
TF timeline timestamp was shifted. The resultant timeline cannot be shown correctly in chrome tracing viewer,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

TF2.12

### Custom code

No

### OS platform and distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA 12, cuDNN 8

### GPU model and memory

_No response_

### Current behavior?

When profiling on GPU. Timestamp in Chrome trace format was incorrect due to shifting. The relevant commit is https://github.com/tensorflow/tensorflow/commit/701c1e97317ace357621b7f0bd4a2e427f16ed42#r144411330

There was a issue on  https://github.com/tensorflow/profiler/issues/238, but does not fix.

**Incorrect** timeline if `CollectData` was called:
https://github.com/tensorflow/tensorflow/blob/aeeeef0ba125dd2b28b59c5d144dd0a237a780c4/tensorflow/core/profiler/lib/device_profiler_session.h#L56-L59

**Correct** timelie if `CollectDataInternal` was called:
https://github.com/tensorflow/tensorflow/blob/a3e2c692c18649329c4210cf8df2487d2028e267/tensorflow/core/profiler/lib/device_profiler_session.h#L56-L59

The shifting was called in `PostProcessSingleHostXSpace` in `CollectData`
https://github.com/tensorflow/tensorflow/blob/aeeeef0ba125dd2b28b59c5d144dd0a237a780c4/third_party/xla/third_party/tsl/tsl/profiler/lib/profiler_session.cc#L81-L88
Is shifting expected behavior? If that, how to use the profiler session correctly?

### Standalone code to reproduce the issue

```shell
from absl import app
import logging

import tensorflow as tf
from tensorflow.python.client import timeline

tf.compat.v1.disable_eager_execution()

def test_profiler():
  shape = tf.constant([1000, 1000], dtype=tf.int64)
  x = tf.random.normal(shape)
  y = x ** 2
  z = y ** 2
  with tf.compat.v1.Session() as sess:
    run_options = tf.compat.v1.RunOptions(trace_level=tf.compat.v1.RunOptions.FULL_TRACE)
    run_metadata = tf.compat.v1.RunMetadata()
    #for i in range(3):
    sess.run(z, options=run_options, run_metadata=run_metadata)
    #logging.info(""run_metadata.step_stats:%s"", run_metadata.step_stats)
    tl = timeline.Timeline(run_metadata.step_stats)
    ctf = tl.generate_chrome_trace_format()
    with open('timeline.json', 'w') as f:
      f.write(ctf)

def main(argv):
  test_profiler()


if __name__ == ""__main__"":
  app.run(main)
```


### Relevant log output

_No response_",1
Error: 'class tensorflow::tools::proto_splitter::SavedModelSplitter' has no member named 'WriteToCord',"Facing below failure while executing test suite on Tensorflow master. 
Command used for test execution: 
`bazel build --define --build_tag_filters=-no_oss,-oss_excluded,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --test_tag_filters=-no_oss,-oss_excluded,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only  --test_size_filters=small,medium --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS -- //tensorflow/... -//tensorflow/compiler/tf2tensorrt/... -//tensorflow/core/tpu/... -//tensorflow/lite/... -//tensorflow/tools/toolchains/... `

gcc --version: 
gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
```
ERROR: /root/tensorflow/tensorflow/cc/saved_model/image_format/BUILD:21:11: Compiling tensorflow/cc/saved_model/image_format/internal_api.cc failed: (Exit 1): gcc failed: error executing command (from target //tensorflow/cc/saved_model/image_format:internal_api) /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 165 arguments skipped)
In file included from ./tensorflow/tools/proto_splitter/cc/composable_splitter.h:20,
                 from ./tensorflow/tools/proto_splitter/cc/saved_model_splitter.h:18,
                 from tensorflow/cc/saved_model/image_format/internal_api.cc:33:
./tensorflow/tools/proto_splitter/cc/composable_splitter_base.h:33: warning: ""IS_OSS"" redefined
   33 | #define IS_OSS true
      |
In file included from tensorflow/cc/saved_model/image_format/internal_api.cc:16:
./tensorflow/cc/saved_model/image_format/internal_api.h:27: note: this is the location of the previous definition
   27 | #define IS_OSS false
      |
tensorflow/cc/saved_model/image_format/internal_api.cc:36: warning: ""IS_OSS"" redefined
   36 | #define IS_OSS false
      |
In file included from ./tensorflow/tools/proto_splitter/cc/composable_splitter.h:20,
                 from ./tensorflow/tools/proto_splitter/cc/saved_model_splitter.h:18,
                 from tensorflow/cc/saved_model/image_format/internal_api.cc:33:
./tensorflow/tools/proto_splitter/cc/composable_splitter_base.h:33: note: this is the location of the previous definition
   33 | #define IS_OSS true
      |
tensorflow/cc/saved_model/image_format/internal_api.cc: In function 'absl::lts_20230802::StatusOr<std::tuple<absl::lts_20230802::Cord, bool> > tensorflow::image_format::WriteSavedModelToCord(tensorflow::SavedModel*)':
tensorflow/cc/saved_model/image_format/internal_api.cc:126:19: error: 'class tensorflow::tools::proto_splitter::SavedModelSplitter' has no member named 'WriteToCord'; did you mean 'WriteToString'?
  126 |   return splitter.WriteToCord();
      |                   ^~~~~~~~~~~
      |                   WriteToString
INFO: Elapsed time: 2594.461s, Critical Path: 70.16s
INFO: 14278 processes: 5059 internal, 9219 local.
FAILED: Build did NOT complete successfully
```",1
Build Error on aarch64 AWS Graviton3 with Ubuntu 22.04 for TensorFlow v2.17.0 with mkl_aarch64,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.17.0

### Custom code

No

### OS platform and distribution

Ubuntu 22.04.2 LTS

### Mobile device

_No response_

### Python version

3.11.9

### Bazel version

6.5.0

### GCC/compiler version

clang version 17.0.6

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I'm encountering build errors when trying to build TensorFlow v2.17.0 from source on an AWS Graviton3 instance (aarch64 architecture) running Ubuntu 22.04. The build fails with errors related to the **MakeOneDnnStream** function.

Expected behavior: The build should complete successfully without any errors.

### Standalone code to reproduce the issue

```shell
1. Set up an AWS Graviton3 instance with Ubuntu 22.04.
2. Clone the TensorFlow repository and checkout version 2.17.0:
   
   git clone https://github.com/tensorflow/tensorflow.git
   cd tensorflow
   git checkout v2.17.0
   
3. Install Bazel and other dependencies as per the TensorFlow build documentation.
4. Run the build command:
   ```bash
   bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu --features=-layering_check --config=mkl_aarch64 --config=opt --copt=-march=armv8-a+sve --copt=-msve-vector-bits=256 --copt=-O3 --copt=-Wno-gnu-offsetof-extensions --copt=-Wno-unused-but-set-variable --jobs=48 --local_cpu_resources=26 --verbose_failures
   ```

Any guidance or fixes to resolve this build error would be greatly appreciated.
```


### Relevant log output

```shell
(tf-venv) user@ip-xxx-xx-x-xxx:~/work_dirr/tensorflow$ taskset -c 6-31 bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu --features=-layering_check --config=mkl_aarch64 --config=opt --copt=-march=armv8-a+sve --copt=-msve-vector-bits=256 --copt=-O3 --copt=-Wno-gnu-offsetof-extensions --copt=-Wno-unused-but-set-variable  --jobs=48 --local_cpu_resources=26 --verbose_failures
INFO: Reading 'startup' options from /home/deepesh/work_dirr/tensorflow/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=113
INFO: Reading rc options for 'build' from /home/deepesh/work_dirr/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/deepesh/work_dirr/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /home/deepesh/work_dirr/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/deepesh/work_dirr/tf-venv/bin/python3 --action_env PYTHON_LIB_PATH=/home/deepesh/work_dirr/tf-venv/lib/python3.11/site-packages --python_path=/home/deepesh/work_dirr/tf-venv/bin/python3 --action_env CLANG_COMPILER_PATH=/usr/lib/llvm-17/bin/clang --repo_env=CC=/usr/lib/llvm-17/bin/clang --repo_env=BAZEL_COMPILER=/usr/lib/llvm-17/bin/clang --copt=-Wno-gnu-offsetof-extensions
INFO: Found applicable config definition build:short_logs in file /home/deepesh/work_dirr/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/deepesh/work_dirr/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:mkl_aarch64 in file /home/deepesh/work_dirr/tensorflow/.bazelrc: --define=build_with_mkl_aarch64=true --define=build_with_openmp=true --define=build_with_acl=true -c opt
INFO: Found applicable config definition build:opt in file /home/deepesh/work_dirr/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare
INFO: Found applicable config definition build:linux in file /home/deepesh/work_dirr/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/deepesh/work_dirr/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Analyzed target //tensorflow/tools/pip_package:wheel (1 packages loaded, 3758 targets configured).
INFO: Found 1 target...
ERROR: /home/deepesh/.cache/bazel/_bazel_deepesh/c2d183634e6ef66bd4ea91e213a12542/external/local_xla/xla/service/cpu/BUILD:1665:11: Compiling xla/service/cpu/onednn_matmul.cc failed: (Exit 1): clang failed: error executing command (from target @local_xla//xla/service/cpu:onednn_matmul) 
  (cd /home/deepesh/.cache/bazel/_bazel_deepesh/c2d183634e6ef66bd4ea91e213a12542/execroot/org_tensorflow && \
  exec env - \
    CLANG_COMPILER_PATH=/usr/lib/llvm-17/bin/clang \
    PATH=/home/deepesh/.cache/bazelisk/downloads/bazelbuild/bazel-6.5.0-linux-arm64/bin:/home/deepesh/work_dirr/tf-venv/bin:/home/deepesh/.vscode-server/cli/servers/Stable-f1e16e1e6214d7c44d078b1f0607b2388f29d729/server/bin/remote-cli:/home/deepesh/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/home/deepesh/work_dirr/tf-venv/bin/python3 \
    PYTHON_LIB_PATH=/home/deepesh/work_dirr/tf-venv/lib/python3.11/site-packages \
    TF2_BEHAVIOR=1 \
  /usr/lib/llvm-17/bin/clang -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -Wunused-but-set-parameter -Wno-free-nonheap-object -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++14' -MD -MF bazel-out/aarch64-opt/bin/external/local_xla/xla/service/cpu/_objs/onednn_matmul/onednn_matmul.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/external/local_xla/xla/service/cpu/_objs/onednn_matmul/onednn_matmul.pic.o' -fPIC '-DEIGEN_MAX_ALIGN_BYTES=64' -DEIGEN_ALLOW_UNALIGNED_SCALARS '-DEIGEN_USE_AVX512_GEMM_KERNELS=0' -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY '-DLLVM_ON_UNIX=1' '-DHAVE_BACKTRACE=1' '-DBACKTRACE_HEADER=<execinfo.h>' '-DLTDL_SHLIB_EXT="".so""' '-DLLVM_PLUGIN_EXT="".so""' '-DLLVM_ENABLE_THREADS=1' '-DHAVE_DEREGISTER_FRAME=1' '-DHAVE_LIBPTHREAD=1' '-DHAVE_PTHREAD_GETNAME_NP=1' '-DHAVE_PTHREAD_H=1' '-DHAVE_PTHREAD_SETNAME_NP=1' '-DHAVE_REGISTER_FRAME=1' '-DHAVE_SETENV_R=1' '-DHAVE_STRERROR_R=1' '-DHAVE_SYSEXITS_H=1' '-DHAVE_UNISTD_H=1' -D_GNU_SOURCE '-DHAVE_LINK_H=1' '-DHAVE_MALLINFO=1' '-DHAVE_SBRK=1' '-DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' -DHAVE_BUILTIN_THREAD_POINTER '-DLLVM_NATIVE_ARCH=""AArch64""' '-DLLVM_NATIVE_ASMPARSER=LLVMInitializeAArch64AsmParser' '-DLLVM_NATIVE_ASMPRINTER=LLVMInitializeAArch64AsmPrinter' '-DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeAArch64Disassembler' '-DLLVM_NATIVE_TARGET=LLVMInitializeAArch64Target' '-DLLVM_NATIVE_TARGETINFO=LLVMInitializeAArch64TargetInfo' '-DLLVM_NATIVE_TARGETMC=LLVMInitializeAArch64TargetMC' '-DLLVM_NATIVE_TARGETMCA=LLVMInitializeAArch64TargetMCA' '-DLLVM_HOST_TRIPLE=""aarch64-unknown-linux-gnu""' '-DLLVM_DEFAULT_TARGET_TRIPLE=""aarch64-unknown-linux-gnu""' '-DLLVM_VERSION_MAJOR=19' '-DLLVM_VERSION_MINOR=0' '-DLLVM_VERSION_PATCH=0' '-DLLVM_VERSION_STRING=""19.0.0git""' -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS '-DBLAKE3_USE_NEON=0' -DBLAKE3_NO_AVX2 -DBLAKE3_NO_AVX512 -DBLAKE3_NO_SSE2 -DBLAKE3_NO_SSE41 -DENABLE_NEON -DARM_COMPUTE_CPU_ENABLED -DARM_COMPUTE_ENABLE_NEON -DARM_COMPUTE_ENABLE_I8MM -DENABLE_FP32_KERNELS -DENABLE_QASYMM8_KERNELS -DENABLE_QASYMM8_SIGNED_KERNELS -DENABLE_QSYMM16_KERNELS -DENABLE_INTEGER_KERNELS -DENABLE_NHWC_KERNELS -DENABLE_NCHW_KERNELS -DARM_COMPUTE_GRAPH_ENABLED -DARM_COMPUTE_ENABLE_SVEF32MM -DARM_COMPUTE_ENABLE_FIXED_FORMAT_KERNELS -D_GLIBCXX_USE_NANOSLEEP -DARM_COMPUTE_OPENMP_SCHEDULER '-DDNNL_AARCH64_USE_ACL=1' '-DBAZEL_CURRENT_REPOSITORY=""local_xla""' -iquote external/local_xla -iquote bazel-out/aarch64-opt/bin/external/local_xla -iquote external/com_google_protobuf -iquote bazel-out/aarch64-opt/bin/external/com_google_protobuf -iquote external/com_google_absl -iquote bazel-out/aarch64-opt/bin/external/com_google_absl -iquote external/eigen_archive -iquote bazel-out/aarch64-opt/bin/external/eigen_archive -iquote external/local_tsl -iquote bazel-out/aarch64-opt/bin/external/local_tsl -iquote external/ml_dtypes -iquote bazel-out/aarch64-opt/bin/external/ml_dtypes -iquote external/nsync -iquote bazel-out/aarch64-opt/bin/external/nsync -iquote external/double_conversion -iquote bazel-out/aarch64-opt/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/aarch64-opt/bin/external/snappy -iquote external/com_googlesource_code_re2 -iquote bazel-out/aarch64-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/aarch64-opt/bin/external/farmhash_archive -iquote external/llvm-project -iquote bazel-out/aarch64-opt/bin/external/llvm-project -iquote external/zlib -iquote bazel-out/aarch64-opt/bin/external/zlib -iquote external/mkl_dnn_acl_compatible -iquote bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible -iquote external/compute_library -iquote bazel-out/aarch64-opt/bin/external/compute_library -Ibazel-out/aarch64-opt/bin/external/ml_dtypes/_virtual_includes/float8 -Ibazel-out/aarch64-opt/bin/external/ml_dtypes/_virtual_includes/intn -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/ArithCanonicalizationIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/AsmParserTokenKinds -Ibazel-out/aarch64-opt/bin/external/compute_library/include/_virtual_includes/include -isystem external/com_google_protobuf/src -isystem bazel-out/aarch64-opt/bin/external/com_google_protobuf/src -isystem external/eigen_archive -isystem bazel-out/aarch64-opt/bin/external/eigen_archive -isystem external/eigen_archive/mkl_include -isystem bazel-out/aarch64-opt/bin/external/eigen_archive/mkl_include -isystem external/ml_dtypes -isystem bazel-out/aarch64-opt/bin/external/ml_dtypes -isystem external/ml_dtypes/ml_dtypes -isystem bazel-out/aarch64-opt/bin/external/ml_dtypes/ml_dtypes -isystem external/nsync/public -isystem bazel-out/aarch64-opt/bin/external/nsync/public -isystem external/farmhash_archive/src -isystem bazel-out/aarch64-opt/bin/external/farmhash_archive/src -isystem external/llvm-project/llvm/include -isystem bazel-out/aarch64-opt/bin/external/llvm-project/llvm/include -isystem external/zlib -isystem bazel-out/aarch64-opt/bin/external/zlib -isystem external/llvm-project/mlir/include -isystem bazel-out/aarch64-opt/bin/external/llvm-project/mlir/include -isystem external/mkl_dnn_acl_compatible/include -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/include -isystem external/mkl_dnn_acl_compatible/src -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src -isystem external/mkl_dnn_acl_compatible/src/common -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src/common -isystem external/mkl_dnn_acl_compatible/src/cpu -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src/cpu -isystem external/mkl_dnn_acl_compatible/src/cpu/aarch64/xbyak_aarch64/src -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src/cpu/aarch64/xbyak_aarch64/src -isystem external/mkl_dnn_acl_compatible/src/cpu/aarch64/xbyak_aarch64/xbyak_aarch64 -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src/cpu/aarch64/xbyak_aarch64/xbyak_aarch64 -isystem external/mkl_dnn_acl_compatible/src/cpu/gemm -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src/cpu/gemm -isystem external/compute_library/arm_compute/runtime -isystem bazel-out/aarch64-opt/bin/external/compute_library/arm_compute/runtime -isystem external/compute_library/src/core/NEON/kernels/assembly -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/NEON/kernels/assembly -isystem external/compute_library/src/core/NEON/kernels/convolution/common -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/NEON/kernels/convolution/common -isystem external/compute_library/src/core/NEON/kernels/convolution/winograd -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/NEON/kernels/convolution/winograd -isystem external/compute_library/src/core/cpu/kernels/assembly -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/cpu/kernels/assembly -isystem external/compute_library/src/cpu/kernels/assembly -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/cpu/kernels/assembly -isystem external/compute_library/src/core/NEON/kernels/arm_conv -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/NEON/kernels/arm_conv -isystem external/compute_library/src/core/NEON/kernels/arm_gemm -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/NEON/kernels/arm_gemm -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -Wno-gnu-offsetof-extensions -Wno-sign-compare '-march=armv8-a+sve' '-msve-vector-bits=256' -O3 -Wno-gnu-offsetof-extensions -Wno-unused-but-set-variable '-std=c++17' -DEIGEN_AVOID_STL_ARRAY -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DTENSORFLOW_USE_XLA=1' -DINTEL_MKL -DENABLE_ONEDNN_V3 '-DDNNL_AARCH64_USE_ACL=1' -DENABLE_ONEDNN_OPENMP '-DXLA_CPU_USE_ACL=1' -fexceptions -pthread -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/local_xla/xla/service/cpu/onednn_matmul.cc -o bazel-out/aarch64-opt/bin/external/local_xla/xla/service/cpu/_objs/onednn_matmul/onednn_matmul.pic.o)
# Configuration: 389168362472d4b0f209a68c4f568adffc1a33297c68acb48d71fb4bd10724d2
# Execution platform: @local_execution_config_platform//:platform
In file included from external/local_xla/xla/service/cpu/onednn_matmul.cc:31:
In file included from external/local_xla/xla/hlo/ir/hlo_instructions.h:37:
In file included from external/local_xla/xla/hlo/ir/hlo_computation.h:31:
external/com_google_absl/absl/log/log.h:199:9: warning: 'LOG' macro redefined [-Wmacro-redefined]
  199 | #define LOG(severity) ABSL_LOG_INTERNAL_LOG_IMPL(_##severity)
      |         ^
external/local_tsl/tsl/platform/default/logging.h:165:9: note: previous definition is here
  165 | #define LOG(severity) _TF_LOG_##severity
      |         ^
In file included from external/local_xla/xla/service/cpu/onednn_matmul.cc:31:
In file included from external/local_xla/xla/hlo/ir/hlo_instructions.h:37:
In file included from external/local_xla/xla/hlo/ir/hlo_computation.h:31:
external/com_google_absl/absl/log/log.h:237:9: warning: 'LOG_EVERY_N' macro redefined [-Wmacro-redefined]
  237 | #define LOG_EVERY_N(severity, n) \
      |         ^
external/local_tsl/tsl/platform/default/logging.h:278:9: note: previous definition is here
  278 | #define LOG_EVERY_N(severity, n)                       \
      |         ^
In file included from external/local_xla/xla/service/cpu/onednn_matmul.cc:31:
In file included from external/local_xla/xla/hlo/ir/hlo_instructions.h:37:
In file included from external/local_xla/xla/hlo/ir/hlo_computation.h:31:
external/com_google_absl/absl/log/log.h:245:9: warning: 'LOG_FIRST_N' macro redefined [-Wmacro-redefined]
  245 | #define LOG_FIRST_N(severity, n) \
      |         ^
external/local_tsl/tsl/platform/default/logging.h:284:9: note: previous definition is here
  284 | #define LOG_FIRST_N(severity, n)                       \
      |         ^
In file included from external/local_xla/xla/service/cpu/onednn_matmul.cc:31:
In file included from external/local_xla/xla/hlo/ir/hlo_instructions.h:37:
In file included from external/local_xla/xla/hlo/ir/hlo_computation.h:31:
external/com_google_absl/absl/log/log.h:253:9: warning: 'LOG_EVERY_POW_2' macro redefined [-Wmacro-redefined]
  253 | #define LOG_EVERY_POW_2(severity) \
      |         ^
external/local_tsl/tsl/platform/default/logging.h:290:9: note: previous definition is here
  290 | #define LOG_EVERY_POW_2(severity)                         \
      |         ^
In file included from external/local_xla/xla/service/cpu/onednn_matmul.cc:31:
In file included from external/local_xla/xla/hlo/ir/hlo_instructions.h:37:
In file included from external/local_xla/xla/hlo/ir/hlo_computation.h:31:
external/com_google_absl/absl/log/log.h:265:9: warning: 'LOG_EVERY_N_SEC' macro redefined [-Wmacro-redefined]
  265 | #define LOG_EVERY_N_SEC(severity, n_seconds) \
      |         ^
external/local_tsl/tsl/platform/default/logging.h:300:9: note: previous definition is here
  300 | #define LOG_EVERY_N_SEC(severity, n_seconds)                      \
      |         ^
external/local_xla/xla/service/cpu/onednn_matmul.cc:270:24: error: no matching function for call to 'MakeOneDnnStream'
  270 |   auto onednn_stream = MakeOneDnnStream(cpu_engine, thread_pool.get());
      |                        ^~~~~~~~~~~~~~~~
external/local_xla/xla/service/cpu/onednn_util.h:57:14: note: candidate function not viable: no known conversion from 'pointer' (aka 'tsl::OneDnnThreadPool *') to 'dnnl::threadpool_interop::threadpool_iface *' for 2nd argument
   57 | dnnl::stream MakeOneDnnStream(
      |              ^
   58 |     const dnnl::engine& cpu_engine,
   59 |     dnnl::threadpool_interop::threadpool_iface* thread_pool);
      |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/service/cpu/onednn_matmul.cc:359:24: error: no matching function for call to 'MakeOneDnnStream'
  359 |   auto onednn_stream = MakeOneDnnStream(cpu_engine, thread_pool.get());
      |                        ^~~~~~~~~~~~~~~~
external/local_xla/xla/service/cpu/onednn_util.h:57:14: note: candidate function not viable: no known conversion from 'pointer' (aka 'tsl::OneDnnThreadPool *') to 'dnnl::threadpool_interop::threadpool_iface *' for 2nd argument
   57 | dnnl::stream MakeOneDnnStream(
      |              ^
   58 |     const dnnl::engine& cpu_engine,
   59 |     dnnl::threadpool_interop::threadpool_iface* thread_pool);
      |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
5 warnings and 2 errors generated.
Target //tensorflow/tools/pip_package:wheel failed to build
INFO: Elapsed time: 720.177s, Critical Path: 142.22s
INFO: 3322 processes: 82 internal, 3240 local.
FAILED: Build did NOT complete successfully
```
",1
"Memory usage with tf.data pipeline (HDF5, TFRecords)","### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.4 LTS

### Mobile device

_No response_

### Python version

3.11.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.3

### GPU model and memory

RTX A6000 Ada

### Current behavior?

Hello.

I am wondering about memory usage for the Tensorflow data API. I am running into OOM issues when I run on very large (360,000 files, >40 MB per file) files. I cannot even iterate through the dataset appropriately; forget about training a (Keras) model.

Here is memory usage (using `psutil` and `memory_info().rss`) over 5 ""epochs"" where I iterate over a dummy HDF5 dataset. The drops that create the sawtooth pattern are each ""epoch,"" where I just iterate over the dataset 5 times.

![image](https://github.com/user-attachments/assets/93e2f201-9c3a-411f-83c6-5861d378fdd1)

Is this expected behavior? I can repeat this memory curve with `TFRecordDataset` with and without `interleave`.

[Gist for creating dummy hdf5 files](https://gist.github.com/dryglicki/dec05492b73416e3829a0440c6024793)
[Gist for reading dummy hdf5 files with Dataset API](https://gist.github.com/dryglicki/cf0a52dd31af3358d5e3cd5351c51f13)

Be warned, that to get the plotting to work without wrecking Tensorflow, you need to use matplotlib <3.8, since that is when it incorporates Numpy 2.0 -- I used MPL v3.7.3.

### Standalone code to reproduce the issue

```shell
#!/usr/bin/env python3

import os ; import sys
from pathlib import Path
import glob

import tensorflow as tf

import matplotlib as mpl
mpl.use('Agg')
import matplotlib.pyplot as plt

import h5py
import psutil
import gc


def unpack_hdf5(hdf5_file):
    '''
    Simple and quick unpack of an hdf5 dummy file for tensorflow.
    '''
    with h5py.File(hdf5_file.numpy().decode('utf-8'), 'r') as h5:
        priors = tf.convert_to_tensor(h5.get('priors')[...], tf.float32)
        forecast = tf.convert_to_tensor(h5.get('forecast')[...], tf.float32)
        model = tf.convert_to_tensor(h5.get('model')[...], tf.float32)

    return priors, model, forecast

def interleave_wrapper(hdf5_file,
        prior_shape,
        model_shape,
        forecast_shape):
    '''
    Wrapper for py_function to work with interleave. Interleave requires a dataset upon output.
    '''
    p, m, f = tf.py_function(unpack_hdf5, [hdf5_file],
                             Tout = [tf.float32, tf.float32, tf.float32])

    p.set_shape(prior_shape)
    m.set_shape(model_shape)
    f.set_shape(forecast_shape)

    return tf.data.Dataset.zip( (tf.data.Dataset.from_tensors((p,m)).map(
        lambda p, m: {'priors' : p, 'model' : m}, num_parallel_calls = tf.data.AUTOTUNE),
        tf.data.Dataset.from_tensors(f)) )

def create_dataset_interleave(file_list,
        priors_shape,
        model_shape,
        forecast_shape,
        batch_size = 32):
    '''
    Create a dataset using pre-globbed file list
    '''

    return tf.data.Dataset.from_tensor_slices(file_list).interleave(
            lambda x: interleave_wrapper(x, priors_shape, model_shape, forecast_shape),
            cycle_length = tf.data.AUTOTUNE,
            num_parallel_calls = tf.data.AUTOTUNE,
            deterministic = False).batch(
                    batch_size, drop_remainder=True)

def main():
    '''
    Program to monitor memory usage of HDF5 file reader.
    '''
    p = psutil.Process(os.getpid())
    hdf_directory = 'hdf_files'
    file_list = list(glob.glob(f'{hdf_directory}/*hdf5'))
    print(len(file_list))

    test_file = file_list[0]
    hdf_vars = ['priors', 'model', 'forecast']
    shapes = {}
    with h5py.File(test_file, 'r') as h5:
        for var in hdf_vars:
            dict_var = f'{var}_shape'
            shapes[dict_var] = h5[var].shape

    ds = create_dataset_interleave(file_list, **shapes)

    rssUse = []
    batch_numbers = []
    for epoch in range(5):
        for ii, batch in enumerate(ds):
            batch_numbers.append((epoch+1)*(ii+1))
            X, Y = batch
            print('Batch: ',ii)
            print(X['model'].shape)
            rssUse.append(p.memory_info().rss / (1024 ** 2))
        gc.collect()

    for b, r in zip(batch_numbers, rssUse):
        print(f'{b:04d}: {r}')

    fig = plt.figure(figsize = (4,3))
    ax = fig.add_subplot()

    ax.plot(rssUse, label = 'rss')
    ax.legend()
    ax.set_ylabel('Memory usage [MB]')
    ax.set_xlabel('Step')

    fig.savefig('memory_use_hdf5.png', dpi=200, bbox_inches='tight')


if __name__ == '__main__':
    main()
```


### Relevant log output

_No response_",1
`tf.data.Dataset.prefetch()` error with basic usage,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The most basic usage of `tf.data.Dataset.prefetch()` raises an error. 

The `buffer_size` argument is documented as requiring a int64 tensor:

> `buffer_size` 	
> A [tf.int64](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [tf.Tensor](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the maximum number of elements that will be buffered when prefetching. If the value [tf.data.AUTOTUNE](https://www.tensorflow.org/api_docs/python/tf/data#AUTOTUNE) is used, then the buffer size is dynamically tuned.

https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch

Probably related to https://github.com/tensorflow/tensorflow/issues/71744, The ""eager fallback"" codepath is broken.

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

x = np.arange(5)
y = np.arange(5)

ds = tf.data.Dataset.from_tensor_slices((x, y)).batch(2)

ds.prefetch(tf.constant(1, dtype = 'int64'))
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1259, in prefetch
    return prefetch_op._prefetch(  # pylint: disable=protected-access
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/data/ops/prefetch_op.py"", line 28, in _prefetch
    return _PrefetchDataset(input_dataset, buffer_size, name=name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/data/ops/prefetch_op.py"", line 46, in __init__
    variant_tensor = gen_dataset_ops.prefetch_dataset(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 6001, in prefetch_dataset
    return prefetch_dataset_eager_fallback(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 6072, in prefetch_dataset_eager_fallback
    legacy_autotune = _execute.make_bool(legacy_autotune, ""legacy_autotune"")
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/eager/execute.py"", line 172, in make_bool
    raise TypeError(""Expected bool for argument '%s' not %s."" %
TypeError: Expected bool for argument 'legacy_autotune' not <tf.Tensor: shape=(), dtype=bool, numpy=False>.
```
",1
Tensorflow distributed + DTensor approach for large outer product,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.9

### Custom code

Yes

### OS platform and distribution

MacOS

### Mobile device

N/A

### Python version

3.10

### Bazel version

N/A

### GCC/compiler version

N/A

### CUDA/cuDNN version

N/A

### GPU model and memory

N/A

### Current behavior?

Hey,

I am computing a large matrix of size $(\mathrm{N_{samples}}, \mathrm{N_{parameters}})$ , which corresponds to a jacobian for each sample. I am obtaining this matrix by using a distributed strategy: `tf.distribute.MirroredStrategy` and calculating $N_\mathrm{samples}/N_\mathrm{devices}$ jacobians on all my devices. I then gather along the first axis to construct the full matrix $X$ from which I calculate the matrix $Y = X X^T$ of size $(\mathrm{N_{samples}} ,\mathrm{N_{samples}})$. 

I am looking for a faster way to perform this calculation by sharding along the direction of $N_\mathrm{parameters}$ and performing the calculation like this:

$$
X X^T =\sum^{\mathrm{N_gpus}−1}_{g=0} X_g X_g^T
$$

See this reference for the specific layout:
<img width=""887"" alt=""image"" src=""https://github.com/user-attachments/assets/47a117c4-906a-4fa3-bf7d-832de810ed7c"">

What I tried is to calculate my jacobians with `strategy.run`, gather them and then copy to a mesh to perform the matrix multiplication. However, I feel like this approach is probably not optimal. 

What is the best way to achieve the same within tensorflow's distributed framework? Can I do something similar to MPI's all-to-all primitive to reorder the data across all my devices and perform the matmul described above?

I added a MWE below that outlines my thinking, in my actual code `train_step` involves a complicated calculation and a call to `tf.jacobians` over all my trainable variables.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.experimental import dtensor
import numpy as np

print('TensorFlow version:', tf.__version__)


def configure_virtual_cpus(ncpu):
    phy_devices = tf.config.list_physical_devices('CPU')
    tf.config.set_logical_device_configuration(phy_devices[0], [
        tf.config.LogicalDeviceConfiguration(),
    ] * ncpu)

# Create virtual devices
number_of_devices = 6
configure_virtual_cpus(number_of_devices)
DEVICES = [f'CPU:{i}' for i in range(number_of_devices)]
# Use a mirrorer strategy
strategy = tf.distribute.MirroredStrategy(DEVICES)

number_of_samples_per_device = 4
# Make sure that the number of parameters is divisble by the number of devices
number_of_parameters = 2 * number_of_devices
# Create 1D mesh
mesh = dtensor.create_mesh([(""x"", number_of_devices), ], devices=DEVICES)
# Shard along Parameter direction
layout = dtensor.Layout([dtensor.UNSHARDED, 'x'], mesh)
tf.random.set_seed(1000)


@tf.function()
def train_step():
    # We create a tensor of (Ns, Np,) as a fictional Jacobian
    return tf.random.uniform((number_of_samples_per_device, number_of_parameters))


@tf.function()
def distributed_step():
    params = strategy.run(train_step)
    # Gathering on axis 1 gives (Ns * N_devices, Np)
    return strategy.gather(params, axis=0)


jacobians = distributed_step()
# Calculate matmul without using a mesh
X_g_full = tf.matmul(jacobians, tf.transpose(jacobians))
# Create a mesh of size (N_devices, )
a = dtensor.copy_to_mesh(jacobians,
                         layout=dtensor.Layout.replicated(layout.mesh, rank=layout.rank))
# Each device gets (Ns * N_devices, Np / N_devices) samples
my_dtensor = dtensor.relayout(a, layout=layout)
for component_tensor in dtensor.unpack(my_dtensor):
    print(""Device:"", component_tensor.device, "","", component_tensor.shape)
# Perform matmul on mesh.
X_g = tf.matmul(my_dtensor, tf.transpose(my_dtensor))
# They match
print(np.allclose(X_g_full.numpy(), X_g.numpy()))
```


### Relevant log output

```shell
Device: /job:localhost/replica:0/task:0/device:CPU:0 , (24, 2)
Device: /job:localhost/replica:0/task:0/device:CPU:1 , (24, 2)
Device: /job:localhost/replica:0/task:0/device:CPU:2 , (24, 2)
Device: /job:localhost/replica:0/task:0/device:CPU:3 , (24, 2)
Device: /job:localhost/replica:0/task:0/device:CPU:4 , (24, 2)
Device: /job:localhost/replica:0/task:0/device:CPU:5 , (24, 2)
True
```
",1
wasm-ld: error: --shared-memory is disallowed by c_api.o because it was not compiled with 'atomics' or 'bulk-memory' features.,"https://github.com/emscripten-core/emsdk/issues/1424

`wasm_cc_binary`with `tensorflow/lite/examples/minimal` failed

my question is can `wasm_cc_binary` without `threads = ""emscripten""`

thank you very much in advance",1
`tf.data.Dataset.from_tensor_slices` allocates GPU RAM,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v2.17.0

### Custom code

No

### OS platform and distribution

Ubuntu 22.04.4 LTS

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Passing a numpy array to `tf.data.Dataset.from_tensor_slices()` attempts to allocate the dataset as a tensor on the GPU device, and raises an exception if there is not enough GPU RAM available. 

This only started with TF 2.17.0. In all previous TF versions, all `tf.data.Dataset` operations were always pinned to the CPU.

To reproduce, create a numpy array larger than can fit on the GPU, and attempt to create a `tf.data.Dataset` from it.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

gpu_ram_gb = 12 # adjust for size of GPU 

gb = gpu_ram_gb+1; dtype = ""float64""
size = (gb * 1024**3) // tf.dtypes.as_dtype(dtype).size

x = np.zeros((size,), dtype = dtype)

tf.data.Dataset.from_tensor_slices(x)
```


### Relevant log output

```shell
2024-07-12 08:20:42.771788: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-07-12 08:20:42.784893: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-07-12 08:20:42.788889: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-07-12 08:20:42.798174: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-07-12 08:20:43.492876: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1720786850.494498   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.527964   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.531351   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.535502   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.538786   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.541878   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.710210   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.711607   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.712908   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-07-12 08:20:50.714170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 34 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:03:00.0, compute capability: 7.5
2024-07-12 08:20:50.715606: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 13958643712 exceeds 10% of free system memory.
2024-07-12 08:21:05.997032: W external/local_tsl/tsl/framework/bfc_allocator.cc:482] Allocator (GPU_0_bfc) ran out of memory trying to allocate 13.00GiB (rounded to 13958643712)requested by op _EagerConst
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. 
Current allocation summary follows.
Current allocation summary follows.
2024-07-12 08:21:05.997054: I external/local_tsl/tsl/framework/bfc_allocator.cc:1039] BFCAllocator dump for GPU_0_bfc
2024-07-12 08:21:05.997064: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (256): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997072: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (512): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997079: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (1024): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997087: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (2048): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997093: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (4096): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997100: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (8192): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997107: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (16384): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997114: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (32768): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997121: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (65536): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997128: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (131072): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997135: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (262144): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997142: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (524288): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997148: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (1048576): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997155: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (2097152): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997162: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (4194304): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997169: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (8388608): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997176: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (16777216): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997183: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997190: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (67108864): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997197: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (134217728): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997203: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (268435456): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997212: I external/local_tsl/tsl/framework/bfc_allocator.cc:1062] Bin for 13.00GiB was 256.00MiB, Chunk State: 
2024-07-12 08:21:05.997219: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100]      Summary of in-use Chunks by size: 
2024-07-12 08:21:05.997225: I external/local_tsl/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 0B
2024-07-12 08:21:05.997232: I external/local_tsl/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 0 memory_limit_: 35651584 available bytes: 35651584 curr_region_allocation_bytes_: 35651584
2024-07-12 08:21:05.997241: I external/local_tsl/tsl/framework/bfc_allocator.cc:1114] Stats: 
Limit:                        35651584
InUse:                               0
MaxInUse:                            0
NumAllocs:                           0
MaxAllocSize:                        0
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2024-07-12 08:21:05.997248: W external/local_tsl/tsl/framework/bfc_allocator.cc:494] <allocator contains no memory>

Traceback (most recent call last):
  File ""/home/tomasz/github/rstudio/keras3/test.py"", line 16, in <module>
    tf.data.Dataset.from_tensor_slices(x)
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 826, in from_tensor_slices
    return from_tensor_slices_op._from_tensor_slices(tensors, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py"", line 25, in _from_tensor_slices
    return _TensorSliceDataset(tensors, name=name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py"", line 33, in __init__
    element = structure.normalize_element(element)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/data/util/structure.py"", line 134, in normalize_element
    ops.convert_to_tensor(t, name=""component_%d"" % i, dtype=dtype))
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/profiler/trace.py"", line 183, in wrapped
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/ops.py"", line 713, in convert_to_tensor
    return tensor_conversion_registry.convert(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion_registry.py"", line 234, in convert
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/constant_tensor_conversion.py"", line 29, in _constant_tensor_conversion_function
    return constant_op.constant(v, dtype=dtype, name=name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/ops/weak_tensor_ops.py"", line 142, in wrapper
    return op(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py"", line 276, in constant
    return _constant_impl(value, dtype, shape, name, verify_shape=False,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py"", line 289, in _constant_impl
    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py"", line 301, in _constant_eager_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py"", line 108, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
```
",1
Updgraded manylinux support?,"As per my understanding, manylinux 2014 (infact CentOS Linux 7) went EOL on June 30 2024.

I still see the tf-nightly-cpu wheels and latest 2.17 prerelease wheels use manylinux 2014.
Do you plan to support updated manylinux (probably 2_28). ?

I also see some notes for the same here: https://docs.google.com/document/d/1l6q1qzEyCzEwXloKZqwP7bmVO1SxeVRhPW3A0BeB4KE/edit#heading=h.u7w1oawlfu32

@MichaelHudgins , any thoughts on this? or any relevant contacts to this?

Refer : https://github.com/mayeut/pep600_compliance/blob/master/EOL.rst
Refer : https://www.centos.org/",0
Advisory GHSA-84mw-34w6-2q43 contains wrong POC codes,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

N/A

### Custom code

No

### OS platform and distribution

N/A

### Mobile device

N/A

### Python version

N/A

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

It seems that description of Gtihub advisory [GHSA-84mw-34w6-2q43](https://github.com/advisories/GHSA-84mw-34w6-2q43) contains POC codes from another advisory [GHSA-772p-x54p-hjrv](https://github.com/advisories/GHSA-772p-x54p-hjrv). This causes issues in understanding the vulnerability.

### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_",1
tf.matmul gives inconsistent results for same data,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.8.0

### Custom code

Yes

### OS platform and distribution

windows 11

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuda 11.8

### GPU model and memory

_No response_

### Current behavior?

I want to do matmul(A,transpose(A)) and matmul(B,transpose(B)). Here, B is just matrix A with additional rows. A is 8 by 512 and b is 64 by 512 with first 8 rows exactly as that of A.

let’s call
A_out = A matmul transpose(A), this is 8 by 8,
B_out = B matmul transpose(B), this is 64 by 64.

So the above matmul just dots each row of a matrix with all the other rows of the same matrix. So if the first 8 rows in B are same as A then the top-left, 8 by 8 sub matrix of B_out should exactly be same as A_out.

tf.matmul operation doesnt produce this. however numpy does.

Please look into this

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.keras.layers import Embedding

# Set up
embedding = Embedding(36711,512,mask_zero=True)
example_sequence = tf.constant([36710,  5095,   466, 16678,     5,     3,  5152, 36711] + [0]*(64-8))

pad_embed    = example_sequence # input with padding       # has shape (64,)
no_pad_embed = example_sequence[:8] # input without padding # has shape (8,)

no_pad_embed = embedding(no_pad_embed) # shape (8,512)  # A
pad_embed    = embedding(pad_embed)    # shape (64,512) # B : has same first 8 rows as A


# Real problem: I want to use matmul to do A @ A.T and B @ B.T
pad_out    = tf.matmul(pad_embed,pad_embed, transpose_b=True)
no_pad_out = tf.matmul(no_pad_embed,no_pad_embed, transpose_b=True)
#print(pad_out[:8,:8])
#print(no_pad_out)

print(tf.reduce_all(pad_out[:8,:8]==no_pad_out)) # False, meaning that the upper left 8 by 8 submatrix of pad_out is not equal to no_pad_out

import numpy as np

# Real problem: I want to use matmul to do A @ A.T and B @ B.T
pad_embed   = pad_embed.numpy()
no_pad_embed= no_pad_embed.numpy()

pad_out    = np.matmul(pad_embed,pad_embed.T)
no_pad_out = np.matmul(no_pad_embed,no_pad_embed.T)
#print(pad_out[:8,:8])
#print(no_pad_out)

print(tf.reduce_all(pad_out[:8,:8]==no_pad_out)) # True, meaning 8 by 8 submatrix of pad_out equals no_pad_out
```


### Relevant log output

```shell
tf.Tensor(False, shape=(), dtype=bool)
tf.Tensor(True, shape=(), dtype=bool)
```
",1
Kubernetes cluster resolver fails when running from within a K8S cluster.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

linux

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

If trying to create a cluster spec from a pod running within a K8s cluster, [this](https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/distribute/cluster_resolver/kubernetes_cluster_resolver.py#L90) try block fails because it can't find the kubectl config file.

The quick is rather straightforward:

```python

if override_client is None:
    try:
      from kubernetes import config as k8sconfig  # pylint: disable=g-import-not-at-top

      k8sconfig.load_kube_config()
    except ImportError:
      if not override_client:
        raise ImportError('The Kubernetes Python client must be installed '
                          'before using the Kubernetes Cluster Resolver. '
                          'To install the Kubernetes Python client, run '
                          '`pip install kubernetes` on your command line.')

...


```

Happy to open a MR for this.

### Standalone code to reproduce the issue

main.py
```python
import os
import tensorflow as tf

from absl import logging
from kubernetes import client, config

logging.set_verbosity(logging.DEBUG)
logging.info(""TF version: %s"", tf.__version__)

config.load_incluster_config()
k8s_cli = client.CoreV1Api()

# Fails here despite providing an override client for talking with the k8s APIs.
cluster_resolver = tf.distribute.cluster_resolver.KubernetesClusterResolver(
    {""worker"": [""job-name=mobileye-0"", ""job-name=mobileye-1""]}, override_client=k8s_cli
)
task_index = int(os.environ.get(""TASK_INDEX""))
cluster_resolver.task_type = ""worker""
cluster_resolver.task_id = task_index

logging.info(""Cluster spec: %s"", cluster_resolver.cluster_spec().as_dict())
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
    cluster_resolver=cluster_resolver
)
```

job.yaml
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: mobileye-0
spec:
  template:
    metadata:
      name: mobileye-training
    spec:
      containers:
      - name: tensorflow
        image: europe-west4-docker.pkg.dev/msteiner-kubeflow/mobileye-test/test-tf-image:latest 
        resources:
          limits:
            cpu: ""1""
            memory: 3Gi
        env:
          - name: TASK_INDEX
            value: ""0""
      restartPolicy: Never
  parallelism: 1
---
apiVersion: batch/v1
kind: Job
metadata:
  name: mobileye-1
spec:
  template:
    metadata:
      name: mobileye-training
    spec:
      containers:
      - name: tensorflow
        image: europe-west4-docker.pkg.dev/msteiner-kubeflow/mobileye-test/test-tf-image:latest 
        resources:
          limits:
            cpu: ""1""
            memory: 3Gi
        env:
          - name: TASK_INDEX
            value: ""1""
      restartPolicy: Never
  parallelism: 1
```
```


### Relevant log output

```shell
2024-06-28 13:23:10.980 CEST
2024-06-28 11:23:10.979832: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-06-28 13:23:10.991 CEST
2024-06-28 11:23:10.991166: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-06-28 13:23:11.109 CEST
2024-06-28 11:23:11.108950: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
2024-06-28 13:23:11.109 CEST
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-28 13:23:14.057 CEST
2024-06-28 11:23:14.056964: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-06-28 13:23:18.311 CEST
INFO:absl:TF version: 2.16.1
2024-06-28 13:23:18.312 CEST
INFO:absl:PATH: /usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
2024-06-28 13:23:18.312 CEST
INFO:absl:HOSTNAME: mobileye-0-v7lkr
2024-06-28 13:23:18.312 CEST
INFO:absl:LANG: C.UTF-8
2024-06-28 13:23:18.312 CEST
INFO:absl:GPG_KEY: A035C8C19219BA821ECEA86B64E628F8D684696D
2024-06-28 13:23:18.312 CEST
INFO:absl:PYTHON_VERSION: 3.11.8
2024-06-28 13:23:18.312 CEST
INFO:absl:PYTHON_PIP_VERSION: 24.0
2024-06-28 13:23:18.312 CEST
INFO:absl:PYTHON_SETUPTOOLS_VERSION: 65.5.1
2024-06-28 13:23:18.312 CEST
INFO:absl:PYTHON_GET_PIP_URL: https://github.com/pypa/get-pip/raw/dbf0c85f76fb6e1ab42aa672ffca6f0a675d9ee4/public/get-pip.py
2024-06-28 13:23:18.312 CEST
INFO:absl:PYTHON_GET_PIP_SHA256: dfe9fd5c28dc98b5ac17979a953ea550cec37ae1b47a5116007395bfacff2ab9
2024-06-28 13:23:18.312 CEST
INFO:absl:TASK_INDEX: 0
2024-06-28 13:23:18.313 CEST
INFO:absl:KUBERNETES_SERVICE_PORT: 443
2024-06-28 13:23:18.313 CEST
INFO:absl:KUBERNETES_SERVICE_PORT_HTTPS: 443
2024-06-28 13:23:18.313 CEST
INFO:absl:KUBERNETES_PORT: tcp://34.118.224.1:443
2024-06-28 13:23:18.313 CEST
INFO:absl:KUBERNETES_PORT_443_TCP: tcp://34.118.224.1:443
2024-06-28 13:23:18.313 CEST
INFO:absl:KUBERNETES_PORT_443_TCP_PROTO: tcp
2024-06-28 13:23:18.313 CEST
INFO:absl:KUBERNETES_PORT_443_TCP_PORT: 443
2024-06-28 13:23:18.313 CEST
INFO:absl:KUBERNETES_PORT_443_TCP_ADDR: 34.118.224.1
2024-06-28 13:23:18.313 CEST
INFO:absl:KUBERNETES_SERVICE_HOST: 34.118.224.1
2024-06-28 13:23:18.313 CEST
INFO:absl:HOME: /root
2024-06-28 13:23:18.313 CEST
INFO:absl:TF2_BEHAVIOR: 1
2024-06-28 13:23:18.313 CEST
INFO:absl:TPU_ML_PLATFORM: Tensorflow
2024-06-28 13:23:18.315 CEST
Traceback (most recent call last):   File ""//src/main.py"", line 20, in <module>     cluster_resolver = tf.distribute.cluster_resolver.KubernetesClusterResolver(
2024-06-28 13:23:18.316 CEST
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-06-28 13:23:18.316 CEST
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/python/distribute/cluster_resolver/kubernetes_cluster_resolver.py"", line 93, in __init__
2024-06-28 13:23:18.317 CEST
    k8sconfig.load_kube_config()
2024-06-28 13:23:18.317 CEST
  File ""/usr/local/lib/python3.11/site-packages/kubernetes/config/kube_config.py"", line 819, in load_kube_config
2024-06-28 13:23:18.318 CEST
    loader = _get_kube_config_loader(
2024-06-28 13:23:18.318 CEST
             ^^^^^^^^^^^^^^^^^^^^^^^^
2024-06-28 13:23:18.318 CEST
  File ""/usr/local/lib/python3.11/site-packages/kubernetes/config/kube_config.py"", line 776, in _get_kube_config_loader
2024-06-28 13:23:18.319 CEST
    raise ConfigException(
2024-06-28 13:23:18.319 CEST
kubernetes.config.config_exception.ConfigException: Invalid kube-config file. No configuration found.
```
",1
Data Service Dynamic sharding policy seems extremely slow,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.15.0-11-g63f5a65c7cd 2.15.1

### Custom code

No

### OS platform and distribution

Linux Debian

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

distributing a simple dataset with sharding policy dynamic should have similar data read performance to sharding policy OFF

### Standalone code to reproduce the issue

```shell
Hi,

I have a very simple example but setting sharding policy to dynamic vs off makes a huge difference in data speed.


dataset = tf.data.Dataset.range(100)
dataset_id = tf.data.experimental.service.register_dataset(dataset=dataset, service=dispatcher_addr)
dataset = tf.data.experimental.service.from_dataset_id(processing_mode=tf.data.experimental.service.ShardingPolicy.OFF, service=dispatcher_addr, dataset_id=dataset_id)

for x in dataset:
    print(x)
```
is almost instantaneous. But If I change the sharding policy to `DYNAMIC` (to ensure at most once visitation) it is horrendously slow even on this simple dataset.

Any idea what Im doing wrong?
```


### Relevant log output

_No response_",1
Document tf.Dataset.zip/map triggers tf.Dataset.shuffle,"I just got bit by this and was wondering if it's intended or needs a warning label

@aaudiber in #41334 commented the below:

`tf.data.Dataset` objects don't eagerly compute all of their data. They work like blueprints, where the data is computed on the fly every time you iterate through the dataset. As a result, iterating through the same dataset multiple times could result in different output.

The `Dataset.zip` transformation takes multiple datasets and ***iterates*** through them in parallel. If you want to iterate through the input just once, use `map` instead of `zip`:
...

_Originally posted by @aaudiber in https://github.com/tensorflow/tensorflow/issues/41334#issuecomment-662747527_

I had a workflow like so for a tf.data pipeline:
```python
filenames, labels = getFileLabels()
loaded = loadFiles(filenames)
transformed = transformData(loaded)
labeled = tf.data.Dataset.zip(transformed, labels).shuffle()
```
I was just relying on the `filenames`, `labels`, and `transformed` datasets being in the same order. To avoid loading all data into RAM, I moved shuffling up to just the filenames

```python
filenames, labels = getFileLabels()
filenames = filenames.shuffle()
loaded = loadFiles(filenames)
transformed = transformData(loaded)
labeled = tf.data.Dataset.zip(transformed, labels)
```
And what I found was that the model's accuracy tanked because it was receiving data with random labels. It's not explicitly stated anywhere in the docs, but it seems calls to `map` and `zip`(and anything else that would iterate) trigger `shuffle`.

The workaround is easy - just map the loading over the shuffled filenames and return a tuple directly rather than relying on `zip` - but originally I was just going off someone's advice to shuffle ""pointers"" to files rather than the actual loaded files (which made a lot of sense). Because I don't actually know what's happening at a low-level in the execution graph, I had just assumed that `shuffle` wouldn't be triggered by its sibling methods, only by `model.fit`

**TL;DR**:  
I think it should be documented how `shuffle` can impact other operations, particularly `zip`",0
C-API support for .keras models,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.16

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

TF_LoadSessionFromSavedModel() not able to load .keras files.
Is C-API support for Keras v3 .keras format planned?

### Standalone code to reproduce the issue

```shell
TF_LoadSessionFromSavedModel(..., ""model.keras"", ...)
```


### Relevant log output

_No response_",1
Tensorflow build without boringssl on Power architecture,"Hello,
We are trying to build tensorflow 2.16.1 without boringssl as boringssl no more supports Power/s390x architecture. 

TF has a direct dependency on boringssl as well as curl and grpc which in turn pulls boringssl. 
For this, our approach is to use TF_SYSTEM_LIBS set to curl, boringssl and grpc and remove direct bazel dependency in tensorflow/workspace2.bzl for boringssl.
I'd first tried to use only boringssl, curl and grpc from system libraries. After doing some changes in the TF's source code, I was able to compile almost 95% of code but linking of libtensorflow_cc.so failed as grpc being used from system is probably not compatible with the abseil-cpp built by bazel. Errors were as follows -

```
ERROR: /home/nishi/ppc/tensorflow/tensorflow/BUILD:1314:21: Linking tensorflow/libtensorflow_cc.so.2.16.1 failed: (Exit 1): gcc failed: error executing command (from target //tensorflow:libtensorflow_cc.so.2.16.1) /opt/rh/gcc-toolset-12/root/usr/bin/gcc @bazel-out/ppc-opt/bin/tensorflow/libtensorflow_cc.so.2.16.1-2.params
bazel-out/ppc-opt/bin/tensorflow/core/debug/_objs/debug_service_cc_grpc_proto/debug_service.grpc.pb.pic.o:debug_service.grpc.pb.cc:function grpc::internal::ClientCallbackReaderWriterImpl<tensorflow::Event, tensorflow::EventReply>::Read(tensorflow::EventReply*): error: undefined reference to 'absl::lts_20211102::Mutex::Lock()'
bazel-out/ppc-opt/bin/tensorflow/core/debug/_objs/debug_service_cc_grpc_proto/debug_service.grpc.pb.pic.o:debug_service.grpc.pb.cc:function grpc::internal::ClientCallbackReaderWriterImpl<tensorflow::Event, tensorflow::EventReply>::Read(tensorflow::EventReply*): error: undefined reference to 'absl::lts_20211102::Mutex::Unlock()'
bazel-out/ppc-opt/bin/tensorflow/core/debug/_objs/debug_service_cc_grpc_proto/debug_service.grpc.pb.pic.o:debug_service.grpc.pb.cc:function grpc::internal::ClientCallbackReaderWriterImpl<tensorflow::Event, tensorflow::EventReply>::Read(tensorflow::EventReply*): error: undefined reference to 'absl::lts_20211102::Mutex::Unlock()'
bazel-out/ppc-opt/bin/tensorflow/core/debug/_objs/debug_service_cc_grpc_proto/debug_service.grpc.pb.pic.o:debug_service.grpc.pb.cc:function grpc::internal::ClientCallbackReaderWriterImpl<tensorflow::Event, tensorflow::EventReply>::Write(tensorflow::Event const*, grpc::WriteOptions): error: undefined reference to 'absl::lts_20211102::Mutex::Lock()'
bazel-out/ppc-opt/bin/tensorflow/core/debug/_objs/debug_service_cc_grpc_proto/debug_service.grpc.pb.pic.o:debug_service.grpc.pb.cc:function grpc::internal::ClientCallbackReaderWriterImpl<tensorflow::Event, tensorflow::EventReply>::Write(tensorflow::Event const*, grpc::WriteOptions): error: undefined reference to 'absl::lts_20211102::Mutex::Unlock()'
```
After investigating I found that system level abseil-cpp library has these symbols but TF doesn't use it probably and it uses its own built abseil-cpp. Explicitly adding options like `-labsl_synchronization -L/usr/lib64` in  `@bazel-out/ppc-opt/bin/tensorflow/libtensorflow_cc.so.2.16.1-2.params` worked and I could see libtensorflow_cc.so generated. But probably this would be fusion of two versions of abseil-cpp which is not guaranteed to work.

Next I tried to use abseil-cpp as well from system by modifying TF_SYSTEM_LIBS. But it looks like absl BUILD files at `third_party/absl/` are absolete and not compatible with TF's code anymore. There were many problems, I fixed a few but there are actually many. So, I want to understand if TF community will be supporting/updating BUILD files for abseil-cpp.

Also, if anyone could help us in understanding if we can replace boringssl with System OpenSSL in Tensorflow, it would be of great help.",1
bucketize -function wrong results on GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.18.0-dev20240624

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Bucketize function returns wrong results when executed on GPU.   

Tested with  2.9.3, 2.15.0 and 2.18.0-dev20240624 and observed same incorrect behavior.

See the example below.   The CPU and GPU results are different so both can't be correct. GPU result seems to be wrong.

Can reproduce in [colab](https://colab.research.google.com/drive/1hSHgt5f31eUG-OsAy0FJ0zlcBhMgPV-c?authuser=0#scrollTo=nCDoKVOFw3ML)





### Standalone code to reproduce the issue

```shell
from tensorflow.python.ops.gen_math_ops import bucketize
import tensorflow as tf

print(tf.__version__)
gpus = tf.config.list_physical_devices('GPU')
assert gpus

x = [0,1,2,3]
boundaries = [0.1, 1.1]
with tf.device(""/CPU:0""):
    print(bucketize(x, boundaries=boundaries))
with tf.device(""/GPU:0""):
    print(bucketize(x, boundaries=boundaries))
```


### Relevant log output

```shell
2.18.0-dev20240624
tf.Tensor([0 1 2 2], shape=(4,), dtype=int32)
tf.Tensor([1 2 2 2], shape=(4,), dtype=int32)
```
",1
`SIGSEGV ` (Address boundary error) in `tf.io.gfile` with `TF_USE_MODULAR_FILESYSTEM=1`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.16.1, 2.17.0rc0, 2.18.0.dev20240617

### Custom code

No

### OS platform and distribution

macOS 14.5

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

A `SIGSEGV` (Address boundary error) fault can be caused by trying to call tensorflow gfile API when a `TF_USE_MODULAR_FILESYSTEM=1` is set. This can be reproduced in version 2.16.1 and 2.17.0rc0 and the latest nightly.

### Standalone code to reproduce the issue

```shell
TF_USE_MODULAR_FILESYSTEM=1 python -c 'import tensorflow as tf;tf.io.gfile.exists(""gs://tfds-data/dataset_info/mnist/3.0.1/dataset_info.json"")'
```


### Relevant log output

```shell
Job 1, 'TF_USE_MODULAR_FILESYSTEM=1 pyt…' terminated by signal SIGSEGV (Address boundary error)
```
",1
ImportError: Failed to load the native TensorFlow runtime due to undefined symbol,"**Issue Description**

I encountered an error while trying to run my project that uses TensorFlow. The error seems to be related to the TensorFlow runtime and an undefined symbol in the `_pywrap_tensorflow_internal.so` file. Below is the stack trace of the error.

**System Information**

- OS Platform and Distribution: Ubuntu 20.04.6 LTS
- Python Version: 3.10.14
- TensorFlow Version: 2.9.3

**Stack Trace**

```
File ""pywrap_tensorflow.py"", line 77, in <module>
    raise ImportError(ImportError: Traceback (most recent call last):
  File ""pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /.../_pywrap_tensorflow_internal.so: undefined symbol: _ZNK10tensorflow4data11DatasetBase8FinalizeEPNS_15OpKernelContextESt8functionIFNS_8StatusOrISt10unique_ptrIS1_NS_4core15RefCountDeleterEEEEvEE

Failed to load the native TensorFlow runtime.
```

Any help to resolve this issue would be greatly appreciated.

Thank you!",1
What is preventing TF to use GPU when used in native windows?,"We keep getting told, repeatedly that TF no longer ""supports"" using GPU on versions `>2.10`. However, the reason why this is so, is nowhere to be found. 

**Can someone from the TF community please explain what is the problem with TF having GPU support for native windows environment?**

* https://github.com/tensorflow/tensorflow/issues/64881
* https://github.com/tensorflow/tensorflow/issues/63362
* https://www.tensorflow.org/install/pip#windows-native

Finally, since the wording is ""supported"" (and it was supported before), how can we go about implementing this support on our own? 

What are the python package requirements, and native windows C/C++/C# compiler requirements? 
",0
Significant Performance Drop When Training Sequential Model Using `tf.data.Dataset.from_generator`,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.1
2.16.1
tf-nightly (2.18.0-dev20240613)

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.11.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I noticed a significant drop in the performance of the Sequential model in a binary-cross entropy problem when I switched to using tf.dataset.from_generator from using pd.DataFrame during the training.
Here is link to a google colab notebook which demonstrate the issue:
~~https://colab.research.google.com/drive/1AoysBY3nL6Tti1fKnCb6ch9i-f20i-z6?usp=sharing~~
https://colab.research.google.com/drive/1kdEQJpRpORiF9ornGzzqn0-2f6pUVims?usp=sharing

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import keras
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd

def set_seeds(seed=999):
    tf.random.set_seed(seed)
    np.random.seed(seed)
    tf.keras.utils.set_random_seed(seed)  # Sets seeds for base-python, numpy, and tf

n_features = 5

# Generate synthetic dataset and split into training and testing sets
X, y = make_classification(n_samples=1_000_000, n_features=n_features, n_classes=2, random_state=42)
X_train = pd.DataFrame(X)
y_train = pd.DataFrame(y)

X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Define a Sequential model
def define_model(n_features=n_features):
    model = keras.models.Sequential([
        keras.layers.BatchNormalization(input_shape=(n_features, )),
        keras.layers.Dense(10, activation='relu'),
        keras.layers.Dense(1, activation='sigmoid')
    ])
    return model

def compile(model):
    # Compile the model
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])


# Train based on pd.DataFrame
set_seeds()
model = define_model()
compile(model)
history = model.fit(X_train, y_train, epochs=10, batch_size=10_000, validation_split=0.0, verbose=False)
ax = pd.DataFrame(history.history).plot()
ax.set_ylim(0, 1.0)
ax.set_title(""Training using pd.DataFrame"")

# Evaluate on the test sample
print(""Performance using pd.DataFrame:"") 
model.evaluate(X_test, y_test, batch_size=10_000)


# Train model using tf.dataset

class Generator:
    def __call__(self):
        yield X_train.values, y_train.values

ds_train = tf.data.Dataset.from_generator(
    Generator(),
    output_signature=(
        tf.TensorSpec(shape=(None, n_features), dtype=tf.float32),
        tf.TensorSpec(shape=(None, 1), dtype=tf.int32)
    )
)

set_seeds()
model = define_model()
compile(model)
history = model.fit(ds_train, epochs=10, batch_size=10_000, verbose=False)
ax = pd.DataFrame(history.history).plot()
ax.set_ylim(0, 1.0)
ax.set_title(""Training using tf.dataset"")


# Evaluate on the test sample
print(""Performance using tf.dataset:"")
model.evaluate(X_test, y_test, batch_size=10_000)
```
```


### Relevant log output

```shell
Performance using pd.DataFrame:
20/20 [==============================] - 0s 3ms/step - loss: 0.1074 - accuracy: 0.9617

Performance using tf.dataset:
20/20 [==============================] - 0s 3ms/step - loss: 0.6245 - accuracy: 0.6626
```
",1
Segmentation fault in `tf.raw_ops.CollectiveAllToAllV2`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific input, tf.raw_ops.CollectiveGatherV2 encounters ""Segmentation fault (core dumped)"".

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

test = tf.Variable([1.0, 2.0, 3.0])

input_dict = {
    'input': tf.constant([], shape=(1, 1, 0), dtype=tf.float64),
    'group_size': tf.constant(1, dtype=tf.int32),
    'group_key': tf.constant(30631, dtype=tf.int32), 
    'instance_key': tf.constant(2, dtype=tf.int32), 
    'ordering_token': [test.handle], 
}


tf.raw_ops.CollectiveAllToAllV2(
    input=input_dict['input'],
    group_size=input_dict['group_size'],
    group_key=input_dict['group_key'],
    instance_key=input_dict['instance_key'],
    ordering_token=input_dict['ordering_token'],
    communication_hint='auto',
    timeout_seconds=0,
    is_stateless=False,
    name=None
)
```


### Relevant log output

```shell
2024-06-13 14:37:50.256604: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Segmentation fault (core dumped)
```
",1
Segmentation fault in `tf.raw_ops.CollectiveGatherV2`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific input, tf.raw_ops.CollectiveGatherV2 encounters ""Segmentation fault (core dumped)"".

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

test = tf.Variable([1.0, 2.0, 3.0])

input_dict = {
    'input': tf.constant([], shape=(1, 1, 0), dtype=tf.float64),
    'group_size': tf.constant(1, dtype=tf.int32),
    'group_key': tf.constant(30631, dtype=tf.int32), 
    'instance_key': tf.constant(2, dtype=tf.int32), 
    'ordering_token': [test.handle], 
}


result = tf.raw_ops.CollectiveGatherV2(
    input=input_dict['input'],
    group_size=input_dict['group_size'],
    group_key=input_dict['group_key'],
    instance_key=input_dict['instance_key'],
    ordering_token=input_dict['ordering_token'],
    communication_hint='auto',
    timeout_seconds=0,
    is_stateless=False
)
```


### Relevant log output

```shell
2024-06-13 14:32:37.690567: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Segmentation fault (core dumped)
```
",1
"""invalid static_cast"" on AVX512FP16 (e.g. Sapphire Rapids)","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.1

### Custom code

No

### OS platform and distribution

RHEL 8

### Mobile device

_No response_

### Python version

3.10

### Bazel version

6.1.0

### GCC/compiler version

GCC 13.1

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Failing build with

```
external/eigen_archive/Eigen/src/Core/MathFunctions.h:429:12: error: invalid ‘static_cast’ from type ‘const Eigen::internal::eigen_packet_wrapper<__vector(4) long long int, 1>’ to type ‘__vector(16) float’
429 | return static_cast(x);
```

when compiling for CPUs supporting AVX512 FP16 extensions, e.g. Sapphire Rapids

This is the same issue as #62047 reported by @OH-AU which was closes by @mihaimaruseac because an unsupported Python version was used which is however unrelated to this failure.

After some analysis it turns out to be an issue in Eigen: https://gitlab.com/libeigen/eigen/-/issues/2829 which might be fixed by https://gitlab.com/libeigen/eigen/-/merge_requests/1639

So TF can either use that MR as a patch or update Eigen once the MR is merged

### Standalone code to reproduce the issue

```shell
TF_PYTHON_VERSION=3.10 CFLAGS=""-O3 -march=native -fPIC"" CXXFLAGS=$CFLAGS LIBRARY_PATH=$LD_RUN_PATH LD_LIBRARY_PATH=$LD_RUN_PATH \
LDFLAGS=""-fPIC  -Wl,--disable-new-dtags -Wl,--rpath -Wl,${LD_RUN_PATH}"" bazel build -j 24 --config=opt -c opt  --copt=-march=native \
--config=mkl --config=tensorrt  //tensorflow/tools/pip_package:build_pip_package --repo_env=TF_PYTHON_VERSION=3.10
```


### Relevant log output

```shell
In file included from external/eigen_archive/Eigen/Core:182,
                 from tensorflow/core/kernels/linalg/matrix_inverse_op.cc:22:
external/eigen_archive/Eigen/src/Core/MathFunctions.h: In instantiation of 'static NewType Eigen::internal::cast_impl<OldType, NewType, EnableIf>::run(const OldType&) [with OldType = Eigen::internal::eigen_packet_wrapper<__vector(4) long long int, 1>; NewType = __vector(16) float; EnableIf = void]':
external/eigen_archive/Eigen/src/Core/GenericPacketMath.h:268:48:   required from 'static TgtPacket Eigen::internal::pcast_generic<SrcPacket, TgtPacket, false, false>::run(const SrcPacket&) [with SrcPacket = Eigen::internal::eigen_packet_wrapper<__vector(4) long long int, 1>; TgtPacket = __vector(16) float]'
external/eigen_archive/Eigen/src/Core/GenericPacketMath.h:289:50:   required from 'TgtPacket Eigen::internal::pcast(const SrcPacket&) [with SrcPacket = eigen_packet_wrapper<__vector(4) long long int, 1>; TgtPacket = __vector(16) float]'
external/eigen_archive/Eigen/src/Core/CoreEvaluators.h:789:52:   required from 'DstPacketType Eigen::internal::unary_evaluator<Eigen::CwiseUnaryOp<Eigen::internal::core_cast_op<SrcType, DstType>, ArgType>, Eigen::internal::IndexBased>::packet(Eigen::Index) const [with int LoadMode = 0; DstPacketType = __vector(16) float; typename std::enable_if<Eigen::internal::find_packet_by_size<SrcType, Eigen::internal::unpacket_traits<DstPacketType>::size>::value, bool>::type <anonymous> = true; SrcType = Eigen::half; DstType = float; ArgType = const Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; typename Eigen::CwiseUnaryOp<Eigen::internal::core_cast_op<SrcType, DstType>, ArgType>::Scalar = float; Eigen::Index = long int]'
external/eigen_archive/Eigen/src/Core/AssignEvaluator.h:706:110:   required from 'void Eigen::internal::generic_dense_assignment_kernel<DstEvaluatorTypeT, SrcEvaluatorTypeT, Functor, Version>::assignPacket(Eigen::Index) [with int StoreMode = 64; int LoadMode = 0; Packet = __vector(16) float; DstEvaluatorTypeT = Eigen::internal::evaluator<Eigen::Matrix<float, -1, -1, 1, -1, -1> >; SrcEvaluatorTypeT = Eigen::internal::evaluator<Eigen::CwiseUnaryOp<Eigen::internal::core_cast_op<Eigen::half, float>, const Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > > >; Functor = Eigen::internal::assign_op<float, float>; int Version = 0; Eigen::Index = long int]'
external/eigen_archive/Eigen/src/Core/AssignEvaluator.h:462:75:   required from 'static constexpr void Eigen::internal::dense_assignment_loop<Kernel, 3, 0>::run(Kernel&) [with Kernel = Eigen::internal::generic_dense_assignment_kernel<Eigen::internal::evaluator<Eigen::Matrix<float, -1, -1, 1, -1, -1> >, Eigen::internal::evaluator<Eigen::CwiseUnaryOp<Eigen::internal::core_cast_op<Eigen::half, float>, const Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > > >, Eigen::internal::assign_op<float, float>, 0>]'
external/eigen_archive/Eigen/src/Core/AssignEvaluator.h:810:37:   [ skipping 3 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]
external/eigen_archive/Eigen/src/Core/AssignEvaluator.h:883:27:   required from 'void Eigen::internal::call_assignment(Dst&, const Src&, const Func&, std::enable_if_t<(! evaluator_assume_aliasing<Src>::value), void*>) [with Dst = Eigen::Matrix<float, -1, -1, 1, -1, -1>; Src = Eigen::CwiseUnaryOp<core_cast_op<Eigen::half, float>, const Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >; Func = assign_op<float, float>; std::enable_if_t<(! evaluator_assume_aliasing<Src>::value), void*> = void*; typename evaluator_traits<SrcXprType>::Shape = Eigen::DenseShape]'
external/eigen_archive/Eigen/src/Core/AssignEvaluator.h:861:18:   required from 'void Eigen::internal::call_assignment(Dst&, const Src&) [with Dst = Eigen::Matrix<float, -1, -1, 1, -1, -1>; Src = Eigen::CwiseUnaryOp<core_cast_op<Eigen::half, float>, const Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >]'
external/eigen_archive/Eigen/src/Core/PlainObjectBase.h:771:32:   required from 'Derived& Eigen::PlainObjectBase<Derived>::_set(const Eigen::DenseBase<OtherDerived>&) [with OtherDerived = Eigen::CwiseUnaryOp<Eigen::internal::core_cast_op<Eigen::half, float>, const Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >; Derived = Eigen::Matrix<float, -1, -1, 1, -1, -1>]'
external/eigen_archive/Eigen/src/Core/Matrix.h:227:24:   required from 'Eigen::Matrix<Scalar_, Rows_, Cols_, Options_, MaxRows_, MaxCols_>& Eigen::Matrix<Scalar_, Rows_, Cols_, Options_, MaxRows_, MaxCols_>::operator=(const Eigen::DenseBase<OtherDerived>&) [with OtherDerived = Eigen::CwiseUnaryOp<Eigen::internal::core_cast_op<Eigen::half, float>, const Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >; Scalar_ = float; int Rows_ = -1; int Cols_ = -1; int Options_ = 1; int MaxRows_ = -1; int MaxCols_ = -1]'
external/eigen_archive/Eigen/src/LU/PartialPivLU.h:135:12:   required from 'Eigen::PartialPivLU<MatrixType, PermutationIndex>& Eigen::PartialPivLU<MatrixType, PermutationIndex>::compute(const Eigen::EigenBase<OtherDerived>&) [with InputType = Eigen::CwiseUnaryOp<Eigen::internal::core_cast_op<Eigen::half, float>, const Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >; MatrixType_ = Eigen::Matrix<float, -1, -1, 1, -1, -1>; PermutationIndex_ = int]'
tensorflow/core/kernels/linalg/matrix_inverse_op.cc:113:31:   required from here
external/eigen_archive/Eigen/src/Core/MathFunctions.h:429:12: error: invalid 'static_cast' from type 'const Eigen::internal::eigen_packet_wrapper<__vector(4) long long int, 1>' to type '__vector(16) float'
  429 |     return static_cast<NewType>(x);
      |            ^~~~~~~~~~~~~~~~~~~~~~~
```
",1
Aborted (core dumped) in `tf.experimental.numpy.diag/tf.compat.v1.linalg.diag/tf.experimental.numpy.diagflat/tf.keras.ops.diag`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific input, tf.experimental.numpy.diag/tf.compat.v1.linalg.diag/tf.experimental.numpy.diagflat/tf.keras.ops.diag encounters ""Aborted (core dumped)"".

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input_dict = {
    'diagonal': tf.constant([1,2], dtype=tf.int32),
    'k': tf.constant([1, 2], dtype=tf.int32)
}

# crash
tf.experimental.numpy.diag(
    v=tf.constant([1,2], dtype=tf.int32), k=tf.constant([1, 2], dtype=tf.int32)
)

# crash
# tf.compat.v1.linalg.diag(
#     diagonal=tf.constant([1,2], dtype=tf.int32),
#     name='diag',
#     k= tf.constant([1, 2], dtype=tf.int32),
#     num_rows=-1,
#     num_cols=-1,
#     padding_value=0,
#     align='RIGHT_LEFT'
# )

# crash
# tf.experimental.numpy.diagflat(
#     v=tf.constant([1,2], dtype=tf.int32), k=tf.constant([1, 2], dtype=tf.int32)
# )

# crash
# tf.keras.ops.diag(
#     x=tf.constant([1,2], dtype=tf.int32), k=tf.constant([1, 2], dtype=tf.int32)
# )
```


### Relevant log output

```shell
2024-06-10 13:23:09.021306: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-10 13:23:10.029041: F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)
```
",2
Crash in `tf.raw_ops.SparseCountSparseOutput `,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

TensorFlow Nightly

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific input, tf.raw_ops.SparseCountSparseOutput will output ""The session crashed because it took up all available RAM."" 

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

binary_output = True
indices = tf.constant(0, shape=[3456,2], dtype=tf.int64)
values = tf.constant(536870912, shape=[3456], dtype=tf.int32)
dense_shape = tf.constant([125099989676412,125099989676412], shape=[2], dtype=tf.int64)
weights = tf.constant(51, shape=[3456], dtype=tf.int32)


tf.raw_ops.SparseCountSparseOutput(
   indices=indices, values=values, dense_shape=dense_shape, weights=weights, binary_output=binary_output,
    minlength=0,
    maxlength=0,
    name=None
)
```


### Relevant log output

```shell
Jun 10, 2024, 11:31:57 AM	WARNING	WARNING:root:kernel 7c5d2f95-9ab7-4b7c-99c0-d8949477c9f1 restarted
Jun 10, 2024, 11:31:57 AM	INFO	KernelRestarter: restarting kernel (1/5), keep random ports
Jun 10, 2024, 11:31:29 AM	WARNING	2024-06-10 03:31:29.145870: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Jun 10, 2024, 11:31:26 AM	WARNING	To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Jun 10, 2024, 11:31:26 AM	WARNING	2024-06-10 03:31:26.481147: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
Jun 10, 2024, 11:31:26 AM	WARNING	2024-06-10 03:31:26.469832: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
Jun 10, 2024, 11:31:26 AM	WARNING	2024-06-10 03:31:26.467591: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
Jun 10, 2024, 11:31:26 AM	WARNING	2024-06-10 03:31:26.467524: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
Jun 10, 2024, 11:31:18 AM	INFO	Kernel started: 7c5d2f95-9ab7-4b7c-99c0-d8949477c9f1, name: python3
Jun 10, 2024, 11:30:19 AM	INFO	Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
```
",2
GPU MaxPool gradient ops do not yet have a deterministic XLA implementation,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9.19

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.4/8.9.7.29

### GPU model and memory

NVIDIA GeForce RTX 3090

### Current behavior?

When TF deterministic was set, runtime exception was thrown at MaxPooling2D().

### Standalone code to reproduce the issue

```shell
When TF deterministic was set, runtime exception was thrown at MaxPooling2D().
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/home/ws/miniconda3/envs/tf216/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 3526, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-3dda39ff370e>"", line 1, in <module>
    runfile('/mnt/projects/Projects/Test_Classification/train_model.py', wdir='/mnt/projects/Projects/Test_Classification')
  File ""/opt/pycharm-community-2024.1/plugins/python-ce/helpers/pydev/_pydev_bundle/pydev_umd.py"", line 197, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File ""/opt/pycharm-community-2024.1/plugins/python-ce/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/mnt/projects/Projects/Test_Classification/train_model.py"", line 956, in <module>
    history = model.fit(x_train, y_train,
  File ""/home/ws/miniconda3/envs/tf216/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/ws/miniconda3/envs/tf216/lib/python3.9/site-packages/tensorflow/python/eager/execute.py"", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.UnimplementedError: Graph execution error:
Detected at node gradient_tape/functional_1_1/max_pooling2d_4_1/MaxPool2d/MaxPoolGrad defined at (most recent call last):
<stack traces unavailable>
GPU MaxPool gradient ops do not yet have a deterministic XLA implementation.
	 [[{{node gradient_tape/functional_1_1/max_pooling2d_4_1/MaxPool2d/MaxPoolGrad}}]]
	tf2xla conversion failed while converting __inference_one_step_on_data_13588[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.
	 [[StatefulPartitionedCall]] [Op:__inference_one_step_on_iterator_14045]
```
",1
"Compilation of mlir:tf-opt fails with error ""The repository '@llvm_zlib' could not be resolved""","### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

dd0c582

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11.9

### Bazel version

6.5.0

### GCC/compiler version

9.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Compiling using the docker image `latest-devel` and `devel`. 

When I try to compile:

```bash
bazel build --override_repository=""llvm-raw=${LLVM_SRC}"" -c opt tensorflow/compiler/mlir:tf-opt
```

I get this error:

```bash
ERROR: /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/llvm-project/llvm/BUILD.bazel:219:11: no such package '@llvm_zlib//': The repository '@llvm_zlib' could not be resolved: Repository '@llvm_zlib' is not defined and referenced by '@llvm-project//llvm:Support'
ERROR: Analysis of target '//tensorflow/compiler/mlir:tf-opt' failed; build aborted:
```

How do I compile it?

### Standalone code to reproduce the issue

```shell
# run docker (latest-devel)
docker run -it -w /tensorflow_src -v $PWD:/mnt -e HOST_PERMS=""$(id -u):$(id -g)"" tensorflow/tensorflow:latest-devel bash
git pull
cd $HOME

# install dependencies
sudo apt-get update
sudo apt-get install -y build-essential gdb lcov pkg-config \
      libbz2-dev libffi-dev libgdbm-dev libgdbm-compat-dev liblzma-dev \
      libncurses-dev libreadline-dev libsqlite3-dev libssl-dev \
      lzma lzma-dev tk-dev uuid-dev zlib1g-dev patchelf
sudo apt-get install -y python3-dev python3-pip
pip install -U --user pip

# install llvm
wget https://apt.llvm.org/llvm.sh
chmod +x llvm.sh
sudo ./llvm.sh 17
sudo ln -sf /usr/bin/clang-17 /usr/bin/clang

# install local python 3.11
mkdir .python3.11
mkdir src
cd src
wget https://www.python.org/ftp/python/3.11.9/Python-3.11.9.tgz
tar -zxvf Python-3.11.9.tgz
cd Python-3.11.9
make clean
./configure --prefix=$HOME/.python3.11
make
make install
export PATH=$PATH:$HOME/.python3.11/bin
cd $HOME

# install virtualenv and activate
pip3.11 install virtualenv
$HOME/.python3.11/bin/virtualenv --python=$HOME/.python3.11/bin/python3.11 $HOME/.tfdev
source $HOME/.tfdev/bin/activate

# get llvm src
cd $HOME
git clone https://github.com/llvm/llvm-project.git
cd llvm-project
git checkout llvmorg-17.0.6
export LLVM_SRC=$HOME/llvm-project
touch ${LLVM_SRC}/BUILD.bazel ${LLVM_SRC}/WORKSPACE

# compile tensorflow
cd /tensorflow_src
export TF_PYTHON_VERSION=3.11
export PYTHON_BIN_PATH=$(which python)
export PYTHON_LIB_PATH=$HOME/.tfdev/lib/python3.11/site-packages
export TF_NEED_ROCM=0
export TF_NEED_CUDA=0
export TF_NEED_CLANG=1
export CLANG_COMPILER_PATH=$(which clang)
export CC_OPT_FLAGS=-Wno-sign-compare
export TF_SET_ANDROID_WORKSPACE=0
./configure

# build tensorflow mlir tools
bazel build --override_repository=""llvm-raw=${LLVM_SRC}"" -c opt tensorflow/compiler/mlir:tf-opt
```


### Relevant log output

```shell
INFO: Reading 'startup' options from /tensorflow_src/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=170
INFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /tensorflow_src/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/root/.tfdev/bin/python --action_env PYTHON_LIB_PATH=/root/.tfdev/lib/python3.11/site-packages --python_path=/root/.tfdev/bin/python --action_env CLANG_COMPILER_PATH=/usr/lib/llvm-17/bin/clang --repo_env=CC=/usr/lib/llvm-17/bin/clang --repo_env=BAZEL_COMPILER=/usr/lib/llvm-17/bin/clang --copt=-Wno-gnu-offsetof-extensions
INFO: Found applicable config definition build:short_logs in file /tensorflow_src/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /tensorflow_src/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:linux in file /tensorflow_src/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow_src/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Repository stablehlo instantiated at:
  /tensorflow_src/WORKSPACE:111:14: in <toplevel>
  /tensorflow_src/tensorflow/workspace2.bzl:947:28: in workspace
  /tensorflow_src/tensorflow/workspace2.bzl:88:14: in _initialize_third_party
  /tensorflow_src/third_party/stablehlo/workspace.bzl:11:20: in repo
  /tensorflow_src/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /tensorflow_src/third_party/repo.bzl:89:35: in <toplevel>
INFO: Repository double_conversion instantiated at:
  /tensorflow_src/WORKSPACE:111:14: in <toplevel>
  /tensorflow_src/tensorflow/workspace2.bzl:954:21: in workspace
  /tensorflow_src/tensorflow/workspace2.bzl:639:20: in _tf_repositories
  /tensorflow_src/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /tensorflow_src/third_party/repo.bzl:89:35: in <toplevel>
INFO: Repository zlib instantiated at:
  /tensorflow_src/WORKSPACE:111:14: in <toplevel>
  /tensorflow_src/tensorflow/workspace2.bzl:954:21: in workspace
  /tensorflow_src/tensorflow/workspace2.bzl:487:20: in _tf_repositories
  /tensorflow_src/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /tensorflow_src/third_party/repo.bzl:89:35: in <toplevel>
INFO: Repository flatbuffers instantiated at:
  /tensorflow_src/WORKSPACE:111:14: in <toplevel>
  /tensorflow_src/tensorflow/workspace2.bzl:947:28: in workspace
  /tensorflow_src/tensorflow/workspace2.bzl:70:16: in _initialize_third_party
  /tensorflow_src/third_party/flatbuffers/workspace.bzl:14:20: in repo
  /tensorflow_src/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /tensorflow_src/third_party/repo.bzl:89:35: in <toplevel>
INFO: Repository farmhash_archive instantiated at:
  /tensorflow_src/WORKSPACE:111:14: in <toplevel>
  /tensorflow_src/tensorflow/workspace2.bzl:947:28: in workspace
  /tensorflow_src/tensorflow/workspace2.bzl:69:13: in _initialize_third_party
  /tensorflow_src/third_party/farmhash/workspace.bzl:14:20: in repo
  /tensorflow_src/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /tensorflow_src/third_party/repo.bzl:89:35: in <toplevel>
INFO: Repository com_googlesource_code_re2 instantiated at:
  /tensorflow_src/WORKSPACE:111:14: in <toplevel>
  /tensorflow_src/tensorflow/workspace2.bzl:954:21: in workspace
  /tensorflow_src/tensorflow/workspace2.bzl:275:20: in _tf_repositories
  /tensorflow_src/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /tensorflow_src/third_party/repo.bzl:89:35: in <toplevel>
INFO: Repository snappy instantiated at:
  /tensorflow_src/WORKSPACE:111:14: in <toplevel>
  /tensorflow_src/tensorflow/workspace2.bzl:954:21: in workspace
  /tensorflow_src/tensorflow/workspace2.bzl:506:20: in _tf_repositories
  /tensorflow_src/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /tensorflow_src/third_party/repo.bzl:89:35: in <toplevel>
INFO: Repository gif instantiated at:
  /tensorflow_src/WORKSPACE:111:14: in <toplevel>
  /tensorflow_src/tensorflow/workspace2.bzl:954:21: in workspace
  /tensorflow_src/tensorflow/workspace2.bzl:340:20: in _tf_repositories
  /tensorflow_src/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /tensorflow_src/third_party/repo.bzl:89:35: in <toplevel>
ERROR: /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/llvm-project/llvm/BUILD.bazel:219:11: no such package '@llvm_zlib//': The repository '@llvm_zlib' could not be resolved: Repository '@llvm_zlib' is not defined and referenced by '@llvm-project//llvm:Support'
ERROR: Analysis of target '//tensorflow/compiler/mlir:tf-opt' failed; build aborted:
INFO: Elapsed time: 41.334s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (253 packages loaded, 3347 targets configured)
    currently loading: tensorflow/compiler/mlir/lite/schema
    Fetching repository @eigen_archive; starting
    Fetching https://storage.googleapis.com/.../github.com/google/flatbuffers/archive/e6463926479bd6b330cbcf673f7e917803fd5831.tar.gz; 1.7 MiB (76.0%)
    Fetching https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/stablehlo/archive/14e2323f0ee3d308c1384fdb806dc6d0c98b16ca.zip; 1.4 MiB (8.2%)
    Fetching https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/double-conversion/archive/v3.2.0.tar.gz
    Fetching https://storage.googleapis.com/mirror.tensorflow.org/zlib.net/fossils/zlib-1.2.13.tar.gz
    Fetching /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/eigen_archive; Extracting eigen-c1d637433e3b3f9012b226c2c9125c494b470ae6.tar.gz
    Fetching https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/farmhash/archive/0d859a811870d10f53a594927d0d0b97573ad06d.tar.gz
    Fetching https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/re2/archive/03da4fc0857c285e3a26782f6bc8931c4c950df4.tar.gz ... (10 fetches)
```
",1
Crash in `tf.raw_ops.ResizeNearestNeighbor/ResizeNearestNeighborGrad/ResizeArea/ResizeBicubic/ResizeBilinear`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

TensorFlow Nightly

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific input, these APIs will output ""The session crashed because it took up all available RAM."" The affected APIs are listed below:

1. tf.raw_ops.ResizeBilinear
2. tf.raw_ops.ResizeBicubic
3. tf.raw_ops.ResizeArea
4. tf.raw_ops.ResizeNearestNeighbor
5. tf.raw_ops.ResizeNearestNeighborGrad

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1a6B_lYMfENTbO4ifQMLYe8Hq6CHcc6hP?usp=sharing
```


### Relevant log output

```shell
Timestamp	Level	Message
Jun 6, 2024, 6:57:43 PM	WARNING	WARNING:root:kernel d0444564-3fb8-4d42-ac83-1338e6842d5a restarted
Jun 6, 2024, 6:57:43 PM	INFO	KernelRestarter: restarting kernel (1/5), keep random ports
Jun 6, 2024, 6:57:25 PM	WARNING	2024-06-06 10:57:25.421101: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 51536461872 exceeds 10% of free system memory.
Jun 6, 2024, 6:57:21 PM	WARNING	2024-06-06 10:57:21.507228: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Jun 6, 2024, 6:57:17 PM	WARNING	To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Jun 6, 2024, 6:57:17 PM	WARNING	2024-06-06 10:57:17.995922: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
Jun 6, 2024, 6:56:13 PM	WARNING	WARNING:root:kernel d0444564-3fb8-4d42-ac83-1338e6842d5a restarted
Jun 6, 2024, 6:56:13 PM	INFO	KernelRestarter: restarting kernel (1/5), keep random ports
Jun 6, 2024, 6:55:56 PM	WARNING	2024-06-06 10:55:56.764126: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 103072923744 exceeds 10% of free system memory.
Jun 6, 2024, 6:55:52 PM	WARNING	2024-06-06 10:55:52.277507: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
```
",2
Aborted (core dumped) in `tf.raw_ops.ResourceApplyCenteredRMSProp/tf.raw_ops.ResourceSparseApplyCenteredRMSProp`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific input, tf.raw_ops.ResourceApplyCenteredRMSProp/tf.raw_ops.ResourceSparseApplyCenteredRMSProp triggers ""Aborted (core dumped)"". We ran the code on colab's latest TensorFlow Nightly, which also triggers the crash. The cause of the crash may be the grad parameter.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

var = tf.Variable([1.0, 2.0, 3.0])
accum = tf.Variable([0.1, 0.2, 0.3], dtype=tf.complex64) 
accum2 = tf.Variable([0, 1.2, -3.3], dtype=tf.complex64) 
lr = 0.01
grad = tf.constant([0.1, 0.2, 0.3])
momentum = 0.9

# crash
tf.raw_ops.ResourceSparseApplyCenteredRMSProp(var=var.handle, mg=accum.handle, ms=accum2.handle, mom=var.handle,
                                              lr=lr, rho=lr,
                                              momentum=momentum,
                                              epsilon=0.9, grad=tf.constant([0.1, 0.2, 0.3]),
                                              indices=tf.constant([1,2,2]),
                                              use_locking=False)
# crash
# tf.raw_ops.ResourceApplyCenteredRMSProp(var=var.handle, mg=accum.handle, ms=accum2.handle, mom=var.handle,
#                                               lr=lr, rho=lr,
#                                               momentum=momentum,
#                                               epsilon=0.9, grad=tf.constant([0.1, 0.2, 0.3]),
#                                               use_locking=False)
```


### Relevant log output

```shell
2024-06-06 01:23:02.820585: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-06-06 01:23:02.853830: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-06 01:23:03.911976: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (8 vs. 1) float expected, got complex64
Aborted (core dumped)
```
",2
Aborted (core dumped) in `tf.raw_ops.ResourceApplyAdagrad/tf.raw_ops.ResourceApplyAdagradDA/tf.raw_ops.ResourceApplyAdagradV2`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific input, tf.raw_ops.ResourceApplyAdagrad/tf.raw_ops.ResourceApplyAdagradDA/tf.raw_ops.ResourceApplyAdagradV2 triggers ""Aborted (core dumped)"". We ran the code on colab's latest TensorFlow Nightly, which also triggers the crash. The cause of the crash may be the grad parameter.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

var = tf.Variable([1.0, 2.0, 3.0])
accum = tf.Variable([0.1, 0.2, 0.3], dtype=tf.complex64) 
accum2 = tf.Variable([0, 1.2, -3.3], dtype=tf.complex64) 
lr = 0.01
grad = tf.constant([0.1, 0.2, 0.3])
momentum = 0.9

# crash
tf.raw_ops.ResourceApplyAdagrad(
    var=var.handle,
    accum=accum.handle,
    lr=lr,
    grad=grad,
    use_locking=False,
)

# crash
# tf.raw_ops.ResourceApplyAdagradDA(var=var.handle,
#               gradient_accumulator=accum.handle,
#               gradient_squared_accumulator=accum2.handle,
#               grad=grad,  lr=lr,
#               l1=lr, l2=lr, global_step=1000,
#               use_locking=False)

# crash
# tf.raw_ops.ResourceApplyAdagradV2(var=var.handle,
#                 accum=accum.handle,
#                 epsilon = 0.9,
#                 grad=grad, lr=lr,
#                 use_locking=False,update_slots=False)
```


### Relevant log output

```shell
2024-06-06 01:18:44.797518: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-06-06 01:18:44.829812: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-06 01:18:45.875807: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (8 vs. 1) float expected, got complex64
Aborted (core dumped)
```
",2
Aborted (core dumped) in `tf.raw_ops.ResourceSparseApplyAdagrad/tf.raw_ops.ResourceSparseApplyAdagradDA/tf.raw_ops.ResourceSparseApplyAdagradV2`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific input, tf.raw_ops.ResourceSparseApplyAdagrad/tf.raw_ops.ResourceSparseApplyAdagradDA/tf.raw_ops.ResourceSparseApplyAdagradV2 triggers ""Aborted (core dumped)"". We ran the code on colab's latest TensorFlow Nightly, which also triggers the crash. The cause of the crash may be the grad parameter.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

var = tf.Variable([1.0, 2.0, 3.0])
accum = tf.Variable([0.1, 0.2, 0.3], dtype=tf.complex64) 
accum2 = tf.Variable([0, 1.2, -3.3], dtype=tf.complex64) 
lr = 0.01
grad = tf.constant([0.1, 0.2, 0.3])
momentum = 0.9

# crash
tf.raw_ops.ResourceSparseApplyAdagrad(
    var=var.handle,
    accum=accum.handle,
    lr=lr,
    grad=grad,
    indices=tf.constant([1,2,3]),
    use_locking=False,
)


# crash only when gpu is available 
# tf.raw_ops.ResourceSparseApplyAdagradDA(var=var.handle,
#                     gradient_accumulator=accum.handle,
#                     gradient_squared_accumulator=accum2.handle,
#                     grad=grad, indices=tf.constant([1,2,3]), lr=lr,
#                     l1=lr, l2=lr, global_step=1000,
#                     use_locking=False)

# crash
# tf.raw_ops.ResourceSparseApplyAdagradV2(var=var.handle,
#                     accum=accum.handle,
#                     epsilon = 0.9,
#                     grad=grad, indices=tf.constant([1,2,3]), lr=lr,
#                     use_locking=False,update_slots=False)
```


### Relevant log output

```shell
2024-06-06 01:14:57.455749: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-06-06 01:14:57.489757: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-06 01:14:58.552846: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (8 vs. 1) float expected, got complex64
Aborted (core dumped)
```
",2
Aborted (core dumped) in `tf.raw_ops.ResourceSparseApplyAdadelta/tf.raw_ops.ResourceApplyAdadelta`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific input, tf.raw_ops.ResourceSparseApplyAdadelta/tf.raw_ops.ResourceApplyAdadelta triggers ""Aborted (core dumped)"". We ran the code on colab's latest TensorFlow Nightly, which also triggers the crash. The cause of the crash may be the grad parameter.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

var = tf.Variable([1.0, 2.0, 3.0])
accum = tf.Variable([0.1, 0.2, 0.3], dtype=tf.complex64) 
accum2 = tf.Variable([0, 1.2, -3.3], dtype=tf.complex64) 
lr = 0.01
grad = tf.constant([0.1, 0.2, 0.3])
momentum = 0.9

# crash
tf.raw_ops.ResourceSparseApplyAdadelta(
    var=var.handle,
    accum=accum.handle,
    accum_update=accum2.handle,
    lr=lr,
    rho=0.9,
    epsilon=0.9,
    grad=grad,
    indices=tf.constant([1,2,2]),
    use_locking=False,
)
# crash
# tf.raw_ops.ResourceApplyAdadelta(
#     var=var.handle,
#     accum=accum.handle,
#     accum_update=accum2.handle,
#     lr=lr,
#     rho=0.9,
#     epsilon=0.9,
#     grad=grad,
#     use_locking=False,
# )
```


### Relevant log output

```shell
2024-06-06 01:10:09.399119: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-06-06 01:10:09.432483: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-06 01:10:10.501032: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (8 vs. 1) float expected, got complex64
Aborted (core dumped)
```
",2
Aborted (core dumped) in `tf.raw_ops.ResourceApplyRMSProp/tf.raw_ops.ResourceSparseApplyRMSProp`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific input, tf.raw_ops.ResourceApplyRMSProp/tf.raw_ops.ResourceSparseApplyRMSProp triggers ""Aborted (core dumped)"". We ran the code on colab's latest TensorFlow Nightly, which also triggers the crash. The cause of the crash may be the grad parameter.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

var = tf.Variable([1.0, 2.0, 3.0])
accum = tf.Variable([0.1, 0.2, 0.3], dtype=tf.complex64) 
accum2 = tf.Variable([0, 1.2, -3.3], dtype=tf.complex64) 
lr = 0.01
grad = tf.constant([0.1, 0.2, 0.3])
momentum = 0.9

# crash
tf.raw_ops.ResourceApplyRMSProp(
    var=var.handle,
    ms=accum.handle,
    mom=accum2.handle,
    lr=lr,
    rho=lr,
    momentum=momentum,
    epsilon=0.9,
    grad=grad,
    use_locking=False,
)

# tf.raw_ops.ResourceSparseApplyRMSProp(
#     var=var.handle,
#     ms=accum.handle,
#     mom=accum2.handle,
#     lr=lr,
#     rho=lr,
#     momentum=momentum,
#     indices=tf.constant([1,2,2]),
#     epsilon=0.9,
#     grad=grad,
#     use_locking=False,
# )
```


### Relevant log output

```shell
2024-06-06 01:07:00.757682: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-06 01:07:01.835541: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (8 vs. 1) float expected, got complex64
Aborted (core dumped)
```
",2
Aborted (core dumped) in `tf.raw_ops.ResourceApplyKerasMomentum/tf.raw_ops.ResourceSparseApplyKerasMomentum`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific input, tf.raw_ops.ResourceApplyKerasMomentum/tf.raw_ops.ResourceSparseApplyKerasMomentum triggers ""Aborted (core dumped)"". We ran the code on colab's latest TensorFlow Nightly, which also triggers the crash. We analyzed it, and the cause of this crash is probably the grad parameter.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

var = tf.Variable([1.0, 2.0, 3.0])
accum = tf.Variable([0.1, 0.2, 0.3], dtype=tf.complex64) 
accum2 = tf.Variable([0, 1.2, -3.3], dtype=tf.complex64) 
lr = 0.01
grad = tf.constant([0.1, 0.2, 0.3])
momentum = 0.9

# crash
tf.raw_ops.ResourceApplyKerasMomentum(
    var=var.handle,
    accum=accum.handle,
    grad=grad,
    lr=lr,
    momentum=0.1,
    use_locking=False,
    use_nesterov=False,
    name=None
)

# crash
# tf.raw_ops.ResourceSparseApplyKerasMomentum(
#     var=var.handle,
#     accum=accum.handle,
#     grad=grad,
#     lr=lr,
#     indices=tf.constant([1,2,2]),
#     momentum=0.1,
#     use_locking=False,
#     use_nesterov=False,
#     name=None
# )
```


### Relevant log output

```shell
2024-06-06 00:59:43.994008: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-06 00:59:44.992853: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (8 vs. 1) float expected, got complex64
Aborted (core dumped)
```
",2
Aborted (core dumped) in `tf.raw_ops.ResourceApplyFtrl/tf.raw_ops.ResourceApplyFtrlV2/tf.raw_ops.ResourceSparseApplyFtrl/tf.raw_ops.ResourceSparseApplyFtrlV2`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific input, tf.raw_ops.ResourceApplyFtrl/tf.raw_ops.ResourceApplyFtrlV2/tf.raw_ops.ResourceSparseApplyFtrl/tf.raw_ops.ResourceSparseApplyFtrlV2 triggers ""Aborted (core dumped)"". We ran the code on colab's latest TensorFlow Nightly, which also triggers the crash. We analyzed it, and the cause of this crash is probably the grad parameter.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

var = tf.Variable([1.0, 2.0, 3.0])
accum = tf.Variable([0.1, 0.2, 0.3], dtype=tf.complex64) 
accum2 = tf.Variable([0, 1.2, -3.3], dtype=tf.complex64) 
lr = 0.01
grad = tf.constant([0.1, 0.2, 0.3])
momentum = 0.9

# crash
tf.raw_ops.ResourceApplyFtrl(
    var=var.handle,
    accum=accum.handle,
    linear=var.handle,
    grad=grad,
    lr=lr,
    l1=0.1,
    l2=0.1,
    lr_power=1,
    use_locking=False,
    multiply_linear_by_lr=False,
    name=None
)
# crash
# tf.raw_ops.ResourceApplyFtrlV2(
#     var=var.handle,
#     accum=accum.handle,
#     linear=var.handle,
#     grad=grad,
#     lr=lr,
#     l1=0.1,
#     l2=0.1,
#     lr_power=1,
#     l2_shrinkage=1,
#     use_locking=False,
#     multiply_linear_by_lr=False,
#     name=None
# )
# crash
# tf.raw_ops.ResourceSparseApplyFtrl(
#    var=var.handle,
#     accum=accum.handle,
#     linear=var.handle,
#     grad=grad,
#     lr=lr,
#     l1=0.1,
#     l2=0.1,
#     lr_power=-1,
#     use_locking=False,
#     indices=tf.constant([1,2,2]),
#     multiply_linear_by_lr=False,
#     name=None
# )
# crash
# tf.raw_ops.ResourceSparseApplyFtrlV2(
#     var=var.handle,
#     accum=accum.handle,
#     linear=var.handle,
#     grad=grad,
#     lr=lr,
#     l1=0.1,
#     l2=0.1,
#     lr_power=-1,
#     use_locking=False,
#     indices=tf.constant([1,2,2]),
#     multiply_linear_by_lr=False,
#     l2_shrinkage=1,
#     name=None
# )
```


### Relevant log output

```shell
2024-06-06 00:53:09.467458: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-06 00:53:10.440397: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (8 vs. 1) float expected, got complex64
Aborted (core dumped)
```
",2
Aborted (core dumped) in `tf.raw_ops.QuantizeAndDequantizeV3`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

A crash is triggered when an unintended value is passed to the input parameter of the tf.raw_ops.QuantizeAndDequantizeV3 function.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input_data = tf.random.uniform(shape=[2, 1], maxval=9.0)
input_min = tf.constant([0, 1.0], dtype=tf.dtypes.float32)
input_max = tf.constant([1.0], dtype=tf.dtypes.float32)
# crash
tf.raw_ops.QuantizeAndDequantizeV3(input=input_data, input_min=input_min, input_max=input_max, num_bits=8, signed_input=True, range_given=True, narrow_range=False, axis=(- 1))
```


### Relevant log output

```shell
2024-06-05 10:39:35.285635: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-05 10:39:36.361982: F tensorflow/core/framework/tensor.cc:852] Check failed: 1 == NumElements() (1 vs. 2)Must have a one element tensor
Aborted (core dumped)
```
",2
Aborted (core dumped) in `tf.transpose`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

A crash is triggered when some boundary values are passed to the perm parameter of these transpose functions. the affected APIs are as follows:

1. tf.transpose
2. tf.experimental.numpy.transpose
3. tf.compat.v1.transpose
4. tf.keras.ops.transpose
5. tf.raw_ops.transpose
6. tf.raw_ops.ConjugateTranspose

tf.raw_ops.ConjugateTranspose is a Segmentation fault crash, all other API crashes are Aborted. 

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

x = tf.random.normal(shape=(3, 3))
# crash
x = tf.transpose(x, perm=((- 1), (- 2))) 
 
# crash 
# tf.experimental.numpy.transpose(     
#     x, axes=((- 1), (- 2))
# )

# crash 
# tf.compat.v1.transpose(  
#     x, perm=((- 1), (- 2)), name='transpose', conjugate=False
# )

# crash
# tf.keras.ops.transpose(
#     x, axes=((- 1), (- 2))
# )

# crash
# tf.raw_ops.ConjugateTranspose(
#     x=x, perm=((- 1), (- 2))
# )

# crash
# tf.raw_ops.Transpose(
#     x=x, perm=((- 1), (- 2))
# )
```


### Relevant log output

```shell
2024-06-05 09:16:55.482645: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-05 09:16:56.526139: F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)

2024-06-05 09:17:47.967585: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Segmentation fault (core dumped)
```
",2
Current tensorflow[and-cuda] installed by pip pulls ptxas which causes Jupyter kernel restart,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.12.1

### Custom code

Yes

### OS platform and distribution

Red Hat Enterprise Linux release 8.5 (Ootpa) x86_64

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

System: 12.3; Python env: nvidia-cuda-runtime-cu12==12.3.101 nvidia-cudnn-cu12==8.9.7.29

### GPU model and memory

Nvidia a10 24GB

### Current behavior?

### Test case
- The first linked standalone code implements a CNN for the CIFAR-10 dataset as a minimal example and test case.
- The second linked code is a kernel.json which implements the environment variables for the production environment.  

### Reproduction of environment 
I set up a conda environment as follows:
`$ conda create -n tf-2.16 python==3.12`
We then source the environment:
`$ source /mmfs1/apps/pyenvs/anaconda3-2022.05/bin/activate tf-2.16`
I then install Tensorflow as per [documentation](https://www.tensorflow.org/install/pip) and add jupyter to this environment:
`$ pip install tensorflow[and-cuda] jupyterlab`
A kernel is produced: 
`$ python -m ipykernel install --user --name=MyEnvName`

### Terminal behavior 
Within the terminal, normal execution of the test code works with some normally expected warnings.
```
# export environment variables for system CUDA and cudnn (nvidia upstream)
$ module load cuda/12.3
$ module load cudnn/8.9
$ source /mmfs1/apps/pyenvs/anaconda3-2022.05/bin/activate tf-2.16
$ python tf_gpu.py
```
When the code calls `model.fit()` we get the following STDERR:
```
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1717178625.148066  121055 service.cc:146] XLA service 0x155468007cb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1717178625.148196  121055 service.cc:154]   StreamExecutor device (0): NVIDIA A10, Compute Capability 8.6
2024-05-31 13:03:45.227617: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2024-05-31 13:03:45.621453: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907
2024-05-31 13:03:48.511964: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1290', 4 bytes spill stores, 4 bytes spill loads

I0000 00:00:1717178635.476383  121055 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
```
Despite warnings, the training and test work begins and main exits normally. 

### Unexpected Jupyter behavior
Within jupyter, the same code run within blocks fails, with **Kernel Restarting: The kernel for .ipynb appears to have died. It will restart automatically** which is unhelpful. The STDERR from Jupyter is logged, and this gives
```
2024-05-31 11:15:06.061841: F external/local_xla/xla/service/gpu/nvptx_compiler.cc:619] ptxas returned an error during compilation of ptx to sass: 'INTERNAL: ptxas 12.3.103 has a bug that we think can affect XLA. Please use a different version.'  If the error message indicates that a file could not be written, please verify that sufficient filesystem space is provided.
```
So, the ptxas shipped with the pip distribution of `tensorflow[and-cuda]` as per current documentation pulls a bad version of `nvidia-cuda-nvcc-cu12`. 
 
### Fix which confirms the bug
I fixed this on production by bumping the offending version:
`$ pip install nvidia-cuda-nvcc-cu12==12.5.40`
Now, `fit()` finishes successfully, and a new warning is logged:
```
2024-05-31 13:19:06.243730: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:764] The NVIDIA driver's CUDA version is 12.3 which is older than the ptxas CUDA version (12.5.40). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.
```
The version of `nvidia-cuda-nvcc-cu12` which current stable pip installed during `pip install tensorflow[and-cuda]` does not work inside of Jupyter if not linked against some other system CUDA distribution (which is not always possible inside of a heterogeneous compute environment). 
A better fix would be to bump the nvidia pip packages so that the offending version of ptxas are avoided completely. 

### nvidia-smi output
```
$ nvidia-smi
Fri May 31 13:25:06 2024
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A10                     Off | 00000000:03:00.0 Off |                    0 |
|  0%   29C    P8               8W / 150W |      4MiB / 23028MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A10                     Off | 00000000:27:00.0 Off |                    0 |
|  0%   30C    P8               9W / 150W |      4MiB / 23028MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA A10                     Off | 00000000:83:00.0 Off |                    0 |
|  0%   27C    P8               8W / 150W |      7MiB / 23028MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA A10                     Off | 00000000:A3:00.0 Off |                    0 |
|  0%   29C    P8              15W / 150W |      4MiB / 23028MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
```

### Standalone code to reproduce the issue

```shell
https://gist.github.com/StephenSzwiec/d4f6f587441f54b9b6c5fed5efd8002d
https://gist.github.com/StephenSzwiec/8107202f0bbdc52deb3c9ebcd5c7f828
```


### Relevant log output

```shell
2024-05-31 11:15:06.061841: F external/local_xla/xla/service/gpu/nvptx_compiler.cc:619] ptxas returned an error during compilation of ptx to sass: 'INTERNAL: ptxas 12.3.103 has a bug that we think can affect XLA. Please use a different version.'  If the error message indicates that a file could not be written, please verify that sufficient filesystem space is provided.
```
",2
segmentation fault when tf.histogram_fixed_width receives large `value_range` and `nbins` on CPU mode ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When setting the `nbins` to a large value and the value range to [-np.inf, np.inf], the API `tf.histogram_fixed_width` will raises a segmentation fault.
Interestingly, I found this issue only occurs on CPU backend while this API works fine on GPU.

### Standalone code to reproduce the issue

```shell
import os
os.environ['CUDA_VISIBLE_DEVICES'] = ''
import numpy as np
import tensorflow as tf
input = tf.constant([1.,2,3])
bins = 23
out = tf.histogram_fixed_width(input, [-np.inf, np.inf], nbins=bins)  # Segmentation fault (core dumped)
print(out)
```
```


### Relevant log output

_No response_",1
Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence when call some methods of `tf.data`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.12.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When calling and iterating over the results of some of tf.data's methods it outputs ""Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence"".

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

def dataset_test():
 
    dataset = tf.data.Dataset.from_tensor_slices([""apple"", ""banana"", ""cherry""])
    # dataset = tf.data.TextLineDataset([""file1.txt"", ""file2.txt""]).range(10)             # same output
    # dataset = tf.data.TFRecordDataset([""file1.tfrecords"", ""file2.tfrecords""]).range(10) # same output
    # dataset = tf.data.Dataset.from_tensors([1,2,3]).range(10)                           # same output

    print(list(dataset.as_numpy_iterator()))

if __name__ == ""__main__"":
    dataset_test()
```


### Relevant log output

```shell
2024-05-24 06:23:06.268182: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-24 06:23:06.309884: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-24 06:23:06.908161: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-24 06:23:07.558225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22137 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:18:00.0, compute capability: 8.6
2024-05-24 06:23:07.558795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22453 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:af:00.0, compute capability: 8.6
2024-05-24 06:23:07.917142: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence
```
",1
TensorRT no longer has NvUtils.h - build from source is failing,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Arch

### Mobile device

_No response_

### Python version

3.12.3

### Bazel version

6.5.0

### GCC/compiler version

14.1.1

### CUDA/cuDNN version

12.4

### GPU model and memory

RTX 4090 - 24gb

### Current behavior?

While building from source, when I specify to use Tensorrt 10.0.1, the bazel configuration uses the header files from version 8 (https://github.com/tensorflow/tensorflow/blob/master/third_party/tensorrt/tensorrt_configure.bzl#L50-L65), which were changed in major 9 of Tensorrt (https://github.com/NVIDIA/TensorRT/tree/v9.1.0/include).

There is no longer `NvUtils.h` and a new header `NvOnnxConfig.h` (since 9.x.x and still like this on TensorRT 10).

I believe that either a new configuration must be created to support TensorRT 9.x.x and 10.x.x, or you should specify that Tensorflow only accepts TensortRT <= 8.6.



### Standalone code to reproduce the issue

```shell
USE_BAZEL_VERSION=6.5.0 bazel build //tensorflow/tools/pip_package/v2:wheel --repo_env=WHEEL_NAME=tensorflow --config=cuda --jobs=30 --verbose_failuresj
```


### Relevant log output

_No response_",1
Couldn't resolve TF-TRT Warning: Could not find TensorRT,"Couldn't resolve TF-TRT Warning: Could not find TensorRT

I'm using wsl2 in windows 11.

```
Distributor ID: Ubuntu
Description:    Ubuntu 22.04.4 LTS
Release:        22.04
Codename:       jammy
```
Previously, I wasn't able to have GPU as the backend, I had tried all the methods in installing tensorflow in wsl. 

But, trying with python3.11 and tensorflow  2.15.1, got me access to the GPU backend. 

```
conda create -n tmpenv python=3.11
conda activate tmpenv
pip install tensorflow[and-cuda]==2.15.1
```

versions:
```
tensorflow                    2.15.1
tensorrt                      10.0.1
```
nvidia-smi:
```
Tue May 21 15:45:23 2024
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.76.01              Driver Version: 552.44         CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 4080 ...    On  |   00000000:01:00.0 Off |                  N/A |
| N/A   38C    P4             17W /   55W |       0MiB /  12282MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
```
```
2024-05-21 15:46:26.487553: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fcc3f7b2c00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-05-21 15:46:26.487663: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4080 Laptop GPU, Compute Capability 8.9
```
I've tried all ways of installing tensorrt - pip, .deb, .tar;
updated path variable, etc.
Its importing in python shell

```
>>> import tensorrt as trt
>>> trt.__version__
'10.0.1'
>>> trt.__file__
'/home/mohan/miniconda3/envs/tmpenv/lib/python3.11/site-packages/tensorrt/__init__.py'
```
All the files are available (.tar installation method)
```
(tmpenv) mohan@LAPTOP-INPO8147:~$ ls TensorRT-10.0.1.6/
bin  data  doc  include  lib  onnx_graphsurgeon  python  samples  targets

(tmpenv) mohan@LAPTOP-INPO8147:~$ ls TensorRT-10.0.1.6/lib/
libnvinfer.so                          libnvinfer_plugin.so.10.0.1
libnvinfer.so.10                       libnvinfer_plugin_static.a
libnvinfer.so.10.0.1                   libnvinfer_static.a
libnvinfer_builder_resource.so.10.0.1  libnvinfer_vc_plugin.so
libnvinfer_dispatch.so                 libnvinfer_vc_plugin.so.10
libnvinfer_dispatch.so.10              libnvinfer_vc_plugin.so.10.0.1
libnvinfer_dispatch.so.10.0.1          libnvinfer_vc_plugin_static.a
libnvinfer_dispatch_static.a           libnvonnxparser.so
libnvinfer_lean.so                     libnvonnxparser.so.10
libnvinfer_lean.so.10                  libnvonnxparser.so.10.0.1
libnvinfer_lean.so.10.0.1              libnvonnxparser_static.a
libnvinfer_lean_static.a               libonnx_proto.a
libnvinfer_plugin.so                   stubs
libnvinfer_plugin.so.10

(tmpenv) mohan@LAPTOP-INPO8147:~/TensorRT-10.0.1.6/python$ ls
tensorrt-10.0.1-cp310-none-linux_x86_64.whl
tensorrt-10.0.1-cp311-none-linux_x86_64.whl
tensorrt-10.0.1-cp312-none-linux_x86_64.whl
tensorrt-10.0.1-cp38-none-linux_x86_64.whl
tensorrt-10.0.1-cp39-none-linux_x86_64.whl
tensorrt_dispatch-10.0.1-cp310-none-linux_x86_64.whl
tensorrt_dispatch-10.0.1-cp311-none-linux_x86_64.whl
tensorrt_dispatch-10.0.1-cp312-none-linux_x86_64.whl
tensorrt_dispatch-10.0.1-cp38-none-linux_x86_64.whl
tensorrt_dispatch-10.0.1-cp39-none-linux_x86_64.whl
tensorrt_lean-10.0.1-cp310-none-linux_x86_64.whl
tensorrt_lean-10.0.1-cp311-none-linux_x86_64.whl
tensorrt_lean-10.0.1-cp312-none-linux_x86_64.whl
tensorrt_lean-10.0.1-cp38-none-linux_x86_64.whl
tensorrt_lean-10.0.1-cp39-none-linux_x86_64.whl
```

I've also used the files from .tar method to install using pip:
```
(tmpenv) mohan@LAPTOP-INPO8147:~/TensorRT-10.0.1.6/python$ python3 -m pip install tensorrt-10.0.1-cp311-none-linux_x86_64.whl
Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
Processing ./tensorrt-10.0.1-cp311-none-linux_x86_64.whl
tensorrt is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.

(tmpenv) mohan@LAPTOP-INPO8147:~/TensorRT-10.0.1.6/python$ python3 -m pip install tensorrt_dispatch-10.0.1-cp311-none-linux_x86_64.whl
Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
Processing ./tensorrt_dispatch-10.0.1-cp311-none-linux_x86_64.whl
tensorrt-dispatch is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.

(tmpenv) mohan@LAPTOP-INPO8147:~/TensorRT-10.0.1.6/python$ python3 -m pip install tensorrt_lean-10.0.1-cp311-none-linux_x86_64.whl
Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
Processing ./tensorrt_lean-10.0.1-cp311-none-linux_x86_64.whl
tensorrt-lean is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.
```
But still I get this error

```
2024-05-21 15:46:19.659564: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
```

_Originally posted by @MohanKrishnaGR in https://github.com/tensorflow/tensorflow/issues/64809#issuecomment-2122328887_
            ",1
TFLite ConvTranspose3D implemented typo,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

in tflite convtranspose3d optimized kernel, pad depth seems typo error
![image](https://github.com/tensorflow/tensorflow/assets/30307463/39bdf859-f236-48f9-b02a-c8231d11cb88)

And also compared with same convtransposed3d with torch interface, tflite have mismatch

### Standalone code to reproduce the issue

```shell
in tflite convtranspose3d optimized kernel, pad depth seems typo error
![image](https://github.com/tensorflow/tensorflow/assets/30307463/39bdf859-f236-48f9-b02a-c8231d11cb88)

here, seems 
const int spatial_dim_1_padding_after =
      params.padding_values.depth + params.padding_values.depth_offset;
```


### Relevant log output

_No response_",1
ValueError: as_list() is not defined on an unknown TensorShape. during training,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0-dev20240517

### Custom code

Yes

### OS platform and distribution

macOS Sonoma 

### Mobile device

_No response_

### Python version

3.10.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I get the following error when training a transformer for translation:

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[9], [line 6](vscode-notebook-cell:?execution_count=9&line=6)
      [1](vscode-notebook-cell:?execution_count=9&line=1) model.compile(optimizer='adam', 
      [2](vscode-notebook-cell:?execution_count=9&line=2)     loss='sparse_categorical_crossentropy', 
      [3](vscode-notebook-cell:?execution_count=9&line=3)     metrics=['accuracy'],
      [4](vscode-notebook-cell:?execution_count=9&line=4)     run_eagerly=False
      [5](vscode-notebook-cell:?execution_count=9&line=5)     )
----> [6](vscode-notebook-cell:?execution_count=9&line=6) model.fit(dataset_train, epochs=10)

File ~/opt/anaconda3/envs/tensorflow_latest/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    [119](https://file+.vscode-resource.vscode-cdn.net/Users/glo/Desktop/deep_learning_training-main/~/opt/anaconda3/envs/tensorflow_latest/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:119)     filtered_tb = _process_traceback_frames(e.__traceback__)
    [120](https://file+.vscode-resource.vscode-cdn.net/Users/glo/Desktop/deep_learning_training-main/~/opt/anaconda3/envs/tensorflow_latest/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:120)     # To get the full stack trace, call:
    [121](https://file+.vscode-resource.vscode-cdn.net/Users/glo/Desktop/deep_learning_training-main/~/opt/anaconda3/envs/tensorflow_latest/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:121)     # `keras.config.disable_traceback_filtering()`
--> [122](https://file+.vscode-resource.vscode-cdn.net/Users/glo/Desktop/deep_learning_training-main/~/opt/anaconda3/envs/tensorflow_latest/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122)     raise e.with_traceback(filtered_tb) from None
    [123](https://file+.vscode-resource.vscode-cdn.net/Users/glo/Desktop/deep_learning_training-main/~/opt/anaconda3/envs/tensorflow_latest/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:123) finally:
    [124](https://file+.vscode-resource.vscode-cdn.net/Users/glo/Desktop/deep_learning_training-main/~/opt/anaconda3/envs/tensorflow_latest/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:124)     del filtered_tb

File ~/opt/anaconda3/envs/tensorflow_latest/lib/python3.10/site-packages/optree/ops.py:594, in tree_map(func, tree, is_leaf, none_is_leaf, namespace, *rests)
    [592](https://file+.vscode-resource.vscode-cdn.net/Users/glo/Desktop/deep_learning_training-main/~/opt/anaconda3/envs/tensorflow_latest/lib/python3.10/site-packages/optree/ops.py:592) leaves, treespec = _C.flatten(tree, is_leaf, none_is_leaf, namespace)
    [593](https://file+.vscode-resource.vscode-cdn.net/Users/glo/Desktop/deep_learning_training-main/~/opt/anaconda3/envs/tensorflow_latest/lib/python3.10/site-packages/optree/ops.py:593) flat_args = [leaves] + [treespec.flatten_up_to(r) for r in rests]
--> [594](https://file+.vscode-resource.vscode-cdn.net/Users/glo/Desktop/deep_learning_training-main/~/opt/anaconda3/envs/tensorflow_latest/lib/python3.10/site-packages/optree/ops.py:594) return treespec.unflatten(map(func, *flat_args))

ValueError: as_list() is not defined on an unknown TensorShape.
```

### Standalone code to reproduce the issue

```shell
Here is a colab link to the code:
https://colab.research.google.com/drive/1BQ4lhaZPP5XGb_IUe-rVacMiFPWCfm9Y?usp=sharing

Running eagerly works fine but an error occurs in graph mode.
```


### Relevant log output

_No response_",1
"Strange finding: When the global seed and @tf.function decorator are used, the random sampling values of the two adjacent periods are equal","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When I set the global seed, applied the @tf.function decorator, and performed gradient updates every two periods, I observed an unexpected phenomenon: the error values were randomly sampled from two consecutive periods with identical values. However, my expectation was that the error values should differ when sampled in each period instead of being equal in adjacent periods. Furthermore, I noticed that removing the @tf.function decorator from the model function prevented this issue of having identical error values in consecutive periods. What could be causing this phenomenon? How should one handle this situation when using @tf.function?""

### Standalone code to reproduce the issue

```shell
import os
import random
import numpy as np
import tensorflow as tf

# set seeds
SEED = 0
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)
os.environ['PYTHONHASHSEED'] = str(SEED)
tf.keras.utils.set_random_seed(SEED)
tf.config.experimental.enable_op_determinism()

@tf.function
def model(x):
    err = tf.random.uniform(shape=(1,))
    loss = x + err
    return err, loss

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
training_periods = 10

# initialize x
x = tf.Variable(tf.random.uniform(shape=(1,)), trainable=True)

for tt in range(training_periods):
    if tt % 2 == 0:
        with tf.GradientTape() as tape:
            err, loss = model(x)
        
        gradients = tape.gradient(loss, [x])  # suppose x need to be optimized
        optimizer.apply_gradients(zip(gradients, [x]))  # update x
        
        print(f""Period: {tt}, err (trained): {err.numpy()}"")
        
    else:
        err, loss = model(x)
        
        print(f""Period: {tt}, err (not trained): {err.numpy()}"")

# outcomes
Period: 0, err (trained): [0.01975703]
Period: 1, err (not trained): [0.01975703]
Period: 2, err (trained): [0.5400312]
Period: 3, err (not trained): [0.5400312]
Period: 4, err (trained): [0.51667833]
Period: 5, err (not trained): [0.51667833]
Period: 6, err (trained): [0.4683528]
Period: 7, err (not trained): [0.4683528]
Period: 8, err (trained): [0.14856052]
Period: 9, err (not trained): [0.14856052]
```


### Relevant log output

_No response_",1
Memory leak from using tf.constant in loop in TF 2.16,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary from pip

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 24.04

### Mobile device

_No response_

### Python version

3.12.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I believe I tracked down a memory leak in my code to a `tf.constant` creation in a short lived object of a class. I can reproduce it in an even simpler way by just creating the constant in a loop. No `@tf.function` decoration or model training necessary to cause it. It seems to happen if I replace the `tf.constant` with a `tf.random.uniform()`. I tried suggestions I've seen elsewhere, like trying to use `del` on the variable followed by `gc.collect()`.

Some similar looking bug reports:
- https://github.com/tensorflow/tensorflow/issues/57982 
- https://github.com/tensorflow/tensorflow/issues/50765
- https://discuss.tensorflow.org/t/tensorflow-memory-leak-during-inference-in-loop/23044

It looks like upgrading to 2.15 solved it for some people, but it appears to be back in 2.16?

is this expected and I'm doing something wrong?

### Standalone code to reproduce the issue

```shell
import time
import psutil
import tensorflow as tf


def loop_copy():
    for _ in range(2000):
        y = tf.constant(5.0)  # still leaks with this


start_mem = psutil.Process().memory_info().rss

for _ in range(500):
    time.sleep(0.1)
    print((psutil.Process().memory_info().rss - start_mem) / 10**6)
    loop_copy()
```
```


### Relevant log output

```shell
0.0
6.029312
6.815744
7.471104
8.126464
8.781824
9.437184
10.092544
10.747904
11.534336
12.189696
12.845056
13.500416
14.155776
14.811136
15.597568
16.252928
...
```
",1
Enabling onednn graph API in TF,How we can  enable oneDNN graph API in tensorflow?,0
Exit code 137 in `tf.raw_ops.ResizeNearestNeighborGrad`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the value in the input `size` is too large, program exceeds the memory limit.

### Standalone code to reproduce the issue

```shell
# Signal --4;2024-05-14 00:40:39.895950: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.2024-05-14 00:40:39.896316: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.2024-05-14 00:40:39.900744: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.2024-05-14 00:40:39.956390: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.2024-05-14 00:40:40.842339: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT

# ResizeNearestNeighborOpGrad

import tensorflow as tf

align_corners = True
half_pixel_centers = False
grads = tf.constant(1.5e+300, shape=[1,8,16,3], dtype=tf.float64)
size = tf.constant([65534,65534], shape=[2], dtype=tf.int32)
tf.raw_ops.ResizeNearestNeighborGrad(grads=grads, size=size, align_corners=align_corners, half_pixel_centers=half_pixel_centers)
```


### Relevant log output

```shell
(tensorflow-2.16.1-orig) root@b29bda27c601:/mnt# python tests/ResizeNearestNeighborGrad.py 
2024-05-15 08:43:01.439844: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-15 08:43:01.440211: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-05-15 08:43:01.444264: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-05-15 08:43:01.498275: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-15 08:43:02.744313: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Killed
```
```
",1
Missing return in error check in mlir::TFTPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.16.1

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Possible null pointer dereference may occur because of missing return in error check:
https://github.com/tensorflow/tensorflow/blob/63bb7339a2bc98af97c61e86eedbbd60c123529c/tensorflow/compiler/mlir/tensorflow/transforms/extract_tpu_copy_with_dynamic_shape_op.cc#L61-L65

### Standalone code to reproduce the issue

```shell
Bug was found with Svace static analyzer
```


### Relevant log output

_No response_",1
Segmentation fault (core dumped) in `tf.raw_ops.FusedResizeAndPadConv2D`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Illegal `size` will trigger a segfault.

### Standalone code to reproduce the issue

```shell
# Signal --4;2024-05-13 05:48:16.272992: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.2024-05-13 05:48:16.273352: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.2024-05-13 05:48:16.277715: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.2024-05-13 05:48:16.331241: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.2024-05-13 05:48:17.210828: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT2024-05-13 05:48:17.792175: E tensorflow/core/util/util.cc:131] oneDNN supports DT_HALF only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.

# FusedResizeConv2DUsingGemmOp

import tensorflow as tf

mode = ""REFLECT""
strides = [1, 1, 1, 1]
padding = ""VALID""
resize_align_corners = False
input = tf.constant(0, shape=[1,2,3,2], dtype=tf.float16)
size = tf.constant([65534,65534], shape=[2], dtype=tf.int32)
paddings = tf.constant(0, shape=[4,2], dtype=tf.int32)
filter = tf.constant(0, shape=[1,2,2,2], dtype=tf.float16)
tf.raw_ops.FusedResizeAndPadConv2D(input=input, size=size, paddings=paddings, filter=filter, mode=mode, strides=strides, padding=padding, resize_align_corners=resize_align_corners)
```


### Relevant log output

```shell
2024-05-14 08:31:20.675971: E tensorflow/core/util/util.cc:131] oneDNN supports DT_HALF only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.
Segmentation fault (core dumped)
```
ASAN report:
```
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
=================================================================
AddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
AddressSanitizerAddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
AddressSanitizerAddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL
AddressSanitizerAddressSanitizer:DEADLYSIGNAL
AddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL
AddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL
AddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL
AddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
AddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
AddressSanitizer==2060653==ERROR: AddressSanitizer: SEGV on unknown address 0x7fbb6ce8a800 (pc 0x7fc21cc305a9 bp 0x7fc1b01c05c0 sp 0x7fc1b01c03b0 T93)
:DEADLYSIGNAL
:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
==2060653==The signal is caused by a WRITE memory access.
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizerAddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
    #0 0x7fc21cc305a9 in Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::EvalParallelContext<Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::NoCallback, true, true, false, 0>::pack_rhs(long, long) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x394ae5a9)
    #1 0x7fc21cc31692 in Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::EvalParallelContext<Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::NoCallback, true, true, false, 0>::enqueue_packing_helper(long, long, long, bool) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x394af692)
    #2 0x7fc24fd0b121 in Eigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41fd121)
    #3 0x7fc24fd00326 in void absl::lts_20230802::internal_any_invocable::RemoteInvoker<false, void, tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}&>(absl::lts_20230802::internal_any_invocable::TypeErasedState*) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41f2326)
    #4 0x7fc24e62625c in tsl::(anonymous namespace)::PThread::ThreadFn(void*) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x2b1825c)
    #5 0x7fc2f9885ac2  (/lib/x86_64-linux-gnu/libc.so.6+0x94ac2)
    #6 0x7fc2f9916a03 in __clone (/lib/x86_64-linux-gnu/libc.so.6+0x125a03)

AddressSanitizer can not provide additional info.
SUMMARY: AddressSanitizer: SEGV (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x394ae5a9) in Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::EvalParallelContext<Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::NoCallback, true, true, false, 0>::pack_rhs(long, long)
Thread T93 created by T0 here:
    #0 0x7fc2f9bab685 in __interceptor_pthread_create ../../../../src/libsanitizer/asan/asan_interceptors.cpp:216
    #1 0x7fc24e633fbf in tsl::(anonymous namespace)::PThread::PThread(tsl::ThreadOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, absl::lts_20230802::AnyInvocable<void ()>) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x2b25fbf)
    #2 0x7fc24e63450a in tsl::(anonymous namespace)::PosixEnv::StartThread(tsl::ThreadOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, absl::lts_20230802::AnyInvocable<void ()>) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x2b2650a)
    #3 0x7fc24fd093a8 in Eigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::ThreadPoolTempl(int, bool, tsl::thread::EigenEnvironment) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41fb3a8)
    #4 0x7fc24fd0d498 in tsl::thread::ThreadPool::ThreadPool(tsl::Env*, tsl::ThreadOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, bool, Eigen::Allocator*) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41ff498)
    #5 0x7fc24d7ef278 in tensorflow::LocalDevice::EigenThreadPoolInfo::EigenThreadPoolInfo(tensorflow::SessionOptions const&, int, tsl::Allocator*) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1ce1278)
    #6 0x7fc24d7f05c5 in tensorflow::LocalDevice::LocalDevice(tensorflow::SessionOptions const&, tensorflow::DeviceAttributes const&) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1ce25c5)
    #7 0x7fc24d7e3111 in tensorflow::ThreadPoolDevice::ThreadPoolDevice(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tsl::gtl::IntType<tensorflow::Bytes_tag_, long>, tensorflow::DeviceLocality const&, tsl::Allocator*) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1cd5111)
    #8 0x7fc24d7dc1dd in tensorflow::ThreadPoolDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1cce1dd)
    #9 0x7fc24d9a667c in tensorflow::DeviceFactory::AddCpuDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1e9867c)
    #10 0x7fc24d9a6bbd in tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1e98bbd)
    #11 0x7fc1f3dd6db8 in TFE_NewContext (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x10654db8)
    #12 0x7fc1df839e70 in pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(TFE_ContextOptions const*)#9}, pybind11::object, TFE_ContextOptions const*, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::return_value_policy>(pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(TFE_ContextOptions const*)#9}&&, pybind11::object (*)(TFE_ContextOptions const*), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&, pybind11::return_value_policy const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call&) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/_pywrap_tfe.so+0x1b6e70)
    #13 0x7fc1df851899 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/_pywrap_tfe.so+0x1ce899)
    #14 0x51ad66  (/usr/bin/python3.11+0x51ad66)

==2060653==ABORTING
```
```
",1
Aborted (core dumped) in `QuantizedInstanceNorm`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Abort is triggered when the input x does not meet the condition of dimension 4.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

output_range_given = False
given_y_min = 0
given_y_max = 0
variance_epsilon = 1e-05
min_separation = 0.001
x = tf.constant([], shape=[0,0,0,1,1,0,0,0,11,9], dtype=tf.quint8)
x_min = tf.constant(-3.5e+35, shape=[], dtype=tf.float32)
x_max = tf.constant(0, shape=[], dtype=tf.float32)
tf.raw_ops.QuantizedInstanceNorm(x=x, x_min=x_min, x_max=x_max, output_range_given=output_range_given, given_y_min=given_y_min, given_y_max=given_y_max, variance_epsilon=variance_epsilon, min_separation=min_separation)
```


### Relevant log output

```shell
2024-05-14 08:25:37.034690: F tensorflow/core/framework/tensor_shape.cc:45] Check failed: NDIMS == dims() (4 vs. 10)Asking for tensor of 4 dimensions from a tensor of 10 dimensions
Aborted (core dumped)
```
",1
Aborted (core dumped) in `tf.raw_ops.FusedPadConv2D`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When `input` dimension is less than 4， it will cause abort.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

mode = ""REFLECT""
strides = [1, 1, 1, 1]
padding = ""VALID""
input = tf.constant(1.5e+300, shape=[], dtype=tf.float64)
paddings = tf.constant(65534, shape=[4,2], dtype=tf.int32)
filter = tf.constant([], shape=[1,1,0,1,0], dtype=tf.float64)
tf.raw_ops.FusedPadConv2D(input=input, paddings=paddings, filter=filter, mode=mode, strides=strides, padding=padding)
```


### Relevant log output

```shell
2024-05-14 08:19:37.953832: F tensorflow/core/framework/tensor_shape.cc:357] Check failed: d < dims() (1 vs. 0)
Aborted (core dumped)
```
",1
"Numerical precision issue of operators selu, leakyRelu, softplus and their corresponding backward operators on Bfloat16 vs float32","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.10.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I'd like to bring to attention an issue concerning the numerical precision of several operators (selu, leaky, relu) when operating on Bfloat16 versus float32 data types. I conducted comparisons using 20,000 random tensors for these operators, assessing the outputs in both Bfloat16 and float32 and computing the discrepancies. My observations indicate that differences generated by TensorFlow are generally more pronounced compared to PyTorch. Particularly noteworthy is the significant error produced by the SeluGrad operator. The results are summarized in the table below:

| Operator           | TensorFlow | PyTorch |
|--------------------|------------|---------|
| selu               | 0.24918    | 0.12243 |
| leakyrelu          | 0.01875    | 0.00094 |
| softplus           | 0.05488    | 0.01554 |
| seluGrad      | 10.41794   | 0.12406 |
| leakyreluGrad | 0.01875    | 0.00094 |
| softplusGrad | 0.13502    | 0.12484 |

In a standalone code to reproduce the issue, I provide illustrative instances for seluGrad operators, where the output discrepancy between Bfloat16 and float32 can be as high as 10.4.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

features = tf.convert_to_tensor(np.array([-0.00112915]), dtype=tf.float32)
gradients = tf.convert_to_tensor(np.array([-14.6875]), dtype=tf.float32)

x = tf.Variable(features)
k = tf.constant(gradients)
with tf.GradientTape(persistent=True) as tape:
    y = tf.nn.selu(features=x)
    z = k*y
    fianl = tf.reduce_mean(z)
    
print('float32 gradient:',tape.gradient(z, x))


features = tf.cast(features, dtype=tf.bfloat16)
gradients = tf.cast(gradients, dtype=tf.bfloat16)
x = tf.Variable(features)
k = tf.constant(gradients)
with tf.GradientTape(persistent=True) as tape:
    y = tf.nn.selu(features=x)
    z = k*y
    fianl = tf.reduce_mean(z)
print('float16 gradient:',tape.gradient(z, x))
```


### Relevant log output

```shell
float32 gradient: tf.Tensor([-25.792944], shape=(1,), dtype=float32)
bfloat16 gradient: tf.Tensor([-15.375], shape=(1,), dtype=bfloat16)
```
",1
Failed to compile on aarch64,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.16.1

### Custom code

No

### OS platform and distribution

aarch64

### Mobile device

_No response_

### Python version

3.10

### Bazel version

bazelisk using the default version requested by tensorflow

### GCC/compiler version

clang 17 (I also tried with clang16).

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When compiling the tensorflow source code on aarch64, I always get the following error:

```
WARN ERROR: /home/build/tensorflow/BUILD:1263:20: Linking tensorflow/libtensorflow.so.2.16.1 failed: (Exit 1): clang-17 failed: error executing command (from target //tensorflow:libtensorflow.so.2.16.1) /usr/bin/clang-17 @bazel-out/aarch64-opt/bin/tensorflow/libtensorflow.so.2.16.1-2.params
```

However the compilation works on x86_64.

``` 
export PYTHON_BIN_PATH=/usr/bin/python
      export TF_PYTHON_VERSION=3.10
      export USE_DEFAULT_PYTHON_LIB_PATH=1
      export TF_NEED_JEMALLOC=1
      export TF_NEED_KAFKA=1
      export TF_NEED_OPENCL_SYCL=0
      export TF_NEED_AWS=1
      export TF_NEED_GCP=1
      export TF_NEED_HDFS=1
      export TF_NEED_S3=1
      export TF_ENABLE_XLA=1
      export TF_NEED_GDR=0
      export TF_NEED_VERBS=0
      export TF_NEED_OPENCL=0
      export TF_NEED_MPI=0
      export TF_NEED_TENSORRT=0
      export TF_NEED_NGRAPH=0
      export TF_NEED_IGNITE=0
      export TF_NEED_ROCM=0
      export TF_SYSTEM_LIBS=""boringssl,curl,gif,icu,libjpeg_turbo,nasm,png,zlib""
      export TF_SET_ANDROID_WORKSPACE=0

      ./configure

      bazel --bazelrc=.tf_configure.bazelrc build \
        --config=opt \
        --config=mkl_threadpool \
        //tensorflow:libtensorflow.so \
        //tensorflow:libtensorflow_cc.so \
        //tensorflow:install_headers \
        //tensorflow/tools/pip_package:build_pip_package
```

### Standalone code to reproduce the issue

```shell
export PYTHON_BIN_PATH=/usr/bin/python
      export TF_PYTHON_VERSION=3.10
      export USE_DEFAULT_PYTHON_LIB_PATH=1
      export TF_NEED_JEMALLOC=1
      export TF_NEED_KAFKA=1
      export TF_NEED_OPENCL_SYCL=0
      export TF_NEED_AWS=1
      export TF_NEED_GCP=1
      export TF_NEED_HDFS=1
      export TF_NEED_S3=1
      export TF_ENABLE_XLA=1
      export TF_NEED_GDR=0
      export TF_NEED_VERBS=0
      export TF_NEED_OPENCL=0
      export TF_NEED_MPI=0
      export TF_NEED_TENSORRT=0
      export TF_NEED_NGRAPH=0
      export TF_NEED_IGNITE=0
      export TF_NEED_ROCM=0
      export TF_SYSTEM_LIBS=""boringssl,curl,gif,icu,libjpeg_turbo,nasm,png,zlib""
      export TF_SET_ANDROID_WORKSPACE=0

      ./configure

      bazel --bazelrc=.tf_configure.bazelrc build \
        --config=opt \
        --config=mkl_threadpool \
        //tensorflow:libtensorflow.so \
        //tensorflow:libtensorflow_cc.so \
        //tensorflow:install_headers \
        //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
2024/05/09 13:03:51 WARN [14,447 / 14,578] Compiling tensorflow/compiler/jit/xla_platform_info.cc; 20s local ... (16 actions running)
2024/05/09 13:03:52 WARN [14,448 / 14,578] Compiling tensorflow/compiler/jit/xla_platform_info.cc; 21s local ... (16 actions running)
2024/05/09 13:03:55 WARN [14,449 / 14,578] Compiling tensorflow/compiler/jit/xla_platform_info.cc; 24s local ... (16 actions, 15 running)
2024/05/09 13:03:57 WARN [14,450 / 14,578] Compiling tensorflow/compiler/jit/xla_platform_info.cc; 26s local ... (16 actions running)
2024/05/09 13:03:58 WARN [14,452 / 14,578] Compiling tensorflow/compiler/jit/get_compiler_ir.cc; 26s local ... (16 actions running)
2024/05/09 13:04:00 WARN [14,454 / 14,578] Compiling tensorflow/compiler/jit/kernels/xla_ops.cc; 27s local ... (16 actions running)
2024/05/09 13:04:01 WARN [14,457 / 14,578] Compiling tensorflow/compiler/jit/xla_cpu_device.cc; 22s local ... (16 actions running)
2024/05/09 13:04:03 WARN [14,459 / 14,578] Compiling tensorflow/compiler/jit/xla_cpu_device.cc; 24s local ... (16 actions running)
2024/05/09 13:04:04 WARN [14,460 / 14,578] Compiling tensorflow/compiler/jit/xla_cpu_device.cc; 25s local ... (16 actions, 15 running)
2024/05/09 13:04:06 WARN [14,461 / 14,578] Compiling tensorflow/compiler/jit/xla_cpu_device.cc; 27s local ... (16 actions running)
2024/05/09 13:04:07 WARN [14,464 / 14,578] Compiling tensorflow/compiler/tf2xla/mlir_tf2xla.cc; 22s local ... (16 actions running)
2024/05/09 13:04:11 WARN [14,465 / 14,578] Compiling tensorflow/compiler/tf2xla/mlir_tf2xla.cc; 26s local ... (16 actions, 15 running)
2024/05/09 13:04:12 WARN [14,467 / 14,578] Compiling tensorflow/compiler/tf2xla/mlir_tf2xla.cc; 27s local ... (16 actions, 15 running)
2024/05/09 13:04:13 WARN [14,468 / 14,578] Compiling tensorflow/compiler/tf2xla/mlir_tf2xla.cc; 28s local ... (16 actions running)
2024/05/09 13:04:14 WARN [14,470 / 14,578] Compiling tensorflow/compiler/tf2xla/mlir_tf2xla.cc; 29s local ... (16 actions, 15 running)
2024/05/09 13:04:16 WARN [14,473 / 14,578] Compiling tensorflow/compiler/aot/codegen.cc; 19s local ... (16 actions, 15 running)
2024/05/09 13:04:17 WARN [14,476 / 14,578] Compiling tensorflow/core/common_runtime/pluggable_device/pluggable_device_plugin_init.cc; 16s local ... (16 actions running)
2024/05/09 13:04:18 WARN [14,478 / 14,578] Compiling tensorflow/core/common_runtime/pluggable_device/pluggable_device_plugin_init.cc; 17s local ... (16 actions, 15 running)
2024/05/09 13:04:20 WARN [14,479 / 14,578] Compiling tensorflow/core/common_runtime/pluggable_device/pluggable_device_plugin_init.cc; 19s local ... (16 actions, 15 running)
2024/05/09 13:04:21 WARN [14,481 / 14,578] Compiling tensorflow/core/common_runtime/pluggable_device/pluggable_device_plugin_init.cc; 20s local ... (16 actions, 14 running)
2024/05/09 13:04:22 WARN [14,482 / 14,578] Compiling tensorflow/core/common_runtime/pluggable_device/pluggable_device_plugin_init.cc; 21s local ... (16 actions, 15 running)
2024/05/09 13:04:23 WARN [14,486 / 14,578] Compiling tensorflow/compiler/mlir/python/mlir.cc; 21s local ... (16 actions running)
2024/05/09 13:04:25 WARN [14,487 / 14,578] Compiling tensorflow/compiler/mlir/python/mlir.cc; 23s local ... (16 actions running)
2024/05/09 13:04:26 WARN [14,489 / 14,578] Compiling tensorflow/compiler/mlir/python/mlir.cc; 24s local ... (16 actions running)
2024/05/09 13:04:28 WARN [14,491 / 14,578] Compiling tensorflow/compiler/mlir/python/mlir.cc; 25s local ... (16 actions, 15 running)
2024/05/09 13:04:29 WARN [14,492 / 14,578] Compiling tensorflow/compiler/mlir/python/mlir.cc; 27s local ... (16 actions, 15 running)
2024/05/09 13:04:30 WARN [14,494 / 14,578] Compiling tensorflow/compiler/mlir/python/mlir.cc; 28s local ... (16 actions, 15 running)
2024/05/09 13:04:31 WARN [14,497 / 14,578] Compiling tensorflow/compiler/mlir/python/mlir.cc; 29s local ... (14 actions running)
2024/05/09 13:04:32 WARN [14,498 / 14,578] Compiling tensorflow/compiler/mlir/python/mlir.cc; 30s local ... (13 actions running)
2024/05/09 13:04:34 WARN [14,500 / 14,578] Compiling tensorflow/compiler/mlir/python/mlir.cc; 32s local ... (11 actions running)
2024/05/09 13:04:36 WARN [14,505 / 14,578] Compiling tensorflow/compiler/mlir/python/mlir.cc; 34s local ... (6 actions running)
2024/05/09 13:04:38 WARN [14,506 / 14,578] Compiling tensorflow/compiler/mlir/python/mlir.cc; 36s local ... (5 actions running)
2024/05/09 13:04:40 WARN [14,508 / 14,578] Compiling tensorflow/compiler/mlir/python/mlir.cc; 38s local ... (3 actions running)
2024/05/09 13:04:40 WARN ERROR: /home/build/tensorflow/BUILD:1263:20: Linking tensorflow/libtensorflow.so.2.16.1 failed: (Exit 1): clang-17 failed: error executing command (from target //tensorflow:libtensorflow.so.2.16.1) /usr/bin/clang-17 @bazel-out/aarch64-opt/bin/tensorflow/libtensorflow.so.2.16.1-2.params
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN clang-17: error: linker command failed with exit code 1 (use -v to see invocation)
2024/05/09 13:04:41 WARN INFO: Elapsed time: 4130.234s, Critical Path: 216.57s
2024/05/09 13:04:41 WARN INFO: 14511 processes: 1423 internal, 13088 local.
2024/05/09 13:04:41 WARN FAILED: Build did NOT complete successfully
```
```
",1
